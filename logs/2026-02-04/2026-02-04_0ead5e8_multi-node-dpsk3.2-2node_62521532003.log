# Run ID: 21679697194
# Commit: 0ead5e8681c4f13c7746d2611268e263f1514ec8
# Job: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
# Date: 2026-02-04
============================================================

ï»¿2026-02-04T18:07:35.0077679Z Current runner version: '2.330.0'
2026-02-04T18:07:35.0082125Z Runner name: 'linux-aarch64-a3-0-wgt9d-runner-6lk8b'
2026-02-04T18:07:35.0082885Z Runner group name: 'Default'
2026-02-04T18:07:35.0083615Z Machine name: 'linux-aarch64-a3-0-wgt9d-runner-6lk8b'
2026-02-04T18:07:35.0087188Z ##[group]GITHUB_TOKEN Permissions
2026-02-04T18:07:35.0089211Z Actions: write
2026-02-04T18:07:35.0089625Z ArtifactMetadata: write
2026-02-04T18:07:35.0090071Z Attestations: write
2026-02-04T18:07:35.0090436Z Checks: write
2026-02-04T18:07:35.0090796Z Contents: write
2026-02-04T18:07:35.0091172Z Deployments: write
2026-02-04T18:07:35.0091576Z Discussions: write
2026-02-04T18:07:35.0091939Z Issues: write
2026-02-04T18:07:35.0092511Z Metadata: read
2026-02-04T18:07:35.0092902Z Models: read
2026-02-04T18:07:35.0093263Z Packages: write
2026-02-04T18:07:35.0093709Z Pages: write
2026-02-04T18:07:35.0094087Z PullRequests: write
2026-02-04T18:07:35.0094574Z RepositoryProjects: write
2026-02-04T18:07:35.0095160Z SecurityEvents: write
2026-02-04T18:07:35.0095578Z Statuses: write
2026-02-04T18:07:35.0095980Z ##[endgroup]
2026-02-04T18:07:35.0097792Z Secret source: Actions
2026-02-04T18:07:35.0098259Z Prepare workflow directory
2026-02-04T18:07:35.0682704Z Prepare all required actions
2026-02-04T18:07:35.0714642Z Getting action download info
2026-02-04T18:07:36.0808471Z Download action repository 'actions/checkout@v6' (SHA:de0fac2e4500dabe0009e67214ff5f5447ce83dd)
2026-02-04T18:07:41.2677959Z Download action repository 'actions/upload-artifact@v6' (SHA:b7c566a772e6b6bfb58ed0dc250532a479d7789f)
2026-02-04T18:07:48.9672173Z Uses: vllm-project/vllm-ascend/.github/workflows/_e2e_nightly_multi_node.yaml@refs/heads/main (0ead5e8681c4f13c7746d2611268e263f1514ec8)
2026-02-04T18:07:48.9675530Z ##[group] Inputs
2026-02-04T18:07:48.9675792Z   soc_version: a3
2026-02-04T18:07:48.9676085Z   runner: linux-aarch64-a3-0
2026-02-04T18:07:48.9676505Z   image: swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3
2026-02-04T18:07:48.9676957Z   config_file_path: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-04T18:07:48.9677282Z   replicas: 1
2026-02-04T18:07:48.9677579Z   size: 2
2026-02-04T18:07:48.9677760Z   vllm_version: v0.15.0
2026-02-04T18:07:48.9678243Z   vllm_ascend_remote_url: https://github.com/vllm-project/vllm-ascend.git
2026-02-04T18:07:48.9678632Z   vllm_ascend_ref: main
2026-02-04T18:07:48.9678850Z ##[endgroup]
2026-02-04T18:07:48.9679366Z Complete job name: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-04T18:07:49.0214734Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-04T18:07:49.0217453Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-04T18:07:49.0218015Z ##[endgroup]
2026-02-04T18:07:56.4971563Z (node:74) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-04T18:07:56.4972449Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-04T18:08:22.9402687Z ##[group]Run # Decode and save kubeconfig
2026-02-04T18:08:22.9403165Z [36;1m# Decode and save kubeconfig[0m
2026-02-04T18:08:22.9435839Z [36;1mecho "***" | base64 -d > "$KUBECONFIG"[0m
2026-02-04T18:08:22.9436421Z shell: bash -el {0}
2026-02-04T18:08:22.9436667Z ##[endgroup]
2026-02-04T18:08:22.9555029Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-04T18:08:22.9555961Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-04T18:08:22.9556286Z ##[endgroup]
2026-02-04T18:08:23.3054984Z (node:5686) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-04T18:08:23.3055792Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-04T18:08:58.9349723Z ##[group]Run actions/checkout@v6
2026-02-04T18:08:58.9350080Z with:
2026-02-04T18:08:58.9350397Z   repository: vllm-project/vllm-ascend
2026-02-04T18:08:58.9351070Z   token: ***
2026-02-04T18:08:58.9351309Z   ssh-strict: true
2026-02-04T18:08:58.9351565Z   ssh-user: git
2026-02-04T18:08:58.9351768Z   persist-credentials: true
2026-02-04T18:08:58.9352263Z   clean: true
2026-02-04T18:08:58.9352523Z   sparse-checkout-cone-mode: true
2026-02-04T18:08:58.9352760Z   fetch-depth: 1
2026-02-04T18:08:58.9352991Z   fetch-tags: false
2026-02-04T18:08:58.9353291Z   show-progress: true
2026-02-04T18:08:58.9353499Z   lfs: false
2026-02-04T18:08:58.9353721Z   submodules: false
2026-02-04T18:08:58.9353912Z   set-safe-directory: true
2026-02-04T18:08:58.9354177Z ##[endgroup]
2026-02-04T18:08:58.9409359Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-04T18:08:58.9410137Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-04T18:08:58.9410440Z ##[endgroup]
2026-02-04T18:08:59.2885329Z (node:6113) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-04T18:08:59.2886110Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-04T18:09:17.2298238Z Syncing repository: vllm-project/vllm-ascend
2026-02-04T18:09:17.2299186Z ##[group]Getting Git version info
2026-02-04T18:09:17.2299439Z Working directory is '/__w/vllm-ascend/vllm-ascend'
2026-02-04T18:09:17.2299848Z [command]/usr/bin/git version
2026-02-04T18:09:17.2300040Z git version 2.34.1
2026-02-04T18:09:17.2301094Z ##[endgroup]
2026-02-04T18:09:17.2304069Z Copying '/root/.gitconfig' to '/__w/_temp/cc3dcd30-2747-4724-aa28-256f09932d29/.gitconfig'
2026-02-04T18:09:17.2304592Z Temporarily overriding HOME='/__w/_temp/cc3dcd30-2747-4724-aa28-256f09932d29' before making global git config changes
2026-02-04T18:09:17.2305122Z Adding repository directory to the temporary git global config as a safe directory
2026-02-04T18:09:17.2308203Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-04T18:09:17.2335987Z Deleting the contents of '/__w/vllm-ascend/vllm-ascend'
2026-02-04T18:09:17.2338616Z ##[group]Initializing the repository
2026-02-04T18:09:17.2341933Z [command]/usr/bin/git init /__w/vllm-ascend/vllm-ascend
2026-02-04T18:09:17.2490704Z hint: Using 'master' as the name for the initial branch. This default branch name
2026-02-04T18:09:17.2491136Z hint: is subject to change. To configure the initial branch name to use in all
2026-02-04T18:09:17.2491499Z hint: of your new repositories, which will suppress this warning, call:
2026-02-04T18:09:17.2491761Z hint: 
2026-02-04T18:09:17.2499209Z hint: 	git config --global init.defaultBranch <name>
2026-02-04T18:09:17.2499460Z hint: 
2026-02-04T18:09:17.2499667Z hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and
2026-02-04T18:09:17.2500061Z hint: 'development'. The just-created branch can be renamed via this command:
2026-02-04T18:09:17.2500339Z hint: 
2026-02-04T18:09:17.2500483Z hint: 	git branch -m <name>
2026-02-04T18:09:17.2500755Z Initialized empty Git repository in /__w/vllm-ascend/vllm-ascend/.git/
2026-02-04T18:09:17.2501444Z [command]/usr/bin/git remote add origin https://github.com/vllm-project/vllm-ascend
2026-02-04T18:09:17.2524549Z ##[endgroup]
2026-02-04T18:09:17.2524878Z ##[group]Disabling automatic garbage collection
2026-02-04T18:09:17.2527028Z [command]/usr/bin/git config --local gc.auto 0
2026-02-04T18:09:17.2551886Z ##[endgroup]
2026-02-04T18:09:17.2568576Z ##[group]Setting up auth
2026-02-04T18:09:17.2568778Z Removing SSH command configuration
2026-02-04T18:09:17.2570131Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-04T18:09:17.2584140Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-04T18:09:17.2776221Z Removing HTTP extra header
2026-02-04T18:09:17.2779355Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-04T18:09:17.2805074Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-04T18:09:17.2995317Z Removing includeIf entries pointing to credentials config files
2026-02-04T18:09:17.3000110Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-04T18:09:17.3025696Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-04T18:09:17.3213893Z [command]/usr/bin/git config --file /__w/_temp/git-credentials-039288b9-4152-4488-8845-7affca6bf092.config http.https://github.com/.extraheader AUTHORIZATION: basic ***
2026-02-04T18:09:17.3246229Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-039288b9-4152-4488-8845-7affca6bf092.config
2026-02-04T18:09:17.3275717Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-039288b9-4152-4488-8845-7affca6bf092.config
2026-02-04T18:09:17.3301557Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-039288b9-4152-4488-8845-7affca6bf092.config
2026-02-04T18:09:17.3327149Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-039288b9-4152-4488-8845-7affca6bf092.config
2026-02-04T18:09:17.3348645Z ##[endgroup]
2026-02-04T18:09:17.3348919Z ##[group]Fetching the repository
2026-02-04T18:09:17.3355968Z [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --no-recurse-submodules --depth=1 origin +0ead5e8681c4f13c7746d2611268e263f1514ec8:refs/remotes/origin/main
2026-02-04T18:09:19.0080364Z From https://gh-proxy.test.osinfra.cn/https://github.com/vllm-project/vllm-ascend
2026-02-04T18:09:19.0080797Z  * [new ref]         0ead5e8681c4f13c7746d2611268e263f1514ec8 -> origin/main
2026-02-04T18:09:19.0103876Z [command]/usr/bin/git branch --list --remote origin/main
2026-02-04T18:09:19.0132191Z   origin/main
2026-02-04T18:09:19.0139008Z [command]/usr/bin/git rev-parse refs/remotes/origin/main
2026-02-04T18:09:19.0159644Z 0ead5e8681c4f13c7746d2611268e263f1514ec8
2026-02-04T18:09:19.0164264Z ##[endgroup]
2026-02-04T18:09:19.0164686Z ##[group]Determining the checkout info
2026-02-04T18:09:19.0165118Z ##[endgroup]
2026-02-04T18:09:19.0168510Z [command]/usr/bin/git sparse-checkout disable
2026-02-04T18:09:19.0211513Z [command]/usr/bin/git config --local --unset-all extensions.worktreeConfig
2026-02-04T18:09:19.0238262Z ##[group]Checking out the ref
2026-02-04T18:09:19.0241567Z [command]/usr/bin/git checkout --progress --force -B main refs/remotes/origin/main
2026-02-04T18:09:19.1098287Z Switched to a new branch 'main'
2026-02-04T18:09:19.1098549Z Branch 'main' set up to track remote branch 'main' from 'origin'.
2026-02-04T18:09:19.1107498Z ##[endgroup]
2026-02-04T18:09:19.1148610Z [command]/usr/bin/git log -1 --format=%H
2026-02-04T18:09:19.1170208Z 0ead5e8681c4f13c7746d2611268e263f1514ec8
2026-02-04T18:09:36.9481810Z ##[group]Run # prepare for lws entrypoint scripts
2026-02-04T18:09:36.9482236Z [36;1m# prepare for lws entrypoint scripts[0m
2026-02-04T18:09:36.9482584Z [36;1minstall -D tests/e2e/nightly/multi_node/scripts/run.sh /root/.cache/tests/run.sh[0m
2026-02-04T18:09:36.9483003Z shell: bash -el {0}
2026-02-04T18:09:36.9483165Z ##[endgroup]
2026-02-04T18:09:36.9576553Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-04T18:09:36.9577252Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-04T18:09:36.9577598Z ##[endgroup]
2026-02-04T18:09:37.3070027Z (node:6699) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-04T18:09:37.3070724Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-04T18:10:13.4339858Z ##[group]Run set -euo pipefail
2026-02-04T18:10:13.4340201Z [36;1mset -euo pipefail[0m
2026-02-04T18:10:13.4340454Z [36;1m[0m
2026-02-04T18:10:13.4340683Z [36;1mCRD_NAME="${CRD_NAME:-vllm}"[0m
2026-02-04T18:10:13.4340938Z [36;1mTIMEOUT=${TIMEOUT:-120}[0m
2026-02-04T18:10:13.4341200Z [36;1mSLEEP_INTERVAL=2[0m
2026-02-04T18:10:13.4341440Z [36;1m[0m
2026-02-04T18:10:13.4341770Z [36;1mecho "Deleting leaderworkerset [$CRD_NAME] in namespace [$NAMESPACE]..."[0m
2026-02-04T18:10:13.4342392Z [36;1mkubectl delete leaderworkerset "$CRD_NAME" -n "$NAMESPACE" --ignore-not-found[0m
2026-02-04T18:10:13.4342778Z [36;1m[0m
2026-02-04T18:10:13.4343036Z [36;1mecho "Waiting for all pods starting with 'vllm' to be deleted..."[0m
2026-02-04T18:10:13.4343398Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-04T18:10:13.4343646Z [36;1m[0m
2026-02-04T18:10:13.4343830Z [36;1mwhile true; do[0m
2026-02-04T18:10:13.4344083Z [36;1m  NOW=$(date +%s)[0m
2026-02-04T18:10:13.4344408Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-04T18:10:13.4344638Z [36;1m[0m
2026-02-04T18:10:13.4344793Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-04T18:10:13.4345071Z [36;1m    echo "Timeout reached ($TIMEOUT seconds), some pods still exist:"[0m
2026-02-04T18:10:13.4345391Z [36;1m    kubectl get pods -n "$NAMESPACE" | grep '^vllm' || true[0m
2026-02-04T18:10:13.4345631Z [36;1m    exit 1[0m
2026-02-04T18:10:13.4345779Z [36;1m  fi[0m
2026-02-04T18:10:13.4345907Z [36;1m[0m
2026-02-04T18:10:13.4346281Z [36;1m  PODS_EXIST=$(kubectl get pods -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null | tr ' ' '\n' | grep '^vllm' || true)[0m
2026-02-04T18:10:13.4346690Z [36;1m[0m
2026-02-04T18:10:13.4346832Z [36;1m  if [[ -z "$PODS_EXIST" ]]; then[0m
2026-02-04T18:10:13.4347044Z [36;1m    echo "All vllm pods deleted."[0m
2026-02-04T18:10:13.4347234Z [36;1m    break[0m
2026-02-04T18:10:13.4347459Z [36;1m  else[0m
2026-02-04T18:10:13.4347666Z [36;1m    echo "Waiting for pods to be deleted: $PODS_EXIST"[0m
2026-02-04T18:10:13.4347899Z [36;1m    sleep $SLEEP_INTERVAL[0m
2026-02-04T18:10:13.4348077Z [36;1m  fi[0m
2026-02-04T18:10:13.4348217Z [36;1mdone[0m
2026-02-04T18:10:13.4348489Z shell: bash -el {0}
2026-02-04T18:10:13.4348646Z ##[endgroup]
2026-02-04T18:10:13.4423480Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-04T18:10:13.4424103Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-04T18:10:13.4424329Z ##[endgroup]
2026-02-04T18:10:13.8128192Z (node:7480) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-04T18:10:13.8128886Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-04T18:10:32.1128158Z Deleting leaderworkerset [vllm] in namespace [vllm-project]...
2026-02-04T18:10:32.4150274Z Waiting for all pods starting with 'vllm' to be deleted...
2026-02-04T18:10:32.4863057Z All vllm pods deleted.
2026-02-04T18:10:50.6044458Z ##[group]Run set -e
2026-02-04T18:10:50.6044680Z [36;1mset -e[0m
2026-02-04T18:10:50.6044828Z [36;1m[0m
2026-02-04T18:10:50.6044958Z [36;1msize="2"[0m
2026-02-04T18:10:50.6045107Z [36;1mreplicas="1"[0m
2026-02-04T18:10:50.6045429Z [36;1mimage="swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3"[0m
2026-02-04T18:10:50.6045832Z [36;1mconfig_file_path="DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-04T18:10:50.6046161Z [36;1mfail_tag=FAIL_TAG_"DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-04T18:10:50.6046441Z [36;1mecho "FAIL_TAG=${fail_tag}" >> "$GITHUB_ENV"[0m
2026-02-04T18:10:50.6046640Z [36;1m[0m
2026-02-04T18:10:50.6046855Z [36;1mrequired_params=("size" "replicas" "image" "config_file_path")[0m
2026-02-04T18:10:50.6047147Z [36;1mfor param in "${required_params[@]}"; do[0m
2026-02-04T18:10:50.6047452Z [36;1m  if [ -z "${!param}" ]; then[0m
2026-02-04T18:10:50.6047918Z [36;1m    echo "Error: Parameter '$param' is required but empty"[0m
2026-02-04T18:10:50.6048185Z [36;1m    exit 1[0m
2026-02-04T18:10:50.6048362Z [36;1m  fi[0m
2026-02-04T18:10:50.6048494Z [36;1mdone[0m
2026-02-04T18:10:50.6048627Z [36;1m[0m
2026-02-04T18:10:50.6048759Z [36;1mif [ "a3" = "a3" ]; then[0m
2026-02-04T18:10:50.6048960Z [36;1m  npu_per_node=16[0m
2026-02-04T18:10:50.6049222Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws.yaml.jinja2"[0m
2026-02-04T18:10:50.6049492Z [36;1melse[0m
2026-02-04T18:10:50.6049636Z [36;1m  npu_per_node=8[0m
2026-02-04T18:10:50.6049894Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws-a2.yaml.jinja2"[0m
2026-02-04T18:10:50.6050166Z [36;1mfi[0m
2026-02-04T18:10:50.6050296Z [36;1m[0m
2026-02-04T18:10:50.6050456Z [36;1mjinja2 $TEMPLATE_FILE \[0m
2026-02-04T18:10:50.6050636Z [36;1m  -D size="$size" \[0m
2026-02-04T18:10:50.6050821Z [36;1m  -D replicas="$replicas" \[0m
2026-02-04T18:10:50.6051005Z [36;1m  -D image="$image" \[0m
2026-02-04T18:10:50.6051222Z [36;1m  -D config_file_path="$config_file_path" \[0m
2026-02-04T18:10:50.6051460Z [36;1m  -D npu_per_node="$npu_per_node" \[0m
2026-02-04T18:10:50.6051660Z [36;1m  -D fail_tag="$fail_tag" \[0m
2026-02-04T18:10:50.6051851Z [36;1m  --outfile lws.yaml[0m
2026-02-04T18:10:50.6052120Z [36;1m[0m
2026-02-04T18:10:50.6052269Z [36;1mkubectl apply -f ./lws.yaml[0m
2026-02-04T18:10:50.6052591Z shell: bash -el {0}
2026-02-04T18:10:50.6052742Z ##[endgroup]
2026-02-04T18:10:50.6121216Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-04T18:10:50.6121835Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-04T18:10:50.6122207Z ##[endgroup]
2026-02-04T18:10:50.9624098Z (node:8447) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-04T18:10:50.9624768Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-04T18:11:10.0408194Z leaderworkerset.leaderworkerset.x-k8s.io/vllm created
2026-02-04T18:11:10.0673488Z service/vllm-leader created
2026-02-04T18:11:28.3059607Z ##[group]Run POD_PREFIX="${POD_PREFIX:-vllm-0}"
2026-02-04T18:11:28.3059936Z [36;1mPOD_PREFIX="${POD_PREFIX:-vllm-0}"[0m
2026-02-04T18:11:28.3060147Z [36;1mSIZE="2"[0m
2026-02-04T18:11:28.3060319Z [36;1mTIMEOUT=1200  # default timeout 20 minutes[0m
2026-02-04T18:11:28.3060526Z [36;1m[0m
2026-02-04T18:11:28.3060825Z [36;1mecho "Waiting for Pods in namespace [$NAMESPACE] to become Running and Ready (timeout ${TIMEOUT}s)..."[0m
2026-02-04T18:11:28.3061171Z [36;1m[0m
2026-02-04T18:11:28.3061314Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-04T18:11:28.3061510Z [36;1m[0m
2026-02-04T18:11:28.3061650Z [36;1mwhile true; do[0m
2026-02-04T18:11:28.3061808Z [36;1m  NOW=$(date +%s)[0m
2026-02-04T18:11:28.3062130Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-04T18:11:28.3062377Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-04T18:11:28.3062635Z [36;1m    echo "Timeout reached after ${ELAPSED}s"[0m
2026-02-04T18:11:28.3062919Z [36;1m    echo "Dumping pod status for debugging:"[0m
2026-02-04T18:11:28.3063145Z [36;1m    kubectl get pods -n "$NAMESPACE"[0m
2026-02-04T18:11:28.3063391Z [36;1m    kubectl describe pod "$LEADER_POD" -n "$NAMESPACE"[0m
2026-02-04T18:11:28.3063621Z [36;1m    exit 1[0m
2026-02-04T18:11:28.3063761Z [36;1m  fi[0m
2026-02-04T18:11:28.3063898Z [36;1m[0m
2026-02-04T18:11:28.3064033Z [36;1m  # 1) check follower pods[0m
2026-02-04T18:11:28.3064225Z [36;1m  ALL_FOLLOWERS_READY=true[0m
2026-02-04T18:11:28.3064412Z [36;1m  for ((i=1; i<SIZE; i++)); do[0m
2026-02-04T18:11:28.3064599Z [36;1m    POD="${POD_PREFIX}-${i}"[0m
2026-02-04T18:11:28.3064967Z [36;1m    PHASE=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-04T18:11:28.3065511Z [36;1m    READY=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-04T18:11:28.3066066Z [36;1m[0m
2026-02-04T18:11:28.3066258Z [36;1m    echo "Follower [$POD] phase=$PHASE ready=$READY"[0m
2026-02-04T18:11:28.3066487Z [36;1m[0m
2026-02-04T18:11:28.3066661Z [36;1m    if [[ "$PHASE" != "Running" || "$READY" != "true" ]]; then[0m
2026-02-04T18:11:28.3066933Z [36;1m      echo "Follower [$POD] not Ready yet..."[0m
2026-02-04T18:11:28.3067154Z [36;1m      ALL_FOLLOWERS_READY=false[0m
2026-02-04T18:11:28.3067331Z [36;1m      break[0m
2026-02-04T18:11:28.3067475Z [36;1m    fi[0m
2026-02-04T18:11:28.3067691Z [36;1m  done[0m
2026-02-04T18:11:28.3067833Z [36;1m[0m
2026-02-04T18:11:28.3067972Z [36;1m  # 2) check leader pod[0m
2026-02-04T18:11:28.3068353Z [36;1m  LEADER_PHASE=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-04T18:11:28.3068949Z [36;1m  LEADER_READY=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-04T18:11:28.3069344Z [36;1m[0m
2026-02-04T18:11:28.3069559Z [36;1m  echo "Leader [$LEADER_POD] phase=$LEADER_PHASE ready=$LEADER_READY"[0m
2026-02-04T18:11:28.3069819Z [36;1m[0m
2026-02-04T18:11:28.3070030Z [36;1m  if [[ "$LEADER_PHASE" != "Running" || "$LEADER_READY" != "true" ]]; then[0m
2026-02-04T18:11:28.3070299Z [36;1m    echo "Leader not Ready yet..."[0m
2026-02-04T18:11:28.3070502Z [36;1m    ALL_FOLLOWERS_READY=false[0m
2026-02-04T18:11:28.3070682Z [36;1m  fi[0m
2026-02-04T18:11:28.3070808Z [36;1m[0m
2026-02-04T18:11:28.3070973Z [36;1m  if [[ "$ALL_FOLLOWERS_READY" == "true" ]]; then[0m
2026-02-04T18:11:28.3071299Z [36;1m    echo "All follower pods and leader pod are Running and Ready â€” continuing."[0m
2026-02-04T18:11:28.3071577Z [36;1m    break[0m
2026-02-04T18:11:28.3071720Z [36;1m  fi[0m
2026-02-04T18:11:28.3071843Z [36;1m[0m
2026-02-04T18:11:28.3071974Z [36;1m  sleep 2[0m
2026-02-04T18:11:28.3072289Z [36;1mdone[0m
2026-02-04T18:11:28.3072560Z shell: bash -el {0}
2026-02-04T18:11:28.3072711Z env:
2026-02-04T18:11:28.3073045Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-04T18:11:28.3073297Z ##[endgroup]
2026-02-04T18:11:28.3179289Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-04T18:11:28.3180003Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-04T18:11:28.3180213Z ##[endgroup]
2026-02-04T18:11:28.6677447Z (node:9591) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-04T18:11:28.6678165Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-04T18:11:47.8893170Z Waiting for Pods in namespace [vllm-project] to become Running and Ready (timeout 1200s)...
2026-02-04T18:11:48.0333941Z Follower [vllm-0-1] phase=Running ready=true
2026-02-04T18:11:48.1623285Z Leader [vllm-0] phase=Pending ready=false
2026-02-04T18:11:48.1623554Z Leader not Ready yet...
2026-02-04T18:11:50.4461051Z Follower [vllm-0-1] phase=Running ready=true
2026-02-04T18:11:50.5630943Z Leader [vllm-0] phase=Pending ready=false
2026-02-04T18:11:50.5631175Z Leader not Ready yet...
2026-02-04T18:11:52.6787145Z Follower [vllm-0-1] phase=Running ready=true
2026-02-04T18:11:52.7942769Z Leader [vllm-0] phase=Pending ready=false
2026-02-04T18:11:52.7943022Z Leader not Ready yet...
2026-02-04T18:11:54.9101077Z Follower [vllm-0-1] phase=Running ready=true
2026-02-04T18:11:55.0292406Z Leader [vllm-0] phase=Pending ready=false
2026-02-04T18:11:55.0292633Z Leader not Ready yet...
2026-02-04T18:11:57.1407823Z Follower [vllm-0-1] phase=Running ready=true
2026-02-04T18:11:57.2524343Z Leader [vllm-0] phase=Pending ready=false
2026-02-04T18:11:57.2524596Z Leader not Ready yet...
2026-02-04T18:11:59.3713578Z Follower [vllm-0-1] phase=Running ready=true
2026-02-04T18:11:59.4927787Z Leader [vllm-0] phase=Pending ready=false
2026-02-04T18:11:59.4928022Z Leader not Ready yet...
2026-02-04T18:12:01.6116554Z Follower [vllm-0-1] phase=Running ready=true
2026-02-04T18:12:01.7261770Z Leader [vllm-0] phase=Pending ready=false
2026-02-04T18:12:01.7262140Z Leader not Ready yet...
2026-02-04T18:12:03.8407436Z Follower [vllm-0-1] phase=Running ready=true
2026-02-04T18:12:03.9557098Z Leader [vllm-0] phase=Pending ready=false
2026-02-04T18:12:03.9557527Z Leader not Ready yet...
2026-02-04T18:12:06.0764054Z Follower [vllm-0-1] phase=Running ready=true
2026-02-04T18:12:06.1890223Z Leader [vllm-0] phase=Pending ready=false
2026-02-04T18:12:06.1890483Z Leader not Ready yet...
2026-02-04T18:12:08.3050718Z Follower [vllm-0-1] phase=Running ready=true
2026-02-04T18:12:08.4316442Z Leader [vllm-0] phase=Pending ready=false
2026-02-04T18:12:08.4316693Z Leader not Ready yet...
2026-02-04T18:12:10.5453593Z Follower [vllm-0-1] phase=Running ready=true
2026-02-04T18:12:10.6533011Z Leader [vllm-0] phase=Pending ready=false
2026-02-04T18:12:10.6533248Z Leader not Ready yet...
2026-02-04T18:12:12.7714337Z Follower [vllm-0-1] phase=Running ready=true
2026-02-04T18:12:12.8850057Z Leader [vllm-0] phase=Running ready=true
2026-02-04T18:12:12.8850638Z All follower pods and leader pod are Running and Ready â€” continuing.
2026-02-04T18:12:31.4878508Z ##[group]Run set -euo pipefail
2026-02-04T18:12:31.4878762Z [36;1mset -euo pipefail[0m
2026-02-04T18:12:31.4878929Z [36;1m[0m
2026-02-04T18:12:31.4879061Z [36;1msize="2"[0m
2026-02-04T18:12:31.4879199Z [36;1mpids=()[0m
2026-02-04T18:12:31.4879335Z [36;1m[0m
2026-02-04T18:12:31.4879484Z [36;1mcleanup() {[0m
2026-02-04T18:12:31.4879675Z [36;1m  echo "Cleaning up background log streams..."[0m
2026-02-04T18:12:31.4879913Z [36;1m  for pid in "${pids[@]}"; do[0m
2026-02-04T18:12:31.4880119Z [36;1m    kill "$pid" 2>/dev/null || true[0m
2026-02-04T18:12:31.4880299Z [36;1m  done[0m
2026-02-04T18:12:31.4880438Z [36;1m}[0m
2026-02-04T18:12:31.4880571Z [36;1mtrap cleanup EXIT[0m
2026-02-04T18:12:31.4880733Z [36;1m[0m
2026-02-04T18:12:31.4880882Z [36;1mfor i in $(seq 1 $((size - 1))); do[0m
2026-02-04T18:12:31.4881086Z [36;1m  POD="vllm-0-${i}"[0m
2026-02-04T18:12:31.4881251Z [36;1m[0m
2026-02-04T18:12:31.4881456Z [36;1m  echo "==== Collecting logs from worker pod: $POD ===="[0m
2026-02-04T18:12:31.4881735Z [36;1m  kubectl logs -f "$POD" -n "$NAMESPACE" \[0m
2026-02-04T18:12:31.4881958Z [36;1m    > "/tmp/${POD}_logs.txt" 2>&1 &[0m
2026-02-04T18:12:31.4882276Z [36;1m[0m
2026-02-04T18:12:31.4882406Z [36;1m  pids+=($!)[0m
2026-02-04T18:12:31.4882606Z [36;1mdone[0m
2026-02-04T18:12:31.4882743Z [36;1m[0m
2026-02-04T18:12:31.4882929Z [36;1mecho "==== Streaming logs from leader pod: $LEADER_POD ===="[0m
2026-02-04T18:12:31.4883210Z [36;1mecho "Looking for logs containing: $FAIL_TAG"[0m
2026-02-04T18:12:31.4883416Z [36;1m[0m
2026-02-04T18:12:31.4883637Z [36;1mkubectl logs -f "$LEADER_POD" -n "$NAMESPACE" | while IFS= read -r line; do[0m
2026-02-04T18:12:31.4883919Z [36;1m  echo "$line"[0m
2026-02-04T18:12:31.4884111Z [36;1m  if echo "$line" | grep -q "$FAIL_TAG"; then[0m
2026-02-04T18:12:31.4884319Z [36;1m    exit 1[0m
2026-02-04T18:12:31.4884462Z [36;1m  fi[0m
2026-02-04T18:12:31.4884589Z [36;1mdone[0m
2026-02-04T18:12:31.4884987Z shell: bash -el {0}
2026-02-04T18:12:31.4885143Z env:
2026-02-04T18:12:31.4885328Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-04T18:12:31.4885566Z ##[endgroup]
2026-02-04T18:12:31.4970386Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-04T18:12:31.4971012Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-04T18:12:31.4971217Z ##[endgroup]
2026-02-04T18:12:31.8505534Z (node:10920) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-04T18:12:31.8506191Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-04T18:12:50.5644192Z ==== Collecting logs from worker pod: vllm-0-1 ====
2026-02-04T18:12:50.5651101Z ==== Streaming logs from leader pod: vllm-0 ====
2026-02-04T18:12:50.5651779Z Looking for logs containing: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-04T18:12:50.6496638Z ====> Check NPU info
2026-02-04T18:12:50.6512401Z +------------------------------------------------------------------------------------------------+
2026-02-04T18:12:50.6524807Z | npu-smi 25.5.0                   Version: 25.5.0                                               |
2026-02-04T18:12:50.6538561Z +---------------------------+---------------+----------------------------------------------------+
2026-02-04T18:12:50.6545901Z | NPU   Name                | Health        | Power(W)    Temp(C)           Hugepages-Usage(page)|
2026-02-04T18:12:50.6553181Z | Chip  Phy-ID              | Bus-Id        | AICore(%)   Memory-Usage(MB)  HBM-Usage(MB)        |
2026-02-04T18:12:50.6564063Z +===========================+===============+====================================================+
2026-02-04T18:12:50.6573921Z | 0     Ascend910           | OK            | 163.6       36                0    / 0             |
2026-02-04T18:12:50.6584040Z | 0     0                   | 0000:9D:00.0  | 0           0    / 0          3151 / 65536         |
2026-02-04T18:12:50.6594691Z +------------------------------------------------------------------------------------------------+
2026-02-04T18:12:50.6605286Z | 0     Ascend910           | OK            | -           37                0    / 0             |
2026-02-04T18:12:50.6614770Z | 1     1                   | 0000:9F:00.0  | 0           0    / 0          2892 / 65536         |
2026-02-04T18:12:50.6625071Z +===========================+===============+====================================================+
2026-02-04T18:12:50.6635156Z | 1     Ascend910           | OK            | 162.9       36                0    / 0             |
2026-02-04T18:12:50.6645365Z | 0     2                   | 0000:99:00.0  | 0           0    / 0          3162 / 65536         |
2026-02-04T18:12:50.6655841Z +------------------------------------------------------------------------------------------------+
2026-02-04T18:12:50.6667200Z | 1     Ascend910           | OK            | -           36                0    / 0             |
2026-02-04T18:12:50.6677618Z | 1     3                   | 0000:9B:00.0  | 0           0    / 0          2883 / 65536         |
2026-02-04T18:12:50.6687634Z +===========================+===============+====================================================+
2026-02-04T18:12:50.6698933Z | 2     Ascend910           | OK            | 164.2       37                0    / 0             |
2026-02-04T18:12:50.6708026Z | 0     4                   | 0000:95:00.0  | 0           0    / 0          3161 / 65536         |
2026-02-04T18:12:50.6717208Z +------------------------------------------------------------------------------------------------+
2026-02-04T18:12:50.6728019Z | 2     Ascend910           | OK            | -           37                0    / 0             |
2026-02-04T18:12:50.6737813Z | 1     5                   | 0000:97:00.0  | 0           0    / 0          2882 / 65536         |
2026-02-04T18:12:50.6747061Z +===========================+===============+====================================================+
2026-02-04T18:12:50.6756859Z | 3     Ascend910           | OK            | 171.5       36                0    / 0             |
2026-02-04T18:12:50.6767300Z | 0     6                   | 0000:91:00.0  | 0           0    / 0          3149 / 65536         |
2026-02-04T18:12:50.6777368Z +------------------------------------------------------------------------------------------------+
2026-02-04T18:12:50.6787137Z | 3     Ascend910           | OK            | -           36                0    / 0             |
2026-02-04T18:12:50.6796794Z | 1     7                   | 0000:93:00.0  | 0           0    / 0          2892 / 65536         |
2026-02-04T18:12:50.6814154Z +===========================+===============+====================================================+
2026-02-04T18:12:50.6946558Z | 4     Ascend910           | OK            | 163.2       36                0    / 0             |
2026-02-04T18:12:50.6962353Z | 0     8                   | 0000:8D:00.0  | 0           0    / 0          3156 / 65536         |
2026-02-04T18:12:50.6964525Z +------------------------------------------------------------------------------------------------+
2026-02-04T18:12:50.6975207Z | 4     Ascend910           | OK            | -           35                0    / 0             |
2026-02-04T18:12:50.6984833Z | 1     9                   | 0000:8F:00.0  | 0           0    / 0          2898 / 65536         |
2026-02-04T18:12:50.6995162Z +===========================+===============+====================================================+
2026-02-04T18:12:50.7005587Z | 5     Ascend910           | OK            | 164.7       34                0    / 0             |
2026-02-04T18:12:50.7015940Z | 0     10                  | 0000:89:00.0  | 0           0    / 0          3153 / 65536         |
2026-02-04T18:12:50.7025489Z +------------------------------------------------------------------------------------------------+
2026-02-04T18:12:50.7040984Z | 5     Ascend910           | OK            | -           36                0    / 0             |
2026-02-04T18:12:50.7047048Z | 1     11                  | 0000:8B:00.0  | 0           0    / 0          2898 / 65536         |
2026-02-04T18:12:50.7056196Z +===========================+===============+====================================================+
2026-02-04T18:12:50.7066468Z | 6     Ascend910           | OK            | 166.1       36                0    / 0             |
2026-02-04T18:12:50.7076359Z | 0     12                  | 0000:85:00.0  | 0           0    / 0          3153 / 65536         |
2026-02-04T18:12:50.7087932Z +------------------------------------------------------------------------------------------------+
2026-02-04T18:12:50.7096533Z | 6     Ascend910           | OK            | -           34                0    / 0             |
2026-02-04T18:12:50.7105942Z | 1     13                  | 0000:87:00.0  | 0           0    / 0          2899 / 65536         |
2026-02-04T18:12:50.7115754Z +===========================+===============+====================================================+
2026-02-04T18:12:50.7126347Z | 7     Ascend910           | OK            | 162.9       36                0    / 0             |
2026-02-04T18:12:50.7136010Z | 0     14                  | 0000:81:00.0  | 0           0    / 0          3161 / 65536         |
2026-02-04T18:12:50.7146108Z +------------------------------------------------------------------------------------------------+
2026-02-04T18:12:50.7155153Z | 7     Ascend910           | OK            | -           36                0    / 0             |
2026-02-04T18:12:50.7165843Z | 1     15                  | 0000:83:00.0  | 0           0    / 0          2881 / 65536         |
2026-02-04T18:12:50.7175625Z +===========================+===============+====================================================+
2026-02-04T18:12:50.7185699Z +---------------------------+---------------+----------------------------------------------------+
2026-02-04T18:12:50.7195477Z | NPU     Chip              | Process id    | Process name             | Process memory(MB)      |
2026-02-04T18:12:50.7206927Z +===========================+===============+====================================================+
2026-02-04T18:12:50.7216050Z | No running processes found in NPU 0                                                            |
2026-02-04T18:12:50.7225282Z +===========================+===============+====================================================+
2026-02-04T18:12:50.7234706Z | No running processes found in NPU 1                                                            |
2026-02-04T18:12:50.7244845Z +===========================+===============+====================================================+
2026-02-04T18:12:50.7254951Z | No running processes found in NPU 2                                                            |
2026-02-04T18:12:50.7267940Z +===========================+===============+====================================================+
2026-02-04T18:12:50.7274947Z | No running processes found in NPU 3                                                            |
2026-02-04T18:12:50.7284050Z +===========================+===============+====================================================+
2026-02-04T18:12:50.7293853Z | No running processes found in NPU 4                                                            |
2026-02-04T18:12:50.7304825Z +===========================+===============+====================================================+
2026-02-04T18:12:50.7313670Z | No running processes found in NPU 5                                                            |
2026-02-04T18:12:50.7323579Z +===========================+===============+====================================================+
2026-02-04T18:12:50.7333933Z | No running processes found in NPU 6                                                            |
2026-02-04T18:12:50.7343118Z +===========================+===============+====================================================+
2026-02-04T18:12:50.7353010Z | No running processes found in NPU 7                                                            |
2026-02-04T18:12:50.7363286Z +===========================+===============+====================================================+
2026-02-04T18:12:50.7372954Z package_name=Ascend-cann-toolkit
2026-02-04T18:12:50.7382126Z version=8.5.0
2026-02-04T18:12:50.7391552Z innerversion=V100R001C25SPC001B232
2026-02-04T18:12:50.7403151Z compatible_version=[V100R001C15],[V100R001C18],[V100R001C19],[V100R001C20],[V100R001C21],[V100R001C23]
2026-02-04T18:12:50.7414070Z arch=aarch64
2026-02-04T18:12:50.7420785Z os=linux
2026-02-04T18:12:50.7432893Z path=/usr/local/Ascend/cann-8.5.0
2026-02-04T18:12:50.7439587Z ====> Configure mirrors and git proxy
2026-02-04T18:12:50.7449240Z Writing to /root/.config/pip/pip.conf
2026-02-04T18:12:50.7459024Z Installed vLLM-related Python packages:
2026-02-04T18:12:50.7468877Z ais_bench_benchmark               3.0.20250930               /vllm-workspace/vllm-ascend/benchmark
2026-02-04T18:12:50.7485094Z vllm                              0.15.0+empty               /vllm-workspace/vllm
2026-02-04T18:12:50.7492377Z vllm_ascend                       0.14.0rc2.dev94+g2dac18afe /vllm-workspace/vllm-ascend
2026-02-04T18:12:50.7496834Z 
2026-02-04T18:12:50.7506622Z ============================
2026-02-04T18:12:50.7519007Z vLLM Git information
2026-02-04T18:12:50.7526782Z ============================
2026-02-04T18:12:50.7535534Z Branch:      HEAD
2026-02-04T18:12:50.7542911Z Commit hash: f176443446f659dbab5315e056e605d8984fd976
2026-02-04T18:12:50.7552335Z Author:      TJian <tunjian.tan@embeddedllm.com>
2026-02-04T18:12:50.7563265Z Date:        2026-01-29 14:45:42 +0800
2026-02-04T18:12:50.7581218Z Message:     [Release] [CI] Optim release pipeline (#33156)
2026-02-04T18:12:50.7583810Z Tags:        v0.15.0
2026-02-04T18:12:50.7592689Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm.git (fetch)
2026-02-04T18:12:50.7599612Z 
2026-02-04T18:12:50.7610604Z 
2026-02-04T18:12:50.7618133Z ============================
2026-02-04T18:12:50.7627800Z vLLM-Ascend Git information
2026-02-04T18:12:50.7641470Z ============================
2026-02-04T18:12:50.7647180Z Branch:      main
2026-02-04T18:12:50.7656527Z Commit hash: 2dac18afeaca01776ffe3b406f67ab7f1b4351f7
2026-02-04T18:12:50.7666008Z Author:      DreamerLeader <88812830+DreamerLeader@users.noreply.github.com>
2026-02-04T18:12:50.7677768Z Date:        2026-02-04 16:35:41 +0800
2026-02-04T18:12:50.7690737Z Message:     [Bugfix]Fix of Pooling Code and Update of Pooling Usage Guide (#6126)
2026-02-04T18:12:50.7695327Z Tags:        
2026-02-04T18:12:50.7705269Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm-ascend (fetch)
2026-02-04T18:12:50.7713302Z 
2026-02-04T18:12:50.7723138Z ====> Check triton ascend info
2026-02-04T18:12:50.7738414Z Ubuntu clang version 15.0.7
2026-02-04T18:12:50.7752918Z Target: aarch64-unknown-linux-gnu
2026-02-04T18:12:50.7760192Z Thread model: posix
2026-02-04T18:12:50.7762995Z InstalledDir: /usr/bin
2026-02-04T18:12:50.7771654Z Found candidate GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-04T18:12:50.7780709Z Selected GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-04T18:12:50.7794755Z Candidate multilib: .;@m64
2026-02-04T18:12:50.7801263Z Selected multilib: .;@m64
2026-02-04T18:12:50.7809356Z /usr/local/Ascend/ascend-toolkit/latest/bin/bishengir-compile
2026-02-04T18:12:50.7821584Z Name: triton-ascend
2026-02-04T18:12:50.7827877Z Version: 3.2.0
2026-02-04T18:12:50.7837785Z Summary: A language and compiler for custom Deep Learning operations on Ascend hardwares
2026-02-04T18:12:50.7847756Z Home-page: https://gitcode.com/Ascend/triton-ascend/
2026-02-04T18:12:50.7857212Z Author: 
2026-02-04T18:12:50.7866337Z Author-email: 
2026-02-04T18:12:50.7875627Z License: 
2026-02-04T18:12:50.7885600Z Location: /usr/local/python3.11.14/lib/python3.11/site-packages
2026-02-04T18:12:50.7894759Z Requires: 
2026-02-04T18:12:50.7904580Z Required-by: vllm_ascend
2026-02-04T18:12:50.7914039Z INFO 02-04 18:12:29 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:12:50.7923720Z INFO 02-04 18:12:29 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:12:50.7934616Z INFO 02-04 18:12:29 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:12:50.7942944Z INFO 02-04 18:12:29 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:12:50.7951832Z ============================= test session starts ==============================
2026-02-04T18:12:50.7961470Z platform linux -- Python 3.11.14, pytest-8.4.2, pluggy-1.6.0 -- /usr/local/python3.11.14/bin/python3
2026-02-04T18:12:50.7975382Z cachedir: .pytest_cache
2026-02-04T18:12:50.7982161Z rootdir: /vllm-workspace/vllm-ascend
2026-02-04T18:12:50.7992137Z configfile: pyproject.toml
2026-02-04T18:12:50.7999690Z plugins: cov-7.0.0, asyncio-1.3.0, mock-3.15.1, anyio-4.12.1
2026-02-04T18:12:50.8050366Z asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
2026-02-04T18:12:50.8050780Z collecting ... collected 1 item
2026-02-04T18:12:50.8050929Z 
2026-02-04T18:12:50.8051241Z [2026-02-04 18:12:34] INFO multi_node_config.py:294: Loading config yaml: tests/e2e/nightly/multi_node/config/DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-04T18:12:50.8051756Z [2026-02-04 18:12:34] INFO multi_node_config.py:351: Resolving cluster IPs via DNS...
2026-02-04T18:12:50.8061881Z [2026-02-04 18:12:34] INFO multi_node_config.py:212: Node 0 envs: {'HCCL_OP_EXPANSION_MODE': 'AIV', 'VLLM_USE_MODELSCOPE': 'True', 'HCCL_BUFFSIZE': '1024', 'SERVER_PORT': '8080', 'OMP_PROC_BIND': 'False', 'OMP_NUM_THREADS': '1', 'PYTORCH_NPU_ALLOC_CONF': 'expandable_segments:True', 'VLLM_ASCEND_ENABLE_FLASHCOMM1': '1', 'ASCEND_A3_EBA_ENABLE': '1', 'HCCL_IF_IP': '10.0.0.76', 'HCCL_SOCKET_IFNAME': 'eth0', 'GLOO_SOCKET_IFNAME': 'eth0', 'TP_SOCKET_IFNAME': 'eth0', 'LOCAL_IP': '10.0.0.76', 'NIC_NAME': 'eth0', 'MASTER_IP': '10.0.0.76'}
2026-02-04T18:12:50.8069885Z [2026-02-04 18:12:34] INFO multi_node_config.py:137: Not launching proxy on non-master node
2026-02-04T18:12:50.8084332Z [2026-02-04 18:12:34] INFO conftest.py:232: Starting server with command: vllm serve vllm-ascend/DeepSeek-V3.2-W8A8 --host 0.0.0.0 --port 8080 --data-parallel-size 4 --data-parallel-size-local 2 --data-parallel-address 10.0.0.76 --data-parallel-rpc-port 13399 --tensor-parallel-size 8 --quantization ascend --seed 1024 --enable-expert-parallel --max-num-seqs 16 --max-model-len 8192 --max-num-batched-tokens 4096 --no-enable-prefix-caching --gpu-memory-utilization 0.85 --trust-remote-code --speculative-config {"num_speculative_tokens": 2, "method":"deepseek_mtp"} --compilation-config {"cudagraph_capture_sizes": [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], "cudagraph_mode": "FULL_DECODE_ONLY"} --additional-config {"layer_sharding": ["q_b_proj", "o_proj"]} --tokenizer-mode deepseek_v32 --reasoning-parser deepseek_v3
2026-02-04T18:12:50.8100604Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node INFO 02-04 18:12:38 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:12:50.8103071Z INFO 02-04 18:12:38 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:12:50.8111173Z INFO 02-04 18:12:38 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:12:50.8120663Z INFO 02-04 18:12:38 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:12:50.8130594Z 2026-02-04 18:12:44,643 - 67 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:12:50.8140903Z INFO 02-04 18:12:44 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:12:50.8150275Z INFO 02-04 18:12:44 [serve.py:100] Defaulting api_server_count to data_parallel_size (4).
2026-02-04T18:12:50.8159558Z INFO 02-04 18:12:44 [utils.py:325] 
2026-02-04T18:12:50.8169740Z INFO 02-04 18:12:44 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
2026-02-04T18:12:50.8178552Z INFO 02-04 18:12:44 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
2026-02-04T18:12:50.8187772Z INFO 02-04 18:12:44 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   vllm-ascend/DeepSeek-V3.2-W8A8
2026-02-04T18:12:50.8196930Z INFO 02-04 18:12:44 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
2026-02-04T18:12:50.8206972Z INFO 02-04 18:12:44 [utils.py:325] 
2026-02-04T18:12:50.8225235Z INFO 02-04 18:12:44 [utils.py:261] non-default args: {'model_tag': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'api_server_count': 4, 'host': '0.0.0.0', 'port': 8080, 'model': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'tokenizer_mode': 'deepseek_v32', 'trust_remote_code': True, 'seed': 1024, 'max_model_len': 8192, 'quantization': 'ascend', 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 8, 'data_parallel_size': 4, 'data_parallel_size_local': 2, 'data_parallel_address': '10.0.0.76', 'data_parallel_rpc_port': 13399, 'enable_expert_parallel': True, 'gpu_memory_utilization': 0.85, 'enable_prefix_caching': False, 'max_num_batched_tokens': 4096, 'max_num_seqs': 16, 'speculative_config': {'num_speculative_tokens': 2, 'method': 'deepseek_mtp'}, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}, 'additional_config': {'layer_sharding': ['q_b_proj', 'o_proj']}}
2026-02-04T18:12:50.8231904Z 2026-02-04 18:12:44,879 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-04T18:12:50.8241373Z INFO 02-04 18:12:44 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-04T18:12:50.8249793Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:12:50.8259763Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:12:50.8268608Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:12:50.8279116Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-04T18:12:50.8288147Z INFO 02-04 18:12:44 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-04T18:12:50.8297423Z INFO 02-04 18:12:44 [model.py:1561] Using max model len 8192
2026-02-04T18:12:50.8307791Z WARNING 02-04 18:12:45 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-04T18:12:50.8317764Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:12:50.8327268Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:12:50.8336504Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-04T18:12:50.8345183Z INFO 02-04 18:12:45 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-04T18:12:50.8357088Z INFO 02-04 18:12:45 [model.py:1561] Using max model len 163840
2026-02-04T18:12:50.8365929Z WARNING 02-04 18:12:45 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-04T18:12:50.8374340Z INFO 02-04 18:12:45 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-04T18:12:50.8383066Z INFO 02-04 18:12:45 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-04T18:12:50.8392615Z INFO 02-04 18:12:45 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-04T18:12:50.8402389Z WARNING 02-04 18:12:45 [platform.py:707] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-04T18:12:50.8412090Z WARNING 02-04 18:12:45 [platform.py:748] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-04T18:12:50.8421768Z INFO 02-04 18:12:45 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:12:50.8431729Z WARNING 02-04 18:12:45 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-04T18:12:50.8440948Z INFO 02-04 18:12:45 [platform.py:310] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-04T18:12:50.8450422Z WARNING 02-04 18:12:45 [platform.py:327] [91m
2026-02-04T18:12:50.8459784Z WARNING 02-04 18:12:45 [platform.py:327]             **********************************************************************************
2026-02-04T18:12:50.8469488Z WARNING 02-04 18:12:45 [platform.py:327]             * WARNING: You have enabled the *full graph* feature.
2026-02-04T18:12:50.8481048Z WARNING 02-04 18:12:45 [platform.py:327]             * This is an early experimental stage and may involve various unknown issues.
2026-02-04T18:12:50.8489842Z WARNING 02-04 18:12:45 [platform.py:327]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-04T18:12:50.8498394Z WARNING 02-04 18:12:45 [platform.py:327]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-04T18:12:50.8507987Z WARNING 02-04 18:12:45 [platform.py:327]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-04T18:12:50.8517186Z WARNING 02-04 18:12:45 [platform.py:327]             * batch size for graph capture.
2026-02-04T18:12:50.8527267Z WARNING 02-04 18:12:45 [platform.py:327]             * For more details, please refer to:
2026-02-04T18:12:50.8536749Z WARNING 02-04 18:12:45 [platform.py:327]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-04T18:12:50.8546187Z WARNING 02-04 18:12:45 [platform.py:327]             **********************************************************************************[0m
2026-02-04T18:12:50.8555042Z WARNING 02-04 18:12:45 [platform.py:327]             
2026-02-04T18:12:50.8567809Z INFO 02-04 18:12:45 [platform.py:420] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-04T18:12:50.8574713Z INFO 02-04 18:12:45 [utils.py:851] Started DP Coordinator process (PID: 80)
2026-02-04T18:12:50.8585140Z INFO 02-04 18:12:49 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:12:50.8593299Z INFO 02-04 18:12:49 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:12:50.8602838Z INFO 02-04 18:12:49 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:12:50.8611899Z INFO 02-04 18:12:49 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:12:50.8621717Z INFO 02-04 18:12:49 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:12:50.8631593Z INFO 02-04 18:12:49 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:12:50.8640995Z INFO 02-04 18:12:49 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:12:50.8649653Z INFO 02-04 18:12:49 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:12:59.3700054Z INFO 02-04 18:12:59 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:12:59.3709311Z INFO 02-04 18:12:59 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:12:59.3722424Z INFO 02-04 18:12:59 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:12:59.3767721Z INFO 02-04 18:12:59 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:13:04.5856463Z INFO 02-04 18:13:04 [utils.py:218] Started 4 API server processes
2026-02-04T18:13:04.7714447Z [0;36m(EngineCore_DP1 pid=102)[0;0m 2026-02-04 18:13:04,769 - 102 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:13:04.7737329Z [0;36m(EngineCore_DP0 pid=83)[0;0m 2026-02-04 18:13:04,770 - 83 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:13:04.7773032Z [0;36m(EngineCore_DP1 pid=102)[0;0m INFO 02-04 18:13:04 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:13:04.7783265Z [0;36m(EngineCore_DP0 pid=83)[0;0m INFO 02-04 18:13:04 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:13:04.7810272Z [0;36m(EngineCore_DP0 pid=83)[0;0m INFO 02-04 18:13:04 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', speculative_config=SpeculativeConfig(method='mtp', model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', num_spec_tokens=2), tokenizer='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', skip_tokenizer_init=False, tokenizer_mode=deepseek_v32, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=4, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=1024, served_model_name=/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [24, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 48, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
2026-02-04T18:13:09.2295879Z INFO 02-04 18:13:09 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:13:09.2777272Z INFO 02-04 18:13:09 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:13:09.2787368Z INFO 02-04 18:13:09 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:13:09.2798213Z INFO 02-04 18:13:09 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:13:09.5509922Z INFO 02-04 18:13:09 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:13:09.5518049Z INFO 02-04 18:13:09 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:13:09.5527320Z INFO 02-04 18:13:09 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:13:09.5584631Z INFO 02-04 18:13:09 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:13:09.8394294Z INFO 02-04 18:13:09 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:13:09.8403077Z INFO 02-04 18:13:09 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:13:09.8412531Z INFO 02-04 18:13:09 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:13:09.8510964Z INFO 02-04 18:13:09 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:13:09.9991766Z INFO 02-04 18:13:09 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:13:10.0000016Z INFO 02-04 18:13:09 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:13:10.0009248Z INFO 02-04 18:13:09 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:13:10.0098982Z INFO 02-04 18:13:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:13:10.2180356Z INFO 02-04 18:13:10 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:13:10.2189515Z INFO 02-04 18:13:10 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:13:10.2211738Z INFO 02-04 18:13:10 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:13:10.2212684Z INFO 02-04 18:13:10 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:13:10.2217144Z INFO 02-04 18:13:10 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:13:10.2227530Z INFO 02-04 18:13:10 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:13:10.2279044Z INFO 02-04 18:13:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:13:10.2311003Z INFO 02-04 18:13:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:13:15.0207398Z [0;36m(ApiServer_1 pid=114)[0;0m 2026-02-04 18:13:15,018 - 114 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:13:15.0352891Z [0;36m(ApiServer_1 pid=114)[0;0m INFO 02-04 18:13:15 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:13:15.0635230Z [0;36m(ApiServer_1 pid=114)[0;0m 2026-02-04 18:13:15,061 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-04T18:13:15.0646591Z [0;36m(ApiServer_1 pid=114)[0;0m INFO 02-04 18:13:15 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-04T18:13:15.1609945Z [0;36m(ApiServer_1 pid=114)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:13:15.1638729Z [0;36m(ApiServer_1 pid=114)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:13:15.1666013Z [0;36m(ApiServer_1 pid=114)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:13:15.1676712Z [0;36m(ApiServer_1 pid=114)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-04T18:13:15.1738454Z [0;36m(ApiServer_1 pid=114)[0;0m INFO 02-04 18:13:15 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-04T18:13:15.1762397Z [0;36m(ApiServer_1 pid=114)[0;0m INFO 02-04 18:13:15 [model.py:1561] Using max model len 8192
2026-02-04T18:13:15.2816061Z [0;36m(ApiServer_1 pid=114)[0;0m WARNING 02-04 18:13:15 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-04T18:13:15.2840588Z [0;36m(ApiServer_1 pid=114)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:13:15.2878441Z [0;36m(ApiServer_1 pid=114)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:13:15.2879404Z [0;36m(ApiServer_1 pid=114)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-04T18:13:15.2924453Z [0;36m(ApiServer_1 pid=114)[0;0m INFO 02-04 18:13:15 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-04T18:13:15.2936666Z [0;36m(ApiServer_1 pid=114)[0;0m INFO 02-04 18:13:15 [model.py:1561] Using max model len 163840
2026-02-04T18:13:15.2947203Z [0;36m(ApiServer_1 pid=114)[0;0m WARNING 02-04 18:13:15 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-04T18:13:15.2955919Z [0;36m(ApiServer_1 pid=114)[0;0m INFO 02-04 18:13:15 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-04T18:13:15.4065848Z [0;36m(ApiServer_1 pid=114)[0;0m INFO 02-04 18:13:15 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-04T18:13:15.4077285Z [0;36m(ApiServer_1 pid=114)[0;0m INFO 02-04 18:13:15 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-04T18:13:15.4099709Z [0;36m(ApiServer_1 pid=114)[0;0m WARNING 02-04 18:13:15 [platform.py:707] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-04T18:13:15.4109756Z [0;36m(ApiServer_1 pid=114)[0;0m WARNING 02-04 18:13:15 [platform.py:748] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-04T18:13:15.4120497Z [0;36m(ApiServer_1 pid=114)[0;0m INFO 02-04 18:13:15 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:13:15.4130161Z [0;36m(ApiServer_1 pid=114)[0;0m WARNING 02-04 18:13:15 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-04T18:13:15.4139700Z [0;36m(ApiServer_1 pid=114)[0;0m INFO 02-04 18:13:15 [platform.py:310] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-04T18:13:15.4149643Z [0;36m(ApiServer_1 pid=114)[0;0m WARNING 02-04 18:13:15 [platform.py:327] [91m
2026-02-04T18:13:15.4160513Z [0;36m(ApiServer_1 pid=114)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             **********************************************************************************
2026-02-04T18:13:15.4169570Z [0;36m(ApiServer_1 pid=114)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             * WARNING: You have enabled the *full graph* feature.
2026-02-04T18:13:15.4178148Z [0;36m(ApiServer_1 pid=114)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             * This is an early experimental stage and may involve various unknown issues.
2026-02-04T18:13:15.4187169Z [0;36m(ApiServer_1 pid=114)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-04T18:13:15.4200772Z [0;36m(ApiServer_1 pid=114)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-04T18:13:15.4208901Z [0;36m(ApiServer_1 pid=114)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-04T18:13:15.4218544Z [0;36m(ApiServer_1 pid=114)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             * batch size for graph capture.
2026-02-04T18:13:15.4228006Z [0;36m(ApiServer_1 pid=114)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             * For more details, please refer to:
2026-02-04T18:13:15.4238523Z [0;36m(ApiServer_1 pid=114)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-04T18:13:15.4248087Z [0;36m(ApiServer_1 pid=114)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             **********************************************************************************[0m
2026-02-04T18:13:15.4257271Z [0;36m(ApiServer_1 pid=114)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             
2026-02-04T18:13:15.4266851Z [0;36m(ApiServer_1 pid=114)[0;0m INFO 02-04 18:13:15 [platform.py:420] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-04T18:13:15.5239467Z [0;36m(ApiServer_2 pid=115)[0;0m 2026-02-04 18:13:15,522 - 115 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:13:15.5398306Z [0;36m(ApiServer_2 pid=115)[0;0m INFO 02-04 18:13:15 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:13:15.5555620Z [0;36m(ApiServer_2 pid=115)[0;0m 2026-02-04 18:13:15,553 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-04T18:13:15.5567307Z [0;36m(ApiServer_2 pid=115)[0;0m INFO 02-04 18:13:15 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-04T18:13:15.6563162Z [0;36m(ApiServer_2 pid=115)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:13:15.6587800Z [0;36m(ApiServer_2 pid=115)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:13:15.6610001Z [0;36m(ApiServer_2 pid=115)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:13:15.6620619Z [0;36m(ApiServer_2 pid=115)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-04T18:13:15.6670899Z [0;36m(ApiServer_2 pid=115)[0;0m INFO 02-04 18:13:15 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-04T18:13:15.6694409Z [0;36m(ApiServer_2 pid=115)[0;0m INFO 02-04 18:13:15 [model.py:1561] Using max model len 8192
2026-02-04T18:13:15.7536856Z 2026-02-04 18:13:15,751 - 131 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:13:15.7574927Z INFO 02-04 18:13:15 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:13:15.7617934Z 2026-02-04 18:13:15,760 - 130 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:13:15.7658382Z INFO 02-04 18:13:15 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:13:15.7783305Z [0;36m(ApiServer_2 pid=115)[0;0m WARNING 02-04 18:13:15 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-04T18:13:15.7810800Z [0;36m(ApiServer_2 pid=115)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:13:15.7838866Z [0;36m(ApiServer_2 pid=115)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:13:15.7840165Z [0;36m(ApiServer_2 pid=115)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-04T18:13:15.7876618Z [0;36m(ApiServer_2 pid=115)[0;0m INFO 02-04 18:13:15 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-04T18:13:15.7901719Z [0;36m(ApiServer_2 pid=115)[0;0m INFO 02-04 18:13:15 [model.py:1561] Using max model len 163840
2026-02-04T18:13:15.7904071Z [0;36m(ApiServer_2 pid=115)[0;0m WARNING 02-04 18:13:15 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-04T18:13:15.7910193Z [0;36m(ApiServer_2 pid=115)[0;0m INFO 02-04 18:13:15 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-04T18:13:15.8655423Z [0;36m(ApiServer_3 pid=116)[0;0m 2026-02-04 18:13:15,863 - 116 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:13:15.8834102Z [0;36m(ApiServer_3 pid=116)[0;0m INFO 02-04 18:13:15 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:13:15.9011610Z [0;36m(ApiServer_3 pid=116)[0;0m 2026-02-04 18:13:15,899 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-04T18:13:15.9031416Z [0;36m(ApiServer_3 pid=116)[0;0m INFO 02-04 18:13:15 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-04T18:13:15.9050674Z [0;36m(ApiServer_2 pid=115)[0;0m INFO 02-04 18:13:15 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-04T18:13:15.9060666Z [0;36m(ApiServer_2 pid=115)[0;0m INFO 02-04 18:13:15 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-04T18:13:15.9070776Z [0;36m(ApiServer_2 pid=115)[0;0m WARNING 02-04 18:13:15 [platform.py:707] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-04T18:13:15.9080427Z [0;36m(ApiServer_2 pid=115)[0;0m WARNING 02-04 18:13:15 [platform.py:748] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-04T18:13:15.9091233Z [0;36m(ApiServer_2 pid=115)[0;0m INFO 02-04 18:13:15 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:13:15.9100972Z [0;36m(ApiServer_2 pid=115)[0;0m WARNING 02-04 18:13:15 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-04T18:13:15.9111030Z [0;36m(ApiServer_2 pid=115)[0;0m INFO 02-04 18:13:15 [platform.py:310] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-04T18:13:15.9120919Z [0;36m(ApiServer_2 pid=115)[0;0m WARNING 02-04 18:13:15 [platform.py:327] [91m
2026-02-04T18:13:15.9131935Z [0;36m(ApiServer_2 pid=115)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             **********************************************************************************
2026-02-04T18:13:15.9141118Z [0;36m(ApiServer_2 pid=115)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             * WARNING: You have enabled the *full graph* feature.
2026-02-04T18:13:15.9151693Z [0;36m(ApiServer_2 pid=115)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             * This is an early experimental stage and may involve various unknown issues.
2026-02-04T18:13:15.9161073Z [0;36m(ApiServer_2 pid=115)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-04T18:13:15.9171918Z [0;36m(ApiServer_2 pid=115)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-04T18:13:15.9181637Z [0;36m(ApiServer_2 pid=115)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-04T18:13:15.9190102Z [0;36m(ApiServer_2 pid=115)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             * batch size for graph capture.
2026-02-04T18:13:15.9200195Z [0;36m(ApiServer_2 pid=115)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             * For more details, please refer to:
2026-02-04T18:13:15.9210364Z [0;36m(ApiServer_2 pid=115)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-04T18:13:15.9267111Z [0;36m(ApiServer_2 pid=115)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             **********************************************************************************[0m
2026-02-04T18:13:15.9267810Z [0;36m(ApiServer_2 pid=115)[0;0m WARNING 02-04 18:13:15 [platform.py:327]             
2026-02-04T18:13:15.9268374Z [0;36m(ApiServer_2 pid=115)[0;0m INFO 02-04 18:13:15 [platform.py:420] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-04T18:13:16.0059496Z [0;36m(ApiServer_3 pid=116)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:13:16.0071763Z [0;36m(ApiServer_3 pid=116)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:13:16.0086514Z [0;36m(ApiServer_3 pid=116)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:13:16.0091253Z [0;36m(ApiServer_3 pid=116)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-04T18:13:16.0136086Z [0;36m(ApiServer_3 pid=116)[0;0m INFO 02-04 18:13:16 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-04T18:13:16.0149841Z [0;36m(ApiServer_3 pid=116)[0;0m INFO 02-04 18:13:16 [model.py:1561] Using max model len 8192
2026-02-04T18:13:16.1297694Z [0;36m(ApiServer_3 pid=116)[0;0m WARNING 02-04 18:13:16 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-04T18:13:16.1326971Z [0;36m(ApiServer_3 pid=116)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:13:16.1335702Z [0;36m(ApiServer_3 pid=116)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:13:16.1343795Z [0;36m(ApiServer_3 pid=116)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-04T18:13:16.1393702Z [0;36m(ApiServer_3 pid=116)[0;0m INFO 02-04 18:13:16 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-04T18:13:16.1407906Z [0;36m(ApiServer_3 pid=116)[0;0m INFO 02-04 18:13:16 [model.py:1561] Using max model len 163840
2026-02-04T18:13:16.1416605Z [0;36m(ApiServer_3 pid=116)[0;0m WARNING 02-04 18:13:16 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-04T18:13:16.1447557Z [0;36m(ApiServer_3 pid=116)[0;0m INFO 02-04 18:13:16 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-04T18:13:16.1712630Z [0;36m(ApiServer_0 pid=113)[0;0m 2026-02-04 18:13:16,169 - 113 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:13:16.1918881Z [0;36m(ApiServer_0 pid=113)[0;0m INFO 02-04 18:13:16 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:13:16.2086258Z [0;36m(ApiServer_0 pid=113)[0;0m 2026-02-04 18:13:16,206 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-04T18:13:16.2131513Z [0;36m(ApiServer_0 pid=113)[0;0m INFO 02-04 18:13:16 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-04T18:13:16.2583807Z [0;36m(ApiServer_3 pid=116)[0;0m INFO 02-04 18:13:16 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-04T18:13:16.2592271Z [0;36m(ApiServer_3 pid=116)[0;0m INFO 02-04 18:13:16 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-04T18:13:16.2607499Z [0;36m(ApiServer_3 pid=116)[0;0m WARNING 02-04 18:13:16 [platform.py:707] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-04T18:13:16.2621846Z [0;36m(ApiServer_3 pid=116)[0;0m WARNING 02-04 18:13:16 [platform.py:748] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-04T18:13:16.2632686Z [0;36m(ApiServer_3 pid=116)[0;0m INFO 02-04 18:13:16 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:13:16.2642761Z [0;36m(ApiServer_3 pid=116)[0;0m WARNING 02-04 18:13:16 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-04T18:13:16.2653088Z [0;36m(ApiServer_3 pid=116)[0;0m INFO 02-04 18:13:16 [platform.py:310] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-04T18:13:16.2662659Z [0;36m(ApiServer_3 pid=116)[0;0m WARNING 02-04 18:13:16 [platform.py:327] [91m
2026-02-04T18:13:16.2671712Z [0;36m(ApiServer_3 pid=116)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             **********************************************************************************
2026-02-04T18:13:16.2681656Z [0;36m(ApiServer_3 pid=116)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             * WARNING: You have enabled the *full graph* feature.
2026-02-04T18:13:16.2691449Z [0;36m(ApiServer_3 pid=116)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             * This is an early experimental stage and may involve various unknown issues.
2026-02-04T18:13:16.2701235Z [0;36m(ApiServer_3 pid=116)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-04T18:13:16.2710223Z [0;36m(ApiServer_3 pid=116)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-04T18:13:16.2720420Z [0;36m(ApiServer_3 pid=116)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-04T18:13:16.2730485Z [0;36m(ApiServer_3 pid=116)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             * batch size for graph capture.
2026-02-04T18:13:16.2739192Z [0;36m(ApiServer_3 pid=116)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             * For more details, please refer to:
2026-02-04T18:13:16.2749077Z [0;36m(ApiServer_3 pid=116)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-04T18:13:16.2758141Z [0;36m(ApiServer_3 pid=116)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             **********************************************************************************[0m
2026-02-04T18:13:16.2767629Z [0;36m(ApiServer_3 pid=116)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             
2026-02-04T18:13:16.2777115Z [0;36m(ApiServer_3 pid=116)[0;0m INFO 02-04 18:13:16 [platform.py:420] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-04T18:13:16.3100035Z [0;36m(ApiServer_0 pid=113)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:13:16.3151837Z [0;36m(ApiServer_0 pid=113)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:13:16.3159371Z [0;36m(ApiServer_0 pid=113)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:13:16.3160278Z [0;36m(ApiServer_0 pid=113)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-04T18:13:16.3201197Z [0;36m(ApiServer_0 pid=113)[0;0m INFO 02-04 18:13:16 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-04T18:13:16.3223180Z [0;36m(ApiServer_0 pid=113)[0;0m INFO 02-04 18:13:16 [model.py:1561] Using max model len 8192
2026-02-04T18:13:16.4300101Z [0;36m(ApiServer_0 pid=113)[0;0m WARNING 02-04 18:13:16 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-04T18:13:16.4322448Z [0;36m(ApiServer_0 pid=113)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:13:16.4370489Z [0;36m(ApiServer_0 pid=113)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-04T18:13:16.4371433Z [0;36m(ApiServer_0 pid=113)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-04T18:13:16.4390889Z [0;36m(ApiServer_0 pid=113)[0;0m INFO 02-04 18:13:16 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-04T18:13:16.4404110Z [0;36m(ApiServer_0 pid=113)[0;0m INFO 02-04 18:13:16 [model.py:1561] Using max model len 163840
2026-02-04T18:13:16.4413818Z [0;36m(ApiServer_0 pid=113)[0;0m WARNING 02-04 18:13:16 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-04T18:13:16.4425625Z [0;36m(ApiServer_0 pid=113)[0;0m INFO 02-04 18:13:16 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-04T18:13:16.5571606Z [0;36m(ApiServer_0 pid=113)[0;0m INFO 02-04 18:13:16 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-04T18:13:16.5580254Z [0;36m(ApiServer_0 pid=113)[0;0m INFO 02-04 18:13:16 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-04T18:13:16.5593094Z [0;36m(ApiServer_0 pid=113)[0;0m WARNING 02-04 18:13:16 [platform.py:707] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-04T18:13:16.5603188Z [0;36m(ApiServer_0 pid=113)[0;0m WARNING 02-04 18:13:16 [platform.py:748] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-04T18:13:16.5613370Z [0;36m(ApiServer_0 pid=113)[0;0m INFO 02-04 18:13:16 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:13:16.5622131Z [0;36m(ApiServer_0 pid=113)[0;0m WARNING 02-04 18:13:16 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-04T18:13:16.5631150Z [0;36m(ApiServer_0 pid=113)[0;0m INFO 02-04 18:13:16 [platform.py:310] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-04T18:13:16.5640310Z [0;36m(ApiServer_0 pid=113)[0;0m WARNING 02-04 18:13:16 [platform.py:327] [91m
2026-02-04T18:13:16.5650286Z [0;36m(ApiServer_0 pid=113)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             **********************************************************************************
2026-02-04T18:13:16.5660460Z [0;36m(ApiServer_0 pid=113)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             * WARNING: You have enabled the *full graph* feature.
2026-02-04T18:13:16.5671622Z [0;36m(ApiServer_0 pid=113)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             * This is an early experimental stage and may involve various unknown issues.
2026-02-04T18:13:16.5683390Z [0;36m(ApiServer_0 pid=113)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-04T18:13:16.5693757Z [0;36m(ApiServer_0 pid=113)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-04T18:13:16.5703308Z [0;36m(ApiServer_0 pid=113)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-04T18:13:16.5713268Z [0;36m(ApiServer_0 pid=113)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             * batch size for graph capture.
2026-02-04T18:13:16.5722815Z [0;36m(ApiServer_0 pid=113)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             * For more details, please refer to:
2026-02-04T18:13:16.5732717Z [0;36m(ApiServer_0 pid=113)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-04T18:13:16.5742336Z [0;36m(ApiServer_0 pid=113)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             **********************************************************************************[0m
2026-02-04T18:13:16.5751432Z [0;36m(ApiServer_0 pid=113)[0;0m WARNING 02-04 18:13:16 [platform.py:327]             
2026-02-04T18:13:16.5761747Z [0;36m(ApiServer_0 pid=113)[0;0m INFO 02-04 18:13:16 [platform.py:420] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-04T18:13:20.2124933Z INFO 02-04 18:13:20 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:13:20.2134220Z INFO 02-04 18:13:20 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:13:20.2145574Z INFO 02-04 18:13:20 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:13:20.2191597Z INFO 02-04 18:13:20 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:13:20.4003420Z INFO 02-04 18:13:20 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:13:20.4030412Z INFO 02-04 18:13:20 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:13:20.4743175Z INFO 02-04 18:13:20 [parallel_state.py:1212] world_size=32 rank=8 local_rank=0 distributed_init_method=tcp://10.0.0.76:46067 backend=hccl
2026-02-04T18:13:20.4751176Z INFO 02-04 18:13:20 [parallel_state.py:1212] world_size=32 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.76:46067 backend=hccl
2026-02-04T18:13:20.4782790Z INFO 02-04 18:13:20 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:13:20.4792332Z INFO 02-04 18:13:20 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:13:20.4802940Z INFO 02-04 18:13:20 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:13:20.4862450Z INFO 02-04 18:13:20 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:13:25.2976010Z 2026-02-04 18:13:25,295 - 183 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:13:25.3008300Z INFO 02-04 18:13:25 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:13:25.4916059Z 2026-02-04 18:13:25,489 - 184 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:13:25.4945082Z INFO 02-04 18:13:25 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:13:28.4721275Z INFO 02-04 18:13:28 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:13:28.5295153Z INFO 02-04 18:13:28 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:13:28.9089044Z INFO 02-04 18:13:28 [parallel_state.py:1212] world_size=32 rank=9 local_rank=1 distributed_init_method=tcp://10.0.0.76:46067 backend=hccl
2026-02-04T18:13:28.9494410Z INFO 02-04 18:13:28 [parallel_state.py:1212] world_size=32 rank=1 local_rank=1 distributed_init_method=tcp://10.0.0.76:46067 backend=hccl
2026-02-04T18:13:29.9396550Z INFO 02-04 18:13:29 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:13:29.9405358Z INFO 02-04 18:13:29 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:13:29.9415382Z INFO 02-04 18:13:29 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:13:29.9466499Z INFO 02-04 18:13:29 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:13:29.9570034Z INFO 02-04 18:13:29 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:13:29.9577705Z INFO 02-04 18:13:29 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:13:29.9586969Z INFO 02-04 18:13:29 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:13:29.9648307Z INFO 02-04 18:13:29 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:13:34.9634697Z 2026-02-04 18:13:34,961 - 301 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:13:34.9666984Z INFO 02-04 18:13:34 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:13:35.2814779Z 2026-02-04 18:13:35,279 - 298 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:13:35.2848453Z INFO 02-04 18:13:35 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:13:38.0007907Z INFO 02-04 18:13:37 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:13:38.4300769Z INFO 02-04 18:13:38 [parallel_state.py:1212] world_size=32 rank=2 local_rank=2 distributed_init_method=tcp://10.0.0.76:46067 backend=hccl
2026-02-04T18:13:38.4519151Z INFO 02-04 18:13:38 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:13:38.9334411Z INFO 02-04 18:13:38 [parallel_state.py:1212] world_size=32 rank=10 local_rank=2 distributed_init_method=tcp://10.0.0.76:46067 backend=hccl
2026-02-04T18:13:39.7358566Z INFO 02-04 18:13:39 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:13:39.7365022Z INFO 02-04 18:13:39 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:13:39.7374974Z INFO 02-04 18:13:39 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:13:39.7430530Z INFO 02-04 18:13:39 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:13:40.1428739Z INFO 02-04 18:13:40 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:13:40.1435919Z INFO 02-04 18:13:40 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:13:40.1445320Z INFO 02-04 18:13:40 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:13:40.1512301Z INFO 02-04 18:13:40 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:13:44.8574736Z 2026-02-04 18:13:44,855 - 402 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:13:44.8606280Z INFO 02-04 18:13:44 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:13:45.1953176Z 2026-02-04 18:13:45,193 - 405 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:13:45.1986097Z INFO 02-04 18:13:45 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:13:47.9251909Z INFO 02-04 18:13:47 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:13:48.2728005Z INFO 02-04 18:13:48 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:13:48.3738545Z INFO 02-04 18:13:48 [parallel_state.py:1212] world_size=32 rank=3 local_rank=3 distributed_init_method=tcp://10.0.0.76:46067 backend=hccl
2026-02-04T18:13:48.7008325Z INFO 02-04 18:13:48 [parallel_state.py:1212] world_size=32 rank=11 local_rank=3 distributed_init_method=tcp://10.0.0.76:46067 backend=hccl
2026-02-04T18:13:49.3400544Z INFO 02-04 18:13:49 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:13:49.3409069Z INFO 02-04 18:13:49 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:13:49.3418393Z INFO 02-04 18:13:49 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:13:49.3480045Z INFO 02-04 18:13:49 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:13:49.8161857Z INFO 02-04 18:13:49 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:13:49.8169419Z INFO 02-04 18:13:49 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:13:49.8179062Z INFO 02-04 18:13:49 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:13:49.8237482Z INFO 02-04 18:13:49 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:13:54.5173870Z 2026-02-04 18:13:54,515 - 506 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:13:54.5206037Z INFO 02-04 18:13:54 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:13:54.9836737Z 2026-02-04 18:13:54,981 - 509 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:13:54.9870455Z INFO 02-04 18:13:54 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:13:57.5042399Z INFO 02-04 18:13:57 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:13:57.9295885Z INFO 02-04 18:13:57 [parallel_state.py:1212] world_size=32 rank=4 local_rank=4 distributed_init_method=tcp://10.0.0.76:46067 backend=hccl
2026-02-04T18:13:58.0497300Z INFO 02-04 18:13:58 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:13:58.4916260Z INFO 02-04 18:13:58 [parallel_state.py:1212] world_size=32 rank=12 local_rank=4 distributed_init_method=tcp://10.0.0.76:46067 backend=hccl
2026-02-04T18:13:59.0674936Z INFO 02-04 18:13:59 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:13:59.0682509Z INFO 02-04 18:13:59 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:13:59.0691688Z INFO 02-04 18:13:59 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:13:59.0752422Z INFO 02-04 18:13:59 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:13:59.6657872Z INFO 02-04 18:13:59 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:13:59.6666935Z INFO 02-04 18:13:59 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:13:59.6680084Z INFO 02-04 18:13:59 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:13:59.6733543Z INFO 02-04 18:13:59 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:14:04.1519836Z 2026-02-04 18:14:04,149 - 610 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:14:04.1557732Z INFO 02-04 18:14:04 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:14:04.8875571Z 2026-02-04 18:14:04,885 - 613 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:14:04.8908291Z INFO 02-04 18:14:04 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:14:07.2244727Z INFO 02-04 18:14:07 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:14:07.6568533Z INFO 02-04 18:14:07 [parallel_state.py:1212] world_size=32 rank=5 local_rank=5 distributed_init_method=tcp://10.0.0.76:46067 backend=hccl
2026-02-04T18:14:07.9554175Z INFO 02-04 18:14:07 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:14:08.3972891Z INFO 02-04 18:14:08 [parallel_state.py:1212] world_size=32 rank=13 local_rank=5 distributed_init_method=tcp://10.0.0.76:46067 backend=hccl
2026-02-04T18:14:08.6844743Z INFO 02-04 18:14:08 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:14:08.6851218Z INFO 02-04 18:14:08 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:14:08.6860316Z INFO 02-04 18:14:08 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:14:08.6925343Z INFO 02-04 18:14:08 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:14:09.3848236Z INFO 02-04 18:14:09 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:14:09.3855372Z INFO 02-04 18:14:09 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:14:09.3865751Z INFO 02-04 18:14:09 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:14:09.3918399Z INFO 02-04 18:14:09 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:14:13.7235955Z 2026-02-04 18:14:13,721 - 714 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:14:13.7268581Z INFO 02-04 18:14:13 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:14:14.3839665Z 2026-02-04 18:14:14,381 - 717 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:14:14.3873625Z INFO 02-04 18:14:14 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:14:16.7910077Z INFO 02-04 18:14:16 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:14:17.2278798Z INFO 02-04 18:14:17 [parallel_state.py:1212] world_size=32 rank=6 local_rank=6 distributed_init_method=tcp://10.0.0.76:46067 backend=hccl
2026-02-04T18:14:17.5814991Z INFO 02-04 18:14:17 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:14:18.0222943Z INFO 02-04 18:14:18 [parallel_state.py:1212] world_size=32 rank=14 local_rank=6 distributed_init_method=tcp://10.0.0.76:46067 backend=hccl
2026-02-04T18:14:18.2824012Z INFO 02-04 18:14:18 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:14:18.2830744Z INFO 02-04 18:14:18 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:14:18.2841268Z INFO 02-04 18:14:18 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:14:18.2896678Z INFO 02-04 18:14:18 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:14:18.9679284Z INFO 02-04 18:14:18 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-04T18:14:18.9687605Z INFO 02-04 18:14:18 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-04T18:14:18.9696794Z INFO 02-04 18:14:18 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-04T18:14:18.9760754Z INFO 02-04 18:14:18 [__init__.py:217] Platform plugin ascend is activated
2026-02-04T18:14:23.3194228Z 2026-02-04 18:14:23,317 - 818 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:14:23.3225395Z INFO 02-04 18:14:23 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:14:23.9784476Z 2026-02-04 18:14:23,976 - 821 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-04T18:14:23.9815379Z INFO 02-04 18:14:23 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-04T18:14:26.3656364Z INFO 02-04 18:14:26 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:14:26.8016882Z INFO 02-04 18:14:26 [parallel_state.py:1212] world_size=32 rank=7 local_rank=7 distributed_init_method=tcp://10.0.0.76:46067 backend=hccl
2026-02-04T18:14:27.0149989Z INFO 02-04 18:14:27 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-04T18:14:27.4521301Z INFO 02-04 18:14:27 [parallel_state.py:1212] world_size=32 rank=15 local_rank=7 distributed_init_method=tcp://10.0.0.76:46067 backend=hccl
2026-02-04T18:14:27.4977136Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.4999783Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.5010213Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.5020559Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.5031770Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.5566629Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.5587471Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.5597988Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.5608311Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.5645031Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.5920901Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.5936738Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.5960322Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.5987912Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.6002529Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.6012509Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.6883093Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-04T18:14:27.6905840Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-04T18:14:27.6919899Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-04T18:14:27.6930045Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-04T18:14:27.6940203Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-04T18:14:27.6950377Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-04T18:14:27.6960675Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-04T18:14:27.6971778Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-04T18:14:27.6981771Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-04T18:14:27.6991900Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-04T18:14:27.7002584Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-04T18:14:27.7014510Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-04T18:14:27.7022982Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-04T18:14:27.7033133Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-04T18:14:27.7044066Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-04T18:14:27.7053735Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-04T18:14:27.7065385Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7077444Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7088073Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7098019Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7107428Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7123226Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7128686Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7137503Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7146618Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7156106Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7167058Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7176984Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7191493Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7197574Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7208818Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7218434Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7227738Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7237835Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7283494Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7285492Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7286026Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7286685Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7290283Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7297521Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7352683Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7381219Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7390910Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7401450Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7410524Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7483810Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7487354Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7497519Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7507474Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7517014Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7526993Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7536646Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7545731Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7556246Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7566502Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7576112Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7586531Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7595476Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7606444Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7616090Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7625272Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7635357Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7645055Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7654231Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.7664506Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.7674030Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.7684135Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.7694005Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-04T18:14:27.7704464Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.7715001Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.7725190Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.7734583Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.7744291Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.7753717Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.7763633Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.7773272Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.7782882Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.7792433Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.7802303Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.7811154Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.7828552Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.7850236Z INFO 02-04 18:14:27 [parallel_state.py:1423] rank 0 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
2026-02-04T18:14:27.8602722Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.8624137Z INFO 02-04 18:14:27 [parallel_state.py:1423] rank 1 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
2026-02-04T18:14:27.8646713Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.8656829Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.8673005Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.8683108Z INFO 02-04 18:14:27 [parallel_state.py:1423] rank 4 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 4, EP rank 4
2026-02-04T18:14:27.8692878Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.8701874Z INFO 02-04 18:14:27 [parallel_state.py:1423] rank 6 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 6, EP rank 6
2026-02-04T18:14:27.8711244Z INFO 02-04 18:14:27 [parallel_state.py:1423] rank 5 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 5, EP rank 5
2026-02-04T18:14:27.8720327Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.8730208Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.8740006Z INFO 02-04 18:14:27 [parallel_state.py:1423] rank 8 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 0, EP rank 8
2026-02-04T18:14:27.8748827Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.8758687Z INFO 02-04 18:14:27 [parallel_state.py:1423] rank 7 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 7, EP rank 7
2026-02-04T18:14:27.8774822Z INFO 02-04 18:14:27 [parallel_state.py:1423] rank 10 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 2, EP rank 10
2026-02-04T18:14:27.8777254Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.8787350Z INFO 02-04 18:14:27 [parallel_state.py:1423] rank 11 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 3, EP rank 11
2026-02-04T18:14:27.8797224Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.8806115Z INFO 02-04 18:14:27 [parallel_state.py:1423] rank 15 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 7, EP rank 15
2026-02-04T18:14:27.8816317Z INFO 02-04 18:14:27 [parallel_state.py:1423] rank 14 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 6, EP rank 14
2026-02-04T18:14:27.8940988Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.8949999Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.8959747Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.8969746Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.8978328Z INFO 02-04 18:14:27 [parallel_state.py:1423] rank 2 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
2026-02-04T18:14:27.8987590Z INFO 02-04 18:14:27 [parallel_state.py:1423] rank 3 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
2026-02-04T18:14:27.8997284Z INFO 02-04 18:14:27 [parallel_state.py:1423] rank 13 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 5, EP rank 13
2026-02-04T18:14:27.9055641Z INFO 02-04 18:14:27 [parallel_state.py:1423] rank 9 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 1, EP rank 9
2026-02-04T18:14:27.9065459Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.9078974Z INFO 02-04 18:14:27 [parallel_state.py:1423] rank 12 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 4, EP rank 12
2026-02-04T18:14:27.9306085Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.9315985Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.9327120Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.9336589Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.9345276Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.9354812Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.9364778Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.9374868Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.9387921Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.9394528Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.9404481Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.9415282Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.9423922Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.9433966Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.9443093Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.9452377Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-04T18:14:27.9461893Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.9471822Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.9482333Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.9491614Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.9500435Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.9510541Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.9520253Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.9529594Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.9539262Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.9548050Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.9557453Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.9567501Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.9576107Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.9585743Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.9595308Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:27.9604960Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-04T18:14:28.0483466Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.0515560Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.0957488Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.0979480Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1002968Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m INFO 02-04 18:14:28 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-04T18:14:28.1095455Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1104904Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1123578Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1131563Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1176706Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1212953Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1237953Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1261868Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1282227Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m INFO 02-04 18:14:28 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-04T18:14:28.1306633Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1315373Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1324601Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1334429Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1344286Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1357913Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1364462Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m INFO 02-04 18:14:28 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-04T18:14:28.1373177Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1383076Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m INFO 02-04 18:14:28 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-04T18:14:28.1392838Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1402098Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1412880Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1422703Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m INFO 02-04 18:14:28 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-04T18:14:28.1432967Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1484833Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m INFO 02-04 18:14:28 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-04T18:14:28.1496995Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1505665Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1515788Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m INFO 02-04 18:14:28 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-04T18:14:28.1538118Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m INFO 02-04 18:14:28 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-04T18:14:28.1547738Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1562967Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1597288Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m INFO 02-04 18:14:28 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-04T18:14:28.1608362Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1617552Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m INFO 02-04 18:14:28 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-04T18:14:28.1628073Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m INFO 02-04 18:14:28 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-04T18:14:28.1640511Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1650255Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.1662664Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m INFO 02-04 18:14:28 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-04T18:14:28.1688205Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m INFO 02-04 18:14:28 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-04T18:14:28.1731071Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m INFO 02-04 18:14:28 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-04T18:14:28.1955996Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.2003958Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m INFO 02-04 18:14:28 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-04T18:14:28.2031212Z WARNING 02-04 18:14:28 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-04T18:14:28.2073785Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m INFO 02-04 18:14:28 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-04T18:14:28.4085131Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m [2026-02-04 18:14:28] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-04T18:14:28.4581344Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m [2026-02-04 18:14:28] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-04T18:14:28.5190326Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m [2026-02-04 18:14:28] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-04T18:14:28.5243724Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m [2026-02-04 18:14:28] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-04T18:14:28.5401713Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m [2026-02-04 18:14:28] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-04T18:14:28.5442324Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m [2026-02-04 18:14:28] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-04T18:14:28.5491909Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m [2026-02-04 18:14:28] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-04T18:14:28.5575507Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m [2026-02-04 18:14:28] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-04T18:14:28.5877217Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m INFO 02-04 18:14:28 [layer.py:475] [EP Rank 4/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-04T18:14:28.5979081Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m INFO 02-04 18:14:28 [layer.py:475] [EP Rank 8/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-04T18:14:28.6331113Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m [2026-02-04 18:14:28] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-04T18:14:28.6352574Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m INFO 02-04 18:14:28 [layer.py:475] [EP Rank 13/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-04T18:14:28.6378132Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m INFO 02-04 18:14:28 [layer.py:475] [EP Rank 1/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-04T18:14:28.6467663Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m INFO 02-04 18:14:28 [layer.py:475] [EP Rank 3/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-04T18:14:28.6513212Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m INFO 02-04 18:14:28 [layer.py:475] [EP Rank 2/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-04T18:14:28.6544160Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m INFO 02-04 18:14:28 [layer.py:475] [EP Rank 6/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-04T18:14:28.6689942Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m INFO 02-04 18:14:28 [layer.py:475] [EP Rank 9/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-04T18:14:28.6843682Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m [2026-02-04 18:14:28] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-04T18:14:28.7415378Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m INFO 02-04 18:14:28 [layer.py:475] [EP Rank 10/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-04T18:14:28.8316620Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m [2026-02-04 18:14:28] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-04T18:14:28.8431515Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m INFO 02-04 18:14:28 [layer.py:475] [EP Rank 0/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-04T18:14:28.8560201Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m [2026-02-04 18:14:28] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-04T18:14:28.8657118Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m [2026-02-04 18:14:28] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-04T18:14:28.8807147Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m [2026-02-04 18:14:28] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-04T18:14:28.9472733Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m [2026-02-04 18:14:28] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-04T18:14:28.9488534Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m INFO 02-04 18:14:28 [layer.py:475] [EP Rank 7/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-04T18:14:28.9742859Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m INFO 02-04 18:14:28 [layer.py:475] [EP Rank 11/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-04T18:14:28.9943682Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m INFO 02-04 18:14:28 [layer.py:475] [EP Rank 15/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-04T18:14:29.0020415Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m INFO 02-04 18:14:28 [layer.py:475] [EP Rank 14/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-04T18:14:29.0694925Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m INFO 02-04 18:14:29 [layer.py:475] [EP Rank 5/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-04T18:14:29.3183679Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m [2026-02-04 18:14:29] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-04T18:14:29.4285032Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m INFO 02-04 18:14:29 [layer.py:475] [EP Rank 12/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-04T18:14:30.4135047Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-04T18:14:30.4143133Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m   return func(*args, **kwargs)
2026-02-04T18:14:30.4811168Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m INFO 02-04 18:14:30 [fused_moe.py:211] [EP Rank 0/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-04T18:14:30.5205888Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-04T18:14:30.5214151Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m   return func(*args, **kwargs)
2026-02-04T18:14:30.5264511Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-04T18:14:30.5277323Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m   return func(*args, **kwargs)
2026-02-04T18:14:30.5410607Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-04T18:14:30.5421597Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m   return func(*args, **kwargs)
2026-02-04T18:14:30.5433984Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-04T18:14:30.5442905Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m   return func(*args, **kwargs)
2026-02-04T18:14:30.5542366Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-04T18:14:30.5550868Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m   return func(*args, **kwargs)
2026-02-04T18:14:30.5814073Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m INFO 02-04 18:14:30 [fused_moe.py:211] [EP Rank 7/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-04T18:14:30.5991265Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-04T18:14:30.6000076Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m   return func(*args, **kwargs)
2026-02-04T18:14:30.6032209Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m INFO 02-04 18:14:30 [fused_moe.py:211] [EP Rank 14/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-04T18:14:30.6051443Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m INFO 02-04 18:14:30 [fused_moe.py:211] [EP Rank 12/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-04T18:14:30.6179981Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-04T18:14:30.6190839Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m   return func(*args, **kwargs)
2026-02-04T18:14:30.6203179Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-04T18:14:30.6211970Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m   return func(*args, **kwargs)
2026-02-04T18:14:30.6345970Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-04T18:14:30.6354473Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m   return func(*args, **kwargs)
2026-02-04T18:14:30.6365036Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m INFO 02-04 18:14:30 [fused_moe.py:211] [EP Rank 11/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-04T18:14:30.6438449Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-04T18:14:30.6452300Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m   return func(*args, **kwargs)
2026-02-04T18:14:30.6548909Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-04T18:14:30.6562630Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m   return func(*args, **kwargs)
2026-02-04T18:14:30.6578104Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m INFO 02-04 18:14:30 [fused_moe.py:211] [EP Rank 8/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-04T18:14:30.6588892Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-04T18:14:30.6599066Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m   return func(*args, **kwargs)
2026-02-04T18:14:30.6610137Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m INFO 02-04 18:14:30 [fused_moe.py:211] [EP Rank 15/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-04T18:14:30.6620602Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-04T18:14:30.6629023Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m   return func(*args, **kwargs)
2026-02-04T18:14:30.6747398Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-04T18:14:30.6763244Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m   return func(*args, **kwargs)
2026-02-04T18:14:30.6777773Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-04T18:14:30.6789190Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m   return func(*args, **kwargs)
2026-02-04T18:14:30.6799572Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m INFO 02-04 18:14:30 [fused_moe.py:211] [EP Rank 3/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-04T18:14:30.6810393Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m INFO 02-04 18:14:30 [fused_moe.py:211] [EP Rank 5/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-04T18:14:30.6934638Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m INFO 02-04 18:14:30 [fused_moe.py:211] [EP Rank 10/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-04T18:14:30.7004963Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m INFO 02-04 18:14:30 [fused_moe.py:211] [EP Rank 6/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-04T18:14:30.7188606Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m INFO 02-04 18:14:30 [fused_moe.py:211] [EP Rank 1/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-04T18:14:30.7216384Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m INFO 02-04 18:14:30 [fused_moe.py:211] [EP Rank 13/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-04T18:14:30.7230364Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m INFO 02-04 18:14:30 [fused_moe.py:211] [EP Rank 9/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-04T18:14:30.7342127Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m INFO 02-04 18:14:30 [fused_moe.py:211] [EP Rank 2/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-04T18:14:30.7421947Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m INFO 02-04 18:14:30 [fused_moe.py:211] [EP Rank 4/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-04T18:16:31.3001664Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-04T18:16:31.3131510Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-04T18:16:31.3134416Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-04T18:16:31.3135514Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-04T18:16:31.3136199Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.3138039Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-04T18:16:31.3149188Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-04T18:16:31.3159383Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-04T18:16:31.3169225Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-04T18:16:31.3179861Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2250, in load_model
2026-02-04T18:16:31.3189521Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-04T18:16:31.3198969Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.3209181Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-04T18:16:31.3218033Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return loader.load_model(
2026-02-04T18:16:31.3228316Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.3238280Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-04T18:16:31.3247668Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     model = initialize_model(
2026-02-04T18:16:31.3266338Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.3267795Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-04T18:16:31.3278461Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-04T18:16:31.3287795Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.3297102Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-04T18:16:31.3306617Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-04T18:16:31.3315635Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-04T18:16:31.3326229Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-04T18:16:31.3336095Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-04T18:16:31.3346115Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-04T18:16:31.3356174Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-04T18:16:31.3367206Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-04T18:16:31.3377338Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-04T18:16:31.3386345Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     + [
2026-02-04T18:16:31.3396884Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]       ^
2026-02-04T18:16:31.3408048Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-04T18:16:31.3417671Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-04T18:16:31.3426903Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.3437262Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-04T18:16:31.3448856Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-04T18:16:31.3457765Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.3468151Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-04T18:16:31.3479899Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-04T18:16:31.3490245Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-04T18:16:31.3497167Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-04T18:16:31.3506626Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-04T18:16:31.3518258Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-04T18:16:31.3527377Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-04T18:16:31.3537476Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-04T18:16:31.3547708Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-04T18:16:31.3559176Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-04T18:16:31.3568168Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-04T18:16:31.3577641Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-04T18:16:31.3587681Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.3598208Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-04T18:16:31.3608630Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-04T18:16:31.3618083Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.3629322Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-04T18:16:31.3638754Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-04T18:16:31.3648426Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.3658979Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-04T18:16:31.3667426Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-04T18:16:31.3676479Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.3687721Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-04T18:16:31.3696944Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [ERROR] 2026-02-04-18:16:30 (PID:613, Device:5, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-04T18:16:31.3708310Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [PID: 613] 2026-02-04-18:16:30.484.953 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-04T18:16:31.3720500Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-04T18:16:31.3728143Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-04T18:16:31.3738446Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-04T18:16:31.3746705Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] 
2026-02-04T18:16:31.3757287Z [0;36m(Worker_DP1_TP5_EP13 pid=613)[0;0m INFO 02-04 18:16:31 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-04T18:16:31.3767569Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-04T18:16:31.3778868Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-04T18:16:31.3800804Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-04T18:16:31.3810975Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-04T18:16:31.3820962Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.3830800Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-04T18:16:31.3840985Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-04T18:16:31.3851144Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-04T18:16:31.3860920Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-04T18:16:31.3871411Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2250, in load_model
2026-02-04T18:16:31.3881218Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-04T18:16:31.3891106Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.3900809Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-04T18:16:31.3910857Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return loader.load_model(
2026-02-04T18:16:31.3920972Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.3930565Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-04T18:16:31.3940209Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     model = initialize_model(
2026-02-04T18:16:31.3949881Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.3960635Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-04T18:16:31.3970071Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-04T18:16:31.3979944Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.3989551Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-04T18:16:31.4002632Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-04T18:16:31.4011298Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-04T18:16:31.4021239Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-04T18:16:31.4031370Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-04T18:16:31.4041747Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-04T18:16:31.4051909Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-04T18:16:31.4061555Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-04T18:16:31.4072436Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-04T18:16:31.4082525Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     + [
2026-02-04T18:16:31.4092116Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]       ^
2026-02-04T18:16:31.4101459Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-04T18:16:31.4111074Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-04T18:16:31.4121013Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.4130786Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-04T18:16:31.4140145Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-04T18:16:31.4149710Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.4159899Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-04T18:16:31.4169539Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-04T18:16:31.4179147Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-04T18:16:31.4189079Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-04T18:16:31.4198674Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-04T18:16:31.4207967Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-04T18:16:31.4217886Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-04T18:16:31.4227376Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-04T18:16:31.4238701Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-04T18:16:31.4248931Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-04T18:16:31.4259599Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-04T18:16:31.4269673Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-04T18:16:31.4282372Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.4290598Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-04T18:16:31.4301116Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-04T18:16:31.4312613Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.4323288Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-04T18:16:31.4333041Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-04T18:16:31.4342945Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.4354021Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-04T18:16:31.4364541Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-04T18:16:31.4375362Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.4387100Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-04T18:16:31.4397214Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [ERROR] 2026-02-04-18:16:30 (PID:298, Device:2, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-04T18:16:31.4411535Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [PID: 298] 2026-02-04-18:16:30.484.154 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-04T18:16:31.4423423Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-04T18:16:31.4432605Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-04T18:16:31.4443675Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-04T18:16:31.4453016Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] 
2026-02-04T18:16:31.4463377Z [0;36m(Worker_DP1_TP2_EP10 pid=298)[0;0m INFO 02-04 18:16:31 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-04T18:16:31.4473249Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m INFO 02-04 18:16:31 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-04T18:16:31.4484480Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-04T18:16:31.4494638Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-04T18:16:31.4505015Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-04T18:16:31.4519453Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-04T18:16:31.4530683Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.4541554Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-04T18:16:31.4553486Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-04T18:16:31.4567112Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-04T18:16:31.4579218Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-04T18:16:31.4592692Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2250, in load_model
2026-02-04T18:16:31.4604267Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-04T18:16:31.4615426Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.4626853Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-04T18:16:31.4637053Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return loader.load_model(
2026-02-04T18:16:31.4648083Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.4659439Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-04T18:16:31.4669420Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     model = initialize_model(
2026-02-04T18:16:31.4679513Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.4690498Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-04T18:16:31.4701052Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-04T18:16:31.4711246Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.4722668Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-04T18:16:31.4733085Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-04T18:16:31.4743042Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-04T18:16:31.4753360Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-04T18:16:31.4772475Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-04T18:16:31.4783361Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-04T18:16:31.4793737Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-04T18:16:31.4804255Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-04T18:16:31.4815146Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-04T18:16:31.4824350Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     + [
2026-02-04T18:16:31.4834319Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]       ^
2026-02-04T18:16:31.4845126Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-04T18:16:31.4855330Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-04T18:16:31.4865014Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.4875849Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-04T18:16:31.4887057Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-04T18:16:31.4897364Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.4907983Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-04T18:16:31.4917212Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-04T18:16:31.4928485Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-04T18:16:31.4939127Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-04T18:16:31.4948846Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-04T18:16:31.4958488Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-04T18:16:31.4970294Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-04T18:16:31.4979734Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-04T18:16:31.4991887Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-04T18:16:31.5001604Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-04T18:16:31.5013088Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-04T18:16:31.5020540Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-04T18:16:31.5032570Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.5042772Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-04T18:16:31.5053289Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-04T18:16:31.5065663Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.5076513Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-04T18:16:31.5086460Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-04T18:16:31.5096346Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.5106794Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-04T18:16:31.5117249Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-04T18:16:31.5127793Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.5139582Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-04T18:16:31.5149480Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [ERROR] 2026-02-04-18:16:30 (PID:131, Device:0, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-04T18:16:31.5162170Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [PID: 131] 2026-02-04-18:16:30.485.090 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-04T18:16:31.5174642Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-04T18:16:31.5183766Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-04T18:16:31.5195425Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-04T18:16:31.5205741Z [0;36m(Worker_DP1_TP0_EP8 pid=131)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] 
2026-02-04T18:16:31.5215752Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-04T18:16:31.5225282Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-04T18:16:31.5235379Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-04T18:16:31.5245424Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-04T18:16:31.5255762Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.5266516Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-04T18:16:31.5275471Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-04T18:16:31.5286666Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-04T18:16:31.5296570Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-04T18:16:31.5306802Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2250, in load_model
2026-02-04T18:16:31.5316498Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-04T18:16:31.5327072Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.5337755Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-04T18:16:31.5347173Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return loader.load_model(
2026-02-04T18:16:31.5356932Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.5367762Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-04T18:16:31.5377698Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     model = initialize_model(
2026-02-04T18:16:31.5387798Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.5398734Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-04T18:16:31.5410200Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-04T18:16:31.5419918Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.5429356Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-04T18:16:31.5438668Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-04T18:16:31.5448733Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-04T18:16:31.5458902Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-04T18:16:31.5468561Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-04T18:16:31.5479486Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-04T18:16:31.5490243Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-04T18:16:31.5503495Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-04T18:16:31.5514639Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-04T18:16:31.5525081Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     + [
2026-02-04T18:16:31.5535561Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]       ^
2026-02-04T18:16:31.5546306Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-04T18:16:31.5556659Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-04T18:16:31.5567482Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.5577726Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-04T18:16:31.5587790Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-04T18:16:31.5598007Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.5608641Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-04T18:16:31.5618445Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-04T18:16:31.5629244Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-04T18:16:31.5639092Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-04T18:16:31.5649172Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-04T18:16:31.5659211Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-04T18:16:31.5669516Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-04T18:16:31.5679228Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-04T18:16:31.5689963Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-04T18:16:31.5699922Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-04T18:16:31.5710721Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-04T18:16:31.5721029Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-04T18:16:31.5731947Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.5741827Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-04T18:16:31.5751466Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-04T18:16:31.5762459Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.5775090Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-04T18:16:31.5784043Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-04T18:16:31.5794499Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.5805890Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-04T18:16:31.5816205Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-04T18:16:31.5826886Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.5838340Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-04T18:16:31.5849966Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [ERROR] 2026-02-04-18:16:30 (PID:402, Device:3, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-04T18:16:31.5861836Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [PID: 402] 2026-02-04-18:16:30.484.305 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-04T18:16:31.5874340Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-04T18:16:31.5883342Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-04T18:16:31.5895070Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-04T18:16:31.5904217Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] 
2026-02-04T18:16:31.5914364Z [0;36m(Worker_DP0_TP3_EP3 pid=402)[0;0m INFO 02-04 18:16:31 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-04T18:16:31.5924446Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m INFO 02-04 18:16:31 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-04T18:16:31.5934354Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m INFO 02-04 18:16:31 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-04T18:16:31.5943744Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-04T18:16:31.5953818Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-04T18:16:31.5964107Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-04T18:16:31.5974256Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-04T18:16:31.5984162Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.5994840Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-04T18:16:31.6005242Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-04T18:16:31.6015774Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-04T18:16:31.6025338Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-04T18:16:31.6035496Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2250, in load_model
2026-02-04T18:16:31.6045990Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-04T18:16:31.6056252Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.6066893Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-04T18:16:31.6076075Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return loader.load_model(
2026-02-04T18:16:31.6086373Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.6097280Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-04T18:16:31.6139553Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     model = initialize_model(
2026-02-04T18:16:31.6140173Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.6140989Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-04T18:16:31.6141887Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-04T18:16:31.6148429Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.6160652Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-04T18:16:31.6171117Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-04T18:16:31.6180853Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-04T18:16:31.6191590Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-04T18:16:31.6203302Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-04T18:16:31.6212687Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-04T18:16:31.6222373Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-04T18:16:31.6232488Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-04T18:16:31.6243179Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-04T18:16:31.6252959Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     + [
2026-02-04T18:16:31.6262605Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]       ^
2026-02-04T18:16:31.6272648Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-04T18:16:31.6282533Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-04T18:16:31.6292924Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.6303404Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-04T18:16:31.6313424Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-04T18:16:31.6323648Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.6334426Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-04T18:16:31.6344223Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-04T18:16:31.6354610Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-04T18:16:31.6365711Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-04T18:16:31.6376403Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-04T18:16:31.6385573Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-04T18:16:31.6396214Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-04T18:16:31.6407165Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-04T18:16:31.6418425Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-04T18:16:31.6427875Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-04T18:16:31.6439280Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-04T18:16:31.6450206Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-04T18:16:31.6460939Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.6472334Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-04T18:16:31.6482849Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-04T18:16:31.6494846Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.6508768Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-04T18:16:31.6519814Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-04T18:16:31.6530150Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.6540920Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-04T18:16:31.6551679Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-04T18:16:31.6562540Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.6575247Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-04T18:16:31.6584822Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [ERROR] 2026-02-04-18:16:30 (PID:301, Device:2, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-04T18:16:31.6597079Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [PID: 301] 2026-02-04-18:16:30.484.754 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-04T18:16:31.6610994Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-04T18:16:31.6619971Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-04T18:16:31.6632121Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-04T18:16:31.6641323Z [0;36m(Worker_DP0_TP2_EP2 pid=301)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] 
2026-02-04T18:16:31.6651958Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-04T18:16:31.6662228Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-04T18:16:31.6673220Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-04T18:16:31.6683433Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-04T18:16:31.6694196Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.6704908Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-04T18:16:31.6715326Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-04T18:16:31.6726093Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-04T18:16:31.6736260Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-04T18:16:31.6747387Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2250, in load_model
2026-02-04T18:16:31.6757613Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-04T18:16:31.6769138Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.6780059Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-04T18:16:31.6790057Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return loader.load_model(
2026-02-04T18:16:31.6800454Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.6811912Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-04T18:16:31.6821426Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     model = initialize_model(
2026-02-04T18:16:31.6831896Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.6843174Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-04T18:16:31.6853015Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-04T18:16:31.6864195Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.6873940Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-04T18:16:31.6883962Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-04T18:16:31.6893945Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-04T18:16:31.6904766Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-04T18:16:31.6914330Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-04T18:16:31.6925594Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-04T18:16:31.6935334Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-04T18:16:31.6945848Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-04T18:16:31.6956208Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-04T18:16:31.6966382Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     + [
2026-02-04T18:16:31.6976075Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]       ^
2026-02-04T18:16:31.6986292Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-04T18:16:31.6997117Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-04T18:16:31.7008561Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.7019212Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-04T18:16:31.7029099Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-04T18:16:31.7039005Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.7049760Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-04T18:16:31.7059788Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-04T18:16:31.7069291Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-04T18:16:31.7080016Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-04T18:16:31.7090214Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-04T18:16:31.7100594Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-04T18:16:31.7111601Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-04T18:16:31.7121852Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-04T18:16:31.7132946Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-04T18:16:31.7146473Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-04T18:16:31.7155795Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-04T18:16:31.7165362Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-04T18:16:31.7174108Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.7184867Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-04T18:16:31.7194791Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-04T18:16:31.7205645Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.7217423Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-04T18:16:31.7224981Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-04T18:16:31.7233759Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.7244258Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-04T18:16:31.7253632Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-04T18:16:31.7263535Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.7274103Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-04T18:16:31.7283653Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [ERROR] 2026-02-04-18:16:30 (PID:506, Device:4, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-04T18:16:31.7295111Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [PID: 506] 2026-02-04-18:16:30.484.247 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-04T18:16:31.7305995Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-04T18:16:31.7314637Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-04T18:16:31.7325212Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-04T18:16:31.7334272Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] 
2026-02-04T18:16:31.7344972Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m INFO 02-04 18:16:31 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-04T18:16:31.7355033Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m INFO 02-04 18:16:31 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-04T18:16:31.7364962Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m INFO 02-04 18:16:31 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-04T18:16:31.7375194Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-04T18:16:31.7385584Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-04T18:16:31.7396011Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-04T18:16:31.7407267Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-04T18:16:31.7417779Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.7429364Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-04T18:16:31.7439551Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-04T18:16:31.7450274Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-04T18:16:31.7459798Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-04T18:16:31.7470441Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2250, in load_model
2026-02-04T18:16:31.7480575Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-04T18:16:31.7491323Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.7501927Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-04T18:16:31.7512446Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return loader.load_model(
2026-02-04T18:16:31.7522315Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.7533697Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-04T18:16:31.7543450Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     model = initialize_model(
2026-02-04T18:16:31.7553877Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.7565057Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-04T18:16:31.7575803Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-04T18:16:31.7585924Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.7596860Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-04T18:16:31.7612116Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-04T18:16:31.7622994Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-04T18:16:31.7641151Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-04T18:16:31.7652984Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-04T18:16:31.7663746Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-04T18:16:31.7678249Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-04T18:16:31.7689364Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-04T18:16:31.7700418Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-04T18:16:31.7709339Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     + [
2026-02-04T18:16:31.7719514Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]       ^
2026-02-04T18:16:31.7729952Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-04T18:16:31.7739608Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-04T18:16:31.7749591Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.7761829Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-04T18:16:31.7772293Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-04T18:16:31.7782377Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.7793144Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-04T18:16:31.7803161Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-04T18:16:31.7813175Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-04T18:16:31.7823497Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-04T18:16:31.7833422Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-04T18:16:31.7843621Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-04T18:16:31.7853891Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-04T18:16:31.7863702Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-04T18:16:31.7873464Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-04T18:16:31.7883011Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-04T18:16:31.7893388Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-04T18:16:31.7902958Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-04T18:16:31.7913009Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.7923511Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-04T18:16:31.7932863Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-04T18:16:31.7943005Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.7954211Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-04T18:16:31.7964017Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-04T18:16:31.7974331Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.7984415Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-04T18:16:31.7994652Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-04T18:16:31.8004963Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.8016506Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-04T18:16:31.8027040Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [ERROR] 2026-02-04-18:16:30 (PID:183, Device:1, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-04T18:16:31.8039499Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [PID: 183] 2026-02-04-18:16:30.484.298 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-04T18:16:31.8052530Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-04T18:16:31.8060901Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-04T18:16:31.8071970Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-04T18:16:31.8081295Z [0;36m(Worker_DP1_TP1_EP9 pid=183)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] 
2026-02-04T18:16:31.8091556Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m INFO 02-04 18:16:31 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-04T18:16:31.8112182Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-04T18:16:31.8135150Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-04T18:16:31.8145114Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-04T18:16:31.8154304Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-04T18:16:31.8164132Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.8180526Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-04T18:16:31.8186102Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-04T18:16:31.8195846Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-04T18:16:31.8206180Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-04T18:16:31.8217120Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2250, in load_model
2026-02-04T18:16:31.8227290Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-04T18:16:31.8237752Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.8249163Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-04T18:16:31.8257831Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return loader.load_model(
2026-02-04T18:16:31.8267895Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.8280266Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-04T18:16:31.8289729Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     model = initialize_model(
2026-02-04T18:16:31.8299763Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.8310382Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-04T18:16:31.8320830Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-04T18:16:31.8331361Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.8341740Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-04T18:16:31.8351621Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-04T18:16:31.8366415Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-04T18:16:31.8374941Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-04T18:16:31.8384694Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-04T18:16:31.8393173Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-04T18:16:31.8403447Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-04T18:16:31.8414037Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-04T18:16:31.8424227Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-04T18:16:31.8433729Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     + [
2026-02-04T18:16:31.8443336Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]       ^
2026-02-04T18:16:31.8453648Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-04T18:16:31.8463292Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-04T18:16:31.8473008Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.8483488Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-04T18:16:31.8492847Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-04T18:16:31.8501944Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.8512480Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-04T18:16:31.8521827Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-04T18:16:31.8531341Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-04T18:16:31.8541686Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-04T18:16:31.8550675Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-04T18:16:31.8560714Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-04T18:16:31.8570524Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-04T18:16:31.8582246Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-04T18:16:31.8594565Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-04T18:16:31.8605775Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-04T18:16:31.8615234Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-04T18:16:31.8625328Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-04T18:16:31.8635191Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.8645982Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-04T18:16:31.8656594Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-04T18:16:31.8665628Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.8675785Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-04T18:16:31.8685132Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-04T18:16:31.8694734Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.8705929Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-04T18:16:31.8714706Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-04T18:16:31.8724213Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.8734974Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-04T18:16:31.8744878Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [ERROR] 2026-02-04-18:16:30 (PID:405, Device:3, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-04T18:16:31.8755130Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [PID: 405] 2026-02-04-18:16:30.485.192 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-04T18:16:31.8783305Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-04T18:16:31.8792564Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-04T18:16:31.8803674Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-04T18:16:31.8813018Z [0;36m(Worker_DP1_TP3_EP11 pid=405)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] 
2026-02-04T18:16:31.8823417Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m INFO 02-04 18:16:31 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-04T18:16:31.8832466Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m INFO 02-04 18:16:31 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-04T18:16:31.8842607Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-04T18:16:31.8852756Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-04T18:16:31.8862881Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-04T18:16:31.8873875Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-04T18:16:31.8883210Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.8893453Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-04T18:16:31.8904331Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-04T18:16:31.8912202Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-04T18:16:31.8921957Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-04T18:16:31.8932386Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2250, in load_model
2026-02-04T18:16:31.8941598Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-04T18:16:31.8950719Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.8961019Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-04T18:16:31.8970691Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return loader.load_model(
2026-02-04T18:16:31.8980271Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.8989853Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-04T18:16:31.8998797Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     model = initialize_model(
2026-02-04T18:16:31.9008677Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.9018827Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-04T18:16:31.9027590Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-04T18:16:31.9037396Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.9049329Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-04T18:16:31.9058700Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-04T18:16:31.9067939Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-04T18:16:31.9078599Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-04T18:16:31.9088455Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-04T18:16:31.9098061Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-04T18:16:31.9107884Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-04T18:16:31.9117596Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-04T18:16:31.9127653Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-04T18:16:31.9136678Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     + [
2026-02-04T18:16:31.9145966Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]       ^
2026-02-04T18:16:31.9156207Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-04T18:16:31.9165852Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-04T18:16:31.9175569Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.9185223Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-04T18:16:31.9194646Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-04T18:16:31.9204877Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.9214936Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-04T18:16:31.9224146Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-04T18:16:31.9233265Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-04T18:16:31.9244047Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-04T18:16:31.9253233Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-04T18:16:31.9262439Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-04T18:16:31.9272359Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-04T18:16:31.9282439Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-04T18:16:31.9292259Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-04T18:16:31.9301352Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-04T18:16:31.9311102Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-04T18:16:31.9321819Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-04T18:16:31.9331009Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.9340956Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-04T18:16:31.9350523Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-04T18:16:31.9360591Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.9370526Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-04T18:16:31.9379754Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-04T18:16:31.9389323Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.9399263Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-04T18:16:31.9409104Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-04T18:16:31.9418300Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.9429029Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-04T18:16:31.9446257Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [ERROR] 2026-02-04-18:16:30 (PID:184, Device:1, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-04T18:16:31.9449847Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [PID: 184] 2026-02-04-18:16:30.484.939 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-04T18:16:31.9460686Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-04T18:16:31.9469932Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-04T18:16:31.9480246Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-04T18:16:31.9489737Z [0;36m(Worker_DP0_TP1_EP1 pid=184)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] 
2026-02-04T18:16:31.9499759Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-04T18:16:31.9509619Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-04T18:16:31.9520419Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-04T18:16:31.9530262Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-04T18:16:31.9540237Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.9549996Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-04T18:16:31.9559386Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-04T18:16:31.9569722Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-04T18:16:31.9579103Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-04T18:16:31.9589080Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2250, in load_model
2026-02-04T18:16:31.9600274Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-04T18:16:31.9610123Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.9619594Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-04T18:16:31.9629265Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return loader.load_model(
2026-02-04T18:16:31.9639208Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.9648855Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-04T18:16:31.9658801Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     model = initialize_model(
2026-02-04T18:16:31.9668685Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.9678787Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-04T18:16:31.9688800Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-04T18:16:31.9698169Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.9708701Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-04T18:16:31.9718026Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-04T18:16:31.9729846Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-04T18:16:31.9739193Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-04T18:16:31.9748673Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-04T18:16:31.9758814Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-04T18:16:31.9769781Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-04T18:16:31.9779663Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-04T18:16:31.9790022Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-04T18:16:31.9800276Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     + [
2026-02-04T18:16:31.9810601Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]       ^
2026-02-04T18:16:31.9821754Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-04T18:16:31.9831826Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-04T18:16:31.9842074Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.9852893Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-04T18:16:31.9862633Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-04T18:16:31.9873009Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:31.9884895Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-04T18:16:31.9894705Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-04T18:16:31.9904220Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-04T18:16:31.9914946Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-04T18:16:31.9924946Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-04T18:16:31.9935337Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-04T18:16:31.9946013Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-04T18:16:31.9955763Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-04T18:16:31.9967144Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-04T18:16:31.9977590Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-04T18:16:31.9987856Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-04T18:16:31.9997421Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-04T18:16:32.0008551Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.0019244Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-04T18:16:32.0028989Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-04T18:16:32.0038791Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.0049209Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-04T18:16:32.0059017Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-04T18:16:32.0068582Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.0079026Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-04T18:16:32.0089376Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-04T18:16:32.0099284Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.0110333Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-04T18:16:32.0120328Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [ERROR] 2026-02-04-18:16:30 (PID:717, Device:6, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-04T18:16:32.0131651Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [PID: 717] 2026-02-04-18:16:30.484.978 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-04T18:16:32.0143273Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-04T18:16:32.0152249Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-04T18:16:32.0163832Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-04T18:16:32.0172145Z [0;36m(Worker_DP1_TP6_EP14 pid=717)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] 
2026-02-04T18:16:32.0182421Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m INFO 02-04 18:16:31 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-04T18:16:32.0192336Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m INFO 02-04 18:16:31 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-04T18:16:32.0250680Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-04T18:16:32.0251357Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-04T18:16:32.0252258Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-04T18:16:32.0253095Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-04T18:16:32.0253767Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.0254567Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-04T18:16:32.0261386Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-04T18:16:32.0277782Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-04T18:16:32.0280626Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-04T18:16:32.0291151Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2250, in load_model
2026-02-04T18:16:32.0301019Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-04T18:16:32.0310853Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.0320990Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-04T18:16:32.0331758Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return loader.load_model(
2026-02-04T18:16:32.0347963Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.0351443Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-04T18:16:32.0362203Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     model = initialize_model(
2026-02-04T18:16:32.0371409Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.0382121Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-04T18:16:32.0391499Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-04T18:16:32.0401513Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.0411683Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-04T18:16:32.0420957Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-04T18:16:32.0430618Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-04T18:16:32.0440116Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-04T18:16:32.0450216Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-04T18:16:32.0459742Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-04T18:16:32.0469531Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-04T18:16:32.0479866Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-04T18:16:32.0490226Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-04T18:16:32.0500331Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     + [
2026-02-04T18:16:32.0510294Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]       ^
2026-02-04T18:16:32.0520451Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-04T18:16:32.0530439Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-04T18:16:32.0540576Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.0551889Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-04T18:16:32.0560980Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-04T18:16:32.0571382Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.0580306Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-04T18:16:32.0589899Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-04T18:16:32.0599847Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-04T18:16:32.0610024Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-04T18:16:32.0619318Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-04T18:16:32.0628854Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-04T18:16:32.0639597Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-04T18:16:32.0649101Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-04T18:16:32.0659313Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-04T18:16:32.0668687Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-04T18:16:32.0679197Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-04T18:16:32.0688997Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-04T18:16:32.0703366Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.0713135Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-04T18:16:32.0723353Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-04T18:16:32.0734745Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.0745281Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-04T18:16:32.0757953Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-04T18:16:32.0770077Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.0780364Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-04T18:16:32.0790635Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-04T18:16:32.0801970Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.0812973Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-04T18:16:32.0822620Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [ERROR] 2026-02-04-18:16:30 (PID:130, Device:0, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-04T18:16:32.0833381Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [PID: 130] 2026-02-04-18:16:30.484.036 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-04T18:16:32.0845602Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-04T18:16:32.0855045Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-04T18:16:32.0869190Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-04T18:16:32.0878305Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] 
2026-02-04T18:16:32.0888646Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-04T18:16:32.0898115Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-04T18:16:32.0908710Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-04T18:16:32.0918868Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-04T18:16:32.0928561Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.0938717Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-04T18:16:32.0948437Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-04T18:16:32.0958811Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-04T18:16:32.0968653Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-04T18:16:32.0979084Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2250, in load_model
2026-02-04T18:16:32.0989176Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-04T18:16:32.0999726Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.1009704Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-04T18:16:32.1019468Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return loader.load_model(
2026-02-04T18:16:32.1029223Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.1039542Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-04T18:16:32.1048878Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     model = initialize_model(
2026-02-04T18:16:32.1089271Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.1090079Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-04T18:16:32.1091051Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-04T18:16:32.1091777Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.1098090Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-04T18:16:32.1107677Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-04T18:16:32.1117540Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-04T18:16:32.1127927Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-04T18:16:32.1137262Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-04T18:16:32.1147236Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-04T18:16:32.1157188Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-04T18:16:32.1167423Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-04T18:16:32.1179237Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-04T18:16:32.1186729Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     + [
2026-02-04T18:16:32.1196369Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]       ^
2026-02-04T18:16:32.1207920Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-04T18:16:32.1218017Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-04T18:16:32.1227981Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.1236612Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-04T18:16:32.1246633Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-04T18:16:32.1255922Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.1265707Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-04T18:16:32.1274920Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-04T18:16:32.1285336Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-04T18:16:32.1294494Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-04T18:16:32.1304119Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-04T18:16:32.1314001Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-04T18:16:32.1323993Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-04T18:16:32.1333604Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-04T18:16:32.1343480Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-04T18:16:32.1352988Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-04T18:16:32.1365772Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-04T18:16:32.1374759Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-04T18:16:32.1389459Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.1396387Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-04T18:16:32.1407094Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-04T18:16:32.1417202Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.1427200Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-04T18:16:32.1437930Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-04T18:16:32.1449519Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.1458636Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-04T18:16:32.1468328Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-04T18:16:32.1477921Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.1489056Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-04T18:16:32.1498795Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [ERROR] 2026-02-04-18:16:30 (PID:821, Device:7, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-04T18:16:32.1509207Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [PID: 821] 2026-02-04-18:16:30.484.725 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-04T18:16:32.1520344Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-04T18:16:32.1529419Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-04T18:16:32.1540612Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-04T18:16:32.1549797Z [0;36m(Worker_DP1_TP7_EP15 pid=821)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] 
2026-02-04T18:16:32.1559492Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m INFO 02-04 18:16:31 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-04T18:16:32.1568905Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-04T18:16:32.1578646Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-04T18:16:32.1588598Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-04T18:16:32.1598055Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-04T18:16:32.1609122Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.1617962Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-04T18:16:32.1626584Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-04T18:16:32.1637357Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-04T18:16:32.1646812Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-04T18:16:32.1656847Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2250, in load_model
2026-02-04T18:16:32.1669016Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-04T18:16:32.1679886Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.1691127Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-04T18:16:32.1701091Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return loader.load_model(
2026-02-04T18:16:32.1710647Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.1721270Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-04T18:16:32.1731902Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     model = initialize_model(
2026-02-04T18:16:32.1740893Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.1751020Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-04T18:16:32.1762073Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-04T18:16:32.1772822Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.1783111Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-04T18:16:32.1792919Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-04T18:16:32.1803769Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-04T18:16:32.1814272Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-04T18:16:32.1823589Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-04T18:16:32.1833532Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-04T18:16:32.1843238Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-04T18:16:32.1853294Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-04T18:16:32.1863125Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-04T18:16:32.1872487Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     + [
2026-02-04T18:16:32.1882474Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]       ^
2026-02-04T18:16:32.1892987Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-04T18:16:32.1901850Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-04T18:16:32.1911519Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.1972674Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-04T18:16:32.1973569Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-04T18:16:32.1974238Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.1975033Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-04T18:16:32.1975791Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-04T18:16:32.1976397Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-04T18:16:32.1980099Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-04T18:16:32.1989645Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-04T18:16:32.1999429Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-04T18:16:32.2009495Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-04T18:16:32.2019176Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-04T18:16:32.2028777Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-04T18:16:32.2038830Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-04T18:16:32.2049523Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-04T18:16:32.2058598Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-04T18:16:32.2067096Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.2077556Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-04T18:16:32.2086992Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-04T18:16:32.2096769Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.2106812Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-04T18:16:32.2115363Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-04T18:16:32.2125018Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.2135255Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-04T18:16:32.2144541Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-04T18:16:32.2153566Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.2164213Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-04T18:16:32.2190921Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [ERROR] 2026-02-04-18:16:30 (PID:509, Device:4, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-04T18:16:32.2192387Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] [PID: 509] 2026-02-04-18:16:30.485.047 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-04T18:16:32.2194570Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-04T18:16:32.2203598Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-04T18:16:32.2214236Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-04T18:16:32.2223168Z [0;36m(Worker_DP1_TP4_EP12 pid=509)[0;0m ERROR 02-04 18:16:31 [multiproc_executor.py:772] 
2026-02-04T18:16:32.2233029Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m INFO 02-04 18:16:31 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-04T18:16:32.2242763Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-04T18:16:32.2252221Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-04T18:16:32.2261296Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-04T18:16:32.2270320Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-04T18:16:32.2280603Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.2289997Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-04T18:16:32.2298802Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-04T18:16:32.2309349Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-04T18:16:32.2319108Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-04T18:16:32.2327878Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2250, in load_model
2026-02-04T18:16:32.2336657Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-04T18:16:32.2346052Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.2355946Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-04T18:16:32.2366261Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     return loader.load_model(
2026-02-04T18:16:32.2378407Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.2387497Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-04T18:16:32.2395883Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     model = initialize_model(
2026-02-04T18:16:32.2405973Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.2415547Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-04T18:16:32.2425016Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-04T18:16:32.2434241Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.2444864Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-04T18:16:32.2453615Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-04T18:16:32.2462962Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-04T18:16:32.2473096Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-04T18:16:32.2486010Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-04T18:16:32.2492752Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-04T18:16:32.2501862Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-04T18:16:32.2511333Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-04T18:16:32.2521584Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-04T18:16:32.2530640Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     + [
2026-02-04T18:16:32.2539787Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]       ^
2026-02-04T18:16:32.2551585Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-04T18:16:32.2559410Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-04T18:16:32.2569080Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.2578893Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-04T18:16:32.2587924Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-04T18:16:32.2597695Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.2607095Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-04T18:16:32.2616691Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-04T18:16:32.2626256Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-04T18:16:32.2635718Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-04T18:16:32.2645719Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-04T18:16:32.2654353Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-04T18:16:32.2664914Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-04T18:16:32.2674736Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-04T18:16:32.2684108Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-04T18:16:32.2692908Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-04T18:16:32.2702755Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-04T18:16:32.2712397Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-04T18:16:32.2722091Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.2731330Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-04T18:16:32.2741677Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-04T18:16:32.2750482Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.2761489Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-04T18:16:32.2770687Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-04T18:16:32.2780306Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.2790534Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-04T18:16:32.2800195Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-04T18:16:32.2810230Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.2820445Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-04T18:16:32.2829617Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772] [ERROR] 2026-02-04-18:16:30 (PID:610, Device:5, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-04T18:16:32.2841349Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772] [PID: 610] 2026-02-04-18:16:30.484.328 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-04T18:16:32.2853062Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-04T18:16:32.2860799Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-04T18:16:32.2871313Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-04T18:16:32.2879866Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772] 
2026-02-04T18:16:32.2893660Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-04T18:16:32.2898761Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-04T18:16:32.2908590Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-04T18:16:32.2918566Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-04T18:16:32.2926797Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.2936514Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-04T18:16:32.2945949Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-04T18:16:32.2955841Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-04T18:16:32.2965725Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-04T18:16:32.2975647Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2250, in load_model
2026-02-04T18:16:32.2985206Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-04T18:16:32.2994740Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3005464Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-04T18:16:32.3013884Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     return loader.load_model(
2026-02-04T18:16:32.3024318Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3033347Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-04T18:16:32.3042718Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     model = initialize_model(
2026-02-04T18:16:32.3054155Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3062715Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-04T18:16:32.3072315Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-04T18:16:32.3082056Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3091982Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-04T18:16:32.3101204Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-04T18:16:32.3110906Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3121424Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-04T18:16:32.3130913Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-04T18:16:32.3141545Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-04T18:16:32.3150308Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-04T18:16:32.3160828Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-04T18:16:32.3171832Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-04T18:16:32.3184733Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     + [
2026-02-04T18:16:32.3188546Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]       ^
2026-02-04T18:16:32.3198263Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-04T18:16:32.3208107Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-04T18:16:32.3218014Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3228581Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-04T18:16:32.3237740Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-04T18:16:32.3246724Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3256258Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-04T18:16:32.3265329Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-04T18:16:32.3274585Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-04T18:16:32.3286612Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-04T18:16:32.3294544Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-04T18:16:32.3304140Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3314441Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-04T18:16:32.3324312Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-04T18:16:32.3333677Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-04T18:16:32.3343167Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-04T18:16:32.3353533Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-04T18:16:32.3363342Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-04T18:16:32.3373187Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3383541Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-04T18:16:32.3392827Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-04T18:16:32.3403062Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3413208Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-04T18:16:32.3422547Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-04T18:16:32.3432108Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3442117Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-04T18:16:32.3452216Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-04T18:16:32.3461733Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3473886Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-04T18:16:32.3481611Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772] [ERROR] 2026-02-04-18:16:30 (PID:818, Device:7, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-04T18:16:32.3493200Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772] [PID: 818] 2026-02-04-18:16:30.485.194 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-04T18:16:32.3504048Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-04T18:16:32.3512580Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-04T18:16:32.3523192Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-04T18:16:32.3532468Z [0;36m(Worker_DP0_TP7_EP7 pid=818)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772] 
2026-02-04T18:16:32.3542211Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-04T18:16:32.3551193Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-04T18:16:32.3561106Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-04T18:16:32.3571039Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-04T18:16:32.3580484Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3590619Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-04T18:16:32.3600102Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-04T18:16:32.3610082Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-04T18:16:32.3621900Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-04T18:16:32.3630300Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2250, in load_model
2026-02-04T18:16:32.3640385Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-04T18:16:32.3648403Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3658203Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-04T18:16:32.3667741Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     return loader.load_model(
2026-02-04T18:16:32.3677264Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3687751Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-04T18:16:32.3697268Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     model = initialize_model(
2026-02-04T18:16:32.3708392Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3716965Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-04T18:16:32.3726300Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-04T18:16:32.3735018Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3745800Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-04T18:16:32.3755018Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-04T18:16:32.3764400Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3776687Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-04T18:16:32.3787357Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-04T18:16:32.3797318Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-04T18:16:32.3807693Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-04T18:16:32.3816750Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-04T18:16:32.3826262Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-04T18:16:32.3836697Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     + [
2026-02-04T18:16:32.3851350Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]       ^
2026-02-04T18:16:32.3862379Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-04T18:16:32.3872481Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-04T18:16:32.3883580Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3893044Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-04T18:16:32.3903757Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-04T18:16:32.3912554Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3923124Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-04T18:16:32.3934323Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-04T18:16:32.3943268Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-04T18:16:32.3951884Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-04T18:16:32.3960512Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-04T18:16:32.3970941Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-04T18:16:32.3979625Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-04T18:16:32.3987626Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-04T18:16:32.3998364Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-04T18:16:32.4007672Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-04T18:16:32.4017737Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-04T18:16:32.4027603Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-04T18:16:32.4036247Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.4048151Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-04T18:16:32.4056213Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-04T18:16:32.4065846Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.4075946Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-04T18:16:32.4085996Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-04T18:16:32.4095534Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.4105398Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-04T18:16:32.4114718Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-04T18:16:32.4125310Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:32.4136024Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-04T18:16:32.4144454Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772] [ERROR] 2026-02-04-18:16:30 (PID:714, Device:6, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-04T18:16:32.4155389Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772] [PID: 714] 2026-02-04-18:16:30.484.345 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-04T18:16:32.4166522Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-04T18:16:32.4175194Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-04T18:16:32.4185717Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-04T18:16:32.4195329Z [0;36m(Worker_DP0_TP6_EP6 pid=714)[0;0m ERROR 02-04 18:16:32 [multiproc_executor.py:772] 
2026-02-04T18:16:36.3613956Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946] EngineCore failed to start.
2026-02-04T18:16:36.3625075Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946] Traceback (most recent call last):
2026-02-04T18:16:36.3633667Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 929, in run_engine_core
2026-02-04T18:16:36.3643458Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946]     engine_core = DPEngineCoreProc(*args, **kwargs)
2026-02-04T18:16:36.3653284Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:36.3667704Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 1279, in __init__
2026-02-04T18:16:36.3680368Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946]     super().__init__(
2026-02-04T18:16:36.3690020Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 691, in __init__
2026-02-04T18:16:36.3700390Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946]     super().__init__(
2026-02-04T18:16:36.3711504Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 105, in __init__
2026-02-04T18:16:36.3721606Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946]     self.model_executor = executor_class(vllm_config)
2026-02-04T18:16:36.3733155Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:36.3741771Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
2026-02-04T18:16:36.3751864Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946]     super().__init__(vllm_config)
2026-02-04T18:16:36.3763253Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
2026-02-04T18:16:36.3773237Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946]     self._init_executor()
2026-02-04T18:16:36.3783162Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
2026-02-04T18:16:36.3793711Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
2026-02-04T18:16:36.3803553Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:36.3813599Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
2026-02-04T18:16:36.3824180Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946]     raise e from None
2026-02-04T18:16:36.3832516Z [0;36m(EngineCore_DP0 pid=83)[0;0m ERROR 02-04 18:16:36 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
2026-02-04T18:16:36.3841831Z [0;36m(EngineCore_DP0 pid=83)[0;0m Process EngineCore_DP0:
2026-02-04T18:16:36.3851361Z [0;36m(EngineCore_DP0 pid=83)[0;0m Traceback (most recent call last):
2026-02-04T18:16:36.3861497Z [0;36m(EngineCore_DP0 pid=83)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-04T18:16:36.3870889Z [0;36m(EngineCore_DP0 pid=83)[0;0m     self.run()
2026-02-04T18:16:36.3882253Z [0;36m(EngineCore_DP0 pid=83)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-04T18:16:36.3891771Z [0;36m(EngineCore_DP0 pid=83)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-04T18:16:36.3901456Z [0;36m(EngineCore_DP0 pid=83)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 950, in run_engine_core
2026-02-04T18:16:36.3910861Z [0;36m(EngineCore_DP0 pid=83)[0;0m     raise e
2026-02-04T18:16:36.3921395Z [0;36m(EngineCore_DP0 pid=83)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 929, in run_engine_core
2026-02-04T18:16:36.3931500Z [0;36m(EngineCore_DP0 pid=83)[0;0m     engine_core = DPEngineCoreProc(*args, **kwargs)
2026-02-04T18:16:36.3942459Z [0;36m(EngineCore_DP0 pid=83)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:36.3951686Z [0;36m(EngineCore_DP0 pid=83)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 1279, in __init__
2026-02-04T18:16:36.3962092Z [0;36m(EngineCore_DP0 pid=83)[0;0m     super().__init__(
2026-02-04T18:16:36.3973298Z [0;36m(EngineCore_DP0 pid=83)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 691, in __init__
2026-02-04T18:16:36.3983575Z [0;36m(EngineCore_DP0 pid=83)[0;0m     super().__init__(
2026-02-04T18:16:36.3993763Z [0;36m(EngineCore_DP0 pid=83)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 105, in __init__
2026-02-04T18:16:36.4048939Z [0;36m(EngineCore_DP0 pid=83)[0;0m     self.model_executor = executor_class(vllm_config)
2026-02-04T18:16:36.4049391Z [0;36m(EngineCore_DP0 pid=83)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:36.4049977Z [0;36m(EngineCore_DP0 pid=83)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
2026-02-04T18:16:36.4050608Z [0;36m(EngineCore_DP0 pid=83)[0;0m     super().__init__(vllm_config)
2026-02-04T18:16:36.4051159Z [0;36m(EngineCore_DP0 pid=83)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
2026-02-04T18:16:36.4053972Z [0;36m(EngineCore_DP0 pid=83)[0;0m     self._init_executor()
2026-02-04T18:16:36.4064428Z [0;36m(EngineCore_DP0 pid=83)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
2026-02-04T18:16:36.4074691Z [0;36m(EngineCore_DP0 pid=83)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
2026-02-04T18:16:36.4084739Z [0;36m(EngineCore_DP0 pid=83)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:36.4108146Z [0;36m(EngineCore_DP0 pid=83)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
2026-02-04T18:16:36.4108679Z [0;36m(EngineCore_DP0 pid=83)[0;0m     raise e from None
2026-02-04T18:16:36.4115650Z [0;36m(EngineCore_DP0 pid=83)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
2026-02-04T18:16:36.4141511Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946] EngineCore failed to start.
2026-02-04T18:16:36.4152521Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946] Traceback (most recent call last):
2026-02-04T18:16:36.4163054Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 929, in run_engine_core
2026-02-04T18:16:36.4173890Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946]     engine_core = DPEngineCoreProc(*args, **kwargs)
2026-02-04T18:16:36.4184234Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:36.4194055Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 1279, in __init__
2026-02-04T18:16:36.4204956Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946]     super().__init__(
2026-02-04T18:16:36.4215581Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 691, in __init__
2026-02-04T18:16:36.4225909Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946]     super().__init__(
2026-02-04T18:16:36.4235917Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 105, in __init__
2026-02-04T18:16:36.4246844Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946]     self.model_executor = executor_class(vllm_config)
2026-02-04T18:16:36.4256671Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:36.4266612Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
2026-02-04T18:16:36.4276459Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946]     super().__init__(vllm_config)
2026-02-04T18:16:36.4287435Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
2026-02-04T18:16:36.4298392Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946]     self._init_executor()
2026-02-04T18:16:36.4307914Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
2026-02-04T18:16:36.4317522Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
2026-02-04T18:16:36.4327587Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:36.4337879Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
2026-02-04T18:16:36.4347414Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946]     raise e from None
2026-02-04T18:16:36.4358080Z [0;36m(EngineCore_DP1 pid=102)[0;0m ERROR 02-04 18:16:36 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
2026-02-04T18:16:36.4367649Z [0;36m(EngineCore_DP1 pid=102)[0;0m Process EngineCore_DP1:
2026-02-04T18:16:36.4377616Z [0;36m(EngineCore_DP1 pid=102)[0;0m Traceback (most recent call last):
2026-02-04T18:16:36.4387526Z [0;36m(EngineCore_DP1 pid=102)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-04T18:16:36.4397640Z [0;36m(EngineCore_DP1 pid=102)[0;0m     self.run()
2026-02-04T18:16:36.4407403Z [0;36m(EngineCore_DP1 pid=102)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-04T18:16:36.4416937Z [0;36m(EngineCore_DP1 pid=102)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-04T18:16:36.4426958Z [0;36m(EngineCore_DP1 pid=102)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 950, in run_engine_core
2026-02-04T18:16:36.4436091Z [0;36m(EngineCore_DP1 pid=102)[0;0m     raise e
2026-02-04T18:16:36.4446546Z [0;36m(EngineCore_DP1 pid=102)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 929, in run_engine_core
2026-02-04T18:16:36.4455946Z [0;36m(EngineCore_DP1 pid=102)[0;0m     engine_core = DPEngineCoreProc(*args, **kwargs)
2026-02-04T18:16:36.4465123Z [0;36m(EngineCore_DP1 pid=102)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:36.4475111Z [0;36m(EngineCore_DP1 pid=102)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 1279, in __init__
2026-02-04T18:16:36.4485472Z [0;36m(EngineCore_DP1 pid=102)[0;0m     super().__init__(
2026-02-04T18:16:36.4495957Z [0;36m(EngineCore_DP1 pid=102)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 691, in __init__
2026-02-04T18:16:36.4505440Z [0;36m(EngineCore_DP1 pid=102)[0;0m     super().__init__(
2026-02-04T18:16:36.4514927Z [0;36m(EngineCore_DP1 pid=102)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 105, in __init__
2026-02-04T18:16:36.4525053Z [0;36m(EngineCore_DP1 pid=102)[0;0m     self.model_executor = executor_class(vllm_config)
2026-02-04T18:16:36.4534741Z [0;36m(EngineCore_DP1 pid=102)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:36.4544984Z [0;36m(EngineCore_DP1 pid=102)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
2026-02-04T18:16:36.4554213Z [0;36m(EngineCore_DP1 pid=102)[0;0m     super().__init__(vllm_config)
2026-02-04T18:16:36.4564066Z [0;36m(EngineCore_DP1 pid=102)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
2026-02-04T18:16:36.4573746Z [0;36m(EngineCore_DP1 pid=102)[0;0m     self._init_executor()
2026-02-04T18:16:36.4583640Z [0;36m(EngineCore_DP1 pid=102)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
2026-02-04T18:16:36.4593107Z [0;36m(EngineCore_DP1 pid=102)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
2026-02-04T18:16:36.4603118Z [0;36m(EngineCore_DP1 pid=102)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-04T18:16:36.4613193Z [0;36m(EngineCore_DP1 pid=102)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
2026-02-04T18:16:36.4622744Z [0;36m(EngineCore_DP1 pid=102)[0;0m     raise e from None
2026-02-04T18:16:36.4632305Z [0;36m(EngineCore_DP1 pid=102)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
2026-02-04T18:16:38.2373920Z Traceback (most recent call last):
2026-02-04T18:16:38.2381944Z   File "/usr/local/python3.11.14/bin/vllm", line 7, in <module>
2026-02-04T18:16:38.2394504Z     sys.exit(main())
2026-02-04T18:16:38.2404402Z              ^^^^^^
2026-02-04T18:16:38.2413715Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/main.py", line 73, in main
2026-02-04T18:16:38.2423960Z     args.dispatch_function(args)
2026-02-04T18:16:38.2434158Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 108, in cmd
2026-02-04T18:16:38.2467306Z     run_multi_api_server(args)
2026-02-04T18:16:38.2477320Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 248, in run_multi_api_server
2026-02-04T18:16:38.2486871Z     with launch_core_engines(
2026-02-04T18:16:38.2495809Z   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 144, in __exit__
2026-02-04T18:16:38.2506194Z     next(self.gen)
2026-02-04T18:16:38.2515414Z   File "/vllm-workspace/vllm/vllm/v1/engine/utils.py", line 933, in launch_core_engines
2026-02-04T18:16:38.2524839Z     wait_for_engine_startup(
2026-02-04T18:16:38.2534180Z   File "/vllm-workspace/vllm/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
2026-02-04T18:16:38.2543303Z     raise RuntimeError(
2026-02-04T18:16:38.2553206Z RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
2026-02-04T18:16:39.0718239Z [ERROR] 2026-02-04-18:16:38 (PID:67, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
2026-02-04T18:16:39.4083770Z sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-04T18:16:41.0195767Z /usr/local/python3.11.14/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 18 leaked shared_memory objects to clean up at shutdown
2026-02-04T18:16:41.0201160Z   warnings.warn('resource_tracker: There appear to be %d '
2026-02-04T18:16:45.7257159Z FAILED
2026-02-04T18:16:45.7266411Z 
2026-02-04T18:16:45.7276544Z =================================== FAILURES ===================================
2026-02-04T18:16:45.7286579Z _______________________________ test_multi_node ________________________________
2026-02-04T18:16:45.7296868Z 
2026-02-04T18:16:45.7305656Z     @pytest.mark.asyncio
2026-02-04T18:16:45.7316851Z     async def test_multi_node() -> None:
2026-02-04T18:16:45.7326707Z         config = MultiNodeConfigLoader.from_yaml()
2026-02-04T18:16:45.7336030Z     
2026-02-04T18:16:45.7345071Z         with ProxyLauncher(
2026-02-04T18:16:45.7354742Z                 nodes=config.nodes,
2026-02-04T18:16:45.7365102Z                 disagg_cfg=config.disagg_cfg,
2026-02-04T18:16:45.7374980Z                 envs=config.envs,
2026-02-04T18:16:45.7385060Z                 proxy_port=config.proxy_port,
2026-02-04T18:16:45.7394480Z                 cur_index=config.cur_index,
2026-02-04T18:16:45.7404002Z         ) as proxy:
2026-02-04T18:16:45.7413381Z     
2026-02-04T18:16:45.7423639Z >           with RemoteOpenAIServer(
2026-02-04T18:16:45.7433389Z                     model=config.model,
2026-02-04T18:16:45.7443149Z                     vllm_serve_args=config.server_cmd,
2026-02-04T18:16:45.7453251Z                     server_port=config.server_port,
2026-02-04T18:16:45.7462872Z                     server_host=config.master_ip,
2026-02-04T18:16:45.7471759Z                     env_dict=config.envs,
2026-02-04T18:16:45.7482224Z                     auto_port=False,
2026-02-04T18:16:45.7490913Z                     proxy_port=proxy.proxy_port,
2026-02-04T18:16:45.7500447Z                     disaggregated_prefill=config.disagg_cfg,
2026-02-04T18:16:45.7510351Z                     nodes_info=config.nodes,
2026-02-04T18:16:45.7520331Z                     max_wait_seconds=2800,
2026-02-04T18:16:45.7529410Z             ) as server:
2026-02-04T18:16:45.7538979Z 
2026-02-04T18:16:45.7548742Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py:21: 
2026-02-04T18:16:45.7558612Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-04T18:16:45.7567910Z tests/e2e/conftest.py:297: in __init__
2026-02-04T18:16:45.7576925Z     self._wait_for_multiple_servers(
2026-02-04T18:16:45.7586751Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-04T18:16:45.7596661Z 
2026-02-04T18:16:45.7606838Z self = <tests.e2e.conftest.RemoteOpenAIServer object at 0xffff3342ba50>
2026-02-04T18:16:45.7615969Z targets = [('10.0.0.76', 'http://10.0.0.76:8080/health')], timeout = 2800
2026-02-04T18:16:45.7625044Z log_interval = 30.0
2026-02-04T18:16:45.7634582Z 
2026-02-04T18:16:45.7644338Z     def _wait_for_multiple_servers(self,
2026-02-04T18:16:45.7653647Z                                    targets,
2026-02-04T18:16:45.7663344Z                                    timeout: float,
2026-02-04T18:16:45.7672764Z                                    log_interval: float = 30.0):
2026-02-04T18:16:45.7683066Z         """
2026-02-04T18:16:45.7695278Z         targets: List[(node_ip, url)]
2026-02-04T18:16:45.7704293Z         log_interval
2026-02-04T18:16:45.7713936Z         """
2026-02-04T18:16:45.7723893Z         start = time.time()
2026-02-04T18:16:45.7733734Z         client = requests
2026-02-04T18:16:45.7743902Z     
2026-02-04T18:16:45.7753417Z         ready = {node_ip: False for node_ip, _ in targets}
2026-02-04T18:16:45.7763531Z     
2026-02-04T18:16:45.7774610Z         last_log_time = 0.0
2026-02-04T18:16:45.7784640Z     
2026-02-04T18:16:45.7794953Z         while True:
2026-02-04T18:16:45.7805103Z             now = time.time()
2026-02-04T18:16:45.7815370Z             all_ready = True
2026-02-04T18:16:45.7826895Z             should_log = (now - last_log_time) >= log_interval
2026-02-04T18:16:45.7836308Z     
2026-02-04T18:16:45.7846974Z             for node_ip, url in targets:
2026-02-04T18:16:45.7856834Z                 if ready[node_ip]:
2026-02-04T18:16:45.7867224Z                     continue
2026-02-04T18:16:45.7876943Z     
2026-02-04T18:16:45.7887298Z                 try:
2026-02-04T18:16:45.7897648Z                     resp = client.get(url)
2026-02-04T18:16:45.7908447Z                     if resp.status_code == 200:
2026-02-04T18:16:45.7918876Z                         ready[node_ip] = True
2026-02-04T18:16:45.7929385Z                         logger.info(f"[READY] Node {node_ip} is ready.")
2026-02-04T18:16:45.7939368Z                 except RequestException:
2026-02-04T18:16:45.7949859Z                     all_ready = False
2026-02-04T18:16:45.7959709Z                     if should_log:
2026-02-04T18:16:45.7970001Z                         logger.debug(f"[WAIT] {url}: connection failed")
2026-02-04T18:16:45.7980623Z     
2026-02-04T18:16:45.7990369Z                     # check unexpected exit
2026-02-04T18:16:45.8001108Z                     result = self._poll()
2026-02-04T18:16:45.8011404Z                     if result is not None and result != 0:
2026-02-04T18:16:45.8021066Z >                       raise RuntimeError(
2026-02-04T18:16:45.8031971Z                             f"Server at {node_ip} exited unexpectedly."
2026-02-04T18:16:45.8039990Z                         ) from None
2026-02-04T18:16:45.8049855Z E                       RuntimeError: Server at 10.0.0.76 exited unexpectedly.
2026-02-04T18:16:45.8059022Z 
2026-02-04T18:16:45.8068360Z tests/e2e/conftest.py:390: RuntimeError
2026-02-04T18:16:45.8078465Z =============================== warnings summary ===============================
2026-02-04T18:16:45.8087724Z <frozen importlib._bootstrap>:241
2026-02-04T18:16:45.8096964Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute
2026-02-04T18:16:45.8105901Z 
2026-02-04T18:16:45.8115785Z <frozen importlib._bootstrap>:241
2026-02-04T18:16:45.8129261Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute
2026-02-04T18:16:45.8135004Z 
2026-02-04T18:16:45.8145016Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2026-02-04T18:16:45.8154034Z =========================== short test summary info ============================
2026-02-04T18:16:45.8164429Z FAILED tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node
2026-02-04T18:16:45.8174410Z ================== 1 failed, 2 warnings in 250.70s (0:04:10) ===================
2026-02-04T18:16:47.5348780Z [0;31mFAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml âœ— ERROR: Some tests failed, please check the error stack above for details. If this is insufficient to pinpoint the error, please download and review the logs of all other nodes from the job's summary.[0m
2026-02-04T18:16:47.6607258Z Cleaning up background log streams...
2026-02-04T18:16:47.7305748Z ##[error]Error: failed to run script step: Error: command terminated with non-zero exit code: command terminated with exit code 1
2026-02-04T18:16:47.7345350Z ##[error]Process completed with exit code 1.
2026-02-04T18:16:47.7436317Z ##[error]Executing the custom container implementation failed. Please contact your self hosted runner administrator.
2026-02-04T18:16:47.7831809Z ##[group]Run actions/upload-artifact@v6
2026-02-04T18:16:47.7832234Z with:
2026-02-04T18:16:47.7832506Z   name: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs
2026-02-04T18:16:47.7832804Z   path: /tmp/vllm*_logs.txt
2026-02-04T18:16:47.7833065Z   retention-days: 7
2026-02-04T18:16:47.7833385Z   if-no-files-found: warn
2026-02-04T18:16:47.7833592Z   compression-level: 6
2026-02-04T18:16:47.7833843Z   overwrite: false
2026-02-04T18:16:47.7834073Z   include-hidden-files: false
2026-02-04T18:16:47.7834289Z env:
2026-02-04T18:16:47.7834549Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-04T18:16:47.7834853Z ##[endgroup]
2026-02-04T18:16:47.7861841Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-04T18:16:47.7863015Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-04T18:16:47.7863457Z ##[endgroup]
2026-02-04T18:16:48.1489880Z (node:11721) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-04T18:16:48.1490995Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-04T18:17:07.1217857Z With the provided path, there will be 1 file uploaded
2026-02-04T18:17:07.1221903Z Artifact name is valid!
2026-02-04T18:17:07.1222196Z Root directory input is valid!
2026-02-04T18:17:08.0172467Z Beginning upload of artifact content to blob storage
2026-02-04T18:17:09.5094064Z Uploaded bytes 15797
2026-02-04T18:17:09.7737521Z Finished uploading artifact content to blob storage!
2026-02-04T18:17:09.7737999Z SHA256 digest of uploaded artifact zip is 9c5441156c0bed0ad0d2fe11a0b0ea97d9a3fb59d7c3cd8bce2166d66af68568
2026-02-04T18:17:09.7741110Z Finalizing artifact upload
2026-02-04T18:17:10.9383464Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs.zip successfully finalized. Artifact ID 5378787089
2026-02-04T18:17:10.9384168Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs has been successfully uploaded! Final size is 15797 bytes. Artifact ID is 5378787089
2026-02-04T18:17:10.9385227Z Artifact download URL: https://github.com/vllm-project/vllm-ascend/actions/runs/21679697194/artifacts/5378787089
2026-02-04T18:17:29.3106015Z ##[group]Run kubectl get pods -n "$NAMESPACE" --ignore-not-found=true
2026-02-04T18:17:29.3106408Z [36;1mkubectl get pods -n "$NAMESPACE" --ignore-not-found=true[0m
2026-02-04T18:17:29.3106739Z [36;1mkubectl delete -f ./lws.yaml --ignore-not-found=true || true[0m
2026-02-04T18:17:29.3107175Z shell: bash -el {0}
2026-02-04T18:17:29.3107338Z env:
2026-02-04T18:17:29.3107548Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-04T18:17:29.3107782Z ##[endgroup]
2026-02-04T18:17:29.3190811Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-04T18:17:29.3191497Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-04T18:17:29.3191710Z ##[endgroup]
2026-02-04T18:17:29.6755922Z (node:13374) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-04T18:17:29.6756732Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-04T18:17:48.7031500Z NAME                                             READY   STATUS    RESTARTS      AGE
2026-02-04T18:17:48.7031932Z linux-aarch64-a3-0-wgt9d-runner-6lk8b            1/1     Running   0             10m
2026-02-04T18:17:48.7032474Z linux-aarch64-a3-0-wgt9d-runner-6lk8b-workflow   1/1     Running   0             9m59s
2026-02-04T18:17:48.7032835Z vllm-0                                           1/1     Running   1 (61s ago)   6m38s
2026-02-04T18:17:48.7033125Z vllm-0-1                                         1/1     Running   0             6m38s
2026-02-04T18:17:48.7633964Z leaderworkerset.leaderworkerset.x-k8s.io "vllm" deleted from vllm-project namespace
2026-02-04T18:17:48.7775320Z service "vllm-leader" deleted from vllm-project namespace
2026-02-04T18:18:07.2680072Z Post job cleanup.
2026-02-04T18:18:07.2703553Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-04T18:18:07.2704381Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-04T18:18:07.2704789Z ##[endgroup]
2026-02-04T18:18:07.6324199Z (node:15211) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-04T18:18:07.6325348Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-04T18:18:26.3341127Z [command]/usr/bin/git version
2026-02-04T18:18:26.3525712Z git version 2.34.1
2026-02-04T18:18:26.3551208Z Copying '/root/.gitconfig' to '/__w/_temp/c442c15b-baa0-4da7-beb9-0c54f3b205f8/.gitconfig'
2026-02-04T18:18:26.3559284Z Temporarily overriding HOME='/__w/_temp/c442c15b-baa0-4da7-beb9-0c54f3b205f8' before making global git config changes
2026-02-04T18:18:26.3559970Z Adding repository directory to the temporary git global config as a safe directory
2026-02-04T18:18:26.3563882Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-04T18:18:26.3604679Z Removing SSH command configuration
2026-02-04T18:18:26.3609935Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-04T18:18:26.3673879Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-04T18:18:26.4212289Z Removing HTTP extra header
2026-02-04T18:18:26.4215503Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-04T18:18:26.4242271Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-04T18:18:26.4434833Z Removing includeIf entries pointing to credentials config files
2026-02-04T18:18:26.4439168Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-04T18:18:26.4458867Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-04T18:18:26.4459212Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-04T18:18:26.4459510Z includeif.gitdir:/github/workspace/.git.path
2026-02-04T18:18:26.4459774Z includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-04T18:18:26.4466627Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-04T18:18:26.4485535Z /__w/_temp/git-credentials-039288b9-4152-4488-8845-7affca6bf092.config
2026-02-04T18:18:26.4493537Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-039288b9-4152-4488-8845-7affca6bf092.config
2026-02-04T18:18:26.4521557Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-04T18:18:26.4539499Z /__w/_temp/git-credentials-039288b9-4152-4488-8845-7affca6bf092.config
2026-02-04T18:18:26.4546533Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-039288b9-4152-4488-8845-7affca6bf092.config
2026-02-04T18:18:26.4573260Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git.path
2026-02-04T18:18:26.4590913Z /github/runner_temp/git-credentials-039288b9-4152-4488-8845-7affca6bf092.config
2026-02-04T18:18:26.4597528Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-039288b9-4152-4488-8845-7affca6bf092.config
2026-02-04T18:18:26.4623765Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-04T18:18:26.4641659Z /github/runner_temp/git-credentials-039288b9-4152-4488-8845-7affca6bf092.config
2026-02-04T18:18:26.4647817Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-039288b9-4152-4488-8845-7affca6bf092.config
2026-02-04T18:18:26.4674684Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-04T18:18:26.4853766Z Removing credentials config '/__w/_temp/git-credentials-039288b9-4152-4488-8845-7affca6bf092.config'
2026-02-04T18:18:44.9876102Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-04T18:18:44.9876862Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-04T18:18:44.9877117Z ##[endgroup]
2026-02-04T18:18:45.3888879Z Cleaning up orphan processes
