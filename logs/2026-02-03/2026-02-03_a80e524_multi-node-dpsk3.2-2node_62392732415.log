# Run ID: 21638756264
# Commit: a80e524fbcaf58576ba7bfe56365a4259a24b692
# Job: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
# Date: 2026-02-03
============================================================

ï»¿2026-02-03T19:26:28.0111844Z Current runner version: '2.330.0'
2026-02-03T19:26:28.0116369Z Runner name: 'linux-aarch64-a3-0-wgt9d-runner-mv8gw'
2026-02-03T19:26:28.0117053Z Runner group name: 'Default'
2026-02-03T19:26:28.0117841Z Machine name: 'linux-aarch64-a3-0-wgt9d-runner-mv8gw'
2026-02-03T19:26:28.0121132Z ##[group]GITHUB_TOKEN Permissions
2026-02-03T19:26:28.0123262Z Actions: write
2026-02-03T19:26:28.0123685Z ArtifactMetadata: write
2026-02-03T19:26:28.0124137Z Attestations: write
2026-02-03T19:26:28.0124507Z Checks: write
2026-02-03T19:26:28.0124879Z Contents: write
2026-02-03T19:26:28.0125290Z Deployments: write
2026-02-03T19:26:28.0125646Z Discussions: write
2026-02-03T19:26:28.0126016Z Issues: write
2026-02-03T19:26:28.0126501Z Metadata: read
2026-02-03T19:26:28.0126880Z Models: read
2026-02-03T19:26:28.0127240Z Packages: write
2026-02-03T19:26:28.0127612Z Pages: write
2026-02-03T19:26:28.0127985Z PullRequests: write
2026-02-03T19:26:28.0128371Z RepositoryProjects: write
2026-02-03T19:26:28.0128912Z SecurityEvents: write
2026-02-03T19:26:28.0129345Z Statuses: write
2026-02-03T19:26:28.0129751Z ##[endgroup]
2026-02-03T19:26:28.0131549Z Secret source: Actions
2026-02-03T19:26:28.0132150Z Prepare workflow directory
2026-02-03T19:26:28.0684826Z Prepare all required actions
2026-02-03T19:26:28.0716557Z Getting action download info
2026-02-03T19:26:29.3101933Z Download action repository 'actions/checkout@v6' (SHA:de0fac2e4500dabe0009e67214ff5f5447ce83dd)
2026-02-03T19:26:33.7718252Z Download action repository 'actions/upload-artifact@v6' (SHA:b7c566a772e6b6bfb58ed0dc250532a479d7789f)
2026-02-03T19:26:40.9257640Z Uses: vllm-project/vllm-ascend/.github/workflows/_e2e_nightly_multi_node.yaml@refs/heads/main (a80e524fbcaf58576ba7bfe56365a4259a24b692)
2026-02-03T19:26:40.9260531Z ##[group] Inputs
2026-02-03T19:26:40.9260855Z   soc_version: a3
2026-02-03T19:26:40.9261112Z   runner: linux-aarch64-a3-0
2026-02-03T19:26:40.9261521Z   image: swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3
2026-02-03T19:26:40.9262185Z   config_file_path: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-03T19:26:40.9262482Z   replicas: 1
2026-02-03T19:26:40.9262697Z   size: 2
2026-02-03T19:26:40.9262924Z   vllm_version: v0.15.0
2026-02-03T19:26:40.9263251Z   vllm_ascend_remote_url: https://github.com/vllm-project/vllm-ascend.git
2026-02-03T19:26:40.9263595Z   vllm_ascend_ref: main
2026-02-03T19:26:40.9263813Z ##[endgroup]
2026-02-03T19:26:40.9264312Z Complete job name: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-03T19:26:40.9760300Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-03T19:26:40.9762866Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-03T19:26:40.9763524Z ##[endgroup]
2026-02-03T19:26:56.4476874Z (node:70) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-03T19:26:56.4477670Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-03T19:27:22.3476037Z ##[group]Run # Decode and save kubeconfig
2026-02-03T19:27:22.3476503Z [36;1m# Decode and save kubeconfig[0m
2026-02-03T19:27:22.3508888Z [36;1mecho "***" | base64 -d > "$KUBECONFIG"[0m
2026-02-03T19:27:22.3509583Z shell: bash -el {0}
2026-02-03T19:27:22.3509796Z ##[endgroup]
2026-02-03T19:27:22.3636358Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-03T19:27:22.3637253Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-03T19:27:22.3637531Z ##[endgroup]
2026-02-03T19:27:22.7234933Z (node:5682) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-03T19:27:22.7235849Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-03T19:27:58.0593554Z ##[group]Run actions/checkout@v6
2026-02-03T19:27:58.0593909Z with:
2026-02-03T19:27:58.0594128Z   repository: vllm-project/vllm-ascend
2026-02-03T19:27:58.0594933Z   token: ***
2026-02-03T19:27:58.0595173Z   ssh-strict: true
2026-02-03T19:27:58.0595459Z   ssh-user: git
2026-02-03T19:27:58.0595720Z   persist-credentials: true
2026-02-03T19:27:58.0595978Z   clean: true
2026-02-03T19:27:58.0596167Z   sparse-checkout-cone-mode: true
2026-02-03T19:27:58.0596460Z   fetch-depth: 1
2026-02-03T19:27:58.0596677Z   fetch-tags: false
2026-02-03T19:27:58.0596857Z   show-progress: true
2026-02-03T19:27:58.0597105Z   lfs: false
2026-02-03T19:27:58.0597285Z   submodules: false
2026-02-03T19:27:58.0597495Z   set-safe-directory: true
2026-02-03T19:27:58.0597826Z ##[endgroup]
2026-02-03T19:27:58.0653443Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-03T19:27:58.0654299Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-03T19:27:58.0654639Z ##[endgroup]
2026-02-03T19:27:58.4206044Z (node:6109) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-03T19:27:58.4206870Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-03T19:28:16.2193532Z Syncing repository: vllm-project/vllm-ascend
2026-02-03T19:28:16.2194611Z ##[group]Getting Git version info
2026-02-03T19:28:16.2194923Z Working directory is '/__w/vllm-ascend/vllm-ascend'
2026-02-03T19:28:16.2195389Z [command]/usr/bin/git version
2026-02-03T19:28:16.2195613Z git version 2.34.1
2026-02-03T19:28:16.2196891Z ##[endgroup]
2026-02-03T19:28:16.2199694Z Copying '/root/.gitconfig' to '/__w/_temp/b5fbb77e-9a1f-4412-89bb-59cccafe482d/.gitconfig'
2026-02-03T19:28:16.2213102Z Temporarily overriding HOME='/__w/_temp/b5fbb77e-9a1f-4412-89bb-59cccafe482d' before making global git config changes
2026-02-03T19:28:16.2213757Z Adding repository directory to the temporary git global config as a safe directory
2026-02-03T19:28:16.2215829Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-03T19:28:16.2249386Z Deleting the contents of '/__w/vllm-ascend/vllm-ascend'
2026-02-03T19:28:16.2252177Z ##[group]Initializing the repository
2026-02-03T19:28:16.2255820Z [command]/usr/bin/git init /__w/vllm-ascend/vllm-ascend
2026-02-03T19:28:16.2414095Z hint: Using 'master' as the name for the initial branch. This default branch name
2026-02-03T19:28:16.2414635Z hint: is subject to change. To configure the initial branch name to use in all
2026-02-03T19:28:16.2415125Z hint: of your new repositories, which will suppress this warning, call:
2026-02-03T19:28:16.2415434Z hint: 
2026-02-03T19:28:16.2415742Z hint: 	git config --global init.defaultBranch <name>
2026-02-03T19:28:16.2416131Z hint: 
2026-02-03T19:28:16.2416393Z hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and
2026-02-03T19:28:16.2416977Z hint: 'development'. The just-created branch can be renamed via this command:
2026-02-03T19:28:16.2417419Z hint: 
2026-02-03T19:28:16.2417629Z hint: 	git branch -m <name>
2026-02-03T19:28:16.2417963Z Initialized empty Git repository in /__w/vllm-ascend/vllm-ascend/.git/
2026-02-03T19:28:16.2418805Z [command]/usr/bin/git remote add origin https://github.com/vllm-project/vllm-ascend
2026-02-03T19:28:16.2419591Z ##[endgroup]
2026-02-03T19:28:16.2419966Z ##[group]Disabling automatic garbage collection
2026-02-03T19:28:16.2420394Z [command]/usr/bin/git config --local gc.auto 0
2026-02-03T19:28:16.2446012Z ##[endgroup]
2026-02-03T19:28:16.2446413Z ##[group]Setting up auth
2026-02-03T19:28:16.2446763Z Removing SSH command configuration
2026-02-03T19:28:16.2451283Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-03T19:28:16.2480320Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-03T19:28:16.2659540Z Removing HTTP extra header
2026-02-03T19:28:16.2662796Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-03T19:28:16.2686864Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-03T19:28:16.2858522Z Removing includeIf entries pointing to credentials config files
2026-02-03T19:28:16.2863381Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-03T19:28:16.2888508Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-03T19:28:16.3072658Z [command]/usr/bin/git config --file /__w/_temp/git-credentials-a2b6e355-4c2e-42f5-adf5-373dce437da2.config http.https://github.com/.extraheader AUTHORIZATION: basic ***
2026-02-03T19:28:16.3103140Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-a2b6e355-4c2e-42f5-adf5-373dce437da2.config
2026-02-03T19:28:16.3127724Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-a2b6e355-4c2e-42f5-adf5-373dce437da2.config
2026-02-03T19:28:16.3152862Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-a2b6e355-4c2e-42f5-adf5-373dce437da2.config
2026-02-03T19:28:16.3178638Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-a2b6e355-4c2e-42f5-adf5-373dce437da2.config
2026-02-03T19:28:16.3201594Z ##[endgroup]
2026-02-03T19:28:16.3201971Z ##[group]Fetching the repository
2026-02-03T19:28:16.3209018Z [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --no-recurse-submodules --depth=1 origin +a80e524fbcaf58576ba7bfe56365a4259a24b692:refs/remotes/origin/main
2026-02-03T19:28:18.0003675Z From https://gh-proxy.test.osinfra.cn/https://github.com/vllm-project/vllm-ascend
2026-02-03T19:28:18.0004427Z  * [new ref]         a80e524fbcaf58576ba7bfe56365a4259a24b692 -> origin/main
2026-02-03T19:28:18.0024948Z [command]/usr/bin/git branch --list --remote origin/main
2026-02-03T19:28:18.0048362Z   origin/main
2026-02-03T19:28:18.0055590Z [command]/usr/bin/git rev-parse refs/remotes/origin/main
2026-02-03T19:28:18.0073733Z a80e524fbcaf58576ba7bfe56365a4259a24b692
2026-02-03T19:28:18.0078058Z ##[endgroup]
2026-02-03T19:28:18.0078487Z ##[group]Determining the checkout info
2026-02-03T19:28:18.0079200Z ##[endgroup]
2026-02-03T19:28:18.0083117Z [command]/usr/bin/git sparse-checkout disable
2026-02-03T19:28:18.0128600Z [command]/usr/bin/git config --local --unset-all extensions.worktreeConfig
2026-02-03T19:28:18.0151043Z ##[group]Checking out the ref
2026-02-03T19:28:18.0154184Z [command]/usr/bin/git checkout --progress --force -B main refs/remotes/origin/main
2026-02-03T19:28:18.0992900Z Switched to a new branch 'main'
2026-02-03T19:28:18.0993232Z Branch 'main' set up to track remote branch 'main' from 'origin'.
2026-02-03T19:28:18.1003412Z ##[endgroup]
2026-02-03T19:28:18.1043066Z [command]/usr/bin/git log -1 --format=%H
2026-02-03T19:28:18.1062315Z a80e524fbcaf58576ba7bfe56365a4259a24b692
2026-02-03T19:28:35.7831741Z ##[group]Run # prepare for lws entrypoint scripts
2026-02-03T19:28:35.7832232Z [36;1m# prepare for lws entrypoint scripts[0m
2026-02-03T19:28:35.7832662Z [36;1minstall -D tests/e2e/nightly/multi_node/scripts/run.sh /root/.cache/tests/run.sh[0m
2026-02-03T19:28:35.7833179Z shell: bash -el {0}
2026-02-03T19:28:35.7833371Z ##[endgroup]
2026-02-03T19:28:35.7918904Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-03T19:28:35.7919774Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-03T19:28:35.7920026Z ##[endgroup]
2026-02-03T19:28:36.1444140Z (node:6697) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-03T19:28:36.1444922Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-03T19:29:11.8417252Z ##[group]Run set -euo pipefail
2026-02-03T19:29:11.8417601Z [36;1mset -euo pipefail[0m
2026-02-03T19:29:11.8417864Z [36;1m[0m
2026-02-03T19:29:11.8418085Z [36;1mCRD_NAME="${CRD_NAME:-vllm}"[0m
2026-02-03T19:29:11.8418312Z [36;1mTIMEOUT=${TIMEOUT:-120}[0m
2026-02-03T19:29:11.8418584Z [36;1mSLEEP_INTERVAL=2[0m
2026-02-03T19:29:11.8418782Z [36;1m[0m
2026-02-03T19:29:11.8419080Z [36;1mecho "Deleting leaderworkerset [$CRD_NAME] in namespace [$NAMESPACE]..."[0m
2026-02-03T19:29:11.8419590Z [36;1mkubectl delete leaderworkerset "$CRD_NAME" -n "$NAMESPACE" --ignore-not-found[0m
2026-02-03T19:29:11.8419931Z [36;1m[0m
2026-02-03T19:29:11.8420204Z [36;1mecho "Waiting for all pods starting with 'vllm' to be deleted..."[0m
2026-02-03T19:29:11.8420576Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-03T19:29:11.8420785Z [36;1m[0m
2026-02-03T19:29:11.8420979Z [36;1mwhile true; do[0m
2026-02-03T19:29:11.8421237Z [36;1m  NOW=$(date +%s)[0m
2026-02-03T19:29:11.8421554Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-03T19:29:11.8421822Z [36;1m[0m
2026-02-03T19:29:11.8422181Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-03T19:29:11.8422551Z [36;1m    echo "Timeout reached ($TIMEOUT seconds), some pods still exist:"[0m
2026-02-03T19:29:11.8422973Z [36;1m    kubectl get pods -n "$NAMESPACE" | grep '^vllm' || true[0m
2026-02-03T19:29:11.8423244Z [36;1m    exit 1[0m
2026-02-03T19:29:11.8423465Z [36;1m  fi[0m
2026-02-03T19:29:11.8423682Z [36;1m[0m
2026-02-03T19:29:11.8424098Z [36;1m  PODS_EXIST=$(kubectl get pods -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null | tr ' ' '\n' | grep '^vllm' || true)[0m
2026-02-03T19:29:11.8424590Z [36;1m[0m
2026-02-03T19:29:11.8424808Z [36;1m  if [[ -z "$PODS_EXIST" ]]; then[0m
2026-02-03T19:29:11.8425058Z [36;1m    echo "All vllm pods deleted."[0m
2026-02-03T19:29:11.8425396Z [36;1m    break[0m
2026-02-03T19:29:11.8425621Z [36;1m  else[0m
2026-02-03T19:29:11.8425868Z [36;1m    echo "Waiting for pods to be deleted: $PODS_EXIST"[0m
2026-02-03T19:29:11.8426188Z [36;1m    sleep $SLEEP_INTERVAL[0m
2026-02-03T19:29:11.8426442Z [36;1m  fi[0m
2026-02-03T19:29:11.8426622Z [36;1mdone[0m
2026-02-03T19:29:11.8426999Z shell: bash -el {0}
2026-02-03T19:29:11.8427260Z ##[endgroup]
2026-02-03T19:29:11.8516806Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-03T19:29:11.8517635Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-03T19:29:11.8517953Z ##[endgroup]
2026-02-03T19:29:12.1982668Z (node:7479) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-03T19:29:12.1984327Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-03T19:29:30.3861379Z Deleting leaderworkerset [vllm] in namespace [vllm-project]...
2026-02-03T19:29:30.7082945Z Waiting for all pods starting with 'vllm' to be deleted...
2026-02-03T19:29:30.7799277Z All vllm pods deleted.
2026-02-03T19:29:48.8739227Z ##[group]Run set -e
2026-02-03T19:29:48.8739492Z [36;1mset -e[0m
2026-02-03T19:29:48.8739635Z [36;1m[0m
2026-02-03T19:29:48.8739769Z [36;1msize="2"[0m
2026-02-03T19:29:48.8739907Z [36;1mreplicas="1"[0m
2026-02-03T19:29:48.8740221Z [36;1mimage="swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3"[0m
2026-02-03T19:29:48.8740641Z [36;1mconfig_file_path="DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-03T19:29:48.8740938Z [36;1mfail_tag=FAIL_TAG_"DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-03T19:29:48.8741209Z [36;1mecho "FAIL_TAG=${fail_tag}" >> "$GITHUB_ENV"[0m
2026-02-03T19:29:48.8741406Z [36;1m[0m
2026-02-03T19:29:48.8741613Z [36;1mrequired_params=("size" "replicas" "image" "config_file_path")[0m
2026-02-03T19:29:48.8741892Z [36;1mfor param in "${required_params[@]}"; do[0m
2026-02-03T19:29:48.8742259Z [36;1m  if [ -z "${!param}" ]; then[0m
2026-02-03T19:29:48.8742699Z [36;1m    echo "Error: Parameter '$param' is required but empty"[0m
2026-02-03T19:29:48.8742931Z [36;1m    exit 1[0m
2026-02-03T19:29:48.8743079Z [36;1m  fi[0m
2026-02-03T19:29:48.8743211Z [36;1mdone[0m
2026-02-03T19:29:48.8743335Z [36;1m[0m
2026-02-03T19:29:48.8743469Z [36;1mif [ "a3" = "a3" ]; then[0m
2026-02-03T19:29:48.8743647Z [36;1m  npu_per_node=16[0m
2026-02-03T19:29:48.8743902Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws.yaml.jinja2"[0m
2026-02-03T19:29:48.8744164Z [36;1melse[0m
2026-02-03T19:29:48.8744297Z [36;1m  npu_per_node=8[0m
2026-02-03T19:29:48.8744557Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws-a2.yaml.jinja2"[0m
2026-02-03T19:29:48.8744832Z [36;1mfi[0m
2026-02-03T19:29:48.8744951Z [36;1m[0m
2026-02-03T19:29:48.8745090Z [36;1mjinja2 $TEMPLATE_FILE \[0m
2026-02-03T19:29:48.8745384Z [36;1m  -D size="$size" \[0m
2026-02-03T19:29:48.8745557Z [36;1m  -D replicas="$replicas" \[0m
2026-02-03T19:29:48.8745749Z [36;1m  -D image="$image" \[0m
2026-02-03T19:29:48.8745962Z [36;1m  -D config_file_path="$config_file_path" \[0m
2026-02-03T19:29:48.8746184Z [36;1m  -D npu_per_node="$npu_per_node" \[0m
2026-02-03T19:29:48.8746388Z [36;1m  -D fail_tag="$fail_tag" \[0m
2026-02-03T19:29:48.8746563Z [36;1m  --outfile lws.yaml[0m
2026-02-03T19:29:48.8746722Z [36;1m[0m
2026-02-03T19:29:48.8746864Z [36;1mkubectl apply -f ./lws.yaml[0m
2026-02-03T19:29:48.8747176Z shell: bash -el {0}
2026-02-03T19:29:48.8747326Z ##[endgroup]
2026-02-03T19:29:48.8829692Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-03T19:29:48.8830643Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-03T19:29:48.8830852Z ##[endgroup]
2026-02-03T19:29:49.2339359Z (node:8445) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-03T19:29:49.2340049Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-03T19:30:08.3496605Z leaderworkerset.leaderworkerset.x-k8s.io/vllm created
2026-02-03T19:30:08.3700450Z service/vllm-leader created
2026-02-03T19:30:26.2921037Z ##[group]Run POD_PREFIX="${POD_PREFIX:-vllm-0}"
2026-02-03T19:30:26.2921509Z [36;1mPOD_PREFIX="${POD_PREFIX:-vllm-0}"[0m
2026-02-03T19:30:26.2921788Z [36;1mSIZE="2"[0m
2026-02-03T19:30:26.2922102Z [36;1mTIMEOUT=1200  # default timeout 20 minutes[0m
2026-02-03T19:30:26.2922426Z [36;1m[0m
2026-02-03T19:30:26.2922796Z [36;1mecho "Waiting for Pods in namespace [$NAMESPACE] to become Running and Ready (timeout ${TIMEOUT}s)..."[0m
2026-02-03T19:30:26.2923166Z [36;1m[0m
2026-02-03T19:30:26.2923399Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-03T19:30:26.2923628Z [36;1m[0m
2026-02-03T19:30:26.2923859Z [36;1mwhile true; do[0m
2026-02-03T19:30:26.2924116Z [36;1m  NOW=$(date +%s)[0m
2026-02-03T19:30:26.2924371Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-03T19:30:26.2924617Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-03T19:30:26.2924957Z [36;1m    echo "Timeout reached after ${ELAPSED}s"[0m
2026-02-03T19:30:26.2925415Z [36;1m    echo "Dumping pod status for debugging:"[0m
2026-02-03T19:30:26.2925726Z [36;1m    kubectl get pods -n "$NAMESPACE"[0m
2026-02-03T19:30:26.2926045Z [36;1m    kubectl describe pod "$LEADER_POD" -n "$NAMESPACE"[0m
2026-02-03T19:30:26.2926320Z [36;1m    exit 1[0m
2026-02-03T19:30:26.2926541Z [36;1m  fi[0m
2026-02-03T19:30:26.2926738Z [36;1m[0m
2026-02-03T19:30:26.2926984Z [36;1m  # 1) check follower pods[0m
2026-02-03T19:30:26.2927249Z [36;1m  ALL_FOLLOWERS_READY=true[0m
2026-02-03T19:30:26.2927504Z [36;1m  for ((i=1; i<SIZE; i++)); do[0m
2026-02-03T19:30:26.2927741Z [36;1m    POD="${POD_PREFIX}-${i}"[0m
2026-02-03T19:30:26.2928186Z [36;1m    PHASE=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-03T19:30:26.2928785Z [36;1m    READY=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-03T19:30:26.2929330Z [36;1m[0m
2026-02-03T19:30:26.2929591Z [36;1m    echo "Follower [$POD] phase=$PHASE ready=$READY"[0m
2026-02-03T19:30:26.2929888Z [36;1m[0m
2026-02-03T19:30:26.2930156Z [36;1m    if [[ "$PHASE" != "Running" || "$READY" != "true" ]]; then[0m
2026-02-03T19:30:26.2930497Z [36;1m      echo "Follower [$POD] not Ready yet..."[0m
2026-02-03T19:30:26.2930862Z [36;1m      ALL_FOLLOWERS_READY=false[0m
2026-02-03T19:30:26.2931086Z [36;1m      break[0m
2026-02-03T19:30:26.2931303Z [36;1m    fi[0m
2026-02-03T19:30:26.2931483Z [36;1m  done[0m
2026-02-03T19:30:26.2931684Z [36;1m[0m
2026-02-03T19:30:26.2931891Z [36;1m  # 2) check leader pod[0m
2026-02-03T19:30:26.2932420Z [36;1m  LEADER_PHASE=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-03T19:30:26.2933098Z [36;1m  LEADER_READY=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-03T19:30:26.2933569Z [36;1m[0m
2026-02-03T19:30:26.2933856Z [36;1m  echo "Leader [$LEADER_POD] phase=$LEADER_PHASE ready=$LEADER_READY"[0m
2026-02-03T19:30:26.2934150Z [36;1m[0m
2026-02-03T19:30:26.2934432Z [36;1m  if [[ "$LEADER_PHASE" != "Running" || "$LEADER_READY" != "true" ]]; then[0m
2026-02-03T19:30:26.2934748Z [36;1m    echo "Leader not Ready yet..."[0m
2026-02-03T19:30:26.2935071Z [36;1m    ALL_FOLLOWERS_READY=false[0m
2026-02-03T19:30:26.2935399Z [36;1m  fi[0m
2026-02-03T19:30:26.2935570Z [36;1m[0m
2026-02-03T19:30:26.2935803Z [36;1m  if [[ "$ALL_FOLLOWERS_READY" == "true" ]]; then[0m
2026-02-03T19:30:26.2936199Z [36;1m    echo "All follower pods and leader pod are Running and Ready â€” continuing."[0m
2026-02-03T19:30:26.2936536Z [36;1m    break[0m
2026-02-03T19:30:26.2936747Z [36;1m  fi[0m
2026-02-03T19:30:26.2936956Z [36;1m[0m
2026-02-03T19:30:26.2937117Z [36;1m  sleep 2[0m
2026-02-03T19:30:26.2937325Z [36;1mdone[0m
2026-02-03T19:30:26.2937671Z shell: bash -el {0}
2026-02-03T19:30:26.2937924Z env:
2026-02-03T19:30:26.2938320Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-03T19:30:26.2938577Z ##[endgroup]
2026-02-03T19:30:26.3010428Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-03T19:30:26.3011138Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-03T19:30:26.3011448Z ##[endgroup]
2026-02-03T19:30:26.6496305Z (node:9588) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-03T19:30:26.6496996Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-03T19:30:44.9740718Z Waiting for Pods in namespace [vllm-project] to become Running and Ready (timeout 1200s)...
2026-02-03T19:30:45.0994706Z Follower [vllm-0-1] phase=Running ready=true
2026-02-03T19:30:45.2230210Z Leader [vllm-0] phase=Running ready=true
2026-02-03T19:30:45.2230840Z All follower pods and leader pod are Running and Ready â€” continuing.
2026-02-03T19:31:03.3803598Z ##[group]Run set -euo pipefail
2026-02-03T19:31:03.3803856Z [36;1mset -euo pipefail[0m
2026-02-03T19:31:03.3804015Z [36;1m[0m
2026-02-03T19:31:03.3804153Z [36;1msize="2"[0m
2026-02-03T19:31:03.3804299Z [36;1mpids=()[0m
2026-02-03T19:31:03.3804428Z [36;1m[0m
2026-02-03T19:31:03.3804559Z [36;1mcleanup() {[0m
2026-02-03T19:31:03.3804745Z [36;1m  echo "Cleaning up background log streams..."[0m
2026-02-03T19:31:03.3804979Z [36;1m  for pid in "${pids[@]}"; do[0m
2026-02-03T19:31:03.3805182Z [36;1m    kill "$pid" 2>/dev/null || true[0m
2026-02-03T19:31:03.3805458Z [36;1m  done[0m
2026-02-03T19:31:03.3805599Z [36;1m}[0m
2026-02-03T19:31:03.3805743Z [36;1mtrap cleanup EXIT[0m
2026-02-03T19:31:03.3805897Z [36;1m[0m
2026-02-03T19:31:03.3806045Z [36;1mfor i in $(seq 1 $((size - 1))); do[0m
2026-02-03T19:31:03.3806242Z [36;1m  POD="vllm-0-${i}"[0m
2026-02-03T19:31:03.3806398Z [36;1m[0m
2026-02-03T19:31:03.3806756Z [36;1m  echo "==== Collecting logs from worker pod: $POD ===="[0m
2026-02-03T19:31:03.3807029Z [36;1m  kubectl logs -f "$POD" -n "$NAMESPACE" \[0m
2026-02-03T19:31:03.3807267Z [36;1m    > "/tmp/${POD}_logs.txt" 2>&1 &[0m
2026-02-03T19:31:03.3807443Z [36;1m[0m
2026-02-03T19:31:03.3807571Z [36;1m  pids+=($!)[0m
2026-02-03T19:31:03.3807724Z [36;1mdone[0m
2026-02-03T19:31:03.3807850Z [36;1m[0m
2026-02-03T19:31:03.3808046Z [36;1mecho "==== Streaming logs from leader pod: $LEADER_POD ===="[0m
2026-02-03T19:31:03.3808326Z [36;1mecho "Looking for logs containing: $FAIL_TAG"[0m
2026-02-03T19:31:03.3808528Z [36;1m[0m
2026-02-03T19:31:03.3808763Z [36;1mkubectl logs -f "$LEADER_POD" -n "$NAMESPACE" | while IFS= read -r line; do[0m
2026-02-03T19:31:03.3809056Z [36;1m  echo "$line"[0m
2026-02-03T19:31:03.3809240Z [36;1m  if echo "$line" | grep -q "$FAIL_TAG"; then[0m
2026-02-03T19:31:03.3809444Z [36;1m    exit 1[0m
2026-02-03T19:31:03.3809577Z [36;1m  fi[0m
2026-02-03T19:31:03.3809711Z [36;1mdone[0m
2026-02-03T19:31:03.3810002Z shell: bash -el {0}
2026-02-03T19:31:03.3810136Z env:
2026-02-03T19:31:03.3810325Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-03T19:31:03.3810552Z ##[endgroup]
2026-02-03T19:31:03.3886027Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-03T19:31:03.3886624Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-03T19:31:03.3886837Z ##[endgroup]
2026-02-03T19:31:03.7359362Z (node:10910) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-03T19:31:03.7360053Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-03T19:31:22.1623637Z ==== Collecting logs from worker pod: vllm-0-1 ====
2026-02-03T19:31:22.1623954Z ==== Streaming logs from leader pod: vllm-0 ====
2026-02-03T19:31:22.1624269Z Looking for logs containing: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-03T19:31:22.2469907Z ====> Check NPU info
2026-02-03T19:31:22.2514355Z +------------------------------------------------------------------------------------------------+
2026-02-03T19:31:22.2524726Z | npu-smi 25.5.0                   Version: 25.5.0                                               |
2026-02-03T19:31:22.2534568Z +---------------------------+---------------+----------------------------------------------------+
2026-02-03T19:31:22.2544284Z | NPU   Name                | Health        | Power(W)    Temp(C)           Hugepages-Usage(page)|
2026-02-03T19:31:22.2553786Z | Chip  Phy-ID              | Bus-Id        | AICore(%)   Memory-Usage(MB)  HBM-Usage(MB)        |
2026-02-03T19:31:22.2563806Z +===========================+===============+====================================================+
2026-02-03T19:31:22.2573321Z | 0     Ascend910           | OK            | 168.1       36                0    / 0             |
2026-02-03T19:31:22.2581333Z | 0     0                   | 0000:9D:00.0  | 0           0    / 0          3165 / 65536         |
2026-02-03T19:31:22.2591710Z +------------------------------------------------------------------------------------------------+
2026-02-03T19:31:22.2601337Z | 0     Ascend910           | OK            | -           36                0    / 0             |
2026-02-03T19:31:22.2610646Z | 1     1                   | 0000:9F:00.0  | 0           0    / 0          2886 / 65536         |
2026-02-03T19:31:22.2621006Z +===========================+===============+====================================================+
2026-02-03T19:31:22.2630205Z | 1     Ascend910           | OK            | 168.1       35                0    / 0             |
2026-02-03T19:31:22.2640481Z | 0     2                   | 0000:99:00.0  | 0           0    / 0          3152 / 65536         |
2026-02-03T19:31:22.2648834Z +------------------------------------------------------------------------------------------------+
2026-02-03T19:31:22.2657638Z | 1     Ascend910           | OK            | -           36                0    / 0             |
2026-02-03T19:31:22.2670645Z | 1     3                   | 0000:9B:00.0  | 0           0    / 0          2890 / 65536         |
2026-02-03T19:31:22.2677383Z +===========================+===============+====================================================+
2026-02-03T19:31:22.2686625Z | 2     Ascend910           | OK            | 166.8       37                0    / 0             |
2026-02-03T19:31:22.2695777Z | 0     4                   | 0000:95:00.0  | 0           0    / 0          3163 / 65536         |
2026-02-03T19:31:22.2705285Z +------------------------------------------------------------------------------------------------+
2026-02-03T19:31:22.2714747Z | 2     Ascend910           | OK            | -           36                0    / 0             |
2026-02-03T19:31:22.2724917Z | 1     5                   | 0000:97:00.0  | 0           0    / 0          2885 / 65536         |
2026-02-03T19:31:22.2734036Z +===========================+===============+====================================================+
2026-02-03T19:31:22.2743799Z | 3     Ascend910           | OK            | 155.5       36                0    / 0             |
2026-02-03T19:31:22.2753453Z | 0     6                   | 0000:91:00.0  | 0           0    / 0          3160 / 65536         |
2026-02-03T19:31:22.2773531Z +------------------------------------------------------------------------------------------------+
2026-02-03T19:31:22.2780138Z | 3     Ascend910           | OK            | -           34                0    / 0             |
2026-02-03T19:31:22.2783145Z | 1     7                   | 0000:93:00.0  | 0           0    / 0          2887 / 65536         |
2026-02-03T19:31:22.2793526Z +===========================+===============+====================================================+
2026-02-03T19:31:22.2801062Z | 4     Ascend910           | OK            | 163.2       36                0    / 0             |
2026-02-03T19:31:22.2810156Z | 0     8                   | 0000:8D:00.0  | 0           0    / 0          3165 / 65536         |
2026-02-03T19:31:22.2819516Z +------------------------------------------------------------------------------------------------+
2026-02-03T19:31:22.2828847Z | 4     Ascend910           | OK            | -           35                0    / 0             |
2026-02-03T19:31:22.2838926Z | 1     9                   | 0000:8F:00.0  | 0           0    / 0          2889 / 65536         |
2026-02-03T19:31:22.2848139Z +===========================+===============+====================================================+
2026-02-03T19:31:22.2857336Z | 5     Ascend910           | OK            | 169.9       37                0    / 0             |
2026-02-03T19:31:22.2866476Z | 0     10                  | 0000:89:00.0  | 0           0    / 0          3148 / 65536         |
2026-02-03T19:31:22.2876413Z +------------------------------------------------------------------------------------------------+
2026-02-03T19:31:22.2886092Z | 5     Ascend910           | OK            | -           37                0    / 0             |
2026-02-03T19:31:22.2895400Z | 1     11                  | 0000:8B:00.0  | 0           0    / 0          2898 / 65536         |
2026-02-03T19:31:22.2904923Z +===========================+===============+====================================================+
2026-02-03T19:31:22.2916105Z | 6     Ascend910           | OK            | 163.4       36                0    / 0             |
2026-02-03T19:31:22.2923965Z | 0     12                  | 0000:85:00.0  | 0           0    / 0          3162 / 65536         |
2026-02-03T19:31:22.2933132Z +------------------------------------------------------------------------------------------------+
2026-02-03T19:31:22.2942823Z | 6     Ascend910           | OK            | -           36                0    / 0             |
2026-02-03T19:31:22.2952334Z | 1     13                  | 0000:87:00.0  | 0           0    / 0          2885 / 65536         |
2026-02-03T19:31:22.2961665Z +===========================+===============+====================================================+
2026-02-03T19:31:22.2971327Z | 7     Ascend910           | OK            | 164.6       34                0    / 0             |
2026-02-03T19:31:22.2980578Z | 0     14                  | 0000:81:00.0  | 0           0    / 0          3146 / 65536         |
2026-02-03T19:31:22.2989809Z +------------------------------------------------------------------------------------------------+
2026-02-03T19:31:22.2999073Z | 7     Ascend910           | OK            | -           37                0    / 0             |
2026-02-03T19:31:22.3008314Z | 1     15                  | 0000:83:00.0  | 0           0    / 0          2897 / 65536         |
2026-02-03T19:31:22.3017452Z +===========================+===============+====================================================+
2026-02-03T19:31:22.3026546Z +---------------------------+---------------+----------------------------------------------------+
2026-02-03T19:31:22.3035877Z | NPU     Chip              | Process id    | Process name             | Process memory(MB)      |
2026-02-03T19:31:22.3045417Z +===========================+===============+====================================================+
2026-02-03T19:31:22.3054518Z | No running processes found in NPU 0                                                            |
2026-02-03T19:31:22.3063597Z +===========================+===============+====================================================+
2026-02-03T19:31:22.3072574Z | No running processes found in NPU 1                                                            |
2026-02-03T19:31:22.3082931Z +===========================+===============+====================================================+
2026-02-03T19:31:22.3092596Z | No running processes found in NPU 2                                                            |
2026-02-03T19:31:22.3101904Z +===========================+===============+====================================================+
2026-02-03T19:31:22.3111328Z | No running processes found in NPU 3                                                            |
2026-02-03T19:31:22.3120930Z +===========================+===============+====================================================+
2026-02-03T19:31:22.3130573Z | No running processes found in NPU 4                                                            |
2026-02-03T19:31:22.3139786Z +===========================+===============+====================================================+
2026-02-03T19:31:22.3149281Z | No running processes found in NPU 5                                                            |
2026-02-03T19:31:22.3158672Z +===========================+===============+====================================================+
2026-02-03T19:31:22.3168663Z | No running processes found in NPU 6                                                            |
2026-02-03T19:31:22.3176410Z +===========================+===============+====================================================+
2026-02-03T19:31:22.3185504Z | No running processes found in NPU 7                                                            |
2026-02-03T19:31:22.3195234Z +===========================+===============+====================================================+
2026-02-03T19:31:22.3204582Z package_name=Ascend-cann-toolkit
2026-02-03T19:31:22.3213514Z version=8.5.0
2026-02-03T19:31:22.3222655Z innerversion=V100R001C25SPC001B232
2026-02-03T19:31:22.3231967Z compatible_version=[V100R001C15],[V100R001C18],[V100R001C19],[V100R001C20],[V100R001C21],[V100R001C23]
2026-02-03T19:31:22.3241653Z arch=aarch64
2026-02-03T19:31:22.3250584Z os=linux
2026-02-03T19:31:22.3259958Z path=/usr/local/Ascend/cann-8.5.0
2026-02-03T19:31:22.3269050Z ====> Configure mirrors and git proxy
2026-02-03T19:31:22.3278962Z Writing to /root/.config/pip/pip.conf
2026-02-03T19:31:22.3288633Z Installed vLLM-related Python packages:
2026-02-03T19:31:22.3297921Z ais_bench_benchmark               3.0.20250930               /vllm-workspace/vllm-ascend/benchmark
2026-02-03T19:31:22.3306698Z vllm                              0.15.0+empty               /vllm-workspace/vllm
2026-02-03T19:31:22.3316692Z vllm_ascend                       0.14.0rc2.dev87+ga80e524fb /vllm-workspace/vllm-ascend
2026-02-03T19:31:22.3325971Z 
2026-02-03T19:31:22.3334528Z ============================
2026-02-03T19:31:22.3343770Z vLLM Git information
2026-02-03T19:31:22.3354136Z ============================
2026-02-03T19:31:22.3363028Z Branch:      HEAD
2026-02-03T19:31:22.3372238Z Commit hash: f176443446f659dbab5315e056e605d8984fd976
2026-02-03T19:31:22.3381656Z Author:      TJian <tunjian.tan@embeddedllm.com>
2026-02-03T19:31:22.3390885Z Date:        2026-01-29 14:45:42 +0800
2026-02-03T19:31:22.3399566Z Message:     [Release] [CI] Optim release pipeline (#33156)
2026-02-03T19:31:22.3409736Z Tags:        v0.15.0
2026-02-03T19:31:22.3421692Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm.git (fetch)
2026-02-03T19:31:22.3427272Z 
2026-02-03T19:31:22.3437256Z 
2026-02-03T19:31:22.3446738Z ============================
2026-02-03T19:31:22.3456060Z vLLM-Ascend Git information
2026-02-03T19:31:22.3466135Z ============================
2026-02-03T19:31:22.3475024Z Branch:      main
2026-02-03T19:31:22.3484175Z Commit hash: a80e524fbcaf58576ba7bfe56365a4259a24b692
2026-02-03T19:31:22.3493631Z Author:      dsxsteven <36877507+dsxsteven@users.noreply.github.com>
2026-02-03T19:31:22.3502166Z Date:        2026-02-03 19:49:58 +0800
2026-02-03T19:31:22.3512700Z Message:     [Quant] GLM4.7-Flash Support W8A8 (#6492)
2026-02-03T19:31:22.3521337Z Tags:        
2026-02-03T19:31:22.3530121Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm-ascend (fetch)
2026-02-03T19:31:22.3538763Z 
2026-02-03T19:31:22.3547878Z ====> Check triton ascend info
2026-02-03T19:31:22.3557035Z Ubuntu clang version 15.0.7
2026-02-03T19:31:22.3565685Z Target: aarch64-unknown-linux-gnu
2026-02-03T19:31:22.3574641Z Thread model: posix
2026-02-03T19:31:22.3584519Z InstalledDir: /usr/bin
2026-02-03T19:31:22.3593035Z Found candidate GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-03T19:31:22.3602158Z Selected GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-03T19:31:22.3610918Z Candidate multilib: .;@m64
2026-02-03T19:31:22.3619812Z Selected multilib: .;@m64
2026-02-03T19:31:22.3629019Z /usr/local/Ascend/ascend-toolkit/latest/bin/bishengir-compile
2026-02-03T19:31:22.3637956Z Name: triton-ascend
2026-02-03T19:31:22.3647088Z Version: 3.2.0
2026-02-03T19:31:22.3656306Z Summary: A language and compiler for custom Deep Learning operations on Ascend hardwares
2026-02-03T19:31:22.3665355Z Home-page: https://gitcode.com/Ascend/triton-ascend/
2026-02-03T19:31:22.3674367Z Author: 
2026-02-03T19:31:22.3683822Z Author-email: 
2026-02-03T19:31:22.3692483Z License: 
2026-02-03T19:31:22.3701604Z Location: /usr/local/python3.11.14/lib/python3.11/site-packages
2026-02-03T19:31:22.3710800Z Requires: 
2026-02-03T19:31:22.3720509Z Required-by: vllm_ascend
2026-02-03T19:31:22.3729169Z INFO 02-03 19:30:48 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:31:22.3738708Z INFO 02-03 19:30:48 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:31:22.3748080Z INFO 02-03 19:30:48 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:31:22.3757749Z INFO 02-03 19:30:48 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:31:22.3766760Z ============================= test session starts ==============================
2026-02-03T19:31:22.3776367Z platform linux -- Python 3.11.14, pytest-8.4.2, pluggy-1.6.0 -- /usr/local/python3.11.14/bin/python3
2026-02-03T19:31:22.3785576Z cachedir: .pytest_cache
2026-02-03T19:31:22.3794880Z rootdir: /vllm-workspace/vllm-ascend
2026-02-03T19:31:22.3804009Z configfile: pyproject.toml
2026-02-03T19:31:22.3814736Z plugins: asyncio-1.3.0, cov-7.0.0, mock-3.15.1, anyio-4.12.1
2026-02-03T19:31:22.3822951Z asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
2026-02-03T19:31:22.3831391Z collecting ... collected 1 item
2026-02-03T19:31:22.3840454Z 
2026-02-03T19:31:22.3850145Z [2026-02-03 19:30:55] INFO multi_node_config.py:294: Loading config yaml: tests/e2e/nightly/multi_node/config/DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-03T19:31:22.3859321Z [2026-02-03 19:30:55] INFO multi_node_config.py:351: Resolving cluster IPs via DNS...
2026-02-03T19:31:22.3870516Z [2026-02-03 19:30:55] INFO multi_node_config.py:212: Node 0 envs: {'HCCL_OP_EXPANSION_MODE': 'AIV', 'VLLM_USE_MODELSCOPE': 'True', 'HCCL_BUFFSIZE': '1024', 'SERVER_PORT': '8080', 'OMP_PROC_BIND': 'False', 'OMP_NUM_THREADS': '1', 'PYTORCH_NPU_ALLOC_CONF': 'expandable_segments:True', 'VLLM_ASCEND_ENABLE_FLASHCOMM1': '1', 'ASCEND_A3_EBA_ENABLE': '1', 'HCCL_IF_IP': '10.0.0.58', 'HCCL_SOCKET_IFNAME': 'eth0', 'GLOO_SOCKET_IFNAME': 'eth0', 'TP_SOCKET_IFNAME': 'eth0', 'LOCAL_IP': '10.0.0.58', 'NIC_NAME': 'eth0', 'MASTER_IP': '10.0.0.58'}
2026-02-03T19:31:22.3878760Z [2026-02-03 19:30:55] INFO multi_node_config.py:137: Not launching proxy on non-master node
2026-02-03T19:31:22.3891894Z [2026-02-03 19:30:55] INFO conftest.py:232: Starting server with command: vllm serve vllm-ascend/DeepSeek-V3.2-W8A8 --host 0.0.0.0 --port 8080 --data-parallel-size 4 --data-parallel-size-local 2 --data-parallel-address 10.0.0.58 --data-parallel-rpc-port 13399 --tensor-parallel-size 8 --quantization ascend --seed 1024 --enable-expert-parallel --max-num-seqs 16 --max-model-len 8192 --max-num-batched-tokens 4096 --no-enable-prefix-caching --gpu-memory-utilization 0.85 --trust-remote-code --speculative-config {"num_speculative_tokens": 2, "method":"deepseek_mtp"} --compilation-config {"cudagraph_capture_sizes": [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], "cudagraph_mode": "FULL_DECODE_ONLY"} --additional-config {"layer_sharding": ["q_b_proj", "o_proj"]} --tokenizer-mode deepseek_v32 --reasoning-parser deepseek_v3
2026-02-03T19:31:22.3903329Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node INFO 02-03 19:31:00 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:31:22.3907767Z INFO 02-03 19:31:00 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:31:22.3917493Z INFO 02-03 19:31:00 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:31:22.3926758Z INFO 02-03 19:31:00 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:31:22.3936207Z 2026-02-03 19:31:06,539 - 67 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:31:22.3945364Z INFO 02-03 19:31:06 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:31:22.3954221Z INFO 02-03 19:31:06 [serve.py:100] Defaulting api_server_count to data_parallel_size (4).
2026-02-03T19:31:22.3963619Z INFO 02-03 19:31:06 [utils.py:325] 
2026-02-03T19:31:22.3973590Z INFO 02-03 19:31:06 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
2026-02-03T19:31:22.3982451Z INFO 02-03 19:31:06 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
2026-02-03T19:31:22.3991014Z INFO 02-03 19:31:06 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   vllm-ascend/DeepSeek-V3.2-W8A8
2026-02-03T19:31:22.4000538Z INFO 02-03 19:31:06 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
2026-02-03T19:31:22.4009197Z INFO 02-03 19:31:06 [utils.py:325] 
2026-02-03T19:31:22.4027536Z INFO 02-03 19:31:06 [utils.py:261] non-default args: {'model_tag': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'api_server_count': 4, 'host': '0.0.0.0', 'port': 8080, 'model': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'tokenizer_mode': 'deepseek_v32', 'trust_remote_code': True, 'seed': 1024, 'max_model_len': 8192, 'quantization': 'ascend', 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 8, 'data_parallel_size': 4, 'data_parallel_size_local': 2, 'data_parallel_address': '10.0.0.58', 'data_parallel_rpc_port': 13399, 'enable_expert_parallel': True, 'gpu_memory_utilization': 0.85, 'enable_prefix_caching': False, 'max_num_batched_tokens': 4096, 'max_num_seqs': 16, 'speculative_config': {'num_speculative_tokens': 2, 'method': 'deepseek_mtp'}, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}, 'additional_config': {'layer_sharding': ['q_b_proj', 'o_proj']}}
2026-02-03T19:31:22.4032088Z 2026-02-03 19:31:06,843 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-03T19:31:22.4043137Z INFO 02-03 19:31:06 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-03T19:31:22.4051448Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:22.4060532Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:22.4069417Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:22.4079724Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-03T19:31:22.4088631Z INFO 02-03 19:31:06 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-03T19:31:22.4097184Z INFO 02-03 19:31:06 [model.py:1561] Using max model len 8192
2026-02-03T19:31:22.4106385Z WARNING 02-03 19:31:07 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-03T19:31:22.4115757Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:22.4124787Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:22.4134002Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-03T19:31:22.4143290Z INFO 02-03 19:31:16 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-03T19:31:22.4152305Z INFO 02-03 19:31:16 [model.py:1561] Using max model len 163840
2026-02-03T19:31:22.4162246Z WARNING 02-03 19:31:16 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-03T19:31:22.4172409Z INFO 02-03 19:31:16 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-03T19:31:22.4184661Z INFO 02-03 19:31:17 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-03T19:31:22.4194047Z INFO 02-03 19:31:17 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-03T19:31:22.4206598Z WARNING 02-03 19:31:17 [platform.py:707] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-03T19:31:22.4215010Z WARNING 02-03 19:31:17 [platform.py:748] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-03T19:31:22.4224502Z INFO 02-03 19:31:17 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:31:22.4233809Z WARNING 02-03 19:31:17 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-03T19:31:22.4243632Z INFO 02-03 19:31:17 [platform.py:310] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-03T19:31:22.4252258Z WARNING 02-03 19:31:17 [platform.py:327] [91m
2026-02-03T19:31:22.4261238Z WARNING 02-03 19:31:17 [platform.py:327]             **********************************************************************************
2026-02-03T19:31:22.4270053Z WARNING 02-03 19:31:17 [platform.py:327]             * WARNING: You have enabled the *full graph* feature.
2026-02-03T19:31:22.4279767Z WARNING 02-03 19:31:17 [platform.py:327]             * This is an early experimental stage and may involve various unknown issues.
2026-02-03T19:31:22.4289236Z WARNING 02-03 19:31:17 [platform.py:327]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-03T19:31:22.4298511Z WARNING 02-03 19:31:17 [platform.py:327]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-03T19:31:22.4307595Z WARNING 02-03 19:31:17 [platform.py:327]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-03T19:31:22.4316866Z WARNING 02-03 19:31:17 [platform.py:327]             * batch size for graph capture.
2026-02-03T19:31:22.4325731Z WARNING 02-03 19:31:17 [platform.py:327]             * For more details, please refer to:
2026-02-03T19:31:22.4334683Z WARNING 02-03 19:31:17 [platform.py:327]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-03T19:31:22.4343700Z WARNING 02-03 19:31:17 [platform.py:327]             **********************************************************************************[0m
2026-02-03T19:31:22.4352780Z WARNING 02-03 19:31:17 [platform.py:327]             
2026-02-03T19:31:22.4362643Z INFO 02-03 19:31:17 [platform.py:420] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-03T19:31:22.4371860Z INFO 02-03 19:31:17 [utils.py:851] Started DP Coordinator process (PID: 87)
2026-02-03T19:31:22.4381210Z INFO 02-03 19:31:21 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:31:22.4390673Z INFO 02-03 19:31:21 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:31:22.4400813Z INFO 02-03 19:31:21 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:31:22.4409631Z INFO 02-03 19:31:21 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:31:22.4418472Z INFO 02-03 19:31:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:31:22.4427320Z INFO 02-03 19:31:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:31:22.4436597Z INFO 02-03 19:31:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:31:22.4445811Z INFO 02-03 19:31:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:31:31.9047212Z INFO 02-03 19:31:31 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:31:31.9057631Z INFO 02-03 19:31:31 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:31:31.9068422Z INFO 02-03 19:31:31 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:31:31.9116422Z INFO 02-03 19:31:31 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:31:37.4381016Z INFO 02-03 19:31:37 [utils.py:218] Started 4 API server processes
2026-02-03T19:31:37.6321347Z [0;36m(EngineCore_DP0 pid=90)[0;0m 2026-02-03 19:31:37,630 - 90 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:31:37.6347847Z [0;36m(EngineCore_DP1 pid=109)[0;0m 2026-02-03 19:31:37,631 - 109 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:31:37.6369366Z [0;36m(EngineCore_DP0 pid=90)[0;0m INFO 02-03 19:31:37 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:31:37.6394713Z [0;36m(EngineCore_DP0 pid=90)[0;0m INFO 02-03 19:31:37 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', speculative_config=SpeculativeConfig(method='mtp', model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', num_spec_tokens=2), tokenizer='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', skip_tokenizer_init=False, tokenizer_mode=deepseek_v32, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=4, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=1024, served_model_name=/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [24, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 48, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
2026-02-03T19:31:37.6402607Z [0;36m(EngineCore_DP1 pid=109)[0;0m INFO 02-03 19:31:37 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:31:42.2589250Z INFO 02-03 19:31:42 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:31:42.2599414Z INFO 02-03 19:31:42 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:31:42.2610887Z INFO 02-03 19:31:42 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:31:42.2659310Z INFO 02-03 19:31:42 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:31:42.4340102Z INFO 02-03 19:31:42 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:31:42.4364437Z INFO 02-03 19:31:42 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:31:42.4373994Z INFO 02-03 19:31:42 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:31:42.4431601Z INFO 02-03 19:31:42 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:31:42.4879054Z INFO 02-03 19:31:42 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:31:42.4888014Z INFO 02-03 19:31:42 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:31:42.4897500Z INFO 02-03 19:31:42 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:31:42.4960734Z INFO 02-03 19:31:42 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:31:42.7624271Z INFO 02-03 19:31:42 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:31:42.7632179Z INFO 02-03 19:31:42 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:31:42.7641831Z INFO 02-03 19:31:42 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:31:42.7734525Z INFO 02-03 19:31:42 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:31:42.9314611Z INFO 02-03 19:31:42 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:31:42.9322887Z INFO 02-03 19:31:42 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:31:42.9332246Z INFO 02-03 19:31:42 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:31:42.9393248Z INFO 02-03 19:31:42 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:31:42.9914886Z INFO 02-03 19:31:42 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:31:42.9925660Z INFO 02-03 19:31:42 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:31:42.9935156Z INFO 02-03 19:31:42 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:31:42.9995445Z INFO 02-03 19:31:42 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:31:48.0948584Z 2026-02-03 19:31:48,092 - 138 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:31:48.0982133Z INFO 02-03 19:31:48 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:31:48.1816401Z [0;36m(ApiServer_1 pid=121)[0;0m 2026-02-03 19:31:48,180 - 121 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:31:48.1969988Z [0;36m(ApiServer_1 pid=121)[0;0m INFO 02-03 19:31:48 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:31:48.2242052Z [0;36m(ApiServer_1 pid=121)[0;0m 2026-02-03 19:31:48,222 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-03T19:31:48.2265302Z [0;36m(ApiServer_1 pid=121)[0;0m INFO 02-03 19:31:48 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-03T19:31:48.3256198Z [0;36m(ApiServer_1 pid=121)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:48.3277823Z [0;36m(ApiServer_1 pid=121)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:48.3299656Z [0;36m(ApiServer_1 pid=121)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:48.3310209Z [0;36m(ApiServer_1 pid=121)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-03T19:31:48.3413116Z [0;36m(ApiServer_1 pid=121)[0;0m INFO 02-03 19:31:48 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-03T19:31:48.3432411Z [0;36m(ApiServer_1 pid=121)[0;0m INFO 02-03 19:31:48 [model.py:1561] Using max model len 8192
2026-02-03T19:31:48.3582361Z 2026-02-03 19:31:48,357 - 137 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:31:48.3623118Z INFO 02-03 19:31:48 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:31:48.3833799Z [0;36m(ApiServer_3 pid=123)[0;0m 2026-02-03 19:31:48,382 - 123 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:31:48.3981675Z [0;36m(ApiServer_3 pid=123)[0;0m INFO 02-03 19:31:48 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:31:48.4158796Z [0;36m(ApiServer_3 pid=123)[0;0m 2026-02-03 19:31:48,414 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-03T19:31:48.4168668Z [0;36m(ApiServer_3 pid=123)[0;0m INFO 02-03 19:31:48 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-03T19:31:48.4534084Z [0;36m(ApiServer_1 pid=121)[0;0m WARNING 02-03 19:31:48 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-03T19:31:48.4554475Z [0;36m(ApiServer_1 pid=121)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:48.4564926Z [0;36m(ApiServer_1 pid=121)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:48.4575002Z [0;36m(ApiServer_1 pid=121)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-03T19:31:48.4647810Z [0;36m(ApiServer_1 pid=121)[0;0m INFO 02-03 19:31:48 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-03T19:31:48.4669509Z [0;36m(ApiServer_1 pid=121)[0;0m INFO 02-03 19:31:48 [model.py:1561] Using max model len 163840
2026-02-03T19:31:48.4679675Z [0;36m(ApiServer_1 pid=121)[0;0m WARNING 02-03 19:31:48 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-03T19:31:48.4689318Z [0;36m(ApiServer_1 pid=121)[0;0m INFO 02-03 19:31:48 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-03T19:31:48.5170649Z [0;36m(ApiServer_3 pid=123)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:48.5185776Z [0;36m(ApiServer_3 pid=123)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:48.5205765Z [0;36m(ApiServer_3 pid=123)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:48.5216105Z [0;36m(ApiServer_3 pid=123)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-03T19:31:48.5263174Z [0;36m(ApiServer_3 pid=123)[0;0m INFO 02-03 19:31:48 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-03T19:31:48.5281855Z [0;36m(ApiServer_3 pid=123)[0;0m INFO 02-03 19:31:48 [model.py:1561] Using max model len 8192
2026-02-03T19:31:48.5870898Z [0;36m(ApiServer_1 pid=121)[0;0m INFO 02-03 19:31:48 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-03T19:31:48.5880184Z [0;36m(ApiServer_1 pid=121)[0;0m INFO 02-03 19:31:48 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-03T19:31:48.5891854Z [0;36m(ApiServer_1 pid=121)[0;0m WARNING 02-03 19:31:48 [platform.py:707] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-03T19:31:48.5901300Z [0;36m(ApiServer_1 pid=121)[0;0m WARNING 02-03 19:31:48 [platform.py:748] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-03T19:31:48.5911090Z [0;36m(ApiServer_1 pid=121)[0;0m INFO 02-03 19:31:48 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:31:48.5920576Z [0;36m(ApiServer_1 pid=121)[0;0m WARNING 02-03 19:31:48 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-03T19:31:48.5930364Z [0;36m(ApiServer_1 pid=121)[0;0m INFO 02-03 19:31:48 [platform.py:310] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-03T19:31:48.5939495Z [0;36m(ApiServer_1 pid=121)[0;0m WARNING 02-03 19:31:48 [platform.py:327] [91m
2026-02-03T19:31:48.5949356Z [0;36m(ApiServer_1 pid=121)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             **********************************************************************************
2026-02-03T19:31:48.5959145Z [0;36m(ApiServer_1 pid=121)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * WARNING: You have enabled the *full graph* feature.
2026-02-03T19:31:48.5969135Z [0;36m(ApiServer_1 pid=121)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * This is an early experimental stage and may involve various unknown issues.
2026-02-03T19:31:48.5978438Z [0;36m(ApiServer_1 pid=121)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-03T19:31:48.5988060Z [0;36m(ApiServer_1 pid=121)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-03T19:31:48.5997804Z [0;36m(ApiServer_1 pid=121)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-03T19:31:48.6007321Z [0;36m(ApiServer_1 pid=121)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * batch size for graph capture.
2026-02-03T19:31:48.6017167Z [0;36m(ApiServer_1 pid=121)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * For more details, please refer to:
2026-02-03T19:31:48.6027049Z [0;36m(ApiServer_1 pid=121)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-03T19:31:48.6036773Z [0;36m(ApiServer_1 pid=121)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             **********************************************************************************[0m
2026-02-03T19:31:48.6046256Z [0;36m(ApiServer_1 pid=121)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             
2026-02-03T19:31:48.6055320Z [0;36m(ApiServer_1 pid=121)[0;0m INFO 02-03 19:31:48 [platform.py:420] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-03T19:31:48.6064841Z [0;36m(ApiServer_2 pid=122)[0;0m 2026-02-03 19:31:48,590 - 122 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:31:48.6076713Z [0;36m(ApiServer_2 pid=122)[0;0m INFO 02-03 19:31:48 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:31:48.6262408Z [0;36m(ApiServer_2 pid=122)[0;0m 2026-02-03 19:31:48,625 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-03T19:31:48.6272161Z [0;36m(ApiServer_2 pid=122)[0;0m INFO 02-03 19:31:48 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-03T19:31:48.6414623Z [0;36m(ApiServer_3 pid=123)[0;0m WARNING 02-03 19:31:48 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-03T19:31:48.6435304Z [0;36m(ApiServer_3 pid=123)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:48.6446063Z [0;36m(ApiServer_3 pid=123)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:48.6456555Z [0;36m(ApiServer_3 pid=123)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-03T19:31:48.6506053Z [0;36m(ApiServer_3 pid=123)[0;0m INFO 02-03 19:31:48 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-03T19:31:48.6524711Z [0;36m(ApiServer_3 pid=123)[0;0m INFO 02-03 19:31:48 [model.py:1561] Using max model len 163840
2026-02-03T19:31:48.6535196Z [0;36m(ApiServer_3 pid=123)[0;0m WARNING 02-03 19:31:48 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-03T19:31:48.6544802Z [0;36m(ApiServer_3 pid=123)[0;0m INFO 02-03 19:31:48 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-03T19:31:48.7311085Z [0;36m(ApiServer_2 pid=122)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:48.7319948Z [0;36m(ApiServer_2 pid=122)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:48.7329554Z [0;36m(ApiServer_2 pid=122)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:48.7339548Z [0;36m(ApiServer_2 pid=122)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-03T19:31:48.7378704Z [0;36m(ApiServer_2 pid=122)[0;0m INFO 02-03 19:31:48 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-03T19:31:48.7403478Z [0;36m(ApiServer_2 pid=122)[0;0m INFO 02-03 19:31:48 [model.py:1561] Using max model len 8192
2026-02-03T19:31:48.7693347Z [0;36m(ApiServer_3 pid=123)[0;0m INFO 02-03 19:31:48 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-03T19:31:48.7702429Z [0;36m(ApiServer_3 pid=123)[0;0m INFO 02-03 19:31:48 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-03T19:31:48.7791471Z [0;36m(ApiServer_3 pid=123)[0;0m WARNING 02-03 19:31:48 [platform.py:707] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-03T19:31:48.7792607Z [0;36m(ApiServer_3 pid=123)[0;0m WARNING 02-03 19:31:48 [platform.py:748] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-03T19:31:48.7793835Z [0;36m(ApiServer_3 pid=123)[0;0m INFO 02-03 19:31:48 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:31:48.7795134Z [0;36m(ApiServer_3 pid=123)[0;0m WARNING 02-03 19:31:48 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-03T19:31:48.7796021Z [0;36m(ApiServer_3 pid=123)[0;0m INFO 02-03 19:31:48 [platform.py:310] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-03T19:31:48.7796607Z [0;36m(ApiServer_3 pid=123)[0;0m WARNING 02-03 19:31:48 [platform.py:327] [91m
2026-02-03T19:31:48.7805365Z [0;36m(ApiServer_3 pid=123)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             **********************************************************************************
2026-02-03T19:31:48.7814988Z [0;36m(ApiServer_3 pid=123)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * WARNING: You have enabled the *full graph* feature.
2026-02-03T19:31:48.7825342Z [0;36m(ApiServer_3 pid=123)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * This is an early experimental stage and may involve various unknown issues.
2026-02-03T19:31:48.7834823Z [0;36m(ApiServer_3 pid=123)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-03T19:31:48.7845037Z [0;36m(ApiServer_3 pid=123)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-03T19:31:48.7854575Z [0;36m(ApiServer_3 pid=123)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-03T19:31:48.7864118Z [0;36m(ApiServer_3 pid=123)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * batch size for graph capture.
2026-02-03T19:31:48.7873881Z [0;36m(ApiServer_3 pid=123)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * For more details, please refer to:
2026-02-03T19:31:48.7884418Z [0;36m(ApiServer_3 pid=123)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-03T19:31:48.7894210Z [0;36m(ApiServer_3 pid=123)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             **********************************************************************************[0m
2026-02-03T19:31:48.7903486Z [0;36m(ApiServer_3 pid=123)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             
2026-02-03T19:31:48.7912515Z [0;36m(ApiServer_3 pid=123)[0;0m INFO 02-03 19:31:48 [platform.py:420] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-03T19:31:48.8466605Z [0;36m(ApiServer_2 pid=122)[0;0m WARNING 02-03 19:31:48 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-03T19:31:48.8489393Z [0;36m(ApiServer_2 pid=122)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:48.8499369Z [0;36m(ApiServer_2 pid=122)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:48.8509511Z [0;36m(ApiServer_2 pid=122)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-03T19:31:48.8593971Z [0;36m(ApiServer_2 pid=122)[0;0m INFO 02-03 19:31:48 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-03T19:31:48.8604427Z [0;36m(ApiServer_2 pid=122)[0;0m INFO 02-03 19:31:48 [model.py:1561] Using max model len 163840
2026-02-03T19:31:48.8614540Z [0;36m(ApiServer_2 pid=122)[0;0m WARNING 02-03 19:31:48 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-03T19:31:48.8623426Z [0;36m(ApiServer_2 pid=122)[0;0m INFO 02-03 19:31:48 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-03T19:31:48.9768243Z [0;36m(ApiServer_2 pid=122)[0;0m INFO 02-03 19:31:48 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-03T19:31:48.9777939Z [0;36m(ApiServer_2 pid=122)[0;0m INFO 02-03 19:31:48 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-03T19:31:48.9787277Z [0;36m(ApiServer_2 pid=122)[0;0m WARNING 02-03 19:31:48 [platform.py:707] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-03T19:31:48.9796605Z [0;36m(ApiServer_2 pid=122)[0;0m WARNING 02-03 19:31:48 [platform.py:748] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-03T19:31:48.9807061Z [0;36m(ApiServer_2 pid=122)[0;0m INFO 02-03 19:31:48 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:31:48.9817130Z [0;36m(ApiServer_2 pid=122)[0;0m WARNING 02-03 19:31:48 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-03T19:31:48.9826083Z [0;36m(ApiServer_2 pid=122)[0;0m INFO 02-03 19:31:48 [platform.py:310] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-03T19:31:48.9835156Z [0;36m(ApiServer_2 pid=122)[0;0m WARNING 02-03 19:31:48 [platform.py:327] [91m
2026-02-03T19:31:48.9844497Z [0;36m(ApiServer_2 pid=122)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             **********************************************************************************
2026-02-03T19:31:48.9854608Z [0;36m(ApiServer_2 pid=122)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * WARNING: You have enabled the *full graph* feature.
2026-02-03T19:31:48.9864423Z [0;36m(ApiServer_2 pid=122)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * This is an early experimental stage and may involve various unknown issues.
2026-02-03T19:31:48.9873058Z [0;36m(ApiServer_2 pid=122)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-03T19:31:48.9882570Z [0;36m(ApiServer_2 pid=122)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-03T19:31:48.9892660Z [0;36m(ApiServer_2 pid=122)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-03T19:31:48.9902239Z [0;36m(ApiServer_2 pid=122)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * batch size for graph capture.
2026-02-03T19:31:48.9912381Z [0;36m(ApiServer_2 pid=122)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * For more details, please refer to:
2026-02-03T19:31:48.9922918Z [0;36m(ApiServer_2 pid=122)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-03T19:31:48.9932331Z [0;36m(ApiServer_2 pid=122)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             **********************************************************************************[0m
2026-02-03T19:31:48.9941668Z [0;36m(ApiServer_2 pid=122)[0;0m WARNING 02-03 19:31:48 [platform.py:327]             
2026-02-03T19:31:48.9950702Z [0;36m(ApiServer_2 pid=122)[0;0m INFO 02-03 19:31:48 [platform.py:420] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-03T19:31:49.0735662Z [0;36m(ApiServer_0 pid=120)[0;0m 2026-02-03 19:31:49,072 - 120 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:31:49.0892822Z [0;36m(ApiServer_0 pid=120)[0;0m INFO 02-03 19:31:49 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:31:49.1050208Z [0;36m(ApiServer_0 pid=120)[0;0m 2026-02-03 19:31:49,103 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-03T19:31:49.1069741Z [0;36m(ApiServer_0 pid=120)[0;0m INFO 02-03 19:31:49 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-03T19:31:49.2083872Z [0;36m(ApiServer_0 pid=120)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:49.2105778Z [0;36m(ApiServer_0 pid=120)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:49.2122446Z [0;36m(ApiServer_0 pid=120)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:49.2132970Z [0;36m(ApiServer_0 pid=120)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-03T19:31:49.2184670Z [0;36m(ApiServer_0 pid=120)[0;0m INFO 02-03 19:31:49 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-03T19:31:49.2202704Z [0;36m(ApiServer_0 pid=120)[0;0m INFO 02-03 19:31:49 [model.py:1561] Using max model len 8192
2026-02-03T19:31:49.3363517Z [0;36m(ApiServer_0 pid=120)[0;0m WARNING 02-03 19:31:49 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-03T19:31:49.3385796Z [0;36m(ApiServer_0 pid=120)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:49.3394919Z [0;36m(ApiServer_0 pid=120)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-03T19:31:49.3404990Z [0;36m(ApiServer_0 pid=120)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-03T19:31:49.3447799Z [0;36m(ApiServer_0 pid=120)[0;0m INFO 02-03 19:31:49 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-03T19:31:49.3471153Z [0;36m(ApiServer_0 pid=120)[0;0m INFO 02-03 19:31:49 [model.py:1561] Using max model len 163840
2026-02-03T19:31:49.3480469Z [0;36m(ApiServer_0 pid=120)[0;0m WARNING 02-03 19:31:49 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-03T19:31:49.3489589Z [0;36m(ApiServer_0 pid=120)[0;0m INFO 02-03 19:31:49 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-03T19:31:49.4622273Z [0;36m(ApiServer_0 pid=120)[0;0m INFO 02-03 19:31:49 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-03T19:31:49.4631933Z [0;36m(ApiServer_0 pid=120)[0;0m INFO 02-03 19:31:49 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-03T19:31:49.4644897Z [0;36m(ApiServer_0 pid=120)[0;0m WARNING 02-03 19:31:49 [platform.py:707] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-03T19:31:49.4653980Z [0;36m(ApiServer_0 pid=120)[0;0m WARNING 02-03 19:31:49 [platform.py:748] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-03T19:31:49.4664022Z [0;36m(ApiServer_0 pid=120)[0;0m INFO 02-03 19:31:49 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:31:49.4672728Z [0;36m(ApiServer_0 pid=120)[0;0m WARNING 02-03 19:31:49 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-03T19:31:49.4682808Z [0;36m(ApiServer_0 pid=120)[0;0m INFO 02-03 19:31:49 [platform.py:310] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-03T19:31:49.4691160Z [0;36m(ApiServer_0 pid=120)[0;0m WARNING 02-03 19:31:49 [platform.py:327] [91m
2026-02-03T19:31:49.4700911Z [0;36m(ApiServer_0 pid=120)[0;0m WARNING 02-03 19:31:49 [platform.py:327]             **********************************************************************************
2026-02-03T19:31:49.4710224Z [0;36m(ApiServer_0 pid=120)[0;0m WARNING 02-03 19:31:49 [platform.py:327]             * WARNING: You have enabled the *full graph* feature.
2026-02-03T19:31:49.4719411Z [0;36m(ApiServer_0 pid=120)[0;0m WARNING 02-03 19:31:49 [platform.py:327]             * This is an early experimental stage and may involve various unknown issues.
2026-02-03T19:31:49.4728390Z [0;36m(ApiServer_0 pid=120)[0;0m WARNING 02-03 19:31:49 [platform.py:327]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-03T19:31:49.4737943Z [0;36m(ApiServer_0 pid=120)[0;0m WARNING 02-03 19:31:49 [platform.py:327]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-03T19:31:49.4746724Z [0;36m(ApiServer_0 pid=120)[0;0m WARNING 02-03 19:31:49 [platform.py:327]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-03T19:31:49.4756374Z [0;36m(ApiServer_0 pid=120)[0;0m WARNING 02-03 19:31:49 [platform.py:327]             * batch size for graph capture.
2026-02-03T19:31:49.4765888Z [0;36m(ApiServer_0 pid=120)[0;0m WARNING 02-03 19:31:49 [platform.py:327]             * For more details, please refer to:
2026-02-03T19:31:49.4776169Z [0;36m(ApiServer_0 pid=120)[0;0m WARNING 02-03 19:31:49 [platform.py:327]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-03T19:31:49.4784690Z [0;36m(ApiServer_0 pid=120)[0;0m WARNING 02-03 19:31:49 [platform.py:327]             **********************************************************************************[0m
2026-02-03T19:31:49.4793599Z [0;36m(ApiServer_0 pid=120)[0;0m WARNING 02-03 19:31:49 [platform.py:327]             
2026-02-03T19:31:49.4803455Z [0;36m(ApiServer_0 pid=120)[0;0m INFO 02-03 19:31:49 [platform.py:420] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-03T19:31:52.4373334Z INFO 02-03 19:31:52 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:31:52.4382189Z INFO 02-03 19:31:52 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:31:52.4412536Z INFO 02-03 19:31:52 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:31:52.4471619Z INFO 02-03 19:31:52 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:31:52.7209915Z INFO 02-03 19:31:52 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:31:52.7218835Z INFO 02-03 19:31:52 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:31:52.7227137Z INFO 02-03 19:31:52 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:31:52.7283605Z INFO 02-03 19:31:52 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:31:53.3363571Z INFO 02-03 19:31:53 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:31:53.3392711Z INFO 02-03 19:31:53 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:31:53.3766304Z INFO 02-03 19:31:53 [parallel_state.py:1212] world_size=32 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.58:48981 backend=hccl
2026-02-03T19:31:53.3774363Z INFO 02-03 19:31:53 [parallel_state.py:1212] world_size=32 rank=8 local_rank=0 distributed_init_method=tcp://10.0.0.58:48981 backend=hccl
2026-02-03T19:31:57.3355777Z 2026-02-03 19:31:57,333 - 187 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:31:57.3389099Z INFO 02-03 19:31:57 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:31:57.6493209Z 2026-02-03 19:31:57,647 - 190 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:31:57.6529725Z INFO 02-03 19:31:57 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:32:00.3205586Z INFO 02-03 19:32:00 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:32:00.6816573Z INFO 02-03 19:32:00 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:32:00.7651915Z INFO 02-03 19:32:00 [parallel_state.py:1212] world_size=32 rank=1 local_rank=1 distributed_init_method=tcp://10.0.0.58:48981 backend=hccl
2026-02-03T19:32:01.1062634Z INFO 02-03 19:32:01 [parallel_state.py:1212] world_size=32 rank=9 local_rank=1 distributed_init_method=tcp://10.0.0.58:48981 backend=hccl
2026-02-03T19:32:01.7372640Z INFO 02-03 19:32:01 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:32:01.7381205Z INFO 02-03 19:32:01 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:32:01.7404673Z INFO 02-03 19:32:01 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:32:01.7445864Z INFO 02-03 19:32:01 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:32:02.2439317Z INFO 02-03 19:32:02 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:32:02.2448719Z INFO 02-03 19:32:02 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:32:02.2458886Z INFO 02-03 19:32:02 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:32:02.2513967Z INFO 02-03 19:32:02 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:32:06.6002551Z 2026-02-03 19:32:06,598 - 305 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:32:06.6034391Z INFO 02-03 19:32:06 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:32:07.3806294Z 2026-02-03 19:32:07,378 - 308 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:32:07.3841661Z INFO 02-03 19:32:07 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:32:09.5993527Z INFO 02-03 19:32:09 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:32:10.0719703Z INFO 02-03 19:32:10 [parallel_state.py:1212] world_size=32 rank=2 local_rank=2 distributed_init_method=tcp://10.0.0.58:48981 backend=hccl
2026-02-03T19:32:10.4131399Z INFO 02-03 19:32:10 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:32:10.8500535Z INFO 02-03 19:32:10 [parallel_state.py:1212] world_size=32 rank=10 local_rank=2 distributed_init_method=tcp://10.0.0.58:48981 backend=hccl
2026-02-03T19:32:11.0906214Z INFO 02-03 19:32:11 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:32:11.0915226Z INFO 02-03 19:32:11 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:32:11.0925048Z INFO 02-03 19:32:11 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:32:11.0979467Z INFO 02-03 19:32:11 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:32:11.9116522Z INFO 02-03 19:32:11 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:32:11.9126537Z INFO 02-03 19:32:11 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:32:11.9138364Z INFO 02-03 19:32:11 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:32:11.9147795Z INFO 02-03 19:32:11 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:32:16.1502419Z 2026-02-03 19:32:16,148 - 409 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:32:16.1535243Z INFO 02-03 19:32:16 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:32:16.9211223Z 2026-02-03 19:32:16,919 - 412 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:32:16.9247795Z INFO 02-03 19:32:16 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:32:19.1605848Z INFO 02-03 19:32:19 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:32:19.6080448Z INFO 02-03 19:32:19 [parallel_state.py:1212] world_size=32 rank=3 local_rank=3 distributed_init_method=tcp://10.0.0.58:48981 backend=hccl
2026-02-03T19:32:19.9446575Z INFO 02-03 19:32:19 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:32:20.3767350Z INFO 02-03 19:32:20 [parallel_state.py:1212] world_size=32 rank=11 local_rank=3 distributed_init_method=tcp://10.0.0.58:48981 backend=hccl
2026-02-03T19:32:20.5771418Z INFO 02-03 19:32:20 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:32:20.5779748Z INFO 02-03 19:32:20 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:32:20.5789315Z INFO 02-03 19:32:20 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:32:20.5844839Z INFO 02-03 19:32:20 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:32:21.4642248Z INFO 02-03 19:32:21 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:32:21.4651650Z INFO 02-03 19:32:21 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:32:21.4659080Z INFO 02-03 19:32:21 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:32:21.4722372Z INFO 02-03 19:32:21 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:32:25.6062838Z 2026-02-03 19:32:25,604 - 513 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:32:25.6138996Z INFO 02-03 19:32:25 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:32:26.4768100Z 2026-02-03 19:32:26,475 - 516 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:32:26.4803917Z INFO 02-03 19:32:26 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:32:28.9121440Z INFO 02-03 19:32:28 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:32:29.3323680Z INFO 02-03 19:32:29 [parallel_state.py:1212] world_size=32 rank=4 local_rank=4 distributed_init_method=tcp://10.0.0.58:48981 backend=hccl
2026-02-03T19:32:29.4748892Z INFO 02-03 19:32:29 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:32:29.8314024Z INFO 02-03 19:32:29 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:32:29.8321933Z INFO 02-03 19:32:29 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:32:29.8331170Z INFO 02-03 19:32:29 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:32:29.8386949Z INFO 02-03 19:32:29 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:32:29.9072886Z INFO 02-03 19:32:29 [parallel_state.py:1212] world_size=32 rank=12 local_rank=4 distributed_init_method=tcp://10.0.0.58:48981 backend=hccl
2026-02-03T19:32:30.9963765Z INFO 02-03 19:32:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:32:30.9973578Z INFO 02-03 19:32:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:32:30.9984140Z INFO 02-03 19:32:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:32:31.0034431Z INFO 02-03 19:32:31 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:32:34.6869130Z 2026-02-03 19:32:34,685 - 617 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:32:34.6928472Z INFO 02-03 19:32:34 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:32:35.9796423Z 2026-02-03 19:32:35,978 - 620 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:32:35.9877688Z INFO 02-03 19:32:35 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:32:37.7564791Z INFO 02-03 19:32:37 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:32:38.1920312Z INFO 02-03 19:32:38 [parallel_state.py:1212] world_size=32 rank=5 local_rank=5 distributed_init_method=tcp://10.0.0.58:48981 backend=hccl
2026-02-03T19:32:38.9937135Z INFO 02-03 19:32:38 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:32:39.1954434Z INFO 02-03 19:32:39 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:32:39.1963174Z INFO 02-03 19:32:39 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:32:39.1973951Z INFO 02-03 19:32:39 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:32:39.2031141Z INFO 02-03 19:32:39 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:32:39.4394650Z INFO 02-03 19:32:39 [parallel_state.py:1212] world_size=32 rank=13 local_rank=5 distributed_init_method=tcp://10.0.0.58:48981 backend=hccl
2026-02-03T19:32:40.5973442Z INFO 02-03 19:32:40 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:32:40.5981312Z INFO 02-03 19:32:40 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:32:40.5991883Z INFO 02-03 19:32:40 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:32:40.6045966Z INFO 02-03 19:32:40 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:32:44.2306524Z 2026-02-03 19:32:44,228 - 721 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:32:44.2339271Z INFO 02-03 19:32:44 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:32:45.6300276Z 2026-02-03 19:32:45,628 - 725 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:32:45.6333168Z INFO 02-03 19:32:45 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:32:47.1706849Z INFO 02-03 19:32:47 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:32:47.6154699Z INFO 02-03 19:32:47 [parallel_state.py:1212] world_size=32 rank=6 local_rank=6 distributed_init_method=tcp://10.0.0.58:48981 backend=hccl
2026-02-03T19:32:48.6906498Z INFO 02-03 19:32:48 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:32:48.7842691Z INFO 02-03 19:32:48 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:32:48.7851714Z INFO 02-03 19:32:48 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:32:48.7862326Z INFO 02-03 19:32:48 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:32:48.7918490Z INFO 02-03 19:32:48 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:32:49.1226061Z INFO 02-03 19:32:49 [parallel_state.py:1212] world_size=32 rank=14 local_rank=6 distributed_init_method=tcp://10.0.0.58:48981 backend=hccl
2026-02-03T19:32:50.2030058Z INFO 02-03 19:32:50 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:32:50.2037792Z INFO 02-03 19:32:50 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:32:50.2047175Z INFO 02-03 19:32:50 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:32:50.2102562Z INFO 02-03 19:32:50 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:32:53.9112114Z 2026-02-03 19:32:53,909 - 825 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:32:53.9144994Z INFO 02-03 19:32:53 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:32:55.2563721Z 2026-02-03 19:32:55,254 - 829 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-03T19:32:55.2597945Z INFO 02-03 19:32:55 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-03T19:32:56.8866027Z INFO 02-03 19:32:56 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:32:57.3206319Z INFO 02-03 19:32:57 [parallel_state.py:1212] world_size=32 rank=7 local_rank=7 distributed_init_method=tcp://10.0.0.58:48981 backend=hccl
2026-02-03T19:32:58.2133301Z INFO 02-03 19:32:58 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:32:58.6308876Z INFO 02-03 19:32:58 [parallel_state.py:1212] world_size=32 rank=15 local_rank=7 distributed_init_method=tcp://10.0.0.58:48981 backend=hccl
2026-02-03T19:32:59.5517057Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.5537133Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.5546531Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.5556181Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.5566694Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.5576058Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.6027061Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.6036402Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.6247999Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.6265237Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.6274751Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.6284803Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.6310347Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.6328708Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.6338045Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.6348076Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.7218087Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-03T19:32:59.7225870Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-03T19:32:59.7235474Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-03T19:32:59.7244684Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-03T19:32:59.7253610Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-03T19:32:59.7263082Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-03T19:32:59.7272097Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-03T19:32:59.7281932Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-03T19:32:59.7291691Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-03T19:32:59.7300747Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-03T19:32:59.7309365Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-03T19:32:59.7319600Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-03T19:32:59.7328348Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-03T19:32:59.7338505Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-03T19:32:59.7347333Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-03T19:32:59.7356136Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-03T19:32:59.7365913Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7375077Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7384116Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7393765Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7403317Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7412601Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7421517Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7430842Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7441275Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7450758Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7471623Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7473537Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7479235Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7488759Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7497743Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7506757Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7516240Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7525493Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7535134Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7544563Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7554360Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7564336Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7573778Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7582837Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7924866Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7945669Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7955753Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7965930Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7975629Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7984794Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.7993702Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.8003402Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.8060305Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.8060700Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.8061079Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.8061466Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.8061854Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.8062324Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.8068815Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.8078239Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.8087644Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.8096922Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.8106171Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.8114453Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.8124426Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.8133217Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.8142370Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.8151919Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-03T19:32:59.8161532Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:32:59.8170848Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:32:59.8179667Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:32:59.8189357Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:32:59.8199389Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:32:59.8208413Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:32:59.8218182Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:32:59.8226763Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:32:59.8236134Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:32:59.8245914Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:32:59.8254835Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:32:59.8264098Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:32:59.8273787Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:32:59.8283475Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:32:59.8293024Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:32:59.8301849Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:32:59.8421333Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.8421847Z INFO 02-03 19:32:59 [parallel_state.py:1423] rank 0 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
2026-02-03T19:32:59.9477976Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9487477Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9497181Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9505593Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9515436Z INFO 02-03 19:32:59 [parallel_state.py:1423] rank 3 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
2026-02-03T19:32:59.9524909Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9533574Z INFO 02-03 19:32:59 [parallel_state.py:1423] rank 10 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 2, EP rank 10
2026-02-03T19:32:59.9542901Z INFO 02-03 19:32:59 [parallel_state.py:1423] rank 1 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
2026-02-03T19:32:59.9551326Z INFO 02-03 19:32:59 [parallel_state.py:1423] rank 12 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 4, EP rank 12
2026-02-03T19:32:59.9560976Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9576603Z INFO 02-03 19:32:59 [parallel_state.py:1423] rank 7 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 7, EP rank 7
2026-02-03T19:32:59.9587865Z INFO 02-03 19:32:59 [parallel_state.py:1423] rank 15 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 7, EP rank 15
2026-02-03T19:32:59.9601741Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9630397Z INFO 02-03 19:32:59 [parallel_state.py:1423] rank 5 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 5, EP rank 5
2026-02-03T19:32:59.9630891Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9631403Z INFO 02-03 19:32:59 [parallel_state.py:1423] rank 13 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 5, EP rank 13
2026-02-03T19:32:59.9639732Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9649849Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9659976Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9661088Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9662478Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9671344Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9680760Z INFO 02-03 19:32:59 [parallel_state.py:1423] rank 2 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
2026-02-03T19:32:59.9691229Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9709809Z INFO 02-03 19:32:59 [parallel_state.py:1423] rank 9 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 1, EP rank 9
2026-02-03T19:32:59.9739418Z INFO 02-03 19:32:59 [parallel_state.py:1423] rank 11 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 3, EP rank 11
2026-02-03T19:32:59.9740004Z INFO 02-03 19:32:59 [parallel_state.py:1423] rank 8 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 0, EP rank 8
2026-02-03T19:32:59.9740589Z INFO 02-03 19:32:59 [parallel_state.py:1423] rank 14 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 6, EP rank 14
2026-02-03T19:32:59.9741142Z INFO 02-03 19:32:59 [parallel_state.py:1423] rank 4 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 4, EP rank 4
2026-02-03T19:32:59.9756553Z INFO 02-03 19:32:59 [parallel_state.py:1423] rank 6 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 6, EP rank 6
2026-02-03T19:32:59.9907650Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9926434Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9935772Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9944481Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9953258Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9963373Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9972291Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9981665Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:32:59.9991023Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:33:00.0000172Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:33:00.0009704Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:33:00.0018830Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:33:00.0028169Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:33:00.0037736Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:33:00.0047249Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:33:00.0056477Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-03T19:33:00.0066218Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:33:00.0076095Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:33:00.0090413Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:33:00.0099550Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:33:00.0108985Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:33:00.0117872Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:33:00.0127526Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:33:00.0135945Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:33:00.0145099Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:33:00.0154331Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:33:00.0164122Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:33:00.0173012Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:33:00.0182262Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:33:00.0191837Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:33:00.0200887Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:33:00.0210681Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-03T19:33:00.0742304Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.0764314Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.0773756Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.0782522Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.0791752Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.0801628Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.0811224Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.0820236Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.0829382Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.0889595Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.0890209Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.0890780Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.0891368Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.0892211Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.0892797Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.0895371Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.1291641Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.1301563Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.1310400Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.1321133Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.1330049Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.1338995Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.1348607Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.1358139Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.1370111Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.1378643Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.1389439Z [0;36m(Worker_DP0_TP1_EP1 pid=187)[0;0m INFO 02-03 19:33:00 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-03T19:33:00.1399296Z [0;36m(Worker_DP0_TP7_EP7 pid=825)[0;0m INFO 02-03 19:33:00 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-03T19:33:00.1408994Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.1417948Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.1427664Z [0;36m(Worker_DP0_TP6_EP6 pid=721)[0;0m INFO 02-03 19:33:00 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-03T19:33:00.1437586Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m INFO 02-03 19:33:00 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-03T19:33:00.1446981Z [0;36m(Worker_DP1_TP3_EP11 pid=412)[0;0m INFO 02-03 19:33:00 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-03T19:33:00.1456077Z [0;36m(Worker_DP1_TP1_EP9 pid=190)[0;0m INFO 02-03 19:33:00 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-03T19:33:00.1465630Z [0;36m(Worker_DP0_TP4_EP4 pid=513)[0;0m INFO 02-03 19:33:00 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-03T19:33:00.1474913Z [0;36m(Worker_DP1_TP6_EP14 pid=725)[0;0m INFO 02-03 19:33:00 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-03T19:33:00.1486124Z [0;36m(Worker_DP1_TP2_EP10 pid=308)[0;0m INFO 02-03 19:33:00 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-03T19:33:00.1495088Z [0;36m(Worker_DP1_TP7_EP15 pid=829)[0;0m INFO 02-03 19:33:00 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-03T19:33:00.1504860Z [0;36m(Worker_DP0_TP2_EP2 pid=305)[0;0m INFO 02-03 19:33:00 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-03T19:33:00.1536399Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.1537066Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.1537750Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.1543396Z WARNING 02-03 19:33:00 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-03T19:33:00.1553556Z [0;36m(Worker_DP1_TP5_EP13 pid=620)[0;0m INFO 02-03 19:33:00 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-03T19:33:00.1563381Z [0;36m(Worker_DP1_TP4_EP12 pid=516)[0;0m INFO 02-03 19:33:00 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-03T19:33:00.1572627Z [0;36m(Worker_DP0_TP3_EP3 pid=409)[0;0m INFO 02-03 19:33:00 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-03T19:33:00.1582369Z [0;36m(Worker_DP0_TP5_EP5 pid=617)[0;0m INFO 02-03 19:33:00 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-03T19:33:00.2108722Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m INFO 02-03 19:33:00 [model_runner_v1.py:2247] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-03T19:33:00.4821111Z [0;36m(Worker_DP1_TP3_EP11 pid=412)[0;0m [2026-02-03 19:33:00] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-03T19:33:00.4851662Z [0;36m(Worker_DP0_TP1_EP1 pid=187)[0;0m [2026-02-03 19:33:00] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-03T19:33:00.4932181Z [0;36m(Worker_DP0_TP7_EP7 pid=825)[0;0m [2026-02-03 19:33:00] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-03T19:33:00.5017761Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m [2026-02-03 19:33:00] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-03T19:33:00.5215288Z [0;36m(Worker_DP0_TP2_EP2 pid=305)[0;0m [2026-02-03 19:33:00] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-03T19:33:00.5299390Z [0;36m(Worker_DP1_TP6_EP14 pid=725)[0;0m [2026-02-03 19:33:00] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-03T19:33:00.5500128Z [0;36m(Worker_DP0_TP6_EP6 pid=721)[0;0m [2026-02-03 19:33:00] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-03T19:33:00.5539321Z [0;36m(Worker_DP1_TP2_EP10 pid=308)[0;0m [2026-02-03 19:33:00] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-03T19:33:00.6464874Z [0;36m(Worker_DP1_TP1_EP9 pid=190)[0;0m [2026-02-03 19:33:00] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-03T19:33:00.6666717Z [0;36m(Worker_DP1_TP4_EP12 pid=516)[0;0m [2026-02-03 19:33:00] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-03T19:33:00.6886015Z [0;36m(Worker_DP0_TP7_EP7 pid=825)[0;0m INFO 02-03 19:33:00 [layer.py:475] [EP Rank 7/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-03T19:33:00.6896056Z [0;36m(Worker_DP0_TP1_EP1 pid=187)[0;0m INFO 02-03 19:33:00 [layer.py:475] [EP Rank 1/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-03T19:33:00.6906251Z [0;36m(Worker_DP0_TP2_EP2 pid=305)[0;0m INFO 02-03 19:33:00 [layer.py:475] [EP Rank 2/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-03T19:33:00.6918880Z [0;36m(Worker_DP1_TP6_EP14 pid=725)[0;0m INFO 02-03 19:33:00 [layer.py:475] [EP Rank 14/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-03T19:33:00.6929627Z [0;36m(Worker_DP1_TP3_EP11 pid=412)[0;0m INFO 02-03 19:33:00 [layer.py:475] [EP Rank 11/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-03T19:33:00.6940101Z [0;36m(Worker_DP0_TP6_EP6 pid=721)[0;0m INFO 02-03 19:33:00 [layer.py:475] [EP Rank 6/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-03T19:33:00.6950432Z [0;36m(Worker_DP1_TP2_EP10 pid=308)[0;0m INFO 02-03 19:33:00 [layer.py:475] [EP Rank 10/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-03T19:33:00.7243173Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m INFO 02-03 19:33:00 [layer.py:475] [EP Rank 8/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-03T19:33:00.7573859Z [0;36m(Worker_DP1_TP1_EP9 pid=190)[0;0m INFO 02-03 19:33:00 [layer.py:475] [EP Rank 9/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-03T19:33:00.7771009Z [0;36m(Worker_DP1_TP4_EP12 pid=516)[0;0m INFO 02-03 19:33:00 [layer.py:475] [EP Rank 12/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-03T19:33:00.7827727Z [0;36m(Worker_DP0_TP3_EP3 pid=409)[0;0m [2026-02-03 19:33:00] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-03T19:33:00.7868374Z [0;36m(Worker_DP1_TP7_EP15 pid=829)[0;0m [2026-02-03 19:33:00] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-03T19:33:00.8342589Z [0;36m(Worker_DP1_TP5_EP13 pid=620)[0;0m [2026-02-03 19:33:00] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-03T19:33:00.8532177Z [0;36m(Worker_DP0_TP4_EP4 pid=513)[0;0m [2026-02-03 19:33:00] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-03T19:33:00.8555174Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m [2026-02-03 19:33:00] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-03T19:33:00.8978606Z [0;36m(Worker_DP0_TP3_EP3 pid=409)[0;0m INFO 02-03 19:33:00 [layer.py:475] [EP Rank 3/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-03T19:33:00.9271523Z [0;36m(Worker_DP1_TP7_EP15 pid=829)[0;0m INFO 02-03 19:33:00 [layer.py:475] [EP Rank 15/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-03T19:33:00.9453459Z [0;36m(Worker_DP1_TP5_EP13 pid=620)[0;0m INFO 02-03 19:33:00 [layer.py:475] [EP Rank 13/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-03T19:33:00.9708879Z [0;36m(Worker_DP0_TP4_EP4 pid=513)[0;0m INFO 02-03 19:33:00 [layer.py:475] [EP Rank 4/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-03T19:33:01.0194376Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m INFO 02-03 19:33:01 [layer.py:475] [EP Rank 0/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-03T19:33:01.3222138Z [0;36m(Worker_DP0_TP5_EP5 pid=617)[0;0m [2026-02-03 19:33:01] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-03T19:33:01.4310561Z [0;36m(Worker_DP0_TP5_EP5 pid=617)[0;0m INFO 02-03 19:33:01 [layer.py:475] [EP Rank 5/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-03T19:33:02.1679216Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-03T19:33:02.1687174Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m   return func(*args, **kwargs)
2026-02-03T19:33:02.2385137Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m INFO 02-03 19:33:02 [fused_moe.py:211] [EP Rank 0/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-03T19:33:02.3503802Z [0;36m(Worker_DP0_TP5_EP5 pid=617)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-03T19:33:02.3511540Z [0;36m(Worker_DP0_TP5_EP5 pid=617)[0;0m   return func(*args, **kwargs)
2026-02-03T19:33:02.3594015Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-03T19:33:02.3620958Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m   return func(*args, **kwargs)
2026-02-03T19:33:02.3887632Z [0;36m(Worker_DP1_TP2_EP10 pid=308)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-03T19:33:02.3895893Z [0;36m(Worker_DP1_TP2_EP10 pid=308)[0;0m   return func(*args, **kwargs)
2026-02-03T19:33:02.3912346Z [0;36m(Worker_DP0_TP1_EP1 pid=187)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-03T19:33:02.3921122Z [0;36m(Worker_DP0_TP1_EP1 pid=187)[0;0m   return func(*args, **kwargs)
2026-02-03T19:33:02.3939985Z [0;36m(Worker_DP1_TP6_EP14 pid=725)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-03T19:33:02.3949747Z [0;36m(Worker_DP1_TP6_EP14 pid=725)[0;0m   return func(*args, **kwargs)
2026-02-03T19:33:02.3964430Z [0;36m(Worker_DP0_TP7_EP7 pid=825)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-03T19:33:02.3972989Z [0;36m(Worker_DP0_TP7_EP7 pid=825)[0;0m   return func(*args, **kwargs)
2026-02-03T19:33:02.4063484Z [0;36m(Worker_DP0_TP4_EP4 pid=513)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-03T19:33:02.4072045Z [0;36m(Worker_DP0_TP4_EP4 pid=513)[0;0m   return func(*args, **kwargs)
2026-02-03T19:33:02.4118659Z [0;36m(Worker_DP0_TP2_EP2 pid=305)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-03T19:33:02.4127721Z [0;36m(Worker_DP0_TP2_EP2 pid=305)[0;0m   return func(*args, **kwargs)
2026-02-03T19:33:02.4192506Z [0;36m(Worker_DP0_TP5_EP5 pid=617)[0;0m INFO 02-03 19:33:02 [fused_moe.py:211] [EP Rank 5/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-03T19:33:02.4292717Z [0;36m(Worker_DP1_TP4_EP12 pid=516)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-03T19:33:02.4299377Z [0;36m(Worker_DP1_TP4_EP12 pid=516)[0;0m   return func(*args, **kwargs)
2026-02-03T19:33:02.4450263Z [0;36m(Worker_DP1_TP7_EP15 pid=829)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-03T19:33:02.4457942Z [0;36m(Worker_DP1_TP7_EP15 pid=829)[0;0m   return func(*args, **kwargs)
2026-02-03T19:33:02.4508801Z [0;36m(Worker_DP1_TP1_EP9 pid=190)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-03T19:33:02.4509895Z [0;36m(Worker_DP1_TP1_EP9 pid=190)[0;0m   return func(*args, **kwargs)
2026-02-03T19:33:02.4510714Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m INFO 02-03 19:33:02 [fused_moe.py:211] [EP Rank 8/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-03T19:33:02.4560187Z [0;36m(Worker_DP0_TP1_EP1 pid=187)[0;0m INFO 02-03 19:33:02 [fused_moe.py:211] [EP Rank 1/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-03T19:33:02.4561760Z [0;36m(Worker_DP0_TP6_EP6 pid=721)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-03T19:33:02.4562946Z [0;36m(Worker_DP0_TP6_EP6 pid=721)[0;0m   return func(*args, **kwargs)
2026-02-03T19:33:02.4563738Z [0;36m(Worker_DP0_TP7_EP7 pid=825)[0;0m INFO 02-03 19:33:02 [fused_moe.py:211] [EP Rank 7/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-03T19:33:02.4585881Z [0;36m(Worker_DP1_TP6_EP14 pid=725)[0;0m INFO 02-03 19:33:02 [fused_moe.py:211] [EP Rank 14/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-03T19:33:02.4611840Z [0;36m(Worker_DP1_TP3_EP11 pid=412)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-03T19:33:02.4620318Z [0;36m(Worker_DP1_TP3_EP11 pid=412)[0;0m   return func(*args, **kwargs)
2026-02-03T19:33:02.4630071Z [0;36m(Worker_DP1_TP2_EP10 pid=308)[0;0m INFO 02-03 19:33:02 [fused_moe.py:211] [EP Rank 10/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-03T19:33:02.4656938Z [0;36m(Worker_DP0_TP3_EP3 pid=409)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-03T19:33:02.4665194Z [0;36m(Worker_DP0_TP3_EP3 pid=409)[0;0m   return func(*args, **kwargs)
2026-02-03T19:33:02.4679602Z [0;36m(Worker_DP0_TP4_EP4 pid=513)[0;0m INFO 02-03 19:33:02 [fused_moe.py:211] [EP Rank 4/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-03T19:33:02.4778854Z [0;36m(Worker_DP0_TP2_EP2 pid=305)[0;0m INFO 02-03 19:33:02 [fused_moe.py:211] [EP Rank 2/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-03T19:33:02.4812189Z [0;36m(Worker_DP1_TP5_EP13 pid=620)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-03T19:33:02.4820595Z [0;36m(Worker_DP1_TP5_EP13 pid=620)[0;0m   return func(*args, **kwargs)
2026-02-03T19:33:02.4888065Z [0;36m(Worker_DP1_TP4_EP12 pid=516)[0;0m INFO 02-03 19:33:02 [fused_moe.py:211] [EP Rank 12/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-03T19:33:02.5050003Z [0;36m(Worker_DP1_TP7_EP15 pid=829)[0;0m INFO 02-03 19:33:02 [fused_moe.py:211] [EP Rank 15/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-03T19:33:02.5078664Z [0;36m(Worker_DP1_TP1_EP9 pid=190)[0;0m INFO 02-03 19:33:02 [fused_moe.py:211] [EP Rank 9/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-03T19:33:02.5103183Z [0;36m(Worker_DP0_TP6_EP6 pid=721)[0;0m INFO 02-03 19:33:02 [fused_moe.py:211] [EP Rank 6/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-03T19:33:02.5210388Z [0;36m(Worker_DP1_TP3_EP11 pid=412)[0;0m INFO 02-03 19:33:02 [fused_moe.py:211] [EP Rank 11/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-03T19:33:02.5391069Z [0;36m(Worker_DP0_TP3_EP3 pid=409)[0;0m INFO 02-03 19:33:02 [fused_moe.py:211] [EP Rank 3/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-03T19:33:02.5415188Z [0;36m(Worker_DP1_TP5_EP13 pid=620)[0;0m INFO 02-03 19:33:02 [fused_moe.py:211] [EP Rank 13/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-03T19:33:02.7656197Z [0;36m(Worker_DP0_TP7_EP7 pid=825)[0;0m INFO 02-03 19:33:02 [fused_moe.py:425] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-03T19:33:02.7681247Z [0;36m(Worker_DP1_TP6_EP14 pid=725)[0;0m INFO 02-03 19:33:02 [fused_moe.py:425] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-03T19:33:02.7706065Z [0;36m(Worker_DP1_TP2_EP10 pid=308)[0;0m INFO 02-03 19:33:02 [fused_moe.py:425] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-03T19:33:02.7715920Z [0;36m(Worker_DP0_TP4_EP4 pid=513)[0;0m INFO 02-03 19:33:02 [fused_moe.py:425] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-03T19:33:02.7759343Z [0;36m(Worker_DP0_TP2_EP2 pid=305)[0;0m INFO 02-03 19:33:02 [fused_moe.py:425] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-03T19:33:02.7859312Z [0;36m(Worker_DP1_TP4_EP12 pid=516)[0;0m INFO 02-03 19:33:02 [fused_moe.py:425] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-03T19:33:02.8133707Z [0;36m(Worker_DP1_TP1_EP9 pid=190)[0;0m INFO 02-03 19:33:02 [fused_moe.py:425] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-03T19:33:02.8362740Z [0;36m(Worker_DP1_TP7_EP15 pid=829)[0;0m INFO 02-03 19:33:02 [fused_moe.py:425] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-03T19:33:02.8414188Z [0;36m(Worker_DP0_TP6_EP6 pid=721)[0;0m INFO 02-03 19:33:02 [fused_moe.py:425] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-03T19:33:02.8469821Z [0;36m(Worker_DP1_TP3_EP11 pid=412)[0;0m INFO 02-03 19:33:02 [fused_moe.py:425] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-03T19:33:02.8558937Z [0;36m(Worker_DP1_TP5_EP13 pid=620)[0;0m INFO 02-03 19:33:02 [fused_moe.py:425] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-03T19:33:02.8676698Z [0;36m(Worker_DP0_TP3_EP3 pid=409)[0;0m INFO 02-03 19:33:02 [fused_moe.py:425] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-03T19:33:02.8893818Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m INFO 02-03 19:33:02 [fused_moe.py:425] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-03T19:33:02.9263168Z [0;36m(Worker_DP0_TP5_EP5 pid=617)[0;0m INFO 02-03 19:33:02 [fused_moe.py:425] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-03T19:33:02.9675040Z [0;36m(Worker_DP0_TP1_EP1 pid=187)[0;0m INFO 02-03 19:33:02 [fused_moe.py:425] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-03T19:33:02.9727137Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m INFO 02-03 19:33:02 [fused_moe.py:425] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-03T19:33:06.0125975Z [0;36m(Worker_DP0_TP4_EP4 pid=513)[0;0m INFO 02-03 19:33:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-03T19:33:06.0750771Z [0;36m(Worker_DP1_TP4_EP12 pid=516)[0;0m INFO 02-03 19:33:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-03T19:33:06.0872466Z [0;36m(Worker_DP0_TP6_EP6 pid=721)[0;0m INFO 02-03 19:33:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-03T19:33:06.1591944Z [0;36m(Worker_DP0_TP7_EP7 pid=825)[0;0m INFO 02-03 19:33:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-03T19:33:06.1687768Z [0;36m(Worker_DP1_TP6_EP14 pid=725)[0;0m INFO 02-03 19:33:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-03T19:33:06.2469950Z [0;36m(Worker_DP1_TP5_EP13 pid=620)[0;0m INFO 02-03 19:33:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-03T19:33:06.2505987Z [0;36m(Worker_DP1_TP3_EP11 pid=412)[0;0m INFO 02-03 19:33:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-03T19:33:06.2564333Z [0;36m(Worker_DP0_TP5_EP5 pid=617)[0;0m INFO 02-03 19:33:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-03T19:33:06.2631320Z [0;36m(Worker_DP1_TP7_EP15 pid=829)[0;0m INFO 02-03 19:33:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-03T19:33:06.2710681Z [0;36m(Worker_DP1_TP2_EP10 pid=308)[0;0m INFO 02-03 19:33:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-03T19:33:06.3043701Z [0;36m(Worker_DP1_TP1_EP9 pid=190)[0;0m INFO 02-03 19:33:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-03T19:33:06.3066340Z [0;36m(Worker_DP0_TP2_EP2 pid=305)[0;0m INFO 02-03 19:33:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-03T19:33:06.4252170Z [0;36m(Worker_DP0_TP1_EP1 pid=187)[0;0m INFO 02-03 19:33:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-03T19:33:06.4751521Z [0;36m(Worker_DP0_TP3_EP3 pid=409)[0;0m INFO 02-03 19:33:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-03T19:33:06.5339656Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m INFO 02-03 19:33:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-03T19:33:06.5776847Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m INFO 02-03 19:33:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-03T19:33:06.6352389Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:06.6352686Z Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
2026-02-03T19:33:07.6454042Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:07.6454401Z Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:01<02:43,  1.01s/it]
2026-02-03T19:33:11.1449064Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:11.1449435Z Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:04<06:38,  2.47s/it]
2026-02-03T19:33:12.1894281Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:12.1894622Z Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:05<04:51,  1.82s/it]
2026-02-03T19:33:13.5939383Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:13.5939770Z Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:06<04:23,  1.66s/it]
2026-02-03T19:33:15.8730770Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:15.8739819Z Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:09<04:57,  1.88s/it]
2026-02-03T19:33:18.8174681Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:18.8175053Z Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:12<05:52,  2.24s/it]
2026-02-03T19:33:21.3441111Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:21.3441470Z Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:14<06:04,  2.34s/it]
2026-02-03T19:33:23.5723891Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:23.5724267Z Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:16<05:56,  2.30s/it]
2026-02-03T19:33:25.3597398Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:25.3597750Z Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:18<05:29,  2.14s/it]
2026-02-03T19:33:27.2141291Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:27.2141694Z Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:20<05:13,  2.05s/it]
2026-02-03T19:33:28.8533388Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:28.8533743Z Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:22<04:52,  1.93s/it]
2026-02-03T19:33:31.0390293Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:31.0390633Z Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:24<05:02,  2.01s/it]
2026-02-03T19:33:32.6373834Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:32.6374223Z Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:26<04:42,  1.88s/it]
2026-02-03T19:33:35.0128502Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:35.0128873Z Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:28<05:02,  2.03s/it]
2026-02-03T19:33:38.2910037Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:38.2910449Z Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:31<05:56,  2.41s/it]
2026-02-03T19:33:40.5344613Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:40.5344986Z Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:33<05:46,  2.36s/it]
2026-02-03T19:33:42.8959996Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:42.8960352Z Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:36<05:44,  2.36s/it]
2026-02-03T19:33:45.2636776Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:45.2637149Z Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:38<05:42,  2.36s/it]
2026-02-03T19:33:46.5083465Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:46.5085003Z Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:39<04:51,  2.03s/it]
2026-02-03T19:33:48.3106995Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:48.3107383Z Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:41<04:39,  1.95s/it]
2026-02-03T19:33:50.9950194Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:50.9950667Z Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:44<05:09,  2.18s/it]
2026-02-03T19:33:52.5135109Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:52.5135469Z Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:45<04:39,  1.98s/it]
2026-02-03T19:33:54.3905016Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:54.3905379Z Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:47<04:32,  1.95s/it]
2026-02-03T19:33:56.1639428Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:56.1639786Z Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:49<04:23,  1.90s/it]
2026-02-03T19:33:59.3255910Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:59.3256308Z Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:52<05:14,  2.28s/it]
2026-02-03T19:33:59.9138286Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:33:59.9139063Z Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:53<04:02,  1.77s/it]
2026-02-03T19:34:00.9459735Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:00.9460092Z Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:54<03:30,  1.55s/it]
2026-02-03T19:34:03.9293502Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:03.9293858Z Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:57<04:27,  1.98s/it]
2026-02-03T19:34:05.2967559Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:05.2967910Z Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:58<04:00,  1.80s/it]
2026-02-03T19:34:06.6869686Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:06.6870057Z Loading safetensors checkpoint shards:  18% Completed | 30/163 [01:00<03:42,  1.67s/it]
2026-02-03T19:34:07.9894396Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:07.9894757Z Loading safetensors checkpoint shards:  19% Completed | 31/163 [01:01<03:26,  1.56s/it]
2026-02-03T19:34:09.7529257Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:09.7529609Z Loading safetensors checkpoint shards:  20% Completed | 32/163 [01:03<03:32,  1.62s/it]
2026-02-03T19:34:11.8123681Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:11.8124048Z Loading safetensors checkpoint shards:  20% Completed | 33/163 [01:05<03:47,  1.75s/it]
2026-02-03T19:34:14.0291506Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:14.0291877Z Loading safetensors checkpoint shards:  21% Completed | 34/163 [01:07<04:04,  1.89s/it]
2026-02-03T19:34:15.9291009Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:15.9291379Z Loading safetensors checkpoint shards:  21% Completed | 35/163 [01:09<04:02,  1.89s/it]
2026-02-03T19:34:20.9299358Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:20.9299741Z Loading safetensors checkpoint shards:  22% Completed | 36/163 [01:14<05:58,  2.83s/it]
2026-02-03T19:34:22.3099421Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:22.3099823Z Loading safetensors checkpoint shards:  23% Completed | 37/163 [01:15<05:01,  2.39s/it]
2026-02-03T19:34:24.0354830Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:24.0355189Z Loading safetensors checkpoint shards:  23% Completed | 38/163 [01:17<04:34,  2.19s/it]
2026-02-03T19:34:26.3566136Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:26.3566516Z Loading safetensors checkpoint shards:  24% Completed | 39/163 [01:19<04:36,  2.23s/it]
2026-02-03T19:34:30.2281196Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:30.2281559Z Loading safetensors checkpoint shards:  25% Completed | 40/163 [01:23<05:34,  2.72s/it]
2026-02-03T19:34:31.0815508Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:31.0815877Z Loading safetensors checkpoint shards:  25% Completed | 41/163 [01:24<04:23,  2.16s/it]
2026-02-03T19:34:32.1149633Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:32.1150536Z Loading safetensors checkpoint shards:  26% Completed | 42/163 [01:25<03:40,  1.82s/it]
2026-02-03T19:34:36.0645052Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:36.0645410Z Loading safetensors checkpoint shards:  26% Completed | 43/163 [01:29<04:55,  2.46s/it]
2026-02-03T19:34:37.3652644Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:37.3653091Z Loading safetensors checkpoint shards:  27% Completed | 44/163 [01:30<04:11,  2.11s/it]
2026-02-03T19:34:38.8278950Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:38.8279347Z Loading safetensors checkpoint shards:  28% Completed | 45/163 [01:32<03:46,  1.92s/it]
2026-02-03T19:34:40.4797867Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:40.4798226Z Loading safetensors checkpoint shards:  28% Completed | 46/163 [01:33<03:35,  1.84s/it]
2026-02-03T19:34:48.5396274Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:48.5396679Z Loading safetensors checkpoint shards:  29% Completed | 47/163 [01:41<07:09,  3.70s/it]
2026-02-03T19:34:51.1161616Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:51.1162070Z Loading safetensors checkpoint shards:  29% Completed | 48/163 [01:44<06:27,  3.37s/it]
2026-02-03T19:34:51.4988720Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:51.4989090Z Loading safetensors checkpoint shards:  30% Completed | 49/163 [01:44<04:41,  2.47s/it]
2026-02-03T19:34:53.3204784Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:53.3205147Z Loading safetensors checkpoint shards:  31% Completed | 50/163 [01:46<04:17,  2.28s/it]
2026-02-03T19:34:55.9921400Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:55.9921776Z Loading safetensors checkpoint shards:  31% Completed | 51/163 [01:49<04:28,  2.39s/it]
2026-02-03T19:34:57.3765372Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:34:57.3765740Z Loading safetensors checkpoint shards:  32% Completed | 52/163 [01:50<03:52,  2.09s/it]
2026-02-03T19:35:00.8483746Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:00.8484163Z Loading safetensors checkpoint shards:  33% Completed | 53/163 [01:54<04:35,  2.50s/it]
2026-02-03T19:35:01.8377656Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:01.8378023Z Loading safetensors checkpoint shards:  33% Completed | 54/163 [01:55<03:43,  2.05s/it]
2026-02-03T19:35:03.0076279Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:03.0076643Z Loading safetensors checkpoint shards:  34% Completed | 55/163 [01:56<03:12,  1.79s/it]
2026-02-03T19:35:04.6228667Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:04.6229033Z Loading safetensors checkpoint shards:  34% Completed | 56/163 [01:57<03:05,  1.74s/it]
2026-02-03T19:35:06.3670430Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:06.3670886Z Loading safetensors checkpoint shards:  35% Completed | 57/163 [01:59<03:04,  1.74s/it]
2026-02-03T19:35:08.4572315Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:08.4572706Z Loading safetensors checkpoint shards:  36% Completed | 58/163 [02:01<03:13,  1.84s/it]
2026-02-03T19:35:13.2025871Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:13.2026232Z Loading safetensors checkpoint shards:  36% Completed | 59/163 [02:06<04:42,  2.71s/it]
2026-02-03T19:35:13.9881738Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:13.9882213Z Loading safetensors checkpoint shards:  37% Completed | 60/163 [02:07<03:39,  2.14s/it]
2026-02-03T19:35:14.8188701Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:14.8189057Z Loading safetensors checkpoint shards:  37% Completed | 61/163 [02:08<02:57,  1.74s/it]
2026-02-03T19:35:18.1338286Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:18.1338664Z Loading safetensors checkpoint shards:  38% Completed | 62/163 [02:11<03:43,  2.22s/it]
2026-02-03T19:35:19.2716022Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:19.2716382Z Loading safetensors checkpoint shards:  39% Completed | 63/163 [02:12<03:09,  1.89s/it]
2026-02-03T19:35:20.9952626Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:20.9953247Z Loading safetensors checkpoint shards:  39% Completed | 64/163 [02:14<03:02,  1.84s/it]
2026-02-03T19:35:25.6877471Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:25.6877840Z Loading safetensors checkpoint shards:  40% Completed | 65/163 [02:19<04:24,  2.70s/it]
2026-02-03T19:35:28.1327375Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:28.1327752Z Loading safetensors checkpoint shards:  40% Completed | 66/163 [02:21<04:14,  2.62s/it]
2026-02-03T19:35:29.5853407Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:29.5853769Z Loading safetensors checkpoint shards:  41% Completed | 67/163 [02:22<03:37,  2.27s/it]
2026-02-03T19:35:32.4273402Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:32.4273770Z Loading safetensors checkpoint shards:  42% Completed | 68/163 [02:25<03:52,  2.44s/it]
2026-02-03T19:35:33.4833354Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:33.4833717Z Loading safetensors checkpoint shards:  42% Completed | 69/163 [02:26<03:10,  2.03s/it]
2026-02-03T19:35:34.4485187Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:34.4485544Z Loading safetensors checkpoint shards:  43% Completed | 70/163 [02:27<02:38,  1.71s/it]
2026-02-03T19:35:36.2006498Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:36.2006870Z Loading safetensors checkpoint shards:  44% Completed | 71/163 [02:29<02:38,  1.72s/it]
2026-02-03T19:35:39.0141385Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:39.0141752Z Loading safetensors checkpoint shards:  44% Completed | 72/163 [02:32<03:06,  2.05s/it]
2026-02-03T19:35:39.7977606Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:39.7977997Z Loading safetensors checkpoint shards:  45% Completed | 73/163 [02:33<02:30,  1.67s/it]
2026-02-03T19:35:41.1318467Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:41.1318858Z Loading safetensors checkpoint shards:  45% Completed | 74/163 [02:34<02:19,  1.57s/it]
2026-02-03T19:35:42.7975733Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:42.7976123Z Loading safetensors checkpoint shards:  46% Completed | 75/163 [02:36<02:20,  1.60s/it]
2026-02-03T19:35:43.9962782Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:43.9963148Z Loading safetensors checkpoint shards:  47% Completed | 76/163 [02:37<02:08,  1.48s/it]
2026-02-03T19:35:46.6390921Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:46.6391281Z Loading safetensors checkpoint shards:  47% Completed | 77/163 [02:40<02:37,  1.83s/it]
2026-02-03T19:35:47.7216240Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:47.7216597Z Loading safetensors checkpoint shards:  48% Completed | 78/163 [02:41<02:16,  1.60s/it]
2026-02-03T19:35:48.7738707Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:48.7739069Z Loading safetensors checkpoint shards:  48% Completed | 79/163 [02:42<02:00,  1.44s/it]
2026-02-03T19:35:51.1289645Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:51.1290053Z Loading safetensors checkpoint shards:  49% Completed | 80/163 [02:44<02:22,  1.71s/it]
2026-02-03T19:35:54.2924599Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:54.2924973Z Loading safetensors checkpoint shards:  50% Completed | 81/163 [02:47<02:56,  2.15s/it]
2026-02-03T19:35:55.2758273Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:55.2758648Z Loading safetensors checkpoint shards:  50% Completed | 82/163 [02:48<02:25,  1.80s/it]
2026-02-03T19:35:57.0039764Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:57.0040135Z Loading safetensors checkpoint shards:  51% Completed | 83/163 [02:50<02:22,  1.78s/it]
2026-02-03T19:35:57.9969225Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:35:57.9969578Z Loading safetensors checkpoint shards:  52% Completed | 84/163 [02:51<02:01,  1.54s/it]
2026-02-03T19:36:00.1348745Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:00.1349230Z Loading safetensors checkpoint shards:  52% Completed | 85/163 [02:53<02:14,  1.72s/it]
2026-02-03T19:36:01.1613265Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:01.1613679Z Loading safetensors checkpoint shards:  53% Completed | 86/163 [02:54<01:56,  1.51s/it]
2026-02-03T19:36:02.8721130Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:02.8721585Z Loading safetensors checkpoint shards:  53% Completed | 87/163 [02:56<01:59,  1.57s/it]
2026-02-03T19:36:06.3262953Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:06.3263430Z Loading safetensors checkpoint shards:  54% Completed | 88/163 [02:59<02:40,  2.14s/it]
2026-02-03T19:36:07.8775857Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:07.8776315Z Loading safetensors checkpoint shards:  55% Completed | 89/163 [03:01<02:25,  1.96s/it]
2026-02-03T19:36:11.4539635Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:11.4540107Z Loading safetensors checkpoint shards:  55% Completed | 90/163 [03:04<02:58,  2.45s/it]
2026-02-03T19:36:12.7891158Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:12.7891913Z Loading safetensors checkpoint shards:  56% Completed | 91/163 [03:06<02:32,  2.11s/it]
2026-02-03T19:36:17.0377423Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:17.0377926Z Loading safetensors checkpoint shards:  56% Completed | 92/163 [03:10<03:15,  2.75s/it]
2026-02-03T19:36:17.8051649Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:17.8052270Z Loading safetensors checkpoint shards:  57% Completed | 93/163 [03:11<02:31,  2.16s/it]
2026-02-03T19:36:18.5580632Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:18.5581033Z Loading safetensors checkpoint shards:  58% Completed | 94/163 [03:11<01:59,  1.74s/it]
2026-02-03T19:36:19.8080239Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:19.8080772Z Loading safetensors checkpoint shards:  58% Completed | 95/163 [03:13<01:48,  1.59s/it]
2026-02-03T19:36:22.7704131Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:22.7704690Z Loading safetensors checkpoint shards:  59% Completed | 96/163 [03:16<02:14,  2.00s/it]
2026-02-03T19:36:23.8684948Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:23.8685436Z Loading safetensors checkpoint shards:  60% Completed | 97/163 [03:17<01:54,  1.73s/it]
2026-02-03T19:36:25.2609948Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:25.2610478Z Loading safetensors checkpoint shards:  60% Completed | 98/163 [03:18<01:45,  1.63s/it]
2026-02-03T19:36:26.8572430Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:26.8572871Z Loading safetensors checkpoint shards:  61% Completed | 99/163 [03:20<01:43,  1.62s/it]
2026-02-03T19:36:28.4975915Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:28.4976362Z Loading safetensors checkpoint shards:  61% Completed | 100/163 [03:21<01:42,  1.63s/it]
2026-02-03T19:36:29.7190092Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:29.7190608Z Loading safetensors checkpoint shards:  62% Completed | 101/163 [03:23<01:33,  1.50s/it]
2026-02-03T19:36:32.7334585Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:32.7335082Z Loading safetensors checkpoint shards:  63% Completed | 102/163 [03:26<01:59,  1.96s/it]
2026-02-03T19:36:35.0260791Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:35.0261251Z Loading safetensors checkpoint shards:  63% Completed | 103/163 [03:28<02:03,  2.06s/it]
2026-02-03T19:36:37.9327955Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:37.9328405Z Loading safetensors checkpoint shards:  64% Completed | 104/163 [03:31<02:16,  2.31s/it]
2026-02-03T19:36:39.0924087Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:39.0924528Z Loading safetensors checkpoint shards:  64% Completed | 105/163 [03:32<01:54,  1.97s/it]
2026-02-03T19:36:42.1063913Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:42.1064419Z Loading safetensors checkpoint shards:  65% Completed | 106/163 [03:35<02:09,  2.28s/it]
2026-02-03T19:36:43.0420703Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:43.0421428Z Loading safetensors checkpoint shards:  66% Completed | 107/163 [03:36<01:45,  1.88s/it]
2026-02-03T19:36:45.9259005Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:45.9259522Z Loading safetensors checkpoint shards:  66% Completed | 108/163 [03:39<01:59,  2.18s/it]
2026-02-03T19:36:46.7089110Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:46.7089520Z Loading safetensors checkpoint shards:  67% Completed | 109/163 [03:40<01:35,  1.76s/it]
2026-02-03T19:36:49.2068636Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:49.2069085Z Loading safetensors checkpoint shards:  67% Completed | 110/163 [03:42<01:45,  1.98s/it]
2026-02-03T19:36:52.5329377Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:52.5329844Z Loading safetensors checkpoint shards:  68% Completed | 111/163 [03:45<02:04,  2.39s/it]
2026-02-03T19:36:54.5070282Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:54.5070752Z Loading safetensors checkpoint shards:  69% Completed | 112/163 [03:47<01:55,  2.26s/it]
2026-02-03T19:36:56.8131017Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:56.8131553Z Loading safetensors checkpoint shards:  69% Completed | 113/163 [03:50<01:53,  2.27s/it]
2026-02-03T19:36:58.0671507Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:58.0672123Z Loading safetensors checkpoint shards:  70% Completed | 114/163 [03:51<01:36,  1.97s/it]
2026-02-03T19:36:59.4718695Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:36:59.4719131Z Loading safetensors checkpoint shards:  71% Completed | 115/163 [03:52<01:26,  1.80s/it]
2026-02-03T19:37:02.9032653Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:02.9033180Z Loading safetensors checkpoint shards:  71% Completed | 116/163 [03:56<01:47,  2.29s/it]
2026-02-03T19:37:04.4529767Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:04.4530322Z Loading safetensors checkpoint shards:  72% Completed | 117/163 [03:57<01:35,  2.07s/it]
2026-02-03T19:37:07.6286309Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:07.6286817Z Loading safetensors checkpoint shards:  72% Completed | 118/163 [04:00<01:47,  2.40s/it]
2026-02-03T19:37:08.8836114Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:08.8836512Z Loading safetensors checkpoint shards:  73% Completed | 119/163 [04:02<01:30,  2.06s/it]
2026-02-03T19:37:10.4544156Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:10.4544719Z Loading safetensors checkpoint shards:  74% Completed | 120/163 [04:03<01:22,  1.91s/it]
2026-02-03T19:37:12.2027259Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:12.2027702Z Loading safetensors checkpoint shards:  74% Completed | 121/163 [04:05<01:18,  1.86s/it]
2026-02-03T19:37:13.5088522Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:13.5089034Z Loading safetensors checkpoint shards:  75% Completed | 122/163 [04:06<01:09,  1.70s/it]
2026-02-03T19:37:16.1779755Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:16.1780391Z Loading safetensors checkpoint shards:  75% Completed | 123/163 [04:09<01:19,  1.99s/it]
2026-02-03T19:37:17.3514948Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:17.3515465Z Loading safetensors checkpoint shards:  76% Completed | 124/163 [04:10<01:07,  1.74s/it]
2026-02-03T19:37:20.2580965Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:20.2581466Z Loading safetensors checkpoint shards:  77% Completed | 125/163 [04:13<01:19,  2.09s/it]
2026-02-03T19:37:21.5982805Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:21.5983284Z Loading safetensors checkpoint shards:  77% Completed | 126/163 [04:14<01:09,  1.87s/it]
2026-02-03T19:37:23.2059755Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:23.2060163Z Loading safetensors checkpoint shards:  78% Completed | 127/163 [04:16<01:04,  1.79s/it]
2026-02-03T19:37:26.0374747Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:26.0375172Z Loading safetensors checkpoint shards:  79% Completed | 128/163 [04:19<01:13,  2.10s/it]
2026-02-03T19:37:28.8523441Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:28.8524139Z Loading safetensors checkpoint shards:  79% Completed | 129/163 [04:22<01:18,  2.32s/it]
2026-02-03T19:37:29.8491509Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:29.8491918Z Loading safetensors checkpoint shards:  80% Completed | 130/163 [04:23<01:03,  1.92s/it]
2026-02-03T19:37:31.1390607Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:31.1391058Z Loading safetensors checkpoint shards:  80% Completed | 131/163 [04:24<00:55,  1.73s/it]
2026-02-03T19:37:32.7453301Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:32.7453750Z Loading safetensors checkpoint shards:  81% Completed | 132/163 [04:26<00:52,  1.69s/it]
2026-02-03T19:37:35.2518490Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:35.2518919Z Loading safetensors checkpoint shards:  82% Completed | 133/163 [04:28<00:58,  1.94s/it]
2026-02-03T19:37:37.0905774Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:37.0906304Z Loading safetensors checkpoint shards:  82% Completed | 134/163 [04:30<00:55,  1.91s/it]
2026-02-03T19:37:40.8700452Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:40.8701009Z Loading safetensors checkpoint shards:  83% Completed | 135/163 [04:34<01:09,  2.47s/it]
2026-02-03T19:37:42.1787212Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:42.1787643Z Loading safetensors checkpoint shards:  83% Completed | 136/163 [04:35<00:57,  2.12s/it]
2026-02-03T19:37:43.8623405Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:43.8623869Z Loading safetensors checkpoint shards:  84% Completed | 137/163 [04:37<00:51,  1.99s/it]
2026-02-03T19:37:48.0840511Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:48.0840997Z Loading safetensors checkpoint shards:  85% Completed | 138/163 [04:41<01:06,  2.66s/it]
2026-02-03T19:37:50.5585914Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:50.5586335Z Loading safetensors checkpoint shards:  85% Completed | 139/163 [04:43<01:02,  2.60s/it]
2026-02-03T19:37:51.3164200Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:51.3164702Z Loading safetensors checkpoint shards:  86% Completed | 140/163 [04:44<00:47,  2.05s/it]
2026-02-03T19:37:52.7775349Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:52.7775819Z Loading safetensors checkpoint shards:  87% Completed | 141/163 [04:46<00:41,  1.87s/it]
2026-02-03T19:37:54.3013304Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:54.3013752Z Loading safetensors checkpoint shards:  87% Completed | 142/163 [04:47<00:37,  1.77s/it]
2026-02-03T19:37:56.9775431Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:37:56.9775869Z Loading safetensors checkpoint shards:  88% Completed | 143/163 [04:50<00:40,  2.04s/it]
2026-02-03T19:38:00.2233551Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:00.2234193Z Loading safetensors checkpoint shards:  88% Completed | 144/163 [04:53<00:45,  2.40s/it]
2026-02-03T19:38:02.3893696Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:02.3894180Z Loading safetensors checkpoint shards:  89% Completed | 145/163 [04:55<00:41,  2.33s/it]
2026-02-03T19:38:04.5792265Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:04.5792708Z Loading safetensors checkpoint shards:  90% Completed | 146/163 [04:57<00:38,  2.29s/it]
2026-02-03T19:38:05.7522238Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:05.7522700Z Loading safetensors checkpoint shards:  90% Completed | 147/163 [04:59<00:31,  1.95s/it]
2026-02-03T19:38:07.0172205Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:07.0172635Z Loading safetensors checkpoint shards:  91% Completed | 148/163 [05:00<00:26,  1.75s/it]
2026-02-03T19:38:08.2109842Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:08.2110401Z Loading safetensors checkpoint shards:  91% Completed | 149/163 [05:01<00:22,  1.58s/it]
2026-02-03T19:38:11.2490667Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:11.2491217Z Loading safetensors checkpoint shards:  93% Completed | 151/163 [05:04<00:18,  1.55s/it]
2026-02-03T19:38:12.7523390Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:12.7523880Z Loading safetensors checkpoint shards:  93% Completed | 152/163 [05:06<00:16,  1.54s/it]
2026-02-03T19:38:14.1678356Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:14.1678823Z Loading safetensors checkpoint shards:  94% Completed | 153/163 [05:07<00:15,  1.51s/it]
2026-02-03T19:38:15.9134092Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:15.9134502Z Loading safetensors checkpoint shards:  94% Completed | 154/163 [05:09<00:14,  1.57s/it]
2026-02-03T19:38:17.6114713Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:17.6115116Z Loading safetensors checkpoint shards:  95% Completed | 155/163 [05:10<00:12,  1.61s/it]
2026-02-03T19:38:18.8877725Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:18.8878171Z Loading safetensors checkpoint shards:  96% Completed | 156/163 [05:12<00:10,  1.51s/it]
2026-02-03T19:38:20.4251930Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:20.4252555Z Loading safetensors checkpoint shards:  96% Completed | 157/163 [05:13<00:09,  1.52s/it]
2026-02-03T19:38:24.3315435Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:24.3315851Z Loading safetensors checkpoint shards:  97% Completed | 158/163 [05:17<00:11,  2.22s/it]
2026-02-03T19:38:25.7025492Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:25.7025903Z Loading safetensors checkpoint shards:  98% Completed | 159/163 [05:19<00:07,  1.97s/it]
2026-02-03T19:38:27.5182936Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:27.5183380Z Loading safetensors checkpoint shards:  98% Completed | 160/163 [05:20<00:05,  1.92s/it]
2026-02-03T19:38:28.9808154Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:28.9808659Z Loading safetensors checkpoint shards:  99% Completed | 161/163 [05:22<00:03,  1.79s/it]
2026-02-03T19:38:30.8598421Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:30.8599005Z Loading safetensors checkpoint shards:  99% Completed | 162/163 [05:24<00:01,  1.81s/it]
2026-02-03T19:38:32.3144738Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:32.3145183Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [05:25<00:00,  1.71s/it]
2026-02-03T19:38:32.3174888Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:32.3175274Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [05:25<00:00,  2.00s/it]
2026-02-03T19:38:32.3188624Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:38:32.3373103Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m INFO 02-03 19:38:32 [default_loader.py:291] Loading weights took 325.69 seconds
2026-02-03T19:38:32.7093907Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m INFO 02-03 19:38:32 [default_loader.py:291] Loading weights took 326.11 seconds
2026-02-03T19:38:46.6708181Z INFO 02-03 19:38:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:38:46.6718201Z INFO 02-03 19:38:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:38:46.6729494Z INFO 02-03 19:38:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:38:46.6739306Z INFO 02-03 19:38:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:38:46.6747684Z INFO 02-03 19:38:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:38:46.6758769Z INFO 02-03 19:38:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:38:46.6768556Z INFO 02-03 19:38:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:38:46.6778561Z INFO 02-03 19:38:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:38:46.6789661Z INFO 02-03 19:38:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:38:46.6799711Z INFO 02-03 19:38:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:38:46.6809537Z INFO 02-03 19:38:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:38:46.6819031Z INFO 02-03 19:38:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:38:46.6828658Z INFO 02-03 19:38:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:38:46.6838268Z INFO 02-03 19:38:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:38:46.6848188Z INFO 02-03 19:38:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:38:46.6857455Z INFO 02-03 19:38:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:38:46.6866538Z INFO 02-03 19:38:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:38:46.6876721Z INFO 02-03 19:38:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:38:46.6887232Z INFO 02-03 19:38:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:38:46.6896353Z INFO 02-03 19:38:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:38:46.6905479Z INFO 02-03 19:38:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:38:46.6915191Z INFO 02-03 19:38:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:38:46.6925109Z INFO 02-03 19:38:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:38:46.6934674Z INFO 02-03 19:38:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:38:46.6944562Z INFO 02-03 19:38:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:38:46.6953041Z INFO 02-03 19:38:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:38:46.6963056Z INFO 02-03 19:38:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:38:46.6972974Z INFO 02-03 19:38:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:38:46.6981853Z INFO 02-03 19:38:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:38:46.6992696Z INFO 02-03 19:38:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:38:46.7002550Z INFO 02-03 19:38:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:38:46.7011910Z INFO 02-03 19:38:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:38:46.7021673Z INFO 02-03 19:38:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:38:46.7031105Z INFO 02-03 19:38:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:38:46.7041603Z INFO 02-03 19:38:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:38:46.7051422Z INFO 02-03 19:38:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:38:46.7060567Z INFO 02-03 19:38:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:38:46.7070524Z INFO 02-03 19:38:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:38:46.7081189Z INFO 02-03 19:38:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:38:46.7090399Z INFO 02-03 19:38:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:38:46.7099339Z INFO 02-03 19:38:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:38:46.7109531Z INFO 02-03 19:38:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:38:46.7119567Z INFO 02-03 19:38:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:38:46.7128521Z INFO 02-03 19:38:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:38:46.7137949Z INFO 02-03 19:38:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:38:46.7146888Z INFO 02-03 19:38:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:38:46.7156958Z INFO 02-03 19:38:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:38:46.7166047Z INFO 02-03 19:38:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:38:46.7175339Z INFO 02-03 19:38:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:38:46.7184992Z INFO 02-03 19:38:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:38:46.7194649Z INFO 02-03 19:38:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:38:46.7204507Z INFO 02-03 19:38:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:38:46.7213957Z INFO 02-03 19:38:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:38:46.7223584Z INFO 02-03 19:38:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:38:46.7233178Z INFO 02-03 19:38:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:38:46.7292619Z INFO 02-03 19:38:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:38:46.7551141Z INFO 02-03 19:38:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:38:46.7564948Z INFO 02-03 19:38:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:38:46.7575360Z INFO 02-03 19:38:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:38:46.7633080Z INFO 02-03 19:38:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:38:46.8377684Z INFO 02-03 19:38:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-03T19:38:46.8385638Z INFO 02-03 19:38:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-03T19:38:46.8394470Z INFO 02-03 19:38:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-03T19:38:46.8458508Z INFO 02-03 19:38:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-03T19:39:11.6417520Z [0;36m(Worker_DP1_TP5_EP13 pid=620)[0;0m WARNING 02-03 19:39:11 [sfa_v1.py:479] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-03T19:39:11.6447118Z [0;36m(Worker_DP0_TP3_EP3 pid=409)[0;0m WARNING 02-03 19:39:11 [sfa_v1.py:479] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-03T19:39:11.6456681Z [0;36m(Worker_DP0_TP2_EP2 pid=305)[0;0m WARNING 02-03 19:39:11 [sfa_v1.py:479] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-03T19:39:11.6564708Z [0;36m(Worker_DP1_TP7_EP15 pid=829)[0;0m WARNING 02-03 19:39:11 [sfa_v1.py:479] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-03T19:39:11.6593760Z [0;36m(Worker_DP0_TP4_EP4 pid=513)[0;0m WARNING 02-03 19:39:11 [sfa_v1.py:479] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-03T19:39:11.6618537Z [0;36m(Worker_DP1_TP1_EP9 pid=190)[0;0m WARNING 02-03 19:39:11 [sfa_v1.py:479] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-03T19:39:11.6641967Z [0;36m(Worker_DP1_TP6_EP14 pid=725)[0;0m WARNING 02-03 19:39:11 [sfa_v1.py:479] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-03T19:39:11.6664972Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m WARNING 02-03 19:39:11 [sfa_v1.py:479] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-03T19:39:11.6694034Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m WARNING 02-03 19:39:11 [sfa_v1.py:479] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-03T19:39:11.6763282Z [0;36m(Worker_DP0_TP6_EP6 pid=721)[0;0m WARNING 02-03 19:39:11 [sfa_v1.py:479] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-03T19:39:11.6773471Z [0;36m(Worker_DP0_TP5_EP5 pid=617)[0;0m WARNING 02-03 19:39:11 [sfa_v1.py:479] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-03T19:39:11.6782845Z [0;36m(Worker_DP0_TP1_EP1 pid=187)[0;0m WARNING 02-03 19:39:11 [sfa_v1.py:479] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-03T19:39:11.6820568Z [0;36m(Worker_DP1_TP4_EP12 pid=516)[0;0m WARNING 02-03 19:39:11 [sfa_v1.py:479] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-03T19:39:11.6922848Z [0;36m(Worker_DP1_TP2_EP10 pid=308)[0;0m WARNING 02-03 19:39:11 [sfa_v1.py:479] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-03T19:39:11.7000970Z [0;36m(Worker_DP1_TP3_EP11 pid=412)[0;0m WARNING 02-03 19:39:11 [sfa_v1.py:479] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-03T19:39:11.7010977Z [0;36m(Worker_DP1_TP5_EP13 pid=620)[0;0m INFO 02-03 19:39:11 [model_runner_v1.py:2254] Loading drafter model...
2026-02-03T19:39:11.7071122Z [0;36m(Worker_DP0_TP2_EP2 pid=305)[0;0m INFO 02-03 19:39:11 [model_runner_v1.py:2254] Loading drafter model...
2026-02-03T19:39:11.7081021Z [0;36m(Worker_DP0_TP3_EP3 pid=409)[0;0m INFO 02-03 19:39:11 [model_runner_v1.py:2254] Loading drafter model...
2026-02-03T19:39:11.7190863Z [0;36m(Worker_DP1_TP1_EP9 pid=190)[0;0m INFO 02-03 19:39:11 [model_runner_v1.py:2254] Loading drafter model...
2026-02-03T19:39:11.7217281Z [0;36m(Worker_DP1_TP7_EP15 pid=829)[0;0m INFO 02-03 19:39:11 [model_runner_v1.py:2254] Loading drafter model...
2026-02-03T19:39:11.7308617Z [0;36m(Worker_DP1_TP6_EP14 pid=725)[0;0m INFO 02-03 19:39:11 [model_runner_v1.py:2254] Loading drafter model...
2026-02-03T19:39:11.7331621Z [0;36m(Worker_DP0_TP5_EP5 pid=617)[0;0m INFO 02-03 19:39:11 [model_runner_v1.py:2254] Loading drafter model...
2026-02-03T19:39:11.7340715Z [0;36m(Worker_DP0_TP4_EP4 pid=513)[0;0m INFO 02-03 19:39:11 [model_runner_v1.py:2254] Loading drafter model...
2026-02-03T19:39:11.7351582Z [0;36m(Worker_DP0_TP7_EP7 pid=825)[0;0m WARNING 02-03 19:39:11 [sfa_v1.py:479] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-03T19:39:11.7376429Z [0;36m(Worker_DP0_TP6_EP6 pid=721)[0;0m INFO 02-03 19:39:11 [model_runner_v1.py:2254] Loading drafter model...
2026-02-03T19:39:11.7431096Z [0;36m(Worker_DP1_TP4_EP12 pid=516)[0;0m INFO 02-03 19:39:11 [model_runner_v1.py:2254] Loading drafter model...
2026-02-03T19:39:11.7433235Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m INFO 02-03 19:39:11 [model_runner_v1.py:2254] Loading drafter model...
2026-02-03T19:39:11.7433960Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m INFO 02-03 19:39:11 [model_runner_v1.py:2254] Loading drafter model...
2026-02-03T19:39:11.7434549Z [0;36m(Worker_DP0_TP1_EP1 pid=187)[0;0m INFO 02-03 19:39:11 [model_runner_v1.py:2254] Loading drafter model...
2026-02-03T19:39:11.7447513Z [0;36m(Worker_DP1_TP2_EP10 pid=308)[0;0m INFO 02-03 19:39:11 [model_runner_v1.py:2254] Loading drafter model...
2026-02-03T19:39:11.7636515Z [0;36m(Worker_DP1_TP3_EP11 pid=412)[0;0m INFO 02-03 19:39:11 [model_runner_v1.py:2254] Loading drafter model...
2026-02-03T19:39:11.8388709Z [0;36m(Worker_DP0_TP7_EP7 pid=825)[0;0m INFO 02-03 19:39:11 [model_runner_v1.py:2254] Loading drafter model...
2026-02-03T19:39:13.7205190Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:13.7205674Z Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
2026-02-03T19:39:14.4909857Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:14.4910241Z Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:41,  3.89it/s]
2026-02-03T19:39:15.6550996Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:15.6551697Z Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:01<01:27,  1.82it/s]
2026-02-03T19:39:16.6931166Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:16.6931594Z Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:02<01:51,  1.42it/s]
2026-02-03T19:39:17.7485688Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:17.7486146Z Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:04<02:07,  1.23it/s]
2026-02-03T19:39:18.8688496Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:18.8688921Z Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:05<02:21,  1.10it/s]
2026-02-03T19:39:19.8598708Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:19.8599123Z Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:06<02:24,  1.07it/s]
2026-02-03T19:39:20.9484255Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:20.9485175Z Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:07<02:30,  1.02it/s]
2026-02-03T19:39:22.0563528Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:22.0564036Z Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:08<02:35,  1.02s/it]
2026-02-03T19:39:23.0922855Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:23.0923356Z Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:09<02:35,  1.02s/it]
2026-02-03T19:39:24.1808209Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:24.1808674Z Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:10<02:37,  1.04s/it]
2026-02-03T19:39:25.2582878Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:25.2583291Z Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:11<02:38,  1.05s/it]
2026-02-03T19:39:26.3526290Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:26.3526765Z Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:12<02:38,  1.07s/it]
2026-02-03T19:39:27.3795130Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:27.3795538Z Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:13<02:36,  1.05s/it]
2026-02-03T19:39:28.3943937Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:28.3944389Z Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:14<02:33,  1.04s/it]
2026-02-03T19:39:29.4224679Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:29.4225168Z Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:15<02:31,  1.04s/it]
2026-02-03T19:39:30.3974214Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:30.3974723Z Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:16<02:27,  1.02s/it]
2026-02-03T19:39:31.4906926Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:31.4907347Z Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:17<02:29,  1.04s/it]
2026-02-03T19:39:32.6087385Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:32.6087837Z Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:18<02:32,  1.06s/it]
2026-02-03T19:39:33.7165126Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:33.7165534Z Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:19<02:32,  1.08s/it]
2026-02-03T19:39:34.8241196Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:34.8241721Z Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:21<02:33,  1.09s/it]
2026-02-03T19:39:35.9509455Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:35.9509915Z Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:22<02:33,  1.10s/it]
2026-02-03T19:39:37.0234767Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:37.0235167Z Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:23<02:31,  1.09s/it]
2026-02-03T19:39:38.0741142Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:38.0741561Z Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:24<02:28,  1.08s/it]
2026-02-03T19:39:39.4404165Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:39.4404622Z Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:25<02:39,  1.16s/it]
2026-02-03T19:39:40.7021017Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:40.7021420Z Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:26<02:42,  1.19s/it]
2026-02-03T19:39:41.7249085Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:41.7249623Z Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:28<02:34,  1.14s/it]
2026-02-03T19:39:42.8247800Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:42.8248237Z Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:29<02:31,  1.13s/it]
2026-02-03T19:39:43.9345534Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:43.9346094Z Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:30<02:29,  1.12s/it]
2026-02-03T19:39:45.1098074Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:45.1098791Z Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:31<02:30,  1.14s/it]
2026-02-03T19:39:46.2200413Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:46.2200837Z Loading safetensors checkpoint shards:  20% Completed | 32/163 [00:32<02:28,  1.13s/it]
2026-02-03T19:39:47.3329185Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:47.3329624Z Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:33<02:26,  1.13s/it]
2026-02-03T19:39:48.4537643Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:48.4538082Z Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:34<02:24,  1.12s/it]
2026-02-03T19:39:49.6327429Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:49.6327869Z Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:35<02:25,  1.14s/it]
2026-02-03T19:39:50.6265522Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:50.6266002Z Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:36<02:19,  1.10s/it]
2026-02-03T19:39:51.7207927Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:51.7208324Z Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:38<02:18,  1.10s/it]
2026-02-03T19:39:52.8445250Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:52.8445640Z Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:39<02:18,  1.10s/it]
2026-02-03T19:39:53.9987141Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:53.9987527Z Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:40<02:18,  1.12s/it]
2026-02-03T19:39:55.0499888Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:55.0500315Z Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:41<02:15,  1.10s/it]
2026-02-03T19:39:56.1663910Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:56.1664308Z Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:42<02:14,  1.10s/it]
2026-02-03T19:39:57.3253352Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:57.3253792Z Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:43<02:15,  1.12s/it]
2026-02-03T19:39:58.3278483Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:58.3279001Z Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:44<02:10,  1.09s/it]
2026-02-03T19:39:59.6965034Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:39:59.6965449Z Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:45<02:19,  1.17s/it]
2026-02-03T19:40:00.6129143Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:00.6129551Z Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:46<02:09,  1.09s/it]
2026-02-03T19:40:01.8485206Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:01.8485685Z Loading safetensors checkpoint shards:  28% Completed | 46/163 [00:48<02:12,  1.14s/it]
2026-02-03T19:40:02.5922303Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:02.5923006Z Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:48<01:58,  1.02s/it]
2026-02-03T19:40:03.7458088Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:03.7458503Z Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:50<01:32,  1.23it/s]
2026-02-03T19:40:04.8803142Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:04.8803575Z Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:51<01:41,  1.12it/s]
2026-02-03T19:40:05.9590341Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:05.9590740Z Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:52<01:45,  1.06it/s]
2026-02-03T19:40:07.1032589Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:07.1033032Z Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:53<01:50,  1.00it/s]
2026-02-03T19:40:08.1178444Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:08.1178868Z Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:54<01:50,  1.00s/it]
2026-02-03T19:40:09.2088345Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:09.2088795Z Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:55<01:51,  1.03s/it]
2026-02-03T19:40:10.3562283Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:10.3562791Z Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:56<01:54,  1.06s/it]
2026-02-03T19:40:11.5018970Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:11.5019364Z Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:57<01:56,  1.09s/it]
2026-02-03T19:40:12.5890348Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:12.5890784Z Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:58<01:55,  1.09s/it]
2026-02-03T19:40:13.7012329Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:13.7012749Z Loading safetensors checkpoint shards:  36% Completed | 58/163 [00:59<01:54,  1.09s/it]
2026-02-03T19:40:14.9103858Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:14.9104385Z Loading safetensors checkpoint shards:  36% Completed | 59/163 [01:01<01:57,  1.13s/it]
2026-02-03T19:40:16.1640614Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:16.1641029Z Loading safetensors checkpoint shards:  37% Completed | 60/163 [01:02<02:00,  1.17s/it]
2026-02-03T19:40:17.3948549Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:17.3948973Z Loading safetensors checkpoint shards:  37% Completed | 61/163 [01:03<02:00,  1.19s/it]
2026-02-03T19:40:18.4719300Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:18.4719795Z Loading safetensors checkpoint shards:  38% Completed | 62/163 [01:04<01:56,  1.15s/it]
2026-02-03T19:40:18.6053804Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:18.6054211Z Loading safetensors checkpoint shards:  40% Completed | 65/163 [01:04<00:51,  1.89it/s]
2026-02-03T19:40:18.7146997Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:18.7147394Z Loading safetensors checkpoint shards:  42% Completed | 68/163 [01:04<00:29,  3.22it/s]
2026-02-03T19:40:18.8227424Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:18.8227952Z Loading safetensors checkpoint shards:  44% Completed | 71/163 [01:05<00:18,  4.91it/s]
2026-02-03T19:40:18.9268127Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:18.9268476Z Loading safetensors checkpoint shards:  45% Completed | 74/163 [01:05<00:12,  6.97it/s]
2026-02-03T19:40:19.0353910Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:19.0354344Z Loading safetensors checkpoint shards:  47% Completed | 77/163 [01:05<00:09,  9.33it/s]
2026-02-03T19:40:19.1403678Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:19.1404061Z Loading safetensors checkpoint shards:  49% Completed | 80/163 [01:05<00:06, 11.95it/s]
2026-02-03T19:40:19.2441071Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:19.2441600Z Loading safetensors checkpoint shards:  51% Completed | 83/163 [01:05<00:05, 14.68it/s]
2026-02-03T19:40:19.3492927Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:19.3493542Z Loading safetensors checkpoint shards:  53% Completed | 86/163 [01:05<00:04, 17.30it/s]
2026-02-03T19:40:19.4554732Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:19.4555130Z Loading safetensors checkpoint shards:  55% Completed | 89/163 [01:05<00:03, 19.65it/s]
2026-02-03T19:40:19.5588457Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:19.5588873Z Loading safetensors checkpoint shards:  56% Completed | 92/163 [01:05<00:03, 21.80it/s]
2026-02-03T19:40:20.7564859Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:20.7565375Z Loading safetensors checkpoint shards:  58% Completed | 95/163 [01:07<00:10,  6.53it/s]
2026-02-03T19:40:24.3353487Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:24.3353999Z Loading safetensors checkpoint shards:  60% Completed | 98/163 [01:10<00:30,  2.14it/s]
2026-02-03T19:40:26.8080575Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:26.8081051Z Loading safetensors checkpoint shards:  61% Completed | 100/163 [01:13<00:40,  1.56it/s]
2026-02-03T19:40:28.9427039Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:28.9427575Z Loading safetensors checkpoint shards:  63% Completed | 102/163 [01:15<00:45,  1.34it/s]
2026-02-03T19:40:30.2116726Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:30.2117416Z Loading safetensors checkpoint shards:  63% Completed | 103/163 [01:16<00:49,  1.22it/s]
2026-02-03T19:40:31.3519266Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:31.3519680Z Loading safetensors checkpoint shards:  64% Completed | 104/163 [01:17<00:51,  1.14it/s]
2026-02-03T19:40:32.4731125Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:32.4731561Z Loading safetensors checkpoint shards:  64% Completed | 105/163 [01:18<00:53,  1.08it/s]
2026-02-03T19:40:33.7925341Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:33.7925791Z Loading safetensors checkpoint shards:  65% Completed | 106/163 [01:20<00:57,  1.01s/it]
2026-02-03T19:40:36.8764563Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:36.8765105Z Loading safetensors checkpoint shards:  66% Completed | 107/163 [01:23<01:24,  1.51s/it]
2026-02-03T19:40:37.6877144Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:37.6877566Z Loading safetensors checkpoint shards:  66% Completed | 108/163 [01:23<01:13,  1.33s/it]
2026-02-03T19:40:38.5827318Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:38.5827868Z Loading safetensors checkpoint shards:  67% Completed | 109/163 [01:24<01:05,  1.21s/it]
2026-02-03T19:40:39.2930278Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:39.2930907Z Loading safetensors checkpoint shards:  67% Completed | 110/163 [01:25<00:57,  1.08s/it]
2026-02-03T19:40:39.9817315Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:39.9817822Z Loading safetensors checkpoint shards:  68% Completed | 111/163 [01:26<00:50,  1.04it/s]
2026-02-03T19:40:40.7983761Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:40.7984219Z Loading safetensors checkpoint shards:  69% Completed | 112/163 [01:27<00:47,  1.08it/s]
2026-02-03T19:40:41.5829274Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:41.5829673Z Loading safetensors checkpoint shards:  69% Completed | 113/163 [01:27<00:44,  1.13it/s]
2026-02-03T19:40:42.3040522Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:42.3040932Z Loading safetensors checkpoint shards:  70% Completed | 114/163 [01:28<00:40,  1.20it/s]
2026-02-03T19:40:43.0699591Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:43.0700073Z Loading safetensors checkpoint shards:  71% Completed | 115/163 [01:29<00:39,  1.23it/s]
2026-02-03T19:40:43.8203444Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:43.8203967Z Loading safetensors checkpoint shards:  71% Completed | 116/163 [01:30<00:37,  1.26it/s]
2026-02-03T19:40:44.5917755Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:44.5918279Z Loading safetensors checkpoint shards:  72% Completed | 117/163 [01:30<00:36,  1.27it/s]
2026-02-03T19:40:45.3307386Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:45.3307810Z Loading safetensors checkpoint shards:  72% Completed | 118/163 [01:31<00:34,  1.31it/s]
2026-02-03T19:40:46.0554924Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:46.0555353Z Loading safetensors checkpoint shards:  73% Completed | 119/163 [01:32<00:33,  1.31it/s]
2026-02-03T19:40:46.8542304Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:46.8542778Z Loading safetensors checkpoint shards:  74% Completed | 120/163 [01:33<00:33,  1.30it/s]
2026-02-03T19:40:47.6791492Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:47.6791901Z Loading safetensors checkpoint shards:  74% Completed | 121/163 [01:33<00:33,  1.27it/s]
2026-02-03T19:40:48.4906308Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:48.4906763Z Loading safetensors checkpoint shards:  75% Completed | 122/163 [01:34<00:32,  1.26it/s]
2026-02-03T19:40:49.2170174Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:49.2170753Z Loading safetensors checkpoint shards:  75% Completed | 123/163 [01:35<00:30,  1.29it/s]
2026-02-03T19:40:50.0126671Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:50.0127165Z Loading safetensors checkpoint shards:  76% Completed | 124/163 [01:36<00:30,  1.28it/s]
2026-02-03T19:40:50.7849289Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:50.7849803Z Loading safetensors checkpoint shards:  77% Completed | 125/163 [01:37<00:29,  1.28it/s]
2026-02-03T19:40:51.6336254Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:51.6336669Z Loading safetensors checkpoint shards:  77% Completed | 126/163 [01:37<00:29,  1.25it/s]
2026-02-03T19:40:52.4737537Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:52.4737943Z Loading safetensors checkpoint shards:  78% Completed | 127/163 [01:38<00:29,  1.23it/s]
2026-02-03T19:40:53.1696815Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:53.1697282Z Loading safetensors checkpoint shards:  79% Completed | 128/163 [01:39<00:27,  1.29it/s]
2026-02-03T19:40:53.9778419Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:53.9778906Z Loading safetensors checkpoint shards:  80% Completed | 130/163 [01:40<00:19,  1.65it/s]
2026-02-03T19:40:54.7637700Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:55.5890734Z Loading safetensors checkpoint shards:  80% Completed | 131/163 [01:41<00:20,  1.54it/s]
2026-02-03T19:40:55.5891298Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:55.5891712Z Loading safetensors checkpoint shards:  81% Completed | 132/163 [01:41<00:21,  1.44it/s]
2026-02-03T19:40:56.4094362Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:56.4094790Z Loading safetensors checkpoint shards:  82% Completed | 133/163 [01:42<00:21,  1.37it/s]
2026-02-03T19:40:57.1313444Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:57.1313873Z Loading safetensors checkpoint shards:  82% Completed | 134/163 [01:43<00:21,  1.37it/s]
2026-02-03T19:40:57.8152322Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:57.8152770Z Loading safetensors checkpoint shards:  83% Completed | 135/163 [01:44<00:20,  1.40it/s]
2026-02-03T19:40:58.5379029Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:58.5379460Z Loading safetensors checkpoint shards:  83% Completed | 136/163 [01:44<00:19,  1.39it/s]
2026-02-03T19:40:59.2829698Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:59.2830111Z Loading safetensors checkpoint shards:  84% Completed | 137/163 [01:45<00:18,  1.38it/s]
2026-02-03T19:40:59.9082763Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:40:59.9083180Z Loading safetensors checkpoint shards:  85% Completed | 138/163 [01:46<00:17,  1.44it/s]
2026-02-03T19:41:00.5472963Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:00.5473371Z Loading safetensors checkpoint shards:  85% Completed | 139/163 [01:46<00:16,  1.47it/s]
2026-02-03T19:41:01.2280476Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:01.2281170Z Loading safetensors checkpoint shards:  86% Completed | 140/163 [01:47<00:15,  1.47it/s]
2026-02-03T19:41:02.0239865Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:02.0240366Z Loading safetensors checkpoint shards:  87% Completed | 141/163 [01:48<00:15,  1.40it/s]
2026-02-03T19:41:02.7068417Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:02.7068985Z Loading safetensors checkpoint shards:  87% Completed | 142/163 [01:48<00:14,  1.42it/s]
2026-02-03T19:41:03.4542523Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:03.4542989Z Loading safetensors checkpoint shards:  88% Completed | 143/163 [01:49<00:14,  1.39it/s]
2026-02-03T19:41:04.0865718Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:04.0866153Z Loading safetensors checkpoint shards:  88% Completed | 144/163 [01:50<00:13,  1.45it/s]
2026-02-03T19:41:04.7737399Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:04.7737952Z Loading safetensors checkpoint shards:  89% Completed | 145/163 [01:51<00:12,  1.45it/s]
2026-02-03T19:41:05.3582181Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:05.3582651Z Loading safetensors checkpoint shards:  90% Completed | 146/163 [01:51<00:11,  1.52it/s]
2026-02-03T19:41:06.0279495Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:06.0279897Z Loading safetensors checkpoint shards:  90% Completed | 147/163 [01:52<00:10,  1.51it/s]
2026-02-03T19:41:06.7486436Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:06.7486887Z Loading safetensors checkpoint shards:  91% Completed | 148/163 [01:53<00:10,  1.47it/s]
2026-02-03T19:41:07.5223007Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:07.5223479Z Loading safetensors checkpoint shards:  91% Completed | 149/163 [01:53<00:09,  1.41it/s]
2026-02-03T19:41:11.4864555Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:11.4864968Z Loading safetensors checkpoint shards:  92% Completed | 150/163 [01:57<00:21,  1.68s/it]
2026-02-03T19:41:12.1768930Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:12.1769367Z Loading safetensors checkpoint shards:  93% Completed | 151/163 [01:58<00:16,  1.39s/it]
2026-02-03T19:41:12.9536820Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:12.9537224Z Loading safetensors checkpoint shards:  93% Completed | 152/163 [01:59<00:13,  1.20s/it]
2026-02-03T19:41:13.6705496Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:13.6705927Z Loading safetensors checkpoint shards:  94% Completed | 153/163 [01:59<00:10,  1.06s/it]
2026-02-03T19:41:14.4346050Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:14.4346511Z Loading safetensors checkpoint shards:  94% Completed | 154/163 [02:00<00:08,  1.03it/s]
2026-02-03T19:41:15.1964160Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:15.1964583Z Loading safetensors checkpoint shards:  95% Completed | 155/163 [02:01<00:07,  1.10it/s]
2026-02-03T19:41:16.5290291Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:16.5290870Z Loading safetensors checkpoint shards:  96% Completed | 156/163 [02:02<00:07,  1.03s/it]
2026-02-03T19:41:17.3948347Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:17.3948851Z Loading safetensors checkpoint shards:  96% Completed | 157/163 [02:03<00:05,  1.02it/s]
2026-02-03T19:41:18.3641094Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:18.3641725Z Loading safetensors checkpoint shards:  97% Completed | 158/163 [02:04<00:04,  1.02it/s]
2026-02-03T19:41:19.4130116Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:19.4130544Z Loading safetensors checkpoint shards:  98% Completed | 159/163 [02:05<00:04,  1.00s/it]
2026-02-03T19:41:20.5079770Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:20.5080208Z Loading safetensors checkpoint shards:  98% Completed | 160/163 [02:06<00:03,  1.03s/it]
2026-02-03T19:41:21.5678119Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:21.5678578Z Loading safetensors checkpoint shards:  99% Completed | 161/163 [02:07<00:02,  1.04s/it]
2026-02-03T19:41:22.6297040Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:22.6297552Z Loading safetensors checkpoint shards:  99% Completed | 162/163 [02:08<00:01,  1.05s/it]
2026-02-03T19:41:23.7614690Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:23.7615205Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [02:10<00:00,  1.07s/it]
2026-02-03T19:41:23.7665064Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:23.7665475Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [02:10<00:00,  1.25it/s]
2026-02-03T19:41:23.7674658Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:41:23.8473812Z [0;36m(Worker_DP0_TP3_EP3 pid=409)[0;0m INFO 02-03 19:41:23 [eagle_proposer.py:261] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-03T19:41:23.9367911Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m INFO 02-03 19:41:23 [default_loader.py:291] Loading weights took 130.22 seconds
2026-02-03T19:41:23.9742488Z [0;36m(Worker_DP1_TP6_EP14 pid=725)[0;0m INFO 02-03 19:41:23 [eagle_proposer.py:261] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-03T19:41:23.9775263Z [0;36m(Worker_DP1_TP5_EP13 pid=620)[0;0m INFO 02-03 19:41:23 [eagle_proposer.py:261] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-03T19:41:23.9801314Z [0;36m(Worker_DP0_TP4_EP4 pid=513)[0;0m INFO 02-03 19:41:23 [eagle_proposer.py:261] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-03T19:41:23.9812128Z [0;36m(Worker_DP0_TP1_EP1 pid=187)[0;0m INFO 02-03 19:41:23 [eagle_proposer.py:261] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-03T19:41:23.9820792Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m INFO 02-03 19:41:23 [eagle_proposer.py:261] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-03T19:41:23.9831332Z [0;36m(Worker_DP0_TP7_EP7 pid=825)[0;0m INFO 02-03 19:41:23 [eagle_proposer.py:261] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-03T19:41:23.9958045Z [0;36m(Worker_DP1_TP2_EP10 pid=308)[0;0m INFO 02-03 19:41:23 [eagle_proposer.py:261] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-03T19:41:24.4977169Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m INFO 02-03 19:41:24 [default_loader.py:291] Loading weights took 131.55 seconds
2026-02-03T19:41:24.5440718Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m INFO 02-03 19:41:24 [eagle_proposer.py:261] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-03T19:41:24.5464010Z [0;36m(Worker_DP1_TP7_EP15 pid=829)[0;0m INFO 02-03 19:41:24 [eagle_proposer.py:261] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-03T19:41:24.5491572Z [0;36m(Worker_DP1_TP1_EP9 pid=190)[0;0m INFO 02-03 19:41:24 [eagle_proposer.py:261] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-03T19:41:24.5500978Z [0;36m(Worker_DP0_TP5_EP5 pid=617)[0;0m INFO 02-03 19:41:24 [eagle_proposer.py:261] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-03T19:41:24.5510728Z [0;36m(Worker_DP1_TP4_EP12 pid=516)[0;0m INFO 02-03 19:41:24 [eagle_proposer.py:261] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-03T19:41:24.5520057Z [0;36m(Worker_DP0_TP2_EP2 pid=305)[0;0m INFO 02-03 19:41:24 [eagle_proposer.py:261] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-03T19:41:24.5541856Z [0;36m(Worker_DP1_TP3_EP11 pid=412)[0;0m INFO 02-03 19:41:24 [eagle_proposer.py:261] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-03T19:41:24.5568645Z [0;36m(Worker_DP0_TP6_EP6 pid=721)[0;0m INFO 02-03 19:41:24 [eagle_proposer.py:261] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-03T19:41:24.9241869Z [0;36m(Worker_DP1_TP6_EP14 pid=725)[0;0m INFO 02-03 19:41:24 [model_runner_v1.py:2262] Loading model weights took 30.8108 GB
2026-02-03T19:41:25.0412692Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m INFO 02-03 19:41:25 [model_runner_v1.py:2262] Loading model weights took 30.8108 GB
2026-02-03T19:41:25.0924262Z [0;36m(Worker_DP0_TP7_EP7 pid=825)[0;0m INFO 02-03 19:41:25 [model_runner_v1.py:2262] Loading model weights took 30.8108 GB
2026-02-03T19:41:25.1696368Z [0;36m(Worker_DP1_TP5_EP13 pid=620)[0;0m INFO 02-03 19:41:25 [model_runner_v1.py:2262] Loading model weights took 30.8108 GB
2026-02-03T19:41:25.6815815Z [0;36m(Worker_DP1_TP4_EP12 pid=516)[0;0m INFO 02-03 19:41:25 [model_runner_v1.py:2262] Loading model weights took 30.8108 GB
2026-02-03T19:41:25.7071823Z [0;36m(Worker_DP1_TP3_EP11 pid=412)[0;0m INFO 02-03 19:41:25 [model_runner_v1.py:2262] Loading model weights took 30.8108 GB
2026-02-03T19:41:25.8390679Z [0;36m(Worker_DP0_TP1_EP1 pid=187)[0;0m INFO 02-03 19:41:25 [model_runner_v1.py:2262] Loading model weights took 30.8108 GB
2026-02-03T19:41:25.9883176Z [0;36m(Worker_DP0_TP3_EP3 pid=409)[0;0m INFO 02-03 19:41:25 [model_runner_v1.py:2262] Loading model weights took 30.8108 GB
2026-02-03T19:41:25.9949905Z [0;36m(Worker_DP0_TP4_EP4 pid=513)[0;0m INFO 02-03 19:41:25 [model_runner_v1.py:2262] Loading model weights took 30.8108 GB
2026-02-03T19:41:26.4036230Z [0;36m(Worker_DP0_TP2_EP2 pid=305)[0;0m INFO 02-03 19:41:26 [model_runner_v1.py:2262] Loading model weights took 30.8108 GB
2026-02-03T19:41:26.4630290Z [0;36m(Worker_DP1_TP7_EP15 pid=829)[0;0m INFO 02-03 19:41:26 [model_runner_v1.py:2262] Loading model weights took 30.8108 GB
2026-02-03T19:41:26.6824446Z [0;36m(Worker_DP0_TP6_EP6 pid=721)[0;0m INFO 02-03 19:41:26 [model_runner_v1.py:2262] Loading model weights took 30.8108 GB
2026-02-03T19:41:26.7155068Z [0;36m(Worker_DP0_TP5_EP5 pid=617)[0;0m INFO 02-03 19:41:26 [model_runner_v1.py:2262] Loading model weights took 30.8108 GB
2026-02-03T19:41:26.7743203Z [0;36m(Worker_DP1_TP2_EP10 pid=308)[0;0m INFO 02-03 19:41:26 [model_runner_v1.py:2262] Loading model weights took 30.8108 GB
2026-02-03T19:41:26.8293284Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m INFO 02-03 19:41:26 [model_runner_v1.py:2262] Loading model weights took 30.8108 GB
2026-02-03T19:41:26.8786773Z [0;36m(Worker_DP1_TP1_EP9 pid=190)[0;0m INFO 02-03 19:41:26 [model_runner_v1.py:2262] Loading model weights took 30.8108 GB
2026-02-03T19:41:32.5439488Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m INFO 02-03 19:41:32 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/cf265e3432/rank_0_0/backbone for vLLM's torch.compile
2026-02-03T19:41:32.5534307Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m INFO 02-03 19:41:32 [backends.py:865] Dynamo bytecode transform time: 4.49 s
2026-02-03T19:41:32.6881348Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m INFO 02-03 19:41:32 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/cf265e3432/rank_0_1/backbone for vLLM's torch.compile
2026-02-03T19:41:32.7012442Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m INFO 02-03 19:41:32 [backends.py:865] Dynamo bytecode transform time: 4.64 s
2026-02-03T19:41:46.3421716Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m INFO 02-03 19:41:46 [backends.py:319] Compiling a graph for compile range (1, 4096) takes 7.62 s
2026-02-03T19:41:46.3451717Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m INFO 02-03 19:41:46 [monitor.py:34] torch.compile takes 12.11 s in total
2026-02-03T19:41:47.0818735Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m INFO 02-03 19:41:47 [backends.py:319] Compiling a graph for compile range (1, 4096) takes 8.34 s
2026-02-03T19:41:47.0849786Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m INFO 02-03 19:41:47 [monitor.py:34] torch.compile takes 12.97 s in total
2026-02-03T19:41:49.4081876Z [0;36m(ApiServer_3 pid=123)[0;0m Process ApiServer_3:
2026-02-03T19:41:49.4092598Z [0;36m(ApiServer_1 pid=121)[0;0m Process ApiServer_1:
2026-02-03T19:41:49.4197142Z [0;36m(ApiServer_3 pid=123)[0;0m Traceback (most recent call last):
2026-02-03T19:41:49.4207428Z [0;36m(ApiServer_1 pid=121)[0;0m Traceback (most recent call last):
2026-02-03T19:41:49.4221766Z [0;36m(ApiServer_3 pid=123)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-03T19:41:49.4231660Z [0;36m(ApiServer_3 pid=123)[0;0m     self.run()
2026-02-03T19:41:49.4241467Z [0;36m(ApiServer_3 pid=123)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-03T19:41:49.4250822Z [0;36m(ApiServer_3 pid=123)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-03T19:41:49.4260082Z [0;36m(ApiServer_3 pid=123)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-03T19:41:49.4269677Z [0;36m(ApiServer_3 pid=123)[0;0m     uvloop.run(
2026-02-03T19:41:49.4279856Z [0;36m(ApiServer_3 pid=123)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-03T19:41:49.4289411Z [0;36m(ApiServer_3 pid=123)[0;0m     return runner.run(wrapper())
2026-02-03T19:41:49.4299486Z [0;36m(ApiServer_3 pid=123)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.4309635Z [0;36m(ApiServer_3 pid=123)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-03T19:41:49.4319156Z [0;36m(ApiServer_3 pid=123)[0;0m     return self._loop.run_until_complete(task)
2026-02-03T19:41:49.4328611Z [0;36m(ApiServer_3 pid=123)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.4338307Z [0;36m(ApiServer_3 pid=123)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-03T19:41:49.4348290Z [0;36m(ApiServer_3 pid=123)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-03T19:41:49.4357654Z [0;36m(ApiServer_3 pid=123)[0;0m     return await main
2026-02-03T19:41:49.4367827Z [0;36m(ApiServer_3 pid=123)[0;0m            ^^^^^^^^^^
2026-02-03T19:41:49.4377750Z [0;36m(ApiServer_3 pid=123)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-03T19:41:49.4387923Z [0;36m(ApiServer_3 pid=123)[0;0m     async with build_async_engine_client(
2026-02-03T19:41:49.4397739Z [0;36m(ApiServer_3 pid=123)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-03T19:41:49.4407352Z [0;36m(ApiServer_3 pid=123)[0;0m     return await anext(self.gen)
2026-02-03T19:41:49.4417397Z [0;36m(ApiServer_1 pid=121)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-03T19:41:49.4427267Z [0;36m(ApiServer_3 pid=123)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.4436977Z [0;36m(ApiServer_1 pid=121)[0;0m     self.run()
2026-02-03T19:41:49.4446953Z [0;36m(ApiServer_3 pid=123)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-03T19:41:49.4456795Z [0;36m(ApiServer_3 pid=123)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-03T19:41:49.4467543Z [0;36m(ApiServer_3 pid=123)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-03T19:41:49.4477525Z [0;36m(ApiServer_1 pid=121)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-03T19:41:49.4487512Z [0;36m(ApiServer_3 pid=123)[0;0m     return await anext(self.gen)
2026-02-03T19:41:49.4499159Z [0;36m(ApiServer_3 pid=123)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.4507003Z [0;36m(ApiServer_1 pid=121)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-03T19:41:49.4517274Z [0;36m(ApiServer_3 pid=123)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-03T19:41:49.4527223Z [0;36m(ApiServer_1 pid=121)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-03T19:41:49.4536550Z [0;36m(ApiServer_3 pid=123)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-03T19:41:49.4546236Z [0;36m(ApiServer_1 pid=121)[0;0m     uvloop.run(
2026-02-03T19:41:49.4556871Z [0;36m(ApiServer_3 pid=123)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.4566662Z [0;36m(ApiServer_1 pid=121)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-03T19:41:49.4575625Z [0;36m(ApiServer_3 pid=123)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-03T19:41:49.4584800Z [0;36m(ApiServer_1 pid=121)[0;0m     return runner.run(wrapper())
2026-02-03T19:41:49.4593953Z [0;36m(ApiServer_3 pid=123)[0;0m     return cls(
2026-02-03T19:41:49.4603457Z [0;36m(ApiServer_1 pid=121)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.4612868Z [0;36m(ApiServer_3 pid=123)[0;0m            ^^^^
2026-02-03T19:41:49.4622811Z [0;36m(ApiServer_1 pid=121)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-03T19:41:49.4631871Z [0;36m(ApiServer_1 pid=121)[0;0m     return self._loop.run_until_complete(task)
2026-02-03T19:41:49.4641558Z [0;36m(ApiServer_3 pid=123)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-03T19:41:49.4650967Z [0;36m(ApiServer_1 pid=121)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.4660025Z [0;36m(ApiServer_3 pid=123)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-03T19:41:49.4669727Z [0;36m(ApiServer_3 pid=123)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.4679654Z [0;36m(ApiServer_1 pid=121)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-03T19:41:49.4689744Z [0;36m(ApiServer_3 pid=123)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-03T19:41:49.4699177Z [0;36m(ApiServer_1 pid=121)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-03T19:41:49.4708595Z [0;36m(ApiServer_3 pid=123)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-03T19:41:49.4717800Z [0;36m(ApiServer_1 pid=121)[0;0m     return await main
2026-02-03T19:41:49.4727136Z [0;36m(ApiServer_3 pid=123)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.4736211Z [0;36m(ApiServer_1 pid=121)[0;0m            ^^^^^^^^^^
2026-02-03T19:41:49.4745724Z [0;36m(ApiServer_3 pid=123)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-03T19:41:49.4754763Z [0;36m(ApiServer_1 pid=121)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-03T19:41:49.4764973Z [0;36m(ApiServer_3 pid=123)[0;0m     super().__init__(
2026-02-03T19:41:49.4774582Z [0;36m(ApiServer_1 pid=121)[0;0m     async with build_async_engine_client(
2026-02-03T19:41:49.4784136Z [0;36m(ApiServer_3 pid=123)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-03T19:41:49.4793222Z [0;36m(ApiServer_3 pid=123)[0;0m     super().__init__(
2026-02-03T19:41:49.4803299Z [0;36m(ApiServer_1 pid=121)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-03T19:41:49.4812628Z [0;36m(ApiServer_1 pid=121)[0;0m     return await anext(self.gen)
2026-02-03T19:41:49.4821750Z [0;36m(ApiServer_3 pid=123)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-03T19:41:49.4831919Z [0;36m(ApiServer_1 pid=121)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.4841137Z [0;36m(ApiServer_3 pid=123)[0;0m     super().__init__(
2026-02-03T19:41:49.4850281Z [0;36m(ApiServer_1 pid=121)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-03T19:41:49.4859027Z [0;36m(ApiServer_1 pid=121)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-03T19:41:49.4868398Z [0;36m(ApiServer_3 pid=123)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-03T19:41:49.4878261Z [0;36m(ApiServer_1 pid=121)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-03T19:41:49.4888430Z [0;36m(ApiServer_3 pid=123)[0;0m     raise TimeoutError(
2026-02-03T19:41:49.4896293Z [0;36m(ApiServer_1 pid=121)[0;0m     return await anext(self.gen)
2026-02-03T19:41:49.4905573Z [0;36m(ApiServer_1 pid=121)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.4915703Z [0;36m(ApiServer_1 pid=121)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-03T19:41:49.4926070Z [0;36m(ApiServer_3 pid=123)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-03T19:41:49.4933943Z [0;36m(ApiServer_1 pid=121)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-03T19:41:49.4943513Z [0;36m(ApiServer_1 pid=121)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.4952532Z [0;36m(ApiServer_1 pid=121)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-03T19:41:49.4962357Z [0;36m(ApiServer_1 pid=121)[0;0m     return cls(
2026-02-03T19:41:49.4971367Z [0;36m(ApiServer_1 pid=121)[0;0m            ^^^^
2026-02-03T19:41:49.4980196Z [0;36m(ApiServer_1 pid=121)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-03T19:41:49.4989055Z [0;36m(ApiServer_1 pid=121)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-03T19:41:49.4998106Z [0;36m(ApiServer_1 pid=121)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.5007587Z [0;36m(ApiServer_1 pid=121)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-03T19:41:49.5016901Z [0;36m(ApiServer_1 pid=121)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-03T19:41:49.5025430Z [0;36m(ApiServer_1 pid=121)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.5035220Z [0;36m(ApiServer_1 pid=121)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-03T19:41:49.5044770Z [0;36m(ApiServer_1 pid=121)[0;0m     super().__init__(
2026-02-03T19:41:49.5054284Z [0;36m(ApiServer_1 pid=121)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-03T19:41:49.5063188Z [0;36m(ApiServer_1 pid=121)[0;0m     super().__init__(
2026-02-03T19:41:49.5072424Z [0;36m(ApiServer_1 pid=121)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-03T19:41:49.5081727Z [0;36m(ApiServer_1 pid=121)[0;0m     super().__init__(
2026-02-03T19:41:49.5091456Z [0;36m(ApiServer_1 pid=121)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-03T19:41:49.5100576Z [0;36m(ApiServer_1 pid=121)[0;0m     raise TimeoutError(
2026-02-03T19:41:49.5109716Z [0;36m(ApiServer_1 pid=121)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-03T19:41:49.5426820Z [0;36m(ApiServer_2 pid=122)[0;0m Process ApiServer_2:
2026-02-03T19:41:49.5447412Z [0;36m(ApiServer_2 pid=122)[0;0m Traceback (most recent call last):
2026-02-03T19:41:49.5456959Z [0;36m(ApiServer_2 pid=122)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-03T19:41:49.5465447Z [0;36m(ApiServer_2 pid=122)[0;0m     self.run()
2026-02-03T19:41:49.5475103Z [0;36m(ApiServer_2 pid=122)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-03T19:41:49.5484469Z [0;36m(ApiServer_2 pid=122)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-03T19:41:49.5493678Z [0;36m(ApiServer_2 pid=122)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-03T19:41:49.5502092Z [0;36m(ApiServer_2 pid=122)[0;0m     uvloop.run(
2026-02-03T19:41:49.5511646Z [0;36m(ApiServer_2 pid=122)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-03T19:41:49.5520911Z [0;36m(ApiServer_2 pid=122)[0;0m     return runner.run(wrapper())
2026-02-03T19:41:49.5530319Z [0;36m(ApiServer_2 pid=122)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.5539851Z [0;36m(ApiServer_2 pid=122)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-03T19:41:49.5551462Z [0;36m(ApiServer_2 pid=122)[0;0m     return self._loop.run_until_complete(task)
2026-02-03T19:41:49.5557673Z [0;36m(ApiServer_2 pid=122)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.5600860Z [0;36m(ApiServer_2 pid=122)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-03T19:41:49.5602839Z [0;36m(ApiServer_2 pid=122)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-03T19:41:49.5603430Z [0;36m(ApiServer_2 pid=122)[0;0m     return await main
2026-02-03T19:41:49.5603785Z [0;36m(ApiServer_2 pid=122)[0;0m            ^^^^^^^^^^
2026-02-03T19:41:49.5604339Z [0;36m(ApiServer_2 pid=122)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-03T19:41:49.5613414Z [0;36m(ApiServer_2 pid=122)[0;0m     async with build_async_engine_client(
2026-02-03T19:41:49.5623172Z [0;36m(ApiServer_2 pid=122)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-03T19:41:49.5632389Z [0;36m(ApiServer_2 pid=122)[0;0m     return await anext(self.gen)
2026-02-03T19:41:49.5642231Z [0;36m(ApiServer_2 pid=122)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.5651372Z [0;36m(ApiServer_2 pid=122)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-03T19:41:49.5660562Z [0;36m(ApiServer_2 pid=122)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-03T19:41:49.5670105Z [0;36m(ApiServer_2 pid=122)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-03T19:41:49.5679695Z [0;36m(ApiServer_2 pid=122)[0;0m     return await anext(self.gen)
2026-02-03T19:41:49.5689493Z [0;36m(ApiServer_2 pid=122)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.5699142Z [0;36m(ApiServer_2 pid=122)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-03T19:41:49.5707309Z [0;36m(ApiServer_2 pid=122)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-03T19:41:49.5716496Z [0;36m(ApiServer_2 pid=122)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.5727576Z [0;36m(ApiServer_2 pid=122)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-03T19:41:49.5735638Z [0;36m(ApiServer_2 pid=122)[0;0m     return cls(
2026-02-03T19:41:49.5744331Z [0;36m(ApiServer_2 pid=122)[0;0m            ^^^^
2026-02-03T19:41:49.5753380Z [0;36m(ApiServer_2 pid=122)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-03T19:41:49.5764892Z [0;36m(ApiServer_2 pid=122)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-03T19:41:49.5773644Z [0;36m(ApiServer_2 pid=122)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.5783212Z [0;36m(ApiServer_2 pid=122)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-03T19:41:49.5792206Z [0;36m(ApiServer_2 pid=122)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-03T19:41:49.5801517Z [0;36m(ApiServer_2 pid=122)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:49.5811248Z [0;36m(ApiServer_2 pid=122)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-03T19:41:49.5820333Z [0;36m(ApiServer_2 pid=122)[0;0m     super().__init__(
2026-02-03T19:41:49.5829923Z [0;36m(ApiServer_2 pid=122)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-03T19:41:49.5839069Z [0;36m(ApiServer_2 pid=122)[0;0m     super().__init__(
2026-02-03T19:41:49.5848531Z [0;36m(ApiServer_2 pid=122)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-03T19:41:49.5857290Z [0;36m(ApiServer_2 pid=122)[0;0m     super().__init__(
2026-02-03T19:41:49.5866955Z [0;36m(ApiServer_2 pid=122)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-03T19:41:49.5875645Z [0;36m(ApiServer_2 pid=122)[0;0m     raise TimeoutError(
2026-02-03T19:41:49.5885071Z [0;36m(ApiServer_2 pid=122)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-03T19:41:49.8599742Z [0;36m(ApiServer_3 pid=123)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-03T19:41:49.8638396Z [0;36m(ApiServer_1 pid=121)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-03T19:41:49.9501020Z [0;36m(ApiServer_2 pid=122)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-03T19:41:50.0188524Z [0;36m(ApiServer_0 pid=120)[0;0m Process ApiServer_0:
2026-02-03T19:41:50.0224643Z [0;36m(ApiServer_0 pid=120)[0;0m Traceback (most recent call last):
2026-02-03T19:41:50.0234244Z [0;36m(ApiServer_0 pid=120)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-03T19:41:50.0243108Z [0;36m(ApiServer_0 pid=120)[0;0m     self.run()
2026-02-03T19:41:50.0252813Z [0;36m(ApiServer_0 pid=120)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-03T19:41:50.0261579Z [0;36m(ApiServer_0 pid=120)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-03T19:41:50.0270824Z [0;36m(ApiServer_0 pid=120)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-03T19:41:50.0279108Z [0;36m(ApiServer_0 pid=120)[0;0m     uvloop.run(
2026-02-03T19:41:50.0289000Z [0;36m(ApiServer_0 pid=120)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-03T19:41:50.0297788Z [0;36m(ApiServer_0 pid=120)[0;0m     return runner.run(wrapper())
2026-02-03T19:41:50.0306733Z [0;36m(ApiServer_0 pid=120)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:50.0315741Z [0;36m(ApiServer_0 pid=120)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-03T19:41:50.0325462Z [0;36m(ApiServer_0 pid=120)[0;0m     return self._loop.run_until_complete(task)
2026-02-03T19:41:50.0335020Z [0;36m(ApiServer_0 pid=120)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:50.0345302Z [0;36m(ApiServer_0 pid=120)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-03T19:41:50.0354611Z [0;36m(ApiServer_0 pid=120)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-03T19:41:50.0364374Z [0;36m(ApiServer_0 pid=120)[0;0m     return await main
2026-02-03T19:41:50.0373450Z [0;36m(ApiServer_0 pid=120)[0;0m            ^^^^^^^^^^
2026-02-03T19:41:50.0382655Z [0;36m(ApiServer_0 pid=120)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-03T19:41:50.0391543Z [0;36m(ApiServer_0 pid=120)[0;0m     async with build_async_engine_client(
2026-02-03T19:41:50.0401745Z [0;36m(ApiServer_0 pid=120)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-03T19:41:50.0411140Z [0;36m(ApiServer_0 pid=120)[0;0m     return await anext(self.gen)
2026-02-03T19:41:50.0420589Z [0;36m(ApiServer_0 pid=120)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:50.0430829Z [0;36m(ApiServer_0 pid=120)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-03T19:41:50.0440314Z [0;36m(ApiServer_0 pid=120)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-03T19:41:50.0449881Z [0;36m(ApiServer_0 pid=120)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-03T19:41:50.0458573Z [0;36m(ApiServer_0 pid=120)[0;0m     return await anext(self.gen)
2026-02-03T19:41:50.0468491Z [0;36m(ApiServer_0 pid=120)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:50.0477998Z [0;36m(ApiServer_0 pid=120)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-03T19:41:50.0486877Z [0;36m(ApiServer_0 pid=120)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-03T19:41:50.0496118Z [0;36m(ApiServer_0 pid=120)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:50.0505414Z [0;36m(ApiServer_0 pid=120)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-03T19:41:50.0514769Z [0;36m(ApiServer_0 pid=120)[0;0m     return cls(
2026-02-03T19:41:50.0526174Z [0;36m(ApiServer_0 pid=120)[0;0m            ^^^^
2026-02-03T19:41:50.0533878Z [0;36m(ApiServer_0 pid=120)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-03T19:41:50.0542919Z [0;36m(ApiServer_0 pid=120)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-03T19:41:50.0568257Z [0;36m(ApiServer_0 pid=120)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:50.0568805Z [0;36m(ApiServer_0 pid=120)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-03T19:41:50.0571483Z [0;36m(ApiServer_0 pid=120)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-03T19:41:50.0580417Z [0;36m(ApiServer_0 pid=120)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-03T19:41:50.0590888Z [0;36m(ApiServer_0 pid=120)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-03T19:41:50.0599511Z [0;36m(ApiServer_0 pid=120)[0;0m     super().__init__(
2026-02-03T19:41:50.0609453Z [0;36m(ApiServer_0 pid=120)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-03T19:41:50.0619358Z [0;36m(ApiServer_0 pid=120)[0;0m     super().__init__(
2026-02-03T19:41:50.0627947Z [0;36m(ApiServer_0 pid=120)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-03T19:41:50.0635973Z [0;36m(ApiServer_0 pid=120)[0;0m     super().__init__(
2026-02-03T19:41:50.0645462Z [0;36m(ApiServer_0 pid=120)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-03T19:41:50.0654468Z [0;36m(ApiServer_0 pid=120)[0;0m     raise TimeoutError(
2026-02-03T19:41:50.0664197Z [0;36m(ApiServer_0 pid=120)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-03T19:41:50.3940883Z [0;36m(ApiServer_0 pid=120)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-03T19:41:51.4682224Z [0;36m(Worker_DP0_TP3_EP3 pid=409)[0;0m INFO 02-03 19:41:51 [worker.py:342] Available memory: 17561560780, total memory: 65796046848
2026-02-03T19:41:51.6183953Z [0;36m(Worker_DP1_TP2_EP10 pid=308)[0;0m INFO 02-03 19:41:51 [worker.py:342] Available memory: 17293404672, total memory: 65787658240
2026-02-03T19:41:51.6215148Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m INFO 02-03 19:41:51 [worker.py:342] Available memory: 16872286720, total memory: 65787658240
2026-02-03T19:41:51.6287783Z [0;36m(Worker_DP1_TP5_EP13 pid=620)[0;0m INFO 02-03 19:41:51 [worker.py:342] Available memory: 17568204492, total memory: 65796046848
2026-02-03T19:41:51.6585901Z [0;36m(Worker_DP1_TP4_EP12 pid=516)[0;0m INFO 02-03 19:41:51 [worker.py:342] Available memory: 17278786048, total memory: 65787658240
2026-02-03T19:41:51.7768058Z [0;36m(Worker_DP1_TP3_EP11 pid=412)[0;0m INFO 02-03 19:41:51 [worker.py:342] Available memory: 17555891916, total memory: 65796046848
2026-02-03T19:41:51.7906672Z [0;36m(Worker_DP1_TP6_EP14 pid=725)[0;0m INFO 02-03 19:41:51 [worker.py:342] Available memory: 17294567936, total memory: 65787658240
2026-02-03T19:41:51.8901377Z [0;36m(Worker_DP0_TP2_EP2 pid=305)[0;0m INFO 02-03 19:41:51 [worker.py:342] Available memory: 17286605312, total memory: 65787658240
2026-02-03T19:41:51.9817441Z [0;36m(Worker_DP1_TP1_EP9 pid=190)[0;0m INFO 02-03 19:41:51 [worker.py:342] Available memory: 17566607052, total memory: 65796046848
2026-02-03T19:41:52.0285255Z [0;36m(Worker_DP0_TP1_EP1 pid=187)[0;0m INFO 02-03 19:41:52 [worker.py:342] Available memory: 17567106764, total memory: 65796046848
2026-02-03T19:41:52.0357807Z [0;36m(Worker_DP0_TP4_EP4 pid=513)[0;0m INFO 02-03 19:41:52 [worker.py:342] Available memory: 17276934656, total memory: 65787658240
2026-02-03T19:41:52.2334025Z [0;36m(Worker_DP0_TP5_EP5 pid=617)[0;0m INFO 02-03 19:41:52 [worker.py:342] Available memory: 17570043596, total memory: 65796046848
2026-02-03T19:41:52.3287647Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m INFO 02-03 19:41:52 [worker.py:342] Available memory: 16870156800, total memory: 65787658240
2026-02-03T19:41:52.4551822Z [0;36m(Worker_DP0_TP7_EP7 pid=825)[0;0m INFO 02-03 19:41:52 [worker.py:342] Available memory: 17566500556, total memory: 65796046848
2026-02-03T19:41:52.7404445Z [0;36m(Worker_DP1_TP7_EP15 pid=829)[0;0m INFO 02-03 19:41:52 [worker.py:342] Available memory: 17557817036, total memory: 65796046848
2026-02-03T19:41:52.7471090Z [0;36m(EngineCore_DP1 pid=109)[0;0m INFO 02-03 19:41:52 [kv_cache_utils.py:1307] GPU KV cache size: 118,016 tokens
2026-02-03T19:41:52.7492895Z [0;36m(EngineCore_DP1 pid=109)[0;0m INFO 02-03 19:41:52 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 14.41x
2026-02-03T19:41:52.7909022Z [0;36m(Worker_DP0_TP6_EP6 pid=721)[0;0m INFO 02-03 19:41:52 [worker.py:342] Available memory: 17281182208, total memory: 65787658240
2026-02-03T19:41:52.8000511Z [0;36m(EngineCore_DP0 pid=90)[0;0m INFO 02-03 19:41:52 [kv_cache_utils.py:1307] GPU KV cache size: 118,016 tokens
2026-02-03T19:41:52.8009195Z [0;36m(EngineCore_DP0 pid=90)[0;0m INFO 02-03 19:41:52 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 14.41x
2026-02-03T19:42:09.7073490Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m 
2026-02-03T19:42:09.7074378Z Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s][rank3]:[W203 19:42:09.632384910 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-03T19:42:09.7092951Z [rank12]:[W203 19:42:09.633040605 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-03T19:42:09.7104117Z [rank7]:[W203 19:42:09.633208776 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-03T19:42:09.7113475Z [rank9]:[W203 19:42:09.633335967 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-03T19:42:09.7123995Z [rank4]:[W203 19:42:09.633798931 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-03T19:42:09.7137312Z [rank0]:[W203 19:42:09.633810641 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-03T19:42:09.7142989Z [rank6]:[W203 19:42:09.634135413 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-03T19:42:09.7154859Z [rank8]:[W203 19:42:09.634175124 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-03T19:42:09.7163948Z [rank11]:[W203 19:42:09.634390835 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-03T19:42:09.7173026Z [rank13]:[W203 19:42:09.634530936 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-03T19:42:09.7182997Z [rank10]:[W203 19:42:09.634529446 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-03T19:42:09.7192575Z [rank5]:[W203 19:42:09.634854239 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-03T19:42:09.7203210Z [rank14]:[W203 19:42:09.634983150 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-03T19:42:09.7212147Z [rank15]:[W203 19:42:09.635132991 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-03T19:42:09.7222114Z [rank1]:[W203 19:42:09.636052588 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-03T19:42:09.7232243Z [rank2]:[W203 19:42:09.636206539 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-03T19:42:11.5188398Z [rank3]:[W203 19:42:11.444320528 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-03T19:42:11.5197700Z [rank4]:[W203 19:42:11.444340768 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-03T19:42:11.5206835Z [rank5]:[W203 19:42:11.444357318 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-03T19:42:11.5219630Z [rank1]:[W203 19:42:11.446688216 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-03T19:42:11.5229488Z [rank6]:[W203 19:42:11.447137730 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-03T19:42:11.5239912Z [rank7]:[W203 19:42:11.447384181 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-03T19:42:11.5248915Z [rank2]:[W203 19:42:11.448115087 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-03T19:42:11.5258755Z [rank0]:[W203 19:42:11.448491610 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-03T19:42:11.5940693Z [rank14]:[W203 19:42:11.520062624 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-03T19:42:11.5970223Z [rank11]:[W203 19:42:11.521357154 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-03T19:42:11.5979624Z [rank12]:[W203 19:42:11.521388054 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-03T19:42:11.5990008Z [rank15]:[W203 19:42:11.521679056 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-03T19:42:11.5998579Z [rank13]:[W203 19:42:11.522802005 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-03T19:42:11.6009218Z [rank8]:[W203 19:42:11.523030096 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-03T19:42:11.6019199Z [rank9]:[W203 19:42:11.523072627 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-03T19:42:11.6028730Z [rank10]:[W203 19:42:11.523525870 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-03T19:42:14.6175695Z 
2026-02-03T19:42:14.6176794Z Capturing CUDA graphs (decode, FULL):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:16<00:16, 16.72s/it]
2026-02-03T19:42:14.6177707Z Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:19<00:00,  8.64s/it]
2026-02-03T19:42:14.6178223Z Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:19<00:00,  9.85s/it]
2026-02-03T19:42:15.1904928Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m INFO 02-03 19:42:15 [gpu_model_runner.py:5051] Graph capturing finished in 21 secs, took 0.27 GiB
2026-02-03T19:42:15.4390438Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m INFO 02-03 19:42:15 [gpu_model_runner.py:5051] Graph capturing finished in 21 secs, took 0.27 GiB
2026-02-03T19:42:15.5483931Z [0;36m(EngineCore_DP0 pid=90)[0;0m INFO 02-03 19:42:15 [core.py:272] init engine (profile, create kv cache, warmup model) took 48.83 seconds
2026-02-03T19:42:16.0911551Z [0;36m(EngineCore_DP1 pid=109)[0;0m INFO 02-03 19:42:16 [core.py:272] init engine (profile, create kv cache, warmup model) took 49.18 seconds
2026-02-03T19:42:17.6406815Z INFO 02-03 19:42:17 [coordinator.py:200] All engine subscriptions received by DP coordinator
2026-02-03T19:42:17.6428638Z [0;36m(EngineCore_DP1 pid=109)[0;0m INFO 02-03 19:42:17 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-03T19:42:17.6438571Z [0;36m(EngineCore_DP0 pid=90)[0;0m INFO 02-03 19:42:17 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-03T19:42:17.6492889Z [0;36m(EngineCore_DP1 pid=109)[0;0m INFO 02-03 19:42:17 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:42:17.6494432Z [0;36m(EngineCore_DP0 pid=90)[0;0m INFO 02-03 19:42:17 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-03T19:42:17.6495633Z [0;36m(EngineCore_DP1 pid=109)[0;0m INFO 02-03 19:42:17 [platform.py:310] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-03T19:42:17.6496495Z [0;36m(EngineCore_DP0 pid=90)[0;0m INFO 02-03 19:42:17 [platform.py:310] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-03T19:42:17.6497168Z [0;36m(EngineCore_DP1 pid=109)[0;0m WARNING 02-03 19:42:17 [platform.py:327] [91m
2026-02-03T19:42:17.6497923Z [0;36m(EngineCore_DP1 pid=109)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             **********************************************************************************
2026-02-03T19:42:17.6507817Z [0;36m(EngineCore_DP1 pid=109)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             * WARNING: You have enabled the *full graph* feature.
2026-02-03T19:42:17.6517807Z [0;36m(EngineCore_DP1 pid=109)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             * This is an early experimental stage and may involve various unknown issues.
2026-02-03T19:42:17.6545364Z [0;36m(EngineCore_DP1 pid=109)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-03T19:42:17.6546173Z [0;36m(EngineCore_DP1 pid=109)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-03T19:42:17.6547024Z [0;36m(EngineCore_DP1 pid=109)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-03T19:42:17.6556663Z [0;36m(EngineCore_DP1 pid=109)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             * batch size for graph capture.
2026-02-03T19:42:17.6566503Z [0;36m(EngineCore_DP1 pid=109)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             * For more details, please refer to:
2026-02-03T19:42:17.6575882Z [0;36m(EngineCore_DP1 pid=109)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-03T19:42:17.6584861Z [0;36m(EngineCore_DP1 pid=109)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             **********************************************************************************[0m
2026-02-03T19:42:17.6594082Z [0;36m(EngineCore_DP1 pid=109)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             
2026-02-03T19:42:17.6603799Z [0;36m(EngineCore_DP0 pid=90)[0;0m WARNING 02-03 19:42:17 [platform.py:327] [91m
2026-02-03T19:42:17.6613968Z [0;36m(EngineCore_DP0 pid=90)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             **********************************************************************************
2026-02-03T19:42:17.6623567Z [0;36m(EngineCore_DP0 pid=90)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             * WARNING: You have enabled the *full graph* feature.
2026-02-03T19:42:17.6632868Z [0;36m(EngineCore_DP0 pid=90)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             * This is an early experimental stage and may involve various unknown issues.
2026-02-03T19:42:17.6643068Z [0;36m(EngineCore_DP0 pid=90)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-03T19:42:17.6653252Z [0;36m(EngineCore_DP0 pid=90)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-03T19:42:17.6663111Z [0;36m(EngineCore_DP0 pid=90)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-03T19:42:17.6672462Z [0;36m(EngineCore_DP0 pid=90)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             * batch size for graph capture.
2026-02-03T19:42:17.6681904Z [0;36m(EngineCore_DP0 pid=90)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             * For more details, please refer to:
2026-02-03T19:42:17.6691964Z [0;36m(EngineCore_DP0 pid=90)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-03T19:42:17.6701571Z [0;36m(EngineCore_DP0 pid=90)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             **********************************************************************************[0m
2026-02-03T19:42:17.6711709Z [0;36m(EngineCore_DP0 pid=90)[0;0m WARNING 02-03 19:42:17 [platform.py:327]             
2026-02-03T19:42:17.6720525Z INFO 02-03 19:42:17 [utils.py:249] Waiting for API servers to complete ...
2026-02-03T19:42:17.6731147Z [0;36m(EngineCore_DP0 pid=90)[0;0m INFO 02-03 19:42:17 [platform.py:420] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-03T19:42:17.6740265Z [0;36m(EngineCore_DP1 pid=109)[0;0m INFO 02-03 19:42:17 [platform.py:420] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-03T19:42:17.6749810Z ERROR 02-03 19:42:17 [utils.py:290] Exception occurred while running API servers: Process ApiServer_0 (PID: 120) died with exit code 1
2026-02-03T19:42:17.6759158Z ERROR 02-03 19:42:17 [utils.py:290] Traceback (most recent call last):
2026-02-03T19:42:17.6768587Z ERROR 02-03 19:42:17 [utils.py:290]   File "/vllm-workspace/vllm/vllm/v1/utils.py", line 277, in wait_for_completion_or_failure
2026-02-03T19:42:17.6777805Z ERROR 02-03 19:42:17 [utils.py:290]     raise RuntimeError(
2026-02-03T19:42:17.6787241Z ERROR 02-03 19:42:17 [utils.py:290] RuntimeError: Process ApiServer_0 (PID: 120) died with exit code 1
2026-02-03T19:42:17.6796994Z INFO 02-03 19:42:17 [utils.py:293] Terminating remaining processes ...
2026-02-03T19:42:17.9339404Z [0;36m(Worker_DP1_TP3_EP11 pid=412)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-03T19:42:17.9347184Z [0;36m(Worker_DP1_TP5_EP13 pid=620)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-03T19:42:17.9357591Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-03T19:42:17.9366429Z [0;36m(Worker_DP0_TP2_EP2 pid=305)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-03T19:42:17.9375519Z [0;36m(Worker_DP0_TP1_EP1 pid=187)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-03T19:42:17.9384987Z [0;36m(Worker_DP1_TP6_EP14 pid=725)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-03T19:42:17.9393985Z [0;36m(Worker_DP1_TP7_EP15 pid=829)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-03T19:42:17.9403779Z [0;36m(Worker_DP0_TP5_EP5 pid=617)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-03T19:42:17.9413541Z [0;36m(Worker_DP0_TP6_EP6 pid=721)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-03T19:42:17.9423008Z [0;36m(Worker_DP0_TP4_EP4 pid=513)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-03T19:42:17.9431525Z [0;36m(Worker_DP1_TP4_EP12 pid=516)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-03T19:42:17.9441423Z [0;36m(Worker_DP1_TP1_EP9 pid=190)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-03T19:42:17.9449595Z [0;36m(Worker_DP1_TP2_EP10 pid=308)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-03T19:42:17.9461825Z [0;36m(Worker_DP1_TP3_EP11 pid=412)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-03T19:42:17.9468725Z [0;36m(Worker_DP1_TP5_EP13 pid=620)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-03T19:42:17.9479013Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-03T19:42:17.9488433Z [0;36m(Worker_DP1_TP6_EP14 pid=725)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-03T19:42:17.9498144Z [0;36m(Worker_DP0_TP7_EP7 pid=825)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-03T19:42:17.9507069Z [0;36m(Worker_DP1_TP7_EP15 pid=829)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-03T19:42:17.9516695Z [0;36m(Worker_DP0_TP2_EP2 pid=305)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-03T19:42:17.9528396Z [0;36m(Worker_DP1_TP0_EP8 pid=137)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-03T19:42:17.9580114Z [0;36m(Worker_DP0_TP4_EP4 pid=513)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-03T19:42:17.9588966Z [0;36m(Worker_DP0_TP1_EP1 pid=187)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-03T19:42:17.9593613Z [0;36m(Worker_DP0_TP3_EP3 pid=409)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-03T19:42:17.9602891Z [0;36m(Worker_DP1_TP1_EP9 pid=190)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-03T19:42:17.9612351Z [0;36m(Worker_DP1_TP4_EP12 pid=516)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-03T19:42:17.9621904Z [0;36m(Worker_DP0_TP5_EP5 pid=617)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-03T19:42:17.9631567Z [0;36m(Worker_DP0_TP6_EP6 pid=721)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-03T19:42:17.9641337Z [0;36m(Worker_DP0_TP0_EP0 pid=138)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-03T19:42:17.9651154Z [0;36m(Worker_DP1_TP2_EP10 pid=308)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-03T19:42:17.9660491Z [0;36m(Worker_DP0_TP3_EP3 pid=409)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-03T19:42:17.9670026Z [0;36m(Worker_DP0_TP7_EP7 pid=825)[0;0m INFO 02-03 19:42:17 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-03T19:42:22.9675903Z Traceback (most recent call last):
2026-02-03T19:42:22.9676287Z   File "/usr/local/python3.11.14/bin/vllm", line 7, in <module>
2026-02-03T19:42:22.9681145Z     sys.exit(main())
2026-02-03T19:42:22.9691163Z              ^^^^^^
2026-02-03T19:42:22.9701362Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/main.py", line 73, in main
2026-02-03T19:42:22.9714870Z     args.dispatch_function(args)
2026-02-03T19:42:22.9725210Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 108, in cmd
2026-02-03T19:42:22.9733789Z     run_multi_api_server(args)
2026-02-03T19:42:22.9743173Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 282, in run_multi_api_server
2026-02-03T19:42:22.9753100Z     wait_for_completion_or_failure(
2026-02-03T19:42:22.9763382Z   File "/vllm-workspace/vllm/vllm/v1/utils.py", line 277, in wait_for_completion_or_failure
2026-02-03T19:42:22.9772960Z     raise RuntimeError(
2026-02-03T19:42:22.9783027Z RuntimeError: Process ApiServer_0 (PID: 120) died with exit code 1
2026-02-03T19:42:23.0756458Z [ERROR] 2026-02-03-19:42:22 (PID:67, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
2026-02-03T19:42:23.4351438Z sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-03T19:42:25.0671428Z /usr/local/python3.11.14/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked shared_memory objects to clean up at shutdown
2026-02-03T19:42:25.0679442Z   warnings.warn('resource_tracker: There appear to be %d '
2026-02-03T19:42:27.1513531Z FAILED
2026-02-03T19:42:27.1520515Z 
2026-02-03T19:42:27.1529346Z =================================== FAILURES ===================================
2026-02-03T19:42:27.1538780Z _______________________________ test_multi_node ________________________________
2026-02-03T19:42:27.1548522Z 
2026-02-03T19:42:27.1557342Z     @pytest.mark.asyncio
2026-02-03T19:42:27.1566370Z     async def test_multi_node() -> None:
2026-02-03T19:42:27.1575672Z         config = MultiNodeConfigLoader.from_yaml()
2026-02-03T19:42:27.1584441Z     
2026-02-03T19:42:27.1593437Z         with ProxyLauncher(
2026-02-03T19:42:27.1603567Z                 nodes=config.nodes,
2026-02-03T19:42:27.1612756Z                 disagg_cfg=config.disagg_cfg,
2026-02-03T19:42:27.1622093Z                 envs=config.envs,
2026-02-03T19:42:27.1631344Z                 proxy_port=config.proxy_port,
2026-02-03T19:42:27.1641149Z                 cur_index=config.cur_index,
2026-02-03T19:42:27.1650534Z         ) as proxy:
2026-02-03T19:42:27.1659438Z     
2026-02-03T19:42:27.1668727Z >           with RemoteOpenAIServer(
2026-02-03T19:42:27.1678034Z                     model=config.model,
2026-02-03T19:42:27.1687514Z                     vllm_serve_args=config.server_cmd,
2026-02-03T19:42:27.1696391Z                     server_port=config.server_port,
2026-02-03T19:42:27.1705136Z                     server_host=config.master_ip,
2026-02-03T19:42:27.1715038Z                     env_dict=config.envs,
2026-02-03T19:42:27.1724360Z                     auto_port=False,
2026-02-03T19:42:27.1732888Z                     proxy_port=proxy.proxy_port,
2026-02-03T19:42:27.1742161Z                     disaggregated_prefill=config.disagg_cfg,
2026-02-03T19:42:27.1751271Z                     nodes_info=config.nodes,
2026-02-03T19:42:27.1761372Z                     max_wait_seconds=2800,
2026-02-03T19:42:27.1770749Z             ) as server:
2026-02-03T19:42:27.1780162Z 
2026-02-03T19:42:27.1790019Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py:21: 
2026-02-03T19:42:27.1799662Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-03T19:42:27.1808733Z tests/e2e/conftest.py:297: in __init__
2026-02-03T19:42:27.1818025Z     self._wait_for_multiple_servers(
2026-02-03T19:42:27.1827449Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-03T19:42:27.1836477Z 
2026-02-03T19:42:27.1846863Z self = <tests.e2e.conftest.RemoteOpenAIServer object at 0xffff038d3c10>
2026-02-03T19:42:27.1855943Z targets = [('10.0.0.58', 'http://10.0.0.58:8080/health')], timeout = 2800
2026-02-03T19:42:27.1865929Z log_interval = 30.0
2026-02-03T19:42:27.1875051Z 
2026-02-03T19:42:27.1884649Z     def _wait_for_multiple_servers(self,
2026-02-03T19:42:27.1894094Z                                    targets,
2026-02-03T19:42:27.1903271Z                                    timeout: float,
2026-02-03T19:42:27.1912755Z                                    log_interval: float = 30.0):
2026-02-03T19:42:27.1922968Z         """
2026-02-03T19:42:27.1932175Z         targets: List[(node_ip, url)]
2026-02-03T19:42:27.1941386Z         log_interval
2026-02-03T19:42:27.1951086Z         """
2026-02-03T19:42:27.1960789Z         start = time.time()
2026-02-03T19:42:27.1970886Z         client = requests
2026-02-03T19:42:27.1980293Z     
2026-02-03T19:42:27.1989537Z         ready = {node_ip: False for node_ip, _ in targets}
2026-02-03T19:42:27.1999104Z     
2026-02-03T19:42:27.2008539Z         last_log_time = 0.0
2026-02-03T19:42:27.2017105Z     
2026-02-03T19:42:27.2026232Z         while True:
2026-02-03T19:42:27.2035448Z             now = time.time()
2026-02-03T19:42:27.2045324Z             all_ready = True
2026-02-03T19:42:27.2054100Z             should_log = (now - last_log_time) >= log_interval
2026-02-03T19:42:27.2062964Z     
2026-02-03T19:42:27.2072253Z             for node_ip, url in targets:
2026-02-03T19:42:27.2081825Z                 if ready[node_ip]:
2026-02-03T19:42:27.2090767Z                     continue
2026-02-03T19:42:27.2099546Z     
2026-02-03T19:42:27.2108199Z                 try:
2026-02-03T19:42:27.2117748Z                     resp = client.get(url)
2026-02-03T19:42:27.2126859Z                     if resp.status_code == 200:
2026-02-03T19:42:27.2135858Z                         ready[node_ip] = True
2026-02-03T19:42:27.2145062Z                         logger.info(f"[READY] Node {node_ip} is ready.")
2026-02-03T19:42:27.2153984Z                 except RequestException:
2026-02-03T19:42:27.2162892Z                     all_ready = False
2026-02-03T19:42:27.2172326Z                     if should_log:
2026-02-03T19:42:27.2181298Z                         logger.debug(f"[WAIT] {url}: connection failed")
2026-02-03T19:42:27.2190516Z     
2026-02-03T19:42:27.2199693Z                     # check unexpected exit
2026-02-03T19:42:27.2208901Z                     result = self._poll()
2026-02-03T19:42:27.2217596Z                     if result is not None and result != 0:
2026-02-03T19:42:27.2226436Z >                       raise RuntimeError(
2026-02-03T19:42:27.2236269Z                             f"Server at {node_ip} exited unexpectedly."
2026-02-03T19:42:27.2245432Z                         ) from None
2026-02-03T19:42:27.2254459Z E                       RuntimeError: Server at 10.0.0.58 exited unexpectedly.
2026-02-03T19:42:27.2262885Z 
2026-02-03T19:42:27.2271773Z tests/e2e/conftest.py:390: RuntimeError
2026-02-03T19:42:27.2281729Z =============================== warnings summary ===============================
2026-02-03T19:42:27.2291013Z <frozen importlib._bootstrap>:241
2026-02-03T19:42:27.2300139Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute
2026-02-03T19:42:27.2308936Z 
2026-02-03T19:42:27.2318440Z <frozen importlib._bootstrap>:241
2026-02-03T19:42:27.2327995Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute
2026-02-03T19:42:27.2336792Z 
2026-02-03T19:42:27.2345968Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2026-02-03T19:42:27.2355227Z =========================== short test summary info ============================
2026-02-03T19:42:27.2365394Z FAILED tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node
2026-02-03T19:42:27.2373926Z ================== 1 failed, 2 warnings in 691.37s (0:11:31) ===================
2026-02-03T19:42:28.7534595Z [0;31mFAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml âœ— ERROR: Some tests failed, please check the error stack above for details. If this is insufficient to pinpoint the error, please download and review the logs of all other nodes from the job's summary.[0m
2026-02-03T19:42:28.9539736Z Cleaning up background log streams...
2026-02-03T19:42:29.0247362Z ##[error]Error: failed to run script step: Error: command terminated with non-zero exit code: command terminated with exit code 1
2026-02-03T19:42:29.0286256Z ##[error]Process completed with exit code 1.
2026-02-03T19:42:29.0377871Z ##[error]Executing the custom container implementation failed. Please contact your self hosted runner administrator.
2026-02-03T19:42:29.0922661Z ##[group]Run actions/upload-artifact@v6
2026-02-03T19:42:29.0922952Z with:
2026-02-03T19:42:29.0923183Z   name: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs
2026-02-03T19:42:29.0923488Z   path: /tmp/vllm*_logs.txt
2026-02-03T19:42:29.0923737Z   retention-days: 7
2026-02-03T19:42:29.0923938Z   if-no-files-found: warn
2026-02-03T19:42:29.0924166Z   compression-level: 6
2026-02-03T19:42:29.0924394Z   overwrite: false
2026-02-03T19:42:29.0924635Z   include-hidden-files: false
2026-02-03T19:42:29.0924860Z env:
2026-02-03T19:42:29.0925138Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-03T19:42:29.0925432Z ##[endgroup]
2026-02-03T19:42:29.0952106Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-03T19:42:29.0952860Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-03T19:42:29.0953152Z ##[endgroup]
2026-02-03T19:42:29.4498144Z (node:11761) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-03T19:42:29.4499007Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-03T19:42:48.5353960Z With the provided path, there will be 1 file uploaded
2026-02-03T19:42:48.5357848Z Artifact name is valid!
2026-02-03T19:42:48.5358116Z Root directory input is valid!
2026-02-03T19:42:49.6981764Z Beginning upload of artifact content to blob storage
2026-02-03T19:42:50.7089465Z Uploaded bytes 11758
2026-02-03T19:42:50.9540890Z Finished uploading artifact content to blob storage!
2026-02-03T19:42:50.9541411Z SHA256 digest of uploaded artifact zip is 659f0f14652206b11170cfcbf6fa65306e4f4c20ee98aaea4955a6e659b58d78
2026-02-03T19:42:50.9541923Z Finalizing artifact upload
2026-02-03T19:42:52.0659726Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs.zip successfully finalized. Artifact ID 5363701044
2026-02-03T19:42:52.0660484Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs has been successfully uploaded! Final size is 11758 bytes. Artifact ID is 5363701044
2026-02-03T19:42:52.0661221Z Artifact download URL: https://github.com/vllm-project/vllm-ascend/actions/runs/21638756264/artifacts/5363701044
2026-02-03T19:43:10.3912645Z ##[group]Run kubectl get pods -n "$NAMESPACE" --ignore-not-found=true
2026-02-03T19:43:10.3913154Z [36;1mkubectl get pods -n "$NAMESPACE" --ignore-not-found=true[0m
2026-02-03T19:43:10.3913552Z [36;1mkubectl delete -f ./lws.yaml --ignore-not-found=true || true[0m
2026-02-03T19:43:10.3913967Z shell: bash -el {0}
2026-02-03T19:43:10.3914168Z env:
2026-02-03T19:43:10.3914432Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-03T19:43:10.3914738Z ##[endgroup]
2026-02-03T19:43:10.3997228Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-03T19:43:10.3998031Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-03T19:43:10.3998296Z ##[endgroup]
2026-02-03T19:43:10.7448507Z (node:13414) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-03T19:43:10.7449707Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-03T19:43:29.3562461Z NAME                                             READY   STATUS    RESTARTS      AGE
2026-02-03T19:43:29.3563002Z linux-aarch64-a3-0-wgt9d-runner-mv8gw            1/1     Running   0             17m
2026-02-03T19:43:29.3563483Z linux-aarch64-a3-0-wgt9d-runner-mv8gw-workflow   1/1     Running   0             16m
2026-02-03T19:43:29.3563907Z vllm-0                                           1/1     Running   1 (61s ago)   13m
2026-02-03T19:43:29.3564307Z vllm-0-1                                         1/1     Running   0             13m
2026-02-03T19:43:29.4185320Z leaderworkerset.leaderworkerset.x-k8s.io "vllm" deleted from vllm-project namespace
2026-02-03T19:43:29.4355346Z service "vllm-leader" deleted from vllm-project namespace
2026-02-03T19:43:47.8198954Z Post job cleanup.
2026-02-03T19:43:47.8443213Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-03T19:43:47.8444003Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-03T19:43:47.8444277Z ##[endgroup]
2026-02-03T19:43:48.2423612Z (node:15248) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-03T19:43:48.2424296Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-03T19:44:07.0387231Z [command]/usr/bin/git version
2026-02-03T19:44:07.0590536Z git version 2.34.1
2026-02-03T19:44:07.0620999Z Copying '/root/.gitconfig' to '/__w/_temp/cfc9419e-dc47-47d0-8df9-79f661cd0474/.gitconfig'
2026-02-03T19:44:07.0627926Z Temporarily overriding HOME='/__w/_temp/cfc9419e-dc47-47d0-8df9-79f661cd0474' before making global git config changes
2026-02-03T19:44:07.0628483Z Adding repository directory to the temporary git global config as a safe directory
2026-02-03T19:44:07.0632441Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-03T19:44:07.0678653Z Removing SSH command configuration
2026-02-03T19:44:07.0681599Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-03T19:44:07.0740985Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-03T19:44:07.1247812Z Removing HTTP extra header
2026-02-03T19:44:07.1250179Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-03T19:44:07.1277507Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-03T19:44:07.1469171Z Removing includeIf entries pointing to credentials config files
2026-02-03T19:44:07.1472558Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-03T19:44:07.1492361Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-03T19:44:07.1492676Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-03T19:44:07.1493020Z includeif.gitdir:/github/workspace/.git.path
2026-02-03T19:44:07.1493283Z includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-03T19:44:07.1500615Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-03T19:44:07.1522621Z /__w/_temp/git-credentials-a2b6e355-4c2e-42f5-adf5-373dce437da2.config
2026-02-03T19:44:07.1526715Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-a2b6e355-4c2e-42f5-adf5-373dce437da2.config
2026-02-03T19:44:07.1559131Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-03T19:44:07.1576571Z /__w/_temp/git-credentials-a2b6e355-4c2e-42f5-adf5-373dce437da2.config
2026-02-03T19:44:07.1583087Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-a2b6e355-4c2e-42f5-adf5-373dce437da2.config
2026-02-03T19:44:07.1608384Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git.path
2026-02-03T19:44:07.1628726Z /github/runner_temp/git-credentials-a2b6e355-4c2e-42f5-adf5-373dce437da2.config
2026-02-03T19:44:07.1633095Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-a2b6e355-4c2e-42f5-adf5-373dce437da2.config
2026-02-03T19:44:07.1658309Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-03T19:44:07.1675674Z /github/runner_temp/git-credentials-a2b6e355-4c2e-42f5-adf5-373dce437da2.config
2026-02-03T19:44:07.1682478Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-a2b6e355-4c2e-42f5-adf5-373dce437da2.config
2026-02-03T19:44:07.1708297Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-03T19:44:07.1891906Z Removing credentials config '/__w/_temp/git-credentials-a2b6e355-4c2e-42f5-adf5-373dce437da2.config'
2026-02-03T19:44:25.5761045Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-03T19:44:25.5761840Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-03T19:44:25.5762294Z ##[endgroup]
2026-02-03T19:44:25.9600741Z Cleaning up orphan processes
