# Run ID: 22190789077
# Commit: f0caeeadcb37261beebd4a6e32934fa9f460db98
# Job: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
# Date: 2026-02-19
============================================================

ï»¿2026-02-19T19:50:34.3321724Z Current runner version: '2.330.0'
2026-02-19T19:50:34.3326944Z Runner name: 'linux-aarch64-a3-0-n4cwm-runner-bv8g4'
2026-02-19T19:50:34.3327620Z Runner group name: 'Default'
2026-02-19T19:50:34.3328353Z Machine name: 'linux-aarch64-a3-0-n4cwm-runner-bv8g4'
2026-02-19T19:50:34.3331607Z ##[group]GITHUB_TOKEN Permissions
2026-02-19T19:50:34.3333729Z Actions: write
2026-02-19T19:50:34.3334294Z ArtifactMetadata: write
2026-02-19T19:50:34.3334843Z Attestations: write
2026-02-19T19:50:34.3335271Z Checks: write
2026-02-19T19:50:34.3335648Z Contents: write
2026-02-19T19:50:34.3336007Z Deployments: write
2026-02-19T19:50:34.3336416Z Discussions: write
2026-02-19T19:50:34.3336800Z Issues: write
2026-02-19T19:50:34.3337152Z Metadata: read
2026-02-19T19:50:34.3337529Z Models: read
2026-02-19T19:50:34.3337903Z Packages: write
2026-02-19T19:50:34.3338261Z Pages: write
2026-02-19T19:50:34.3338640Z PullRequests: write
2026-02-19T19:50:34.3339019Z RepositoryProjects: write
2026-02-19T19:50:34.3339676Z SecurityEvents: write
2026-02-19T19:50:34.3340118Z Statuses: write
2026-02-19T19:50:34.3340492Z ##[endgroup]
2026-02-19T19:50:34.3342421Z Secret source: Actions
2026-02-19T19:50:34.3342974Z Prepare workflow directory
2026-02-19T19:50:34.3888426Z Prepare all required actions
2026-02-19T19:50:34.3919170Z Getting action download info
2026-02-19T19:50:35.6307802Z Download action repository 'actions/checkout@v6' (SHA:de0fac2e4500dabe0009e67214ff5f5447ce83dd)
2026-02-19T19:50:40.1905663Z Download action repository 'actions/upload-artifact@v6' (SHA:b7c566a772e6b6bfb58ed0dc250532a479d7789f)
2026-02-19T19:50:48.2582890Z Uses: vllm-project/vllm-ascend/.github/workflows/_e2e_nightly_multi_node.yaml@refs/heads/main (f0caeeadcb37261beebd4a6e32934fa9f460db98)
2026-02-19T19:50:48.2586074Z ##[group] Inputs
2026-02-19T19:50:48.2586364Z   soc_version: a3
2026-02-19T19:50:48.2586573Z   runner: linux-aarch64-a3-0
2026-02-19T19:50:48.2587037Z   image: swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3
2026-02-19T19:50:48.2587504Z   config_file_path: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-19T19:50:48.2587894Z   replicas: 1
2026-02-19T19:50:48.2588151Z   size: 2
2026-02-19T19:50:48.2588372Z   vllm_version: v0.15.0
2026-02-19T19:50:48.2588697Z   vllm_ascend_remote_url: https://github.com/vllm-project/vllm-ascend.git
2026-02-19T19:50:48.2589142Z   vllm_ascend_ref: main
2026-02-19T19:50:48.2589386Z ##[endgroup]
2026-02-19T19:50:48.2589825Z Complete job name: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-19T19:50:48.3065077Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-19T19:50:48.3067980Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-19T19:50:48.3068517Z ##[endgroup]
2026-02-19T19:51:03.8196246Z (node:70) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-19T19:51:03.8197045Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-19T19:51:05.6473816Z ##[group]Run # Decode and save kubeconfig
2026-02-19T19:51:05.6474332Z [36;1m# Decode and save kubeconfig[0m
2026-02-19T19:51:05.6506566Z [36;1mecho "***" | base64 -d > "$KUBECONFIG"[0m
2026-02-19T19:51:05.6507108Z shell: bash -el {0}
2026-02-19T19:51:05.6507400Z ##[endgroup]
2026-02-19T19:51:05.6627599Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-19T19:51:05.6628544Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-19T19:51:05.6628852Z ##[endgroup]
2026-02-19T19:51:06.0128377Z (node:401) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-19T19:51:06.0129171Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-19T19:51:06.8644894Z ##[group]Run actions/checkout@v6
2026-02-19T19:51:06.8645196Z with:
2026-02-19T19:51:06.8645523Z   repository: vllm-project/vllm-ascend
2026-02-19T19:51:06.8646246Z   token: ***
2026-02-19T19:51:06.8646437Z   ssh-strict: true
2026-02-19T19:51:06.8646783Z   ssh-user: git
2026-02-19T19:51:06.8646993Z   persist-credentials: true
2026-02-19T19:51:06.8647228Z   clean: true
2026-02-19T19:51:06.8647491Z   sparse-checkout-cone-mode: true
2026-02-19T19:51:06.8647735Z   fetch-depth: 1
2026-02-19T19:51:06.8647957Z   fetch-tags: false
2026-02-19T19:51:06.8648206Z   show-progress: true
2026-02-19T19:51:06.8648404Z   lfs: false
2026-02-19T19:51:06.8648603Z   submodules: false
2026-02-19T19:51:06.8648828Z   set-safe-directory: true
2026-02-19T19:51:06.8649069Z ##[endgroup]
2026-02-19T19:51:06.8689783Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-19T19:51:06.8690640Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-19T19:51:06.8690993Z ##[endgroup]
2026-02-19T19:51:07.2186804Z (node:431) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-19T19:51:07.2187673Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-19T19:51:07.7769953Z Syncing repository: vllm-project/vllm-ascend
2026-02-19T19:51:07.7771190Z ##[group]Getting Git version info
2026-02-19T19:51:07.7771522Z Working directory is '/__w/vllm-ascend/vllm-ascend'
2026-02-19T19:51:07.7772247Z [command]/usr/bin/git version
2026-02-19T19:51:07.7842848Z git version 2.34.1
2026-02-19T19:51:07.7858651Z ##[endgroup]
2026-02-19T19:51:07.7866360Z Copying '/root/.gitconfig' to '/__w/_temp/de754dc1-5821-47d6-b269-b98234a34dcd/.gitconfig'
2026-02-19T19:51:07.7874230Z Temporarily overriding HOME='/__w/_temp/de754dc1-5821-47d6-b269-b98234a34dcd' before making global git config changes
2026-02-19T19:51:07.7878985Z Adding repository directory to the temporary git global config as a safe directory
2026-02-19T19:51:07.7879494Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-19T19:51:07.7913801Z Deleting the contents of '/__w/vllm-ascend/vllm-ascend'
2026-02-19T19:51:07.7916140Z ##[group]Initializing the repository
2026-02-19T19:51:07.7920103Z [command]/usr/bin/git init /__w/vllm-ascend/vllm-ascend
2026-02-19T19:51:07.8045570Z hint: Using 'master' as the name for the initial branch. This default branch name
2026-02-19T19:51:07.8046037Z hint: is subject to change. To configure the initial branch name to use in all
2026-02-19T19:51:07.8046434Z hint: of your new repositories, which will suppress this warning, call:
2026-02-19T19:51:07.8046771Z hint: 
2026-02-19T19:51:07.8047084Z hint: 	git config --global init.defaultBranch <name>
2026-02-19T19:51:07.8047346Z hint: 
2026-02-19T19:51:07.8047709Z hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and
2026-02-19T19:51:07.8048144Z hint: 'development'. The just-created branch can be renamed via this command:
2026-02-19T19:51:07.8048463Z hint: 
2026-02-19T19:51:07.8048685Z hint: 	git branch -m <name>
2026-02-19T19:51:07.8052612Z Initialized empty Git repository in /__w/vllm-ascend/vllm-ascend/.git/
2026-02-19T19:51:07.8060122Z [command]/usr/bin/git remote add origin https://github.com/vllm-project/vllm-ascend
2026-02-19T19:51:07.8108061Z ##[endgroup]
2026-02-19T19:51:07.8108454Z ##[group]Disabling automatic garbage collection
2026-02-19T19:51:07.8111193Z [command]/usr/bin/git config --local gc.auto 0
2026-02-19T19:51:07.8134371Z ##[endgroup]
2026-02-19T19:51:07.8134737Z ##[group]Setting up auth
2026-02-19T19:51:07.8135332Z Removing SSH command configuration
2026-02-19T19:51:07.8140382Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-19T19:51:07.8165511Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-19T19:51:07.8566172Z Removing HTTP extra header
2026-02-19T19:51:07.8568787Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-19T19:51:07.8592792Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-19T19:51:07.8764471Z Removing includeIf entries pointing to credentials config files
2026-02-19T19:51:07.8768906Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-19T19:51:07.8793719Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-19T19:51:07.8971392Z [command]/usr/bin/git config --file /__w/_temp/git-credentials-f99c08ad-2fad-474c-98da-8f3d672b4975.config http.https://github.com/.extraheader AUTHORIZATION: basic ***
2026-02-19T19:51:07.9002076Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-f99c08ad-2fad-474c-98da-8f3d672b4975.config
2026-02-19T19:51:07.9026004Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-f99c08ad-2fad-474c-98da-8f3d672b4975.config
2026-02-19T19:51:07.9050598Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-f99c08ad-2fad-474c-98da-8f3d672b4975.config
2026-02-19T19:51:07.9077721Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-f99c08ad-2fad-474c-98da-8f3d672b4975.config
2026-02-19T19:51:07.9099573Z ##[endgroup]
2026-02-19T19:51:07.9100036Z ##[group]Fetching the repository
2026-02-19T19:51:07.9107062Z [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --no-recurse-submodules --depth=1 origin +f0caeeadcb37261beebd4a6e32934fa9f460db98:refs/remotes/origin/main
2026-02-19T19:51:09.5706958Z From https://gh-proxy.test.osinfra.cn/https://github.com/vllm-project/vllm-ascend
2026-02-19T19:51:09.5707767Z  * [new ref]         f0caeeadcb37261beebd4a6e32934fa9f460db98 -> origin/main
2026-02-19T19:51:09.5727830Z [command]/usr/bin/git branch --list --remote origin/main
2026-02-19T19:51:09.5748739Z   origin/main
2026-02-19T19:51:09.5755171Z [command]/usr/bin/git rev-parse refs/remotes/origin/main
2026-02-19T19:51:09.5773577Z f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-19T19:51:09.5785814Z ##[endgroup]
2026-02-19T19:51:09.5786179Z ##[group]Determining the checkout info
2026-02-19T19:51:09.5806005Z ##[endgroup]
2026-02-19T19:51:09.5808693Z [command]/usr/bin/git sparse-checkout disable
2026-02-19T19:51:09.5848077Z [command]/usr/bin/git config --local --unset-all extensions.worktreeConfig
2026-02-19T19:51:09.5871299Z ##[group]Checking out the ref
2026-02-19T19:51:09.5874524Z [command]/usr/bin/git checkout --progress --force -B main refs/remotes/origin/main
2026-02-19T19:51:09.6733210Z Switched to a new branch 'main'
2026-02-19T19:51:09.6733905Z Branch 'main' set up to track remote branch 'main' from 'origin'.
2026-02-19T19:51:09.6742574Z ##[endgroup]
2026-02-19T19:51:09.6773229Z [command]/usr/bin/git log -1 --format=%H
2026-02-19T19:51:09.6793378Z f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-19T19:51:10.0787656Z ##[group]Run # prepare for lws entrypoint scripts
2026-02-19T19:51:10.0788050Z [36;1m# prepare for lws entrypoint scripts[0m
2026-02-19T19:51:10.0788464Z [36;1minstall -D tests/e2e/nightly/multi_node/scripts/run.sh /root/.cache/tests/run.sh[0m
2026-02-19T19:51:10.0788992Z shell: bash -el {0}
2026-02-19T19:51:10.0789203Z ##[endgroup]
2026-02-19T19:51:10.0914793Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-19T19:51:10.0915733Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-19T19:51:10.0915992Z ##[endgroup]
2026-02-19T19:51:10.4498218Z (node:472) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-19T19:51:10.4498930Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-19T19:51:11.3739213Z ##[group]Run set -euo pipefail
2026-02-19T19:51:11.3739659Z [36;1mset -euo pipefail[0m
2026-02-19T19:51:11.3739872Z [36;1m[0m
2026-02-19T19:51:11.3740074Z [36;1mCRD_NAME="${CRD_NAME:-vllm}"[0m
2026-02-19T19:51:11.3740426Z [36;1mTIMEOUT=${TIMEOUT:-120}[0m
2026-02-19T19:51:11.3740653Z [36;1mSLEEP_INTERVAL=2[0m
2026-02-19T19:51:11.3740853Z [36;1m[0m
2026-02-19T19:51:11.3741425Z [36;1mecho "Deleting leaderworkerset [$CRD_NAME] in namespace [$NAMESPACE]..."[0m
2026-02-19T19:51:11.3741888Z [36;1mkubectl delete leaderworkerset "$CRD_NAME" -n "$NAMESPACE" --ignore-not-found[0m
2026-02-19T19:51:11.3742441Z [36;1m[0m
2026-02-19T19:51:11.3742748Z [36;1mecho "Waiting for all pods starting with 'vllm' to be deleted..."[0m
2026-02-19T19:51:11.3743066Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-19T19:51:11.3743304Z [36;1m[0m
2026-02-19T19:51:11.3743574Z [36;1mwhile true; do[0m
2026-02-19T19:51:11.3743821Z [36;1m  NOW=$(date +%s)[0m
2026-02-19T19:51:11.3744216Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-19T19:51:11.3744486Z [36;1m[0m
2026-02-19T19:51:11.3744728Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-19T19:51:11.3745044Z [36;1m    echo "Timeout reached ($TIMEOUT seconds), some pods still exist:"[0m
2026-02-19T19:51:11.3745452Z [36;1m    kubectl get pods -n "$NAMESPACE" | grep '^vllm' || true[0m
2026-02-19T19:51:11.3745772Z [36;1m    exit 1[0m
2026-02-19T19:51:11.3745952Z [36;1m  fi[0m
2026-02-19T19:51:11.3746178Z [36;1m[0m
2026-02-19T19:51:11.3746616Z [36;1m  PODS_EXIST=$(kubectl get pods -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null | tr ' ' '\n' | grep '^vllm' || true)[0m
2026-02-19T19:51:11.3747104Z [36;1m[0m
2026-02-19T19:51:11.3747340Z [36;1m  if [[ -z "$PODS_EXIST" ]]; then[0m
2026-02-19T19:51:11.3747627Z [36;1m    echo "All vllm pods deleted."[0m
2026-02-19T19:51:11.3747853Z [36;1m    break[0m
2026-02-19T19:51:11.3748091Z [36;1m  else[0m
2026-02-19T19:51:11.3748362Z [36;1m    echo "Waiting for pods to be deleted: $PODS_EXIST"[0m
2026-02-19T19:51:11.3748684Z [36;1m    sleep $SLEEP_INTERVAL[0m
2026-02-19T19:51:11.3748952Z [36;1m  fi[0m
2026-02-19T19:51:11.3749162Z [36;1mdone[0m
2026-02-19T19:51:11.3749493Z shell: bash -el {0}
2026-02-19T19:51:11.3749736Z ##[endgroup]
2026-02-19T19:51:11.3819112Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-19T19:51:11.3819966Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-19T19:51:11.3820264Z ##[endgroup]
2026-02-19T19:51:11.7399595Z (node:526) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-19T19:51:11.7400307Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-19T19:51:12.2470293Z Deleting leaderworkerset [vllm] in namespace [vllm-project]...
2026-02-19T19:51:12.5521221Z Waiting for all pods starting with 'vllm' to be deleted...
2026-02-19T19:51:12.6295008Z All vllm pods deleted.
2026-02-19T19:51:13.0471803Z ##[group]Run set -e
2026-02-19T19:51:13.0472306Z [36;1mset -e[0m
2026-02-19T19:51:13.0472548Z [36;1m[0m
2026-02-19T19:51:13.0472740Z [36;1msize="2"[0m
2026-02-19T19:51:13.0472965Z [36;1mreplicas="1"[0m
2026-02-19T19:51:13.0473431Z [36;1mimage="swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3"[0m
2026-02-19T19:51:13.0473898Z [36;1mconfig_file_path="DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-19T19:51:13.0474289Z [36;1mfail_tag=FAIL_TAG_"DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-19T19:51:13.0474741Z [36;1mecho "FAIL_TAG=${fail_tag}" >> "$GITHUB_ENV"[0m
2026-02-19T19:51:13.0474999Z [36;1m[0m
2026-02-19T19:51:13.0475359Z [36;1mrequired_params=("size" "replicas" "image" "config_file_path")[0m
2026-02-19T19:51:13.0475739Z [36;1mfor param in "${required_params[@]}"; do[0m
2026-02-19T19:51:13.0476028Z [36;1m  if [ -z "${!param}" ]; then[0m
2026-02-19T19:51:13.0476585Z [36;1m    echo "Error: Parameter '$param' is required but empty"[0m
2026-02-19T19:51:13.0476899Z [36;1m    exit 1[0m
2026-02-19T19:51:13.0477086Z [36;1m  fi[0m
2026-02-19T19:51:13.0477302Z [36;1mdone[0m
2026-02-19T19:51:13.0477488Z [36;1m[0m
2026-02-19T19:51:13.0477698Z [36;1mif [ "a3" = "a3" ]; then[0m
2026-02-19T19:51:13.0477963Z [36;1m  npu_per_node=16[0m
2026-02-19T19:51:13.0478283Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws.yaml.jinja2"[0m
2026-02-19T19:51:13.0478696Z [36;1melse[0m
2026-02-19T19:51:13.0478907Z [36;1m  npu_per_node=8[0m
2026-02-19T19:51:13.0479229Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws-a2.yaml.jinja2"[0m
2026-02-19T19:51:13.0479587Z [36;1mfi[0m
2026-02-19T19:51:13.0479783Z [36;1m[0m
2026-02-19T19:51:13.0479983Z [36;1mjinja2 $TEMPLATE_FILE \[0m
2026-02-19T19:51:13.0480247Z [36;1m  -D size="$size" \[0m
2026-02-19T19:51:13.0480490Z [36;1m  -D replicas="$replicas" \[0m
2026-02-19T19:51:13.0480736Z [36;1m  -D image="$image" \[0m
2026-02-19T19:51:13.0481033Z [36;1m  -D config_file_path="$config_file_path" \[0m
2026-02-19T19:51:13.0481297Z [36;1m  -D npu_per_node="$npu_per_node" \[0m
2026-02-19T19:51:13.0481590Z [36;1m  -D fail_tag="$fail_tag" \[0m
2026-02-19T19:51:13.0481933Z [36;1m  --outfile lws.yaml[0m
2026-02-19T19:51:13.0482365Z [36;1m[0m
2026-02-19T19:51:13.0482604Z [36;1mkubectl apply -f ./lws.yaml[0m
2026-02-19T19:51:13.0483025Z shell: bash -el {0}
2026-02-19T19:51:13.0483207Z ##[endgroup]
2026-02-19T19:51:13.0555003Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-19T19:51:13.0556047Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-19T19:51:13.0556324Z ##[endgroup]
2026-02-19T19:51:13.4044935Z (node:592) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-19T19:51:13.4045689Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-19T19:51:14.3695074Z leaderworkerset.leaderworkerset.x-k8s.io/vllm created
2026-02-19T19:51:14.3946679Z service/vllm-leader created
2026-02-19T19:51:16.0126308Z ##[group]Run POD_PREFIX="${POD_PREFIX:-vllm-0}"
2026-02-19T19:51:16.0126688Z [36;1mPOD_PREFIX="${POD_PREFIX:-vllm-0}"[0m
2026-02-19T19:51:16.0126986Z [36;1mSIZE="2"[0m
2026-02-19T19:51:16.0127238Z [36;1mTIMEOUT=1200  # default timeout 20 minutes[0m
2026-02-19T19:51:16.0127484Z [36;1m[0m
2026-02-19T19:51:16.0127870Z [36;1mecho "Waiting for Pods in namespace [$NAMESPACE] to become Running and Ready (timeout ${TIMEOUT}s)..."[0m
2026-02-19T19:51:16.0128279Z [36;1m[0m
2026-02-19T19:51:16.0128521Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-19T19:51:16.0128787Z [36;1m[0m
2026-02-19T19:51:16.0129015Z [36;1mwhile true; do[0m
2026-02-19T19:51:16.0129243Z [36;1m  NOW=$(date +%s)[0m
2026-02-19T19:51:16.0129521Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-19T19:51:16.0129809Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-19T19:51:16.0130130Z [36;1m    echo "Timeout reached after ${ELAPSED}s"[0m
2026-02-19T19:51:16.0130536Z [36;1m    echo "Dumping pod status for debugging:"[0m
2026-02-19T19:51:16.0130807Z [36;1m    kubectl get pods -n "$NAMESPACE"[0m
2026-02-19T19:51:16.0131131Z [36;1m    kubectl describe pod "$LEADER_POD" -n "$NAMESPACE"[0m
2026-02-19T19:51:16.0131436Z [36;1m    exit 1[0m
2026-02-19T19:51:16.0131681Z [36;1m  fi[0m
2026-02-19T19:51:16.0131888Z [36;1m[0m
2026-02-19T19:51:16.0132186Z [36;1m  # 1) check follower pods[0m
2026-02-19T19:51:16.0132455Z [36;1m  ALL_FOLLOWERS_READY=true[0m
2026-02-19T19:51:16.0132724Z [36;1m  for ((i=1; i<SIZE; i++)); do[0m
2026-02-19T19:51:16.0132965Z [36;1m    POD="${POD_PREFIX}-${i}"[0m
2026-02-19T19:51:16.0133401Z [36;1m    PHASE=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-19T19:51:16.0133994Z [36;1m    READY=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-19T19:51:16.0134558Z [36;1m[0m
2026-02-19T19:51:16.0134823Z [36;1m    echo "Follower [$POD] phase=$PHASE ready=$READY"[0m
2026-02-19T19:51:16.0135128Z [36;1m[0m
2026-02-19T19:51:16.0135434Z [36;1m    if [[ "$PHASE" != "Running" || "$READY" != "true" ]]; then[0m
2026-02-19T19:51:16.0135819Z [36;1m      echo "Follower [$POD] not Ready yet..."[0m
2026-02-19T19:51:16.0136125Z [36;1m      ALL_FOLLOWERS_READY=false[0m
2026-02-19T19:51:16.0136348Z [36;1m      break[0m
2026-02-19T19:51:16.0136567Z [36;1m    fi[0m
2026-02-19T19:51:16.0136784Z [36;1m  done[0m
2026-02-19T19:51:16.0136956Z [36;1m[0m
2026-02-19T19:51:16.0137163Z [36;1m  # 2) check leader pod[0m
2026-02-19T19:51:16.0137623Z [36;1m  LEADER_PHASE=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-19T19:51:16.0138245Z [36;1m  LEADER_READY=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-19T19:51:16.0138700Z [36;1m[0m
2026-02-19T19:51:16.0138999Z [36;1m  echo "Leader [$LEADER_POD] phase=$LEADER_PHASE ready=$LEADER_READY"[0m
2026-02-19T19:51:16.0139296Z [36;1m[0m
2026-02-19T19:51:16.0139636Z [36;1m  if [[ "$LEADER_PHASE" != "Running" || "$LEADER_READY" != "true" ]]; then[0m
2026-02-19T19:51:16.0139984Z [36;1m    echo "Leader not Ready yet..."[0m
2026-02-19T19:51:16.0140228Z [36;1m    ALL_FOLLOWERS_READY=false[0m
2026-02-19T19:51:16.0140486Z [36;1m  fi[0m
2026-02-19T19:51:16.0140693Z [36;1m[0m
2026-02-19T19:51:16.0140899Z [36;1m  if [[ "$ALL_FOLLOWERS_READY" == "true" ]]; then[0m
2026-02-19T19:51:16.0141299Z [36;1m    echo "All follower pods and leader pod are Running and Ready â€” continuing."[0m
2026-02-19T19:51:16.0141665Z [36;1m    break[0m
2026-02-19T19:51:16.0141854Z [36;1m  fi[0m
2026-02-19T19:51:16.0142236Z [36;1m[0m
2026-02-19T19:51:16.0142409Z [36;1m  sleep 2[0m
2026-02-19T19:51:16.0142696Z [36;1mdone[0m
2026-02-19T19:51:16.0143066Z shell: bash -el {0}
2026-02-19T19:51:16.0143247Z env:
2026-02-19T19:51:16.0143670Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-19T19:51:16.0143981Z ##[endgroup]
2026-02-19T19:51:16.0227303Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-19T19:51:16.0228106Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-19T19:51:16.0228447Z ##[endgroup]
2026-02-19T19:51:16.4127633Z (node:704) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-19T19:51:16.4128366Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-19T19:51:16.9563668Z Waiting for Pods in namespace [vllm-project] to become Running and Ready (timeout 1200s)...
2026-02-19T19:51:17.0877947Z Follower [vllm-0-1] phase=Pending ready=
2026-02-19T19:51:17.0878342Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:51:17.1982289Z Leader [vllm-0] phase=Pending ready=
2026-02-19T19:51:17.1982632Z Leader not Ready yet...
2026-02-19T19:51:19.3126175Z Follower [vllm-0-1] phase=Pending ready=
2026-02-19T19:51:19.3126593Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:51:19.4340180Z Leader [vllm-0] phase=Pending ready=
2026-02-19T19:51:19.4340576Z Leader not Ready yet...
2026-02-19T19:51:21.5568093Z Follower [vllm-0-1] phase=Pending ready=
2026-02-19T19:51:21.5568484Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:51:21.6644473Z Leader [vllm-0] phase=Pending ready=
2026-02-19T19:51:21.6644870Z Leader not Ready yet...
2026-02-19T19:51:23.7869617Z Follower [vllm-0-1] phase=Pending ready=
2026-02-19T19:51:23.7870049Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:51:23.8973902Z Leader [vllm-0] phase=Pending ready=
2026-02-19T19:51:23.8974236Z Leader not Ready yet...
2026-02-19T19:51:26.0180188Z Follower [vllm-0-1] phase=Pending ready=
2026-02-19T19:51:26.0180543Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:51:26.1358868Z Leader [vllm-0] phase=Pending ready=
2026-02-19T19:51:26.1359466Z Leader not Ready yet...
2026-02-19T19:51:28.2573104Z Follower [vllm-0-1] phase=Pending ready=
2026-02-19T19:51:28.2573519Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:51:28.3737358Z Leader [vllm-0] phase=Pending ready=false
2026-02-19T19:51:28.3737705Z Leader not Ready yet...
2026-02-19T19:51:30.4906119Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:51:30.4906457Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:51:30.6088784Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:51:32.7340126Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:51:32.7340625Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:51:32.8459762Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:51:34.9648564Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:51:34.9648946Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:51:35.0816439Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:51:37.1966677Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:51:37.1967064Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:51:37.3227858Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:51:39.4434260Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:51:39.4434642Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:51:39.5614147Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:51:41.6916506Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:51:41.6916947Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:51:41.8048416Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:51:43.9235942Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:51:43.9236281Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:51:44.0337184Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:51:46.1517071Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:51:46.1517424Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:51:46.2650305Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:51:48.3795902Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:51:48.3796334Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:51:48.4920835Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:51:50.6121164Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:51:50.6121535Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:51:50.7298120Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:51:52.8455385Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:51:52.8455845Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:51:52.9621675Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:51:55.0867303Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:51:55.0867688Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:51:55.2047059Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:51:57.3189953Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:51:57.3190307Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:51:57.4406789Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:51:59.5658728Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:51:59.5659086Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:51:59.6916269Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:52:01.8131980Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:52:01.8132456Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:52:01.9230081Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:52:04.0456008Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:52:04.0456446Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:52:04.1621556Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:52:06.2802653Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:52:06.2803092Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:52:06.3958818Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:52:08.5198086Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:52:08.5198517Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:52:08.6337535Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:52:10.7518302Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:52:10.7518836Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:52:10.8675505Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:52:12.9863342Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:52:12.9863724Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:52:13.1006904Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:52:15.2367924Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:52:15.2368272Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:52:15.3712871Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:52:17.4947853Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-19T19:52:17.4948260Z Follower [vllm-0-1] not Ready yet...
2026-02-19T19:52:17.6130730Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:52:19.7257108Z Follower [vllm-0-1] phase=Running ready=true
2026-02-19T19:52:19.8383965Z Leader [vllm-0] phase=Running ready=true
2026-02-19T19:52:19.8384678Z All follower pods and leader pod are Running and Ready â€” continuing.
2026-02-19T19:52:20.2936766Z ##[group]Run set -euo pipefail
2026-02-19T19:52:20.2937193Z [36;1mset -euo pipefail[0m
2026-02-19T19:52:20.2937435Z [36;1m[0m
2026-02-19T19:52:20.2937669Z [36;1msize="2"[0m
2026-02-19T19:52:20.2937923Z [36;1mpids=()[0m
2026-02-19T19:52:20.2938125Z [36;1m[0m
2026-02-19T19:52:20.2938423Z [36;1mcleanup() {[0m
2026-02-19T19:52:20.2938706Z [36;1m  echo "Cleaning up background log streams..."[0m
2026-02-19T19:52:20.2939013Z [36;1m  for pid in "${pids[@]}"; do[0m
2026-02-19T19:52:20.2939322Z [36;1m    kill "$pid" 2>/dev/null || true[0m
2026-02-19T19:52:20.2939559Z [36;1m  done[0m
2026-02-19T19:52:20.2939763Z [36;1m}[0m
2026-02-19T19:52:20.2939975Z [36;1mtrap cleanup EXIT[0m
2026-02-19T19:52:20.2940214Z [36;1m[0m
2026-02-19T19:52:20.2940429Z [36;1mfor i in $(seq 1 $((size - 1))); do[0m
2026-02-19T19:52:20.2940702Z [36;1m  POD="vllm-0-${i}"[0m
2026-02-19T19:52:20.2940941Z [36;1m[0m
2026-02-19T19:52:20.2941267Z [36;1m  echo "==== Collecting logs from worker pod: $POD ===="[0m
2026-02-19T19:52:20.2941640Z [36;1m  kubectl logs -f "$POD" -n "$NAMESPACE" \[0m
2026-02-19T19:52:20.2941948Z [36;1m    > "/tmp/${POD}_logs.txt" 2>&1 &[0m
2026-02-19T19:52:20.2942315Z [36;1m[0m
2026-02-19T19:52:20.2942599Z [36;1m  pids+=($!)[0m
2026-02-19T19:52:20.2942837Z [36;1mdone[0m
2026-02-19T19:52:20.2943023Z [36;1m[0m
2026-02-19T19:52:20.2943304Z [36;1mecho "==== Streaming logs from leader pod: $LEADER_POD ===="[0m
2026-02-19T19:52:20.2943661Z [36;1mecho "Looking for logs containing: $FAIL_TAG"[0m
2026-02-19T19:52:20.2943916Z [36;1m[0m
2026-02-19T19:52:20.2944233Z [36;1mkubectl logs -f "$LEADER_POD" -n "$NAMESPACE" | while IFS= read -r line; do[0m
2026-02-19T19:52:20.2944610Z [36;1m  echo "$line"[0m
2026-02-19T19:52:20.2944844Z [36;1m  if echo "$line" | grep -q "$FAIL_TAG"; then[0m
2026-02-19T19:52:20.2945133Z [36;1m    exit 1[0m
2026-02-19T19:52:20.2945324Z [36;1m  fi[0m
2026-02-19T19:52:20.2945541Z [36;1mdone[0m
2026-02-19T19:52:20.2945971Z shell: bash -el {0}
2026-02-19T19:52:20.2946171Z env:
2026-02-19T19:52:20.2946512Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-19T19:52:20.2946830Z ##[endgroup]
2026-02-19T19:52:20.3045710Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-19T19:52:20.3046452Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-19T19:52:20.3046755Z ##[endgroup]
2026-02-19T19:52:20.6616837Z (node:807) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-19T19:52:20.6617637Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-19T19:52:21.1893252Z ==== Collecting logs from worker pod: vllm-0-1 ====
2026-02-19T19:52:21.1893650Z ==== Streaming logs from leader pod: vllm-0 ====
2026-02-19T19:52:21.1894030Z Looking for logs containing: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-19T19:52:21.2718062Z /usr/local/Ascend/ascend-toolkit/set_env.sh: line 31: CMAKE_PREFIX_PATH: unbound variable
2026-02-19T19:52:21.2729516Z ====> Check NPU info
2026-02-19T19:52:21.2738801Z +------------------------------------------------------------------------------------------------+
2026-02-19T19:52:21.2747379Z | npu-smi 25.5.0                   Version: 25.5.0                                               |
2026-02-19T19:52:21.2756731Z +---------------------------+---------------+----------------------------------------------------+
2026-02-19T19:52:21.2767503Z | NPU   Name                | Health        | Power(W)    Temp(C)           Hugepages-Usage(page)|
2026-02-19T19:52:21.2778693Z | Chip  Phy-ID              | Bus-Id        | AICore(%)   Memory-Usage(MB)  HBM-Usage(MB)        |
2026-02-19T19:52:21.2788645Z +===========================+===============+====================================================+
2026-02-19T19:52:21.2798763Z | 0     Ascend910           | OK            | 165.3       36                0    / 0             |
2026-02-19T19:52:21.2808521Z | 0     0                   | 0000:9D:00.0  | 0           0    / 0          3158 / 65536         |
2026-02-19T19:52:21.2818146Z +------------------------------------------------------------------------------------------------+
2026-02-19T19:52:21.2828205Z | 0     Ascend910           | OK            | -           36                0    / 0             |
2026-02-19T19:52:21.2837619Z | 1     1                   | 0000:9F:00.0  | 0           0    / 0          2892 / 65536         |
2026-02-19T19:52:21.2847021Z +===========================+===============+====================================================+
2026-02-19T19:52:21.2856577Z | 1     Ascend910           | OK            | 163.2       35                0    / 0             |
2026-02-19T19:52:21.2867731Z | 0     2                   | 0000:99:00.0  | 0           0    / 0          3168 / 65536         |
2026-02-19T19:52:21.2875197Z +------------------------------------------------------------------------------------------------+
2026-02-19T19:52:21.2885590Z | 1     Ascend910           | OK            | -           34                0    / 0             |
2026-02-19T19:52:21.2894951Z | 1     3                   | 0000:9B:00.0  | 0           0    / 0          2883 / 65536         |
2026-02-19T19:52:21.2904462Z +===========================+===============+====================================================+
2026-02-19T19:52:21.2914370Z | 2     Ascend910           | OK            | 162.2       34                0    / 0             |
2026-02-19T19:52:21.2924170Z | 0     4                   | 0000:95:00.0  | 0           0    / 0          3153 / 65536         |
2026-02-19T19:52:21.2933874Z +------------------------------------------------------------------------------------------------+
2026-02-19T19:52:21.2942621Z | 2     Ascend910           | OK            | -           35                0    / 0             |
2026-02-19T19:52:21.2951783Z | 1     5                   | 0000:97:00.0  | 0           0    / 0          2895 / 65536         |
2026-02-19T19:52:21.2961172Z +===========================+===============+====================================================+
2026-02-19T19:52:21.2970799Z | 3     Ascend910           | OK            | 165.2       34                0    / 0             |
2026-02-19T19:52:21.2980182Z | 0     6                   | 0000:91:00.0  | 0           0    / 0          3162 / 65536         |
2026-02-19T19:52:21.2992397Z +------------------------------------------------------------------------------------------------+
2026-02-19T19:52:21.3003406Z | 3     Ascend910           | OK            | -           34                0    / 0             |
2026-02-19T19:52:21.3012850Z | 1     7                   | 0000:93:00.0  | 0           0    / 0          2883 / 65536         |
2026-02-19T19:52:21.3021911Z +===========================+===============+====================================================+
2026-02-19T19:52:21.3032094Z | 4     Ascend910           | OK            | 167.2       34                0    / 0             |
2026-02-19T19:52:21.3041714Z | 0     8                   | 0000:8D:00.0  | 0           0    / 0          3156 / 65536         |
2026-02-19T19:52:21.3051924Z +------------------------------------------------------------------------------------------------+
2026-02-19T19:52:21.3060698Z | 4     Ascend910           | OK            | -           35                0    / 0             |
2026-02-19T19:52:21.3070343Z | 1     9                   | 0000:8F:00.0  | 0           0    / 0          2895 / 65536         |
2026-02-19T19:52:21.3079352Z +===========================+===============+====================================================+
2026-02-19T19:52:21.3093245Z | 5     Ascend910           | OK            | 165.9       34                0    / 0             |
2026-02-19T19:52:21.3098411Z | 0     10                  | 0000:89:00.0  | 0           0    / 0          3147 / 65536         |
2026-02-19T19:52:21.3107628Z +------------------------------------------------------------------------------------------------+
2026-02-19T19:52:21.3117507Z | 5     Ascend910           | OK            | -           37                0    / 0             |
2026-02-19T19:52:21.3127799Z | 1     11                  | 0000:8B:00.0  | 0           0    / 0          2895 / 65536         |
2026-02-19T19:52:21.3136541Z +===========================+===============+====================================================+
2026-02-19T19:52:21.3145345Z | 6     Ascend910           | OK            | 161.7       36                0    / 0             |
2026-02-19T19:52:21.3154552Z | 0     12                  | 0000:85:00.0  | 0           0    / 0          3149 / 65536         |
2026-02-19T19:52:21.3164028Z +------------------------------------------------------------------------------------------------+
2026-02-19T19:52:21.3173819Z | 6     Ascend910           | OK            | -           36                0    / 0             |
2026-02-19T19:52:21.3182895Z | 1     13                  | 0000:87:00.0  | 0           0    / 0          2891 / 65536         |
2026-02-19T19:52:21.3193381Z +===========================+===============+====================================================+
2026-02-19T19:52:21.3201727Z | 7     Ascend910           | OK            | 158.4       34                0    / 0             |
2026-02-19T19:52:21.3211196Z | 0     14                  | 0000:81:00.0  | 0           0    / 0          3159 / 65536         |
2026-02-19T19:52:21.3219844Z +------------------------------------------------------------------------------------------------+
2026-02-19T19:52:21.3228821Z | 7     Ascend910           | OK            | -           36                0    / 0             |
2026-02-19T19:52:21.3238484Z | 1     15                  | 0000:83:00.0  | 0           0    / 0          2885 / 65536         |
2026-02-19T19:52:21.3248208Z +===========================+===============+====================================================+
2026-02-19T19:52:21.3257234Z +---------------------------+---------------+----------------------------------------------------+
2026-02-19T19:52:21.3266539Z | NPU     Chip              | Process id    | Process name             | Process memory(MB)      |
2026-02-19T19:52:21.3276131Z +===========================+===============+====================================================+
2026-02-19T19:52:21.3285821Z | No running processes found in NPU 0                                                            |
2026-02-19T19:52:21.3294870Z +===========================+===============+====================================================+
2026-02-19T19:52:21.3303917Z | No running processes found in NPU 1                                                            |
2026-02-19T19:52:21.3313033Z +===========================+===============+====================================================+
2026-02-19T19:52:21.3322997Z | No running processes found in NPU 2                                                            |
2026-02-19T19:52:21.3331767Z +===========================+===============+====================================================+
2026-02-19T19:52:21.3340793Z | No running processes found in NPU 3                                                            |
2026-02-19T19:52:21.3349843Z +===========================+===============+====================================================+
2026-02-19T19:52:21.3359504Z | No running processes found in NPU 4                                                            |
2026-02-19T19:52:21.3369115Z +===========================+===============+====================================================+
2026-02-19T19:52:21.3378092Z | No running processes found in NPU 5                                                            |
2026-02-19T19:52:21.3386840Z +===========================+===============+====================================================+
2026-02-19T19:52:21.3396373Z | No running processes found in NPU 6                                                            |
2026-02-19T19:52:21.3405558Z +===========================+===============+====================================================+
2026-02-19T19:52:21.3415161Z | No running processes found in NPU 7                                                            |
2026-02-19T19:52:21.3423906Z +===========================+===============+====================================================+
2026-02-19T19:52:21.3433038Z package_name=Ascend-cann-toolkit
2026-02-19T19:52:21.3442912Z version=8.5.0
2026-02-19T19:52:21.3452524Z innerversion=V100R001C25SPC001B232
2026-02-19T19:52:21.3461309Z compatible_version=[V100R001C15],[V100R001C18],[V100R001C19],[V100R001C20],[V100R001C21],[V100R001C23]
2026-02-19T19:52:21.3470245Z arch=aarch64
2026-02-19T19:52:21.3479362Z os=linux
2026-02-19T19:52:21.3489057Z path=/usr/local/Ascend/cann-8.5.0
2026-02-19T19:52:21.3498553Z ====> Configure mirrors and git proxy
2026-02-19T19:52:21.3507886Z Writing to /root/.config/pip/pip.conf
2026-02-19T19:52:21.3518007Z Installed vLLM-related Python packages:
2026-02-19T19:52:21.3528217Z ais_bench_benchmark               3.0.20250930                /vllm-workspace/vllm-ascend/benchmark
2026-02-19T19:52:21.3537211Z vllm                              0.15.0+empty                /vllm-workspace/vllm
2026-02-19T19:52:21.3547053Z vllm_ascend                       0.14.0rc2.dev171+gf0caeeadc /vllm-workspace/vllm-ascend
2026-02-19T19:52:21.3555547Z 
2026-02-19T19:52:21.3565421Z ============================
2026-02-19T19:52:21.3574366Z vLLM Git information
2026-02-19T19:52:21.3583349Z ============================
2026-02-19T19:52:21.3591954Z Branch:      HEAD
2026-02-19T19:52:21.3601717Z Commit hash: f176443446f659dbab5315e056e605d8984fd976
2026-02-19T19:52:21.3611150Z Author:      TJian <tunjian.tan@embeddedllm.com>
2026-02-19T19:52:21.3619709Z Date:        2026-01-29 14:45:42 +0800
2026-02-19T19:52:21.3628891Z Message:     [Release] [CI] Optim release pipeline (#33156)
2026-02-19T19:52:21.3638268Z Tags:        v0.15.0
2026-02-19T19:52:21.3647110Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm.git (fetch)
2026-02-19T19:52:21.3655865Z 
2026-02-19T19:52:21.3665033Z 
2026-02-19T19:52:21.3673453Z ============================
2026-02-19T19:52:21.3683022Z vLLM-Ascend Git information
2026-02-19T19:52:21.3691844Z ============================
2026-02-19T19:52:21.3700448Z Branch:      main
2026-02-19T19:52:21.3709930Z Commit hash: f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-19T19:52:21.3718746Z Author:      Nengjun Ma <nengjunma@outlook.com>
2026-02-19T19:52:21.3727905Z Date:        2026-02-14 18:54:04 +0800
2026-02-19T19:52:21.3736803Z Message:     [CI] unlock when load model (#6771)
2026-02-19T19:52:21.3745643Z Tags:        
2026-02-19T19:52:21.3755087Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm-ascend (fetch)
2026-02-19T19:52:21.3764354Z 
2026-02-19T19:52:21.3774257Z ====> Check triton ascend info
2026-02-19T19:52:21.3783755Z Ubuntu clang version 15.0.7
2026-02-19T19:52:21.3793465Z Target: aarch64-unknown-linux-gnu
2026-02-19T19:52:21.3802746Z Thread model: posix
2026-02-19T19:52:21.3811900Z InstalledDir: /usr/bin
2026-02-19T19:52:21.3821603Z Found candidate GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-19T19:52:21.3830462Z Selected GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-19T19:52:21.3840272Z Candidate multilib: .;@m64
2026-02-19T19:52:21.3849505Z Selected multilib: .;@m64
2026-02-19T19:52:21.3858838Z /usr/local/Ascend/cann-8.5.0/tools/bishengir/bin/bishengir-compile
2026-02-19T19:52:21.3867825Z Name: triton-ascend
2026-02-19T19:52:21.3877101Z Version: 3.2.0
2026-02-19T19:52:21.3886664Z Summary: A language and compiler for custom Deep Learning operations on Ascend hardwares
2026-02-19T19:52:21.3895098Z Home-page: https://gitcode.com/Ascend/triton-ascend/
2026-02-19T19:52:21.3903996Z Author: 
2026-02-19T19:52:21.3913089Z Author-email: 
2026-02-19T19:52:21.3921840Z License: 
2026-02-19T19:52:21.3931658Z Location: /usr/local/python3.11.14/lib/python3.11/site-packages
2026-02-19T19:52:21.3940301Z Requires: 
2026-02-19T19:52:21.3949576Z Required-by: vllm_ascend
2026-02-19T19:52:21.3959189Z INFO 02-19 19:51:45 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:52:21.3968303Z INFO 02-19 19:51:45 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:52:21.3977826Z INFO 02-19 19:51:45 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:52:21.3986597Z INFO 02-19 19:51:45 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:52:21.3995531Z ============================= test session starts ==============================
2026-02-19T19:52:21.4005610Z platform linux -- Python 3.11.14, pytest-8.4.2, pluggy-1.6.0 -- /usr/local/python3.11.14/bin/python3
2026-02-19T19:52:21.4014151Z cachedir: .pytest_cache
2026-02-19T19:52:21.4023241Z rootdir: /vllm-workspace/vllm-ascend
2026-02-19T19:52:21.4031888Z configfile: pyproject.toml
2026-02-19T19:52:21.4041423Z plugins: cov-7.0.0, mock-3.15.1, asyncio-1.3.0, anyio-4.12.1
2026-02-19T19:52:21.4051330Z asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
2026-02-19T19:52:21.4060118Z collecting ... collected 1 item
2026-02-19T19:52:21.4068307Z 
2026-02-19T19:52:21.4079045Z [2026-02-19 19:51:50] INFO multi_node_config.py:294: Loading config yaml: tests/e2e/nightly/multi_node/config/DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-19T19:52:21.4087979Z [2026-02-19 19:51:50] INFO multi_node_config.py:351: Resolving cluster IPs via DNS...
2026-02-19T19:52:21.4099406Z [2026-02-19 19:52:20] INFO multi_node_config.py:212: Node 0 envs: {'HCCL_OP_EXPANSION_MODE': 'AIV', 'VLLM_USE_MODELSCOPE': 'True', 'HCCL_BUFFSIZE': '1024', 'SERVER_PORT': '8080', 'OMP_PROC_BIND': 'False', 'OMP_NUM_THREADS': '1', 'PYTORCH_NPU_ALLOC_CONF': 'expandable_segments:True', 'VLLM_ASCEND_ENABLE_FLASHCOMM1': '1', 'ASCEND_A3_EBA_ENABLE': '1', 'HCCL_IF_IP': '10.0.0.219', 'HCCL_SOCKET_IFNAME': 'eth0', 'GLOO_SOCKET_IFNAME': 'eth0', 'TP_SOCKET_IFNAME': 'eth0', 'LOCAL_IP': '10.0.0.219', 'NIC_NAME': 'eth0', 'MASTER_IP': '10.0.0.219'}
2026-02-19T19:52:21.4107458Z [2026-02-19 19:52:20] INFO multi_node_config.py:137: Not launching proxy on non-master node
2026-02-19T19:52:21.4120977Z [2026-02-19 19:52:20] INFO conftest.py:241: Starting server with command: vllm serve vllm-ascend/DeepSeek-V3.2-W8A8 --host 0.0.0.0 --port 8080 --data-parallel-size 4 --data-parallel-size-local 2 --data-parallel-address 10.0.0.219 --data-parallel-rpc-port 13399 --tensor-parallel-size 8 --quantization ascend --seed 1024 --enable-expert-parallel --max-num-seqs 16 --max-model-len 8192 --max-num-batched-tokens 4096 --no-enable-prefix-caching --gpu-memory-utilization 0.85 --trust-remote-code --speculative-config {"num_speculative_tokens": 2, "method":"deepseek_mtp"} --compilation-config {"cudagraph_capture_sizes": [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], "cudagraph_mode": "FULL_DECODE_ONLY"} --additional-config {"layer_sharding": ["q_b_proj", "o_proj"]} --tokenizer-mode deepseek_v32 --reasoning-parser deepseek_v3
2026-02-19T19:52:25.2511537Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node INFO 02-19 19:52:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:52:25.2518153Z INFO 02-19 19:52:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:52:25.2529112Z INFO 02-19 19:52:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:52:25.2575856Z INFO 02-19 19:52:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:52:31.5597078Z 2026-02-19 19:52:31,556 - 138 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:52:31.5893718Z INFO 02-19 19:52:31 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:52:31.7347757Z INFO 02-19 19:52:31 [serve.py:100] Defaulting api_server_count to data_parallel_size (4).
2026-02-19T19:52:31.7370557Z INFO 02-19 19:52:31 [utils.py:325] 
2026-02-19T19:52:31.7379096Z INFO 02-19 19:52:31 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
2026-02-19T19:52:31.7390526Z INFO 02-19 19:52:31 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
2026-02-19T19:52:31.7401691Z INFO 02-19 19:52:31 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   vllm-ascend/DeepSeek-V3.2-W8A8
2026-02-19T19:52:31.7412194Z INFO 02-19 19:52:31 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
2026-02-19T19:52:31.7421428Z INFO 02-19 19:52:31 [utils.py:325] 
2026-02-19T19:52:31.7440889Z INFO 02-19 19:52:31 [utils.py:261] non-default args: {'model_tag': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'api_server_count': 4, 'host': '0.0.0.0', 'port': 8080, 'model': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'tokenizer_mode': 'deepseek_v32', 'trust_remote_code': True, 'seed': 1024, 'max_model_len': 8192, 'quantization': 'ascend', 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 8, 'data_parallel_size': 4, 'data_parallel_size_local': 2, 'data_parallel_address': '10.0.0.219', 'data_parallel_rpc_port': 13399, 'enable_expert_parallel': True, 'gpu_memory_utilization': 0.85, 'enable_prefix_caching': False, 'max_num_batched_tokens': 4096, 'max_num_seqs': 16, 'speculative_config': {'num_speculative_tokens': 2, 'method': 'deepseek_mtp'}, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}, 'additional_config': {'layer_sharding': ['q_b_proj', 'o_proj']}}
2026-02-19T19:52:31.7891244Z 2026-02-19 19:52:31,787 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-19T19:52:31.8019870Z INFO 02-19 19:52:31 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-19T19:52:31.8034239Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:52:31.8049347Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:52:31.8049838Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:52:31.8059716Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-19T19:52:31.8252938Z INFO 02-19 19:52:31 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-19T19:52:31.8261282Z INFO 02-19 19:52:31 [model.py:1561] Using max model len 8192
2026-02-19T19:52:32.1162552Z WARNING 02-19 19:52:32 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-19T19:52:32.1183868Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:52:32.1193092Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:52:32.1204064Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-19T19:52:41.5809685Z INFO 02-19 19:52:41 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-19T19:52:41.5835052Z INFO 02-19 19:52:41 [model.py:1561] Using max model len 163840
2026-02-19T19:52:41.5846182Z WARNING 02-19 19:52:41 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-19T19:52:41.5855061Z INFO 02-19 19:52:41 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-19T19:52:41.9177858Z INFO 02-19 19:52:41 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-19T19:52:41.9186433Z INFO 02-19 19:52:41 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-19T19:52:41.9200009Z WARNING 02-19 19:52:41 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-19T19:52:41.9210675Z WARNING 02-19 19:52:41 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-19T19:52:41.9219938Z INFO 02-19 19:52:41 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:52:41.9230118Z INFO 02-19 19:52:41 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:52:41.9240176Z INFO 02-19 19:52:41 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:52:41.9249724Z WARNING 02-19 19:52:41 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-19T19:52:41.9259733Z INFO 02-19 19:52:41 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-19T19:52:41.9269286Z WARNING 02-19 19:52:41 [platform.py:335] [91m
2026-02-19T19:52:41.9279283Z WARNING 02-19 19:52:41 [platform.py:335]             **********************************************************************************
2026-02-19T19:52:41.9289067Z WARNING 02-19 19:52:41 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-19T19:52:41.9298676Z WARNING 02-19 19:52:41 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-19T19:52:41.9307902Z WARNING 02-19 19:52:41 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-19T19:52:41.9318302Z WARNING 02-19 19:52:41 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-19T19:52:41.9326856Z WARNING 02-19 19:52:41 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-19T19:52:41.9336222Z WARNING 02-19 19:52:41 [platform.py:335]             * batch size for graph capture.
2026-02-19T19:52:41.9345648Z WARNING 02-19 19:52:41 [platform.py:335]             * For more details, please refer to:
2026-02-19T19:52:41.9357419Z WARNING 02-19 19:52:41 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-19T19:52:41.9366055Z WARNING 02-19 19:52:41 [platform.py:335]             **********************************************************************************[0m
2026-02-19T19:52:41.9374719Z WARNING 02-19 19:52:41 [platform.py:335]             
2026-02-19T19:52:41.9383986Z INFO 02-19 19:52:41 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-19T19:52:41.9393019Z INFO 02-19 19:52:41 [utils.py:851] Started DP Coordinator process (PID: 158)
2026-02-19T19:52:46.6162289Z INFO 02-19 19:52:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:52:46.6162810Z INFO 02-19 19:52:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:52:46.6163602Z INFO 02-19 19:52:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:52:46.6205599Z INFO 02-19 19:52:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:52:46.6916641Z INFO 02-19 19:52:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:52:46.6927909Z INFO 02-19 19:52:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:52:46.6936838Z INFO 02-19 19:52:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:52:46.7003575Z INFO 02-19 19:52:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:52:56.4421875Z INFO 02-19 19:52:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:52:56.4433274Z INFO 02-19 19:52:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:52:56.4445015Z INFO 02-19 19:52:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:52:56.4512319Z INFO 02-19 19:52:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:53:01.6901397Z INFO 02-19 19:53:01 [utils.py:218] Started 4 API server processes
2026-02-19T19:53:06.2201611Z INFO 02-19 19:53:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:53:06.2209877Z INFO 02-19 19:53:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:53:06.2222370Z INFO 02-19 19:53:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:53:06.2267289Z INFO 02-19 19:53:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:53:06.7853158Z INFO 02-19 19:53:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:53:06.7859568Z INFO 02-19 19:53:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:53:06.7868548Z INFO 02-19 19:53:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:53:06.7927317Z INFO 02-19 19:53:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:53:06.7993516Z INFO 02-19 19:53:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:53:06.8003681Z INFO 02-19 19:53:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:53:06.8013161Z INFO 02-19 19:53:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:53:06.8070929Z INFO 02-19 19:53:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:53:06.8170477Z INFO 02-19 19:53:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:53:06.8179383Z INFO 02-19 19:53:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:53:06.8188553Z INFO 02-19 19:53:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:53:06.8250425Z INFO 02-19 19:53:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:53:11.1034430Z [0;36m(EngineCore_DP0 pid=161)[0;0m 2026-02-19 19:53:11,101 - 161 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:53:11.1096245Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-19 19:53:11 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:53:11.1137168Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-19 19:53:11 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', speculative_config=SpeculativeConfig(method='mtp', model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', num_spec_tokens=2), tokenizer='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', skip_tokenizer_init=False, tokenizer_mode=deepseek_v32, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=4, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=bfloat16, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=1024, served_model_name=/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [24, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 48, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
2026-02-19T19:53:11.1156345Z [0;36m(EngineCore_DP1 pid=180)[0;0m 2026-02-19 19:53:11,114 - 180 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:53:11.1207541Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-19 19:53:11 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:53:12.0276223Z [0;36m(ApiServer_3 pid=194)[0;0m 2026-02-19 19:53:12,026 - 194 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:53:12.0418896Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-19 19:53:12 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:53:12.0701783Z [0;36m(ApiServer_3 pid=194)[0;0m 2026-02-19 19:53:12,066 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-19T19:53:12.0725773Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-19 19:53:12 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-19T19:53:12.1714710Z [0;36m(ApiServer_3 pid=194)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:53:12.1723585Z [0;36m(ApiServer_3 pid=194)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:53:12.1736503Z [0;36m(ApiServer_3 pid=194)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:53:12.1747400Z [0;36m(ApiServer_3 pid=194)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-19T19:53:12.1811592Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-19 19:53:12 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-19T19:53:12.1831593Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-19 19:53:12 [model.py:1561] Using max model len 8192
2026-02-19T19:53:12.2975233Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-19 19:53:12 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-19T19:53:12.2983265Z [0;36m(ApiServer_3 pid=194)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:53:12.2993370Z [0;36m(ApiServer_3 pid=194)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:53:12.3003422Z [0;36m(ApiServer_3 pid=194)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-19T19:53:12.3043995Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-19 19:53:12 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-19T19:53:12.3066601Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-19 19:53:12 [model.py:1561] Using max model len 163840
2026-02-19T19:53:12.3075604Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-19 19:53:12 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-19T19:53:12.3085737Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-19 19:53:12 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-19T19:53:12.4218666Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-19 19:53:12 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-19T19:53:12.4227688Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-19 19:53:12 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-19T19:53:12.4243676Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-19 19:53:12 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-19T19:53:12.4267691Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-19 19:53:12 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-19T19:53:12.4279810Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-19 19:53:12 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:53:12.4289293Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-19 19:53:12 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:53:12.4300112Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-19 19:53:12 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:53:12.4308719Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-19 19:53:12 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-19T19:53:12.4319184Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-19 19:53:12 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-19T19:53:12.4329076Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-19 19:53:12 [platform.py:335] [91m
2026-02-19T19:53:12.4338850Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-19 19:53:12 [platform.py:335]             **********************************************************************************
2026-02-19T19:53:12.4349786Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-19 19:53:12 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-19T19:53:12.4358826Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-19 19:53:12 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-19T19:53:12.4368400Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-19 19:53:12 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-19T19:53:12.4377168Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-19 19:53:12 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-19T19:53:12.4387238Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-19 19:53:12 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-19T19:53:12.4396923Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-19 19:53:12 [platform.py:335]             * batch size for graph capture.
2026-02-19T19:53:12.4406460Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-19 19:53:12 [platform.py:335]             * For more details, please refer to:
2026-02-19T19:53:12.4416781Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-19 19:53:12 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-19T19:53:12.4426355Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-19 19:53:12 [platform.py:335]             **********************************************************************************[0m
2026-02-19T19:53:12.4435869Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-19 19:53:12 [platform.py:335]             
2026-02-19T19:53:12.4445826Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-19 19:53:12 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-19T19:53:12.8918182Z [0;36m(ApiServer_2 pid=193)[0;0m 2026-02-19 19:53:12,890 - 193 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:53:12.9089091Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-19 19:53:12 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:53:12.9244592Z [0;36m(ApiServer_2 pid=193)[0;0m 2026-02-19 19:53:12,923 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-19T19:53:12.9303861Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-19 19:53:12 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-19T19:53:13.0331321Z [0;36m(ApiServer_2 pid=193)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:53:13.0423512Z [0;36m(ApiServer_2 pid=193)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:53:13.0440816Z [0;36m(ApiServer_2 pid=193)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:53:13.0446838Z [0;36m(ApiServer_2 pid=193)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-19T19:53:13.0457067Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-19 19:53:13 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-19T19:53:13.0466468Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-19 19:53:13 [model.py:1561] Using max model len 8192
2026-02-19T19:53:13.0554671Z [0;36m(ApiServer_0 pid=191)[0;0m 2026-02-19 19:53:13,053 - 191 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:53:13.0707988Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-19 19:53:13 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:53:13.0860519Z [0;36m(ApiServer_0 pid=191)[0;0m 2026-02-19 19:53:13,085 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-19T19:53:13.0916852Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-19 19:53:13 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-19T19:53:13.1516778Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-19 19:53:13 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-19T19:53:13.1536935Z [0;36m(ApiServer_2 pid=193)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:53:13.1546139Z [0;36m(ApiServer_2 pid=193)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:53:13.1556269Z [0;36m(ApiServer_2 pid=193)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-19T19:53:13.1600034Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-19 19:53:13 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-19T19:53:13.1660130Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-19 19:53:13 [model.py:1561] Using max model len 163840
2026-02-19T19:53:13.1664321Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-19 19:53:13 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-19T19:53:13.1665334Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-19 19:53:13 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-19T19:53:13.1955222Z [0;36m(ApiServer_0 pid=191)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:53:13.1995381Z [0;36m(ApiServer_0 pid=191)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:53:13.2004838Z [0;36m(ApiServer_0 pid=191)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:53:13.2014202Z [0;36m(ApiServer_0 pid=191)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-19T19:53:13.2046941Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-19 19:53:13 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-19T19:53:13.2056102Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-19 19:53:13 [model.py:1561] Using max model len 8192
2026-02-19T19:53:13.2465778Z [0;36m(ApiServer_1 pid=192)[0;0m 2026-02-19 19:53:13,245 - 192 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:53:13.2619764Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-19 19:53:13 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:53:13.2786752Z [0;36m(ApiServer_1 pid=192)[0;0m 2026-02-19 19:53:13,277 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-19T19:53:13.2809434Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-19 19:53:13 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-19T19:53:13.2818753Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-19 19:53:13 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-19T19:53:13.2828291Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-19 19:53:13 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-19T19:53:13.2838809Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-19 19:53:13 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-19T19:53:13.2847613Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-19 19:53:13 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:53:13.2857084Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-19 19:53:13 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:53:13.2867181Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-19 19:53:13 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:53:13.2876988Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-19 19:53:13 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-19T19:53:13.2886803Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-19 19:53:13 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-19T19:53:13.2895409Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-19 19:53:13 [platform.py:335] [91m
2026-02-19T19:53:13.2904962Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             **********************************************************************************
2026-02-19T19:53:13.2914701Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-19T19:53:13.2924535Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-19T19:53:13.2934518Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-19T19:53:13.2944024Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-19T19:53:13.2953779Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-19T19:53:13.2963467Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * batch size for graph capture.
2026-02-19T19:53:13.2974112Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * For more details, please refer to:
2026-02-19T19:53:13.2983347Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-19T19:53:13.2992935Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             **********************************************************************************[0m
2026-02-19T19:53:13.3003798Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             
2026-02-19T19:53:13.3012458Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-19 19:53:13 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-19T19:53:13.3022865Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-19 19:53:13 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-19T19:53:13.3136900Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-19 19:53:13 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-19T19:53:13.3157123Z [0;36m(ApiServer_0 pid=191)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:53:13.3167528Z [0;36m(ApiServer_0 pid=191)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:53:13.3177091Z [0;36m(ApiServer_0 pid=191)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-19T19:53:13.3217609Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-19 19:53:13 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-19T19:53:13.3239240Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-19 19:53:13 [model.py:1561] Using max model len 163840
2026-02-19T19:53:13.3250228Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-19 19:53:13 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-19T19:53:13.3258962Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-19 19:53:13 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-19T19:53:13.3839730Z [0;36m(ApiServer_1 pid=192)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:53:13.3864997Z [0;36m(ApiServer_1 pid=192)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:53:13.3874289Z [0;36m(ApiServer_1 pid=192)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:53:13.3890599Z [0;36m(ApiServer_1 pid=192)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-19T19:53:13.3967606Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-19 19:53:13 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-19T19:53:13.4018736Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-19 19:53:13 [model.py:1561] Using max model len 8192
2026-02-19T19:53:13.4491159Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-19 19:53:13 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-19T19:53:13.4499461Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-19 19:53:13 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-19T19:53:13.4514613Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-19 19:53:13 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-19T19:53:13.4524295Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-19 19:53:13 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-19T19:53:13.4533449Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-19 19:53:13 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:53:13.4543552Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-19 19:53:13 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:53:13.4553744Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-19 19:53:13 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:53:13.4563880Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-19 19:53:13 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-19T19:53:13.4573424Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-19 19:53:13 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-19T19:53:13.4582718Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-19 19:53:13 [platform.py:335] [91m
2026-02-19T19:53:13.4592634Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             **********************************************************************************
2026-02-19T19:53:13.4602071Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-19T19:53:13.4611763Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-19T19:53:13.4622256Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-19T19:53:13.4631804Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-19T19:53:13.4642081Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-19T19:53:13.4651427Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * batch size for graph capture.
2026-02-19T19:53:13.4661160Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * For more details, please refer to:
2026-02-19T19:53:13.4670624Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-19T19:53:13.4680158Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             **********************************************************************************[0m
2026-02-19T19:53:13.4689501Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             
2026-02-19T19:53:13.4698990Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-19 19:53:13 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-19T19:53:13.5171206Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-19 19:53:13 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-19T19:53:13.5194089Z [0;36m(ApiServer_1 pid=192)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:53:13.5203238Z [0;36m(ApiServer_1 pid=192)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-19T19:53:13.5213270Z [0;36m(ApiServer_1 pid=192)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-19T19:53:13.5272918Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-19 19:53:13 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-19T19:53:13.5295136Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-19 19:53:13 [model.py:1561] Using max model len 163840
2026-02-19T19:53:13.5304462Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-19 19:53:13 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-19T19:53:13.5314010Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-19 19:53:13 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-19T19:53:13.6504370Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-19 19:53:13 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-19T19:53:13.6512980Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-19 19:53:13 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-19T19:53:13.6522641Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-19 19:53:13 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-19T19:53:13.6532151Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-19 19:53:13 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-19T19:53:13.6541206Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-19 19:53:13 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:53:13.6549955Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-19 19:53:13 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:53:13.6560584Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-19 19:53:13 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:53:13.6570126Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-19 19:53:13 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-19T19:53:13.6579921Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-19 19:53:13 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-19T19:53:13.6589132Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-19 19:53:13 [platform.py:335] [91m
2026-02-19T19:53:13.6598752Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             **********************************************************************************
2026-02-19T19:53:13.6608588Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-19T19:53:13.6624751Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-19T19:53:13.6637866Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-19T19:53:13.6649241Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-19T19:53:13.6658803Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-19T19:53:13.6668568Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * batch size for graph capture.
2026-02-19T19:53:13.6679239Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * For more details, please refer to:
2026-02-19T19:53:13.6689421Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-19T19:53:13.6699211Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             **********************************************************************************[0m
2026-02-19T19:53:13.6708463Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-19 19:53:13 [platform.py:335]             
2026-02-19T19:53:13.6718237Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-19 19:53:13 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-19T19:53:16.0076591Z INFO 02-19 19:53:16 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:53:16.0085424Z INFO 02-19 19:53:16 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:53:16.0095895Z INFO 02-19 19:53:16 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:53:16.0144636Z INFO 02-19 19:53:16 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:53:16.0896369Z INFO 02-19 19:53:16 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:53:16.0903933Z INFO 02-19 19:53:16 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:53:16.0916933Z INFO 02-19 19:53:16 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:53:16.0971662Z INFO 02-19 19:53:16 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:53:21.2412395Z 2026-02-19 19:53:21,239 - 240 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:53:21.2448415Z INFO 02-19 19:53:21 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:53:21.3144879Z 2026-02-19 19:53:21,313 - 241 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:53:21.3204811Z INFO 02-19 19:53:21 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:53:23.2566089Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-19T19:53:23.2573389Z   warnings.warn(
2026-02-19T19:53:23.2584413Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-19T19:53:23.2593867Z   warnings.warn(
2026-02-19T19:53:25.8928596Z INFO 02-19 19:53:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:53:25.8941966Z INFO 02-19 19:53:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:53:25.8953581Z INFO 02-19 19:53:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:53:25.9001602Z INFO 02-19 19:53:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:53:25.9849782Z INFO 02-19 19:53:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:53:25.9859971Z INFO 02-19 19:53:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:53:25.9892192Z INFO 02-19 19:53:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:53:25.9934530Z INFO 02-19 19:53:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:53:26.8695662Z INFO 02-19 19:53:26 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:53:26.8704892Z INFO 02-19 19:53:26 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:53:26.8718052Z INFO 02-19 19:53:26 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:53:26.8727986Z INFO 02-19 19:53:26 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:53:26.8737567Z INFO 02-19 19:53:26 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:53:26.8747744Z INFO 02-19 19:53:26 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:53:26.9266933Z INFO 02-19 19:53:26 [parallel_state.py:1212] world_size=32 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.219:51283 backend=hccl
2026-02-19T19:53:26.9281802Z INFO 02-19 19:53:26 [parallel_state.py:1212] world_size=32 rank=8 local_rank=0 distributed_init_method=tcp://10.0.0.219:51283 backend=hccl
2026-02-19T19:53:31.1308585Z 2026-02-19 19:53:31,125 - 282 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:53:31.1309769Z INFO 02-19 19:53:31 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:53:31.2913836Z 2026-02-19 19:53:31,289 - 283 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:53:31.2948118Z INFO 02-19 19:53:31 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:53:32.5528494Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-19T19:53:32.5536185Z   warnings.warn(
2026-02-19T19:53:32.6384794Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-19T19:53:32.6440608Z   warnings.warn(
2026-02-19T19:53:34.6679990Z INFO 02-19 19:53:34 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:53:34.6687520Z INFO 02-19 19:53:34 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:53:34.6698658Z INFO 02-19 19:53:34 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:53:34.7589452Z INFO 02-19 19:53:34 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:53:34.7597585Z INFO 02-19 19:53:34 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:53:34.7608302Z INFO 02-19 19:53:34 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:53:35.0907896Z INFO 02-19 19:53:35 [parallel_state.py:1212] world_size=32 rank=1 local_rank=1 distributed_init_method=tcp://10.0.0.219:51283 backend=hccl
2026-02-19T19:53:35.1911788Z INFO 02-19 19:53:35 [parallel_state.py:1212] world_size=32 rank=9 local_rank=1 distributed_init_method=tcp://10.0.0.219:51283 backend=hccl
2026-02-19T19:53:36.0902068Z INFO 02-19 19:53:36 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:53:36.0909935Z INFO 02-19 19:53:36 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:53:36.0919363Z INFO 02-19 19:53:36 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:53:36.0974503Z INFO 02-19 19:53:36 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:53:36.1378794Z INFO 02-19 19:53:36 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:53:36.1386965Z INFO 02-19 19:53:36 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:53:36.1395910Z INFO 02-19 19:53:36 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:53:36.1530366Z INFO 02-19 19:53:36 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:53:41.3815342Z 2026-02-19 19:53:41,379 - 379 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:53:41.3850865Z INFO 02-19 19:53:41 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:53:41.4883878Z 2026-02-19 19:53:41,486 - 376 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:53:41.4926818Z INFO 02-19 19:53:41 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:53:42.6989722Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-19T19:53:42.7001432Z   warnings.warn(
2026-02-19T19:53:42.8366893Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-19T19:53:42.8373478Z   warnings.warn(
2026-02-19T19:53:44.8026588Z INFO 02-19 19:53:44 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:53:44.8038292Z INFO 02-19 19:53:44 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:53:44.8050173Z INFO 02-19 19:53:44 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:53:44.9367294Z INFO 02-19 19:53:44 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:53:44.9375176Z INFO 02-19 19:53:44 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:53:44.9385768Z INFO 02-19 19:53:44 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:53:45.2405013Z INFO 02-19 19:53:45 [parallel_state.py:1212] world_size=32 rank=10 local_rank=2 distributed_init_method=tcp://10.0.0.219:51283 backend=hccl
2026-02-19T19:53:45.3777365Z INFO 02-19 19:53:45 [parallel_state.py:1212] world_size=32 rank=2 local_rank=2 distributed_init_method=tcp://10.0.0.219:51283 backend=hccl
2026-02-19T19:53:45.9974971Z INFO 02-19 19:53:45 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:53:45.9982926Z INFO 02-19 19:53:45 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:53:45.9995555Z INFO 02-19 19:53:45 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:53:46.0047094Z INFO 02-19 19:53:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:53:46.1535912Z INFO 02-19 19:53:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:53:46.1544141Z INFO 02-19 19:53:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:53:46.1554243Z INFO 02-19 19:53:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:53:46.1583823Z INFO 02-19 19:53:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:53:51.1462196Z 2026-02-19 19:53:51,144 - 480 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:53:51.1493811Z INFO 02-19 19:53:51 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:53:51.2897059Z 2026-02-19 19:53:51,288 - 482 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:53:51.2933295Z INFO 02-19 19:53:51 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:53:52.4504551Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-19T19:53:52.4513481Z   warnings.warn(
2026-02-19T19:53:52.6829395Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-19T19:53:52.6839129Z   warnings.warn(
2026-02-19T19:53:54.5605634Z INFO 02-19 19:53:54 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:53:54.5633892Z INFO 02-19 19:53:54 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:53:54.5634712Z INFO 02-19 19:53:54 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:53:54.7631467Z INFO 02-19 19:53:54 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:53:54.7639304Z INFO 02-19 19:53:54 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:53:54.7649462Z INFO 02-19 19:53:54 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:53:54.9857503Z INFO 02-19 19:53:54 [parallel_state.py:1212] world_size=32 rank=11 local_rank=3 distributed_init_method=tcp://10.0.0.219:51283 backend=hccl
2026-02-19T19:53:55.1913500Z INFO 02-19 19:53:55 [parallel_state.py:1212] world_size=32 rank=3 local_rank=3 distributed_init_method=tcp://10.0.0.219:51283 backend=hccl
2026-02-19T19:53:55.6684494Z INFO 02-19 19:53:55 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:53:55.6693348Z INFO 02-19 19:53:55 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:53:55.6703741Z INFO 02-19 19:53:55 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:53:55.6759560Z INFO 02-19 19:53:55 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:53:55.8610749Z INFO 02-19 19:53:55 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:53:55.8618961Z INFO 02-19 19:53:55 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:53:55.8627242Z INFO 02-19 19:53:55 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:53:55.8685904Z INFO 02-19 19:53:55 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:54:00.7960110Z 2026-02-19 19:54:00,794 - 584 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:54:00.8012620Z INFO 02-19 19:54:00 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:54:00.9414047Z 2026-02-19 19:54:00,940 - 587 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:54:00.9449632Z INFO 02-19 19:54:00 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:54:02.1609801Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-19T19:54:02.1615865Z   warnings.warn(
2026-02-19T19:54:02.2936751Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-19T19:54:02.2943314Z   warnings.warn(
2026-02-19T19:54:04.2663171Z INFO 02-19 19:54:04 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:54:04.2670124Z INFO 02-19 19:54:04 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:54:04.2681656Z INFO 02-19 19:54:04 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:54:04.3628619Z INFO 02-19 19:54:04 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:54:04.3635537Z INFO 02-19 19:54:04 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:54:04.3645992Z INFO 02-19 19:54:04 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:54:04.7051337Z INFO 02-19 19:54:04 [parallel_state.py:1212] world_size=32 rank=12 local_rank=4 distributed_init_method=tcp://10.0.0.219:51283 backend=hccl
2026-02-19T19:54:04.8006509Z INFO 02-19 19:54:04 [parallel_state.py:1212] world_size=32 rank=4 local_rank=4 distributed_init_method=tcp://10.0.0.219:51283 backend=hccl
2026-02-19T19:54:05.6252520Z INFO 02-19 19:54:05 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:54:05.6260931Z INFO 02-19 19:54:05 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:54:05.6271139Z INFO 02-19 19:54:05 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:54:05.6324101Z INFO 02-19 19:54:05 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:54:05.6898439Z INFO 02-19 19:54:05 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:54:05.6907380Z INFO 02-19 19:54:05 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:54:05.6916220Z INFO 02-19 19:54:05 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:54:05.6976026Z INFO 02-19 19:54:05 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:54:10.9537844Z 2026-02-19 19:54:10,952 - 688 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:54:10.9572252Z INFO 02-19 19:54:10 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:54:10.9717378Z 2026-02-19 19:54:10,970 - 691 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:54:10.9753566Z INFO 02-19 19:54:10 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:54:12.2795997Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-19T19:54:12.2803192Z   warnings.warn(
2026-02-19T19:54:12.2943810Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-19T19:54:12.2955203Z   warnings.warn(
2026-02-19T19:54:14.4316580Z INFO 02-19 19:54:14 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:54:14.4326252Z INFO 02-19 19:54:14 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:54:14.4338066Z INFO 02-19 19:54:14 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:54:14.4371907Z INFO 02-19 19:54:14 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:54:14.4386007Z INFO 02-19 19:54:14 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:54:14.4396291Z INFO 02-19 19:54:14 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:54:14.8559618Z INFO 02-19 19:54:14 [parallel_state.py:1212] world_size=32 rank=5 local_rank=5 distributed_init_method=tcp://10.0.0.219:51283 backend=hccl
2026-02-19T19:54:14.8647578Z INFO 02-19 19:54:14 [parallel_state.py:1212] world_size=32 rank=13 local_rank=5 distributed_init_method=tcp://10.0.0.219:51283 backend=hccl
2026-02-19T19:54:15.5869710Z INFO 02-19 19:54:15 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:54:15.5871176Z INFO 02-19 19:54:15 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:54:15.5880786Z INFO 02-19 19:54:15 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:54:15.5947264Z INFO 02-19 19:54:15 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:54:15.7382074Z INFO 02-19 19:54:15 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:54:15.7391417Z INFO 02-19 19:54:15 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:54:15.7404390Z INFO 02-19 19:54:15 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:54:15.7456655Z INFO 02-19 19:54:15 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:54:20.5948576Z 2026-02-19 19:54:20,593 - 792 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:54:20.6003923Z INFO 02-19 19:54:20 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:54:20.8899862Z 2026-02-19 19:54:20,888 - 793 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:54:20.8933925Z INFO 02-19 19:54:20 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:54:21.9049474Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-19T19:54:21.9063657Z   warnings.warn(
2026-02-19T19:54:22.1939789Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-19T19:54:22.1952730Z   warnings.warn(
2026-02-19T19:54:24.0459261Z INFO 02-19 19:54:24 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:54:24.0469612Z INFO 02-19 19:54:24 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:54:24.0480943Z INFO 02-19 19:54:24 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:54:24.3677076Z INFO 02-19 19:54:24 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:54:24.3683913Z INFO 02-19 19:54:24 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:54:24.3693784Z INFO 02-19 19:54:24 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:54:24.4825352Z INFO 02-19 19:54:24 [parallel_state.py:1212] world_size=32 rank=14 local_rank=6 distributed_init_method=tcp://10.0.0.219:51283 backend=hccl
2026-02-19T19:54:24.8106475Z INFO 02-19 19:54:24 [parallel_state.py:1212] world_size=32 rank=6 local_rank=6 distributed_init_method=tcp://10.0.0.219:51283 backend=hccl
2026-02-19T19:54:25.1886017Z INFO 02-19 19:54:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:54:25.1894594Z INFO 02-19 19:54:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:54:25.1905005Z INFO 02-19 19:54:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:54:25.1964981Z INFO 02-19 19:54:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:54:25.4394395Z INFO 02-19 19:54:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-19T19:54:25.4404009Z INFO 02-19 19:54:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-19T19:54:25.4413236Z INFO 02-19 19:54:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-19T19:54:25.4465650Z INFO 02-19 19:54:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-19T19:54:30.3436294Z 2026-02-19 19:54:30,341 - 896 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:54:30.3466259Z INFO 02-19 19:54:30 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:54:30.5875795Z 2026-02-19 19:54:30,584 - 899 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-19T19:54:30.5895137Z INFO 02-19 19:54:30 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-19T19:54:31.6347712Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-19T19:54:31.6353930Z   warnings.warn(
2026-02-19T19:54:31.8881167Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-19T19:54:31.8888575Z   warnings.warn(
2026-02-19T19:54:33.7168020Z INFO 02-19 19:54:33 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:54:33.7176261Z INFO 02-19 19:54:33 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:54:33.7187829Z INFO 02-19 19:54:33 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:54:33.9048285Z INFO 02-19 19:54:33 [ascend_config.py:412] Dynamic EPLB is False
2026-02-19T19:54:33.9055879Z INFO 02-19 19:54:33 [ascend_config.py:413] The number of redundant experts is 0
2026-02-19T19:54:33.9066041Z INFO 02-19 19:54:33 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-19T19:54:34.1275238Z INFO 02-19 19:54:34 [parallel_state.py:1212] world_size=32 rank=15 local_rank=7 distributed_init_method=tcp://10.0.0.219:51283 backend=hccl
2026-02-19T19:54:34.3286755Z INFO 02-19 19:54:34 [parallel_state.py:1212] world_size=32 rank=7 local_rank=7 distributed_init_method=tcp://10.0.0.219:51283 backend=hccl
2026-02-19T19:54:34.3697379Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.3705838Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.3728694Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.4297148Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.4305686Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.4315263Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.4325287Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.4335728Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.4345558Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.4355230Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.4365169Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.4374263Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.4383558Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.4392975Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.4403088Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.4415179Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.5603741Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-19T19:54:34.5617339Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-19T19:54:34.5627310Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-19T19:54:34.5637504Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-19T19:54:34.5647176Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-19T19:54:34.5656394Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-19T19:54:34.5665427Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-19T19:54:34.5675195Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-19T19:54:34.5686374Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-19T19:54:34.5695388Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-19T19:54:34.5705363Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-19T19:54:34.5715323Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-19T19:54:34.5724812Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-19T19:54:34.5734068Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-19T19:54:34.5743761Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-19T19:54:34.5753907Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-19T19:54:34.5765238Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5775265Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5785111Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5794859Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5805183Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5814364Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5824435Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5834296Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5844811Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5853920Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5864477Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5874010Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5884604Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5894789Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5904450Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5914312Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5924896Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5934378Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5943329Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5953061Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5963041Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5972716Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5982995Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.5991717Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6522605Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6541212Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6552713Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6562922Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6571567Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6581371Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6590848Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6601144Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6611123Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6620155Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6630115Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6640597Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6649796Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6659125Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6668799Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6678407Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6687837Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6697010Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6706474Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6716332Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6725871Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6735069Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6743775Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6753142Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-19T19:54:34.6764031Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.6774261Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.6783868Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.6794149Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.6804392Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.6813982Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.6822885Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.6832666Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.6842246Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.6851510Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.6861095Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.6871289Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.6880996Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.6890526Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.6899897Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.6909915Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.7038665Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.7057017Z INFO 02-19 19:54:34 [parallel_state.py:1423] rank 0 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
2026-02-19T19:54:34.7655260Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.7679875Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.7688732Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.7697754Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.7706559Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.7716396Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.7725238Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.7734900Z INFO 02-19 19:54:34 [parallel_state.py:1423] rank 3 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
2026-02-19T19:54:34.7743753Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.7753101Z INFO 02-19 19:54:34 [parallel_state.py:1423] rank 7 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 7, EP rank 7
2026-02-19T19:54:34.7763424Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.7774259Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.7785604Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.7795550Z INFO 02-19 19:54:34 [parallel_state.py:1423] rank 4 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 4, EP rank 4
2026-02-19T19:54:34.7806302Z INFO 02-19 19:54:34 [parallel_state.py:1423] rank 6 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 6, EP rank 6
2026-02-19T19:54:34.7816168Z INFO 02-19 19:54:34 [parallel_state.py:1423] rank 5 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 5, EP rank 5
2026-02-19T19:54:34.7825720Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.7836481Z INFO 02-19 19:54:34 [parallel_state.py:1423] rank 11 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 3, EP rank 11
2026-02-19T19:54:34.7845631Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.7855528Z INFO 02-19 19:54:34 [parallel_state.py:1423] rank 13 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 5, EP rank 13
2026-02-19T19:54:34.7864553Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.7874629Z INFO 02-19 19:54:34 [parallel_state.py:1423] rank 1 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
2026-02-19T19:54:34.7884922Z INFO 02-19 19:54:34 [parallel_state.py:1423] rank 9 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 1, EP rank 9
2026-02-19T19:54:34.7894964Z INFO 02-19 19:54:34 [parallel_state.py:1423] rank 10 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 2, EP rank 10
2026-02-19T19:54:34.7904694Z INFO 02-19 19:54:34 [parallel_state.py:1423] rank 12 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 4, EP rank 12
2026-02-19T19:54:34.7914382Z INFO 02-19 19:54:34 [parallel_state.py:1423] rank 8 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 0, EP rank 8
2026-02-19T19:54:34.7925563Z INFO 02-19 19:54:34 [parallel_state.py:1423] rank 15 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 7, EP rank 15
2026-02-19T19:54:34.7935097Z INFO 02-19 19:54:34 [parallel_state.py:1423] rank 14 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 6, EP rank 14
2026-02-19T19:54:34.8416611Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.8439439Z INFO 02-19 19:54:34 [parallel_state.py:1423] rank 2 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
2026-02-19T19:54:34.8796382Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.8806239Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.8824089Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.8833166Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.8842892Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.8852428Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.8861857Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.8871306Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.8880784Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.8890326Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.8899765Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.8910073Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.8922462Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.8931447Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.8941189Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.8950867Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-19T19:54:34.8961019Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.8971955Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.8980257Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.8989524Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.8999286Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.9010173Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.9018986Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.9028676Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.9039148Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.9049145Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.9057456Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.9065764Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.9074796Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.9084550Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.9093614Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.9103149Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-19T19:54:34.9973563Z WARNING 02-19 19:54:34 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:34.9996492Z WARNING 02-19 19:54:34 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0005901Z WARNING 02-19 19:54:34 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0015691Z WARNING 02-19 19:54:34 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0025269Z WARNING 02-19 19:54:34 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0035329Z WARNING 02-19 19:54:34 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0047020Z WARNING 02-19 19:54:34 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0057005Z WARNING 02-19 19:54:34 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0068182Z WARNING 02-19 19:54:34 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0080482Z WARNING 02-19 19:54:34 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0086624Z WARNING 02-19 19:54:34 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0096259Z WARNING 02-19 19:54:34 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0105863Z WARNING 02-19 19:54:34 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0115861Z WARNING 02-19 19:54:34 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0126155Z WARNING 02-19 19:54:35 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0136549Z WARNING 02-19 19:54:35 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0497166Z WARNING 02-19 19:54:35 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0506318Z WARNING 02-19 19:54:35 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0516336Z WARNING 02-19 19:54:35 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0526474Z WARNING 02-19 19:54:35 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0536089Z WARNING 02-19 19:54:35 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0545169Z WARNING 02-19 19:54:35 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0554825Z WARNING 02-19 19:54:35 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0565102Z WARNING 02-19 19:54:35 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0575171Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m INFO 02-19 19:54:35 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-19T19:54:35.0584589Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m INFO 02-19 19:54:35 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-19T19:54:35.0594203Z WARNING 02-19 19:54:35 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0604289Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m INFO 02-19 19:54:35 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-19T19:54:35.0613919Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m INFO 02-19 19:54:35 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-19T19:54:35.0623184Z WARNING 02-19 19:54:35 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0631970Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m INFO 02-19 19:54:35 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-19T19:54:35.0642133Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m INFO 02-19 19:54:35 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-19T19:54:35.0651542Z WARNING 02-19 19:54:35 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0660986Z WARNING 02-19 19:54:35 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0670574Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m INFO 02-19 19:54:35 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-19T19:54:35.0680821Z WARNING 02-19 19:54:35 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0689924Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m INFO 02-19 19:54:35 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-19T19:54:35.0699039Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m INFO 02-19 19:54:35 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-19T19:54:35.0707769Z WARNING 02-19 19:54:35 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0717759Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m INFO 02-19 19:54:35 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-19T19:54:35.0727409Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m INFO 02-19 19:54:35 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-19T19:54:35.0736303Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m INFO 02-19 19:54:35 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-19T19:54:35.0812189Z WARNING 02-19 19:54:35 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0832097Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m INFO 02-19 19:54:35 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-19T19:54:35.0852923Z WARNING 02-19 19:54:35 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-19T19:54:35.0872563Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m INFO 02-19 19:54:35 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-19T19:54:35.0899363Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m INFO 02-19 19:54:35 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-19T19:54:35.1150081Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m INFO 02-19 19:54:35 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-19T19:54:35.4706332Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m [2026-02-19 19:54:35] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-19T19:54:35.4727304Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m [2026-02-19 19:54:35] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-19T19:54:35.4736433Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m [2026-02-19 19:54:35] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-19T19:54:35.4861420Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m [2026-02-19 19:54:35] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-19T19:54:35.5063775Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m [2026-02-19 19:54:35] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-19T19:54:35.5074964Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m [2026-02-19 19:54:35] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-19T19:54:35.5342367Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m [2026-02-19 19:54:35] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-19T19:54:35.5463236Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m [2026-02-19 19:54:35] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-19T19:54:35.5760030Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m [2026-02-19 19:54:35] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-19T19:54:35.6637682Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m [2026-02-19 19:54:35] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-19T19:54:35.6665809Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m INFO 02-19 19:54:35 [layer.py:475] [EP Rank 12/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-19T19:54:35.6675702Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m INFO 02-19 19:54:35 [layer.py:475] [EP Rank 1/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-19T19:54:35.6686527Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m INFO 02-19 19:54:35 [layer.py:475] [EP Rank 15/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-19T19:54:35.6696894Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m INFO 02-19 19:54:35 [layer.py:475] [EP Rank 7/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-19T19:54:35.6707270Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m INFO 02-19 19:54:35 [layer.py:475] [EP Rank 5/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-19T19:54:35.6718104Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m INFO 02-19 19:54:35 [layer.py:475] [EP Rank 14/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-19T19:54:35.6727474Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m INFO 02-19 19:54:35 [layer.py:475] [EP Rank 11/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-19T19:54:35.6736685Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m [2026-02-19 19:54:35] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-19T19:54:35.7043306Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m INFO 02-19 19:54:35 [layer.py:475] [EP Rank 8/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-19T19:54:35.7367851Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m INFO 02-19 19:54:35 [layer.py:475] [EP Rank 0/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-19T19:54:35.7718910Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m INFO 02-19 19:54:35 [layer.py:475] [EP Rank 3/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-19T19:54:35.7809882Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m INFO 02-19 19:54:35 [layer.py:475] [EP Rank 4/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-19T19:54:35.7890655Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m [2026-02-19 19:54:35] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-19T19:54:35.8730045Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m [2026-02-19 19:54:35] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-19T19:54:35.8760619Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m [2026-02-19 19:54:35] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-19T19:54:35.8924010Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m INFO 02-19 19:54:35 [layer.py:475] [EP Rank 10/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-19T19:54:35.9857152Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m INFO 02-19 19:54:35 [layer.py:475] [EP Rank 13/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-19T19:54:35.9956714Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m INFO 02-19 19:54:35 [layer.py:475] [EP Rank 9/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-19T19:54:36.0298247Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m [2026-02-19 19:54:36] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-19T19:54:36.1534732Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m INFO 02-19 19:54:36 [layer.py:475] [EP Rank 6/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-19T19:54:36.1985409Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m [2026-02-19 19:54:36] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-19T19:54:36.3061962Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m INFO 02-19 19:54:36 [layer.py:475] [EP Rank 2/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-19T19:54:36.9385375Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-19T19:54:36.9391882Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m   return func(*args, **kwargs)
2026-02-19T19:54:36.9403278Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-19T19:54:36.9411700Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m   return func(*args, **kwargs)
2026-02-19T19:54:36.9421310Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-19T19:54:36.9430231Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m   return func(*args, **kwargs)
2026-02-19T19:54:36.9441003Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-19T19:54:36.9448832Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m   return func(*args, **kwargs)
2026-02-19T19:54:36.9505803Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-19T19:54:36.9513709Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m   return func(*args, **kwargs)
2026-02-19T19:54:36.9525500Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-19T19:54:36.9533308Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m   return func(*args, **kwargs)
2026-02-19T19:54:36.9594002Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-19T19:54:36.9602818Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m   return func(*args, **kwargs)
2026-02-19T19:54:36.9613340Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-19T19:54:36.9621441Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m   return func(*args, **kwargs)
2026-02-19T19:54:37.0148390Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-19T19:54:37.0155387Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m   return func(*args, **kwargs)
2026-02-19T19:54:37.0201692Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m INFO 02-19 19:54:37 [fused_moe.py:207] [EP Rank 11/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-19T19:54:37.0210470Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m INFO 02-19 19:54:37 [fused_moe.py:207] [EP Rank 15/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-19T19:54:37.0219944Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m INFO 02-19 19:54:37 [fused_moe.py:207] [EP Rank 13/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-19T19:54:37.0230278Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-19T19:54:37.0238852Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m   return func(*args, **kwargs)
2026-02-19T19:54:37.0248696Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m INFO 02-19 19:54:37 [fused_moe.py:207] [EP Rank 1/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-19T19:54:37.0258566Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m INFO 02-19 19:54:37 [fused_moe.py:207] [EP Rank 9/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-19T19:54:37.0267493Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m INFO 02-19 19:54:37 [fused_moe.py:207] [EP Rank 3/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-19T19:54:37.0287817Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m INFO 02-19 19:54:37 [fused_moe.py:207] [EP Rank 4/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-19T19:54:37.0518497Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-19T19:54:37.0526223Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m   return func(*args, **kwargs)
2026-02-19T19:54:37.0536507Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m INFO 02-19 19:54:37 [fused_moe.py:207] [EP Rank 7/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-19T19:54:37.0741109Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m INFO 02-19 19:54:37 [fused_moe.py:207] [EP Rank 10/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-19T19:54:37.0808189Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-19T19:54:37.0816206Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m   return func(*args, **kwargs)
2026-02-19T19:54:37.0886921Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m INFO 02-19 19:54:37 [fused_moe.py:207] [EP Rank 2/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-19T19:54:37.0913547Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-19T19:54:37.0922150Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m   return func(*args, **kwargs)
2026-02-19T19:54:37.1124905Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m INFO 02-19 19:54:37 [fused_moe.py:207] [EP Rank 6/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-19T19:54:37.1209183Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-19T19:54:37.1217787Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m   return func(*args, **kwargs)
2026-02-19T19:54:37.1371213Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m INFO 02-19 19:54:37 [fused_moe.py:207] [EP Rank 8/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-19T19:54:37.1394843Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-19T19:54:37.1403279Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m   return func(*args, **kwargs)
2026-02-19T19:54:37.1462101Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m INFO 02-19 19:54:37 [fused_moe.py:207] [EP Rank 0/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-19T19:54:37.1488887Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-19T19:54:37.1497252Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m   return func(*args, **kwargs)
2026-02-19T19:54:37.1807550Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m INFO 02-19 19:54:37 [fused_moe.py:207] [EP Rank 12/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-19T19:54:37.1978898Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m INFO 02-19 19:54:37 [fused_moe.py:207] [EP Rank 14/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-19T19:54:37.2063114Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m INFO 02-19 19:54:37 [fused_moe.py:207] [EP Rank 5/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-19T19:56:37.9459057Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-19T19:56:37.9642760Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-19T19:56:37.9653185Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-19T19:56:37.9663281Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-19T19:56:37.9673580Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:37.9683594Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-19T19:56:37.9693183Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-19T19:56:37.9702711Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 404, in load_model
2026-02-19T19:56:37.9712161Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-19T19:56:37.9722521Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2311, in load_model
2026-02-19T19:56:37.9731816Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-19T19:56:37.9774610Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:37.9775740Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-19T19:56:37.9776521Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     return loader.load_model(
2026-02-19T19:56:37.9786009Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:37.9796756Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-19T19:56:37.9805558Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     model = initialize_model(
2026-02-19T19:56:37.9815528Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-19T19:56:37.9826403Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-19T19:56:37.9836264Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-19T19:56:37.9845932Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:37.9856405Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-19T19:56:37.9865428Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-19T19:56:37.9875717Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-19T19:56:37.9886035Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-19T19:56:37.9896243Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-19T19:56:37.9905701Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-19T19:56:37.9915074Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-19T19:56:37.9925865Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-19T19:56:37.9936635Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-19T19:56:37.9949213Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     + [
2026-02-19T19:56:37.9960498Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]       ^
2026-02-19T19:56:37.9970282Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-19T19:56:37.9980104Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-19T19:56:37.9990438Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.0000013Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-19T19:56:38.0010636Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-19T19:56:38.0021381Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.0031618Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-19T19:56:38.0042437Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-19T19:56:38.0052325Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-19T19:56:38.0061977Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-19T19:56:38.0071076Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-19T19:56:38.0080917Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.0091497Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 401, in __init__
2026-02-19T19:56:38.0102565Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-19T19:56:38.0111142Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 238, in __init__
2026-02-19T19:56:38.0123208Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-19T19:56:38.0150368Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 50, in setup_moe_comm_method
2026-02-19T19:56:38.0151343Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-19T19:56:38.0152281Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.0160112Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 82, in __init__
2026-02-19T19:56:38.0169864Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-19T19:56:38.0179251Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.0189377Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 260, in _get_token_dispatcher
2026-02-19T19:56:38.0199300Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-19T19:56:38.0209261Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.0218823Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 415, in __init__
2026-02-19T19:56:38.0228117Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-19T19:56:38.0237919Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.0248490Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-19T19:56:38.0257825Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772] [ERROR] 2026-02-19-19:56:37 (PID:792, Device:6, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-19T19:56:38.0268705Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772] [PID: 792] 2026-02-19-19:56:37.149.889 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-19T19:56:38.0280247Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-19T19:56:38.0288639Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-19T19:56:38.0298791Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-19T19:56:38.0307580Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m ERROR 02-19 19:56:37 [multiproc_executor.py:772] 
2026-02-19T19:56:38.0318053Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m INFO 02-19 19:56:37 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-19T19:56:38.0327965Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-19T19:56:38.0337445Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-19T19:56:38.0347625Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-19T19:56:38.0357451Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-19T19:56:38.0367450Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.0377292Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-19T19:56:38.0386785Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-19T19:56:38.0396942Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 404, in load_model
2026-02-19T19:56:38.0407265Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-19T19:56:38.0416953Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2311, in load_model
2026-02-19T19:56:38.0426247Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-19T19:56:38.0435594Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.0446795Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-19T19:56:38.0456159Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return loader.load_model(
2026-02-19T19:56:38.0465001Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.0474752Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-19T19:56:38.0484285Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     model = initialize_model(
2026-02-19T19:56:38.0493862Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.0503426Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-19T19:56:38.0513499Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-19T19:56:38.0522736Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.0532744Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-19T19:56:38.0541689Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-19T19:56:38.0551874Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.0563698Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-19T19:56:38.0574323Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-19T19:56:38.0580939Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-19T19:56:38.0589965Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-19T19:56:38.0599540Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-19T19:56:38.0609279Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-19T19:56:38.0617696Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     + [
2026-02-19T19:56:38.0627265Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]       ^
2026-02-19T19:56:38.0638274Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-19T19:56:38.0647731Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-19T19:56:38.0657372Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.0675433Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-19T19:56:38.0677782Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-19T19:56:38.0686828Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.0696143Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-19T19:56:38.0705055Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-19T19:56:38.0714775Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-19T19:56:38.0725332Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-19T19:56:38.0736643Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-19T19:56:38.0747794Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.0759128Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 401, in __init__
2026-02-19T19:56:38.0769082Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-19T19:56:38.0779476Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 238, in __init__
2026-02-19T19:56:38.0789232Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-19T19:56:38.0800132Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 50, in setup_moe_comm_method
2026-02-19T19:56:38.0810263Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-19T19:56:38.0819557Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.0829686Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 82, in __init__
2026-02-19T19:56:38.0840181Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-19T19:56:38.0849669Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.0859849Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 260, in _get_token_dispatcher
2026-02-19T19:56:38.0868667Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-19T19:56:38.0878354Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.0889011Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 415, in __init__
2026-02-19T19:56:38.0898355Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-19T19:56:38.0907902Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.0917927Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-19T19:56:38.0927684Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [ERROR] 2026-02-19-19:56:37 (PID:899, Device:7, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-19T19:56:38.0937960Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [PID: 899] 2026-02-19-19:56:37.150.159 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-19T19:56:38.0948596Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-19T19:56:38.0958113Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-19T19:56:38.0969031Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-19T19:56:38.0977897Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] 
2026-02-19T19:56:38.0986867Z [0;36m(Worker_DP0_TP7_EP7 pid=899)[0;0m INFO 02-19 19:56:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-19T19:56:38.0996603Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m INFO 02-19 19:56:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-19T19:56:38.1006028Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-19T19:56:38.1015946Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-19T19:56:38.1026117Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-19T19:56:38.1036591Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-19T19:56:38.1045484Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.1055296Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-19T19:56:38.1065234Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-19T19:56:38.1073790Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 404, in load_model
2026-02-19T19:56:38.1083522Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-19T19:56:38.1093712Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2311, in load_model
2026-02-19T19:56:38.1102603Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-19T19:56:38.1112252Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.1122262Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-19T19:56:38.1131040Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return loader.load_model(
2026-02-19T19:56:38.1162209Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.1162984Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-19T19:56:38.1163755Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     model = initialize_model(
2026-02-19T19:56:38.1167694Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.1177478Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-19T19:56:38.1186836Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-19T19:56:38.1195550Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.1205935Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-19T19:56:38.1215171Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-19T19:56:38.1224362Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.1233899Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-19T19:56:38.1243798Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-19T19:56:38.1253518Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-19T19:56:38.1263283Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-19T19:56:38.1272715Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-19T19:56:38.1282078Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-19T19:56:38.1291417Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     + [
2026-02-19T19:56:38.1299969Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]       ^
2026-02-19T19:56:38.1309786Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-19T19:56:38.1319678Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-19T19:56:38.1329616Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.1338769Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-19T19:56:38.1347611Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-19T19:56:38.1358126Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.1368073Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-19T19:56:38.1377322Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-19T19:56:38.1386450Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-19T19:56:38.1395959Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-19T19:56:38.1405752Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-19T19:56:38.1415372Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.1424940Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 401, in __init__
2026-02-19T19:56:38.1434082Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-19T19:56:38.1444798Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 238, in __init__
2026-02-19T19:56:38.1455000Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-19T19:56:38.1466797Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 50, in setup_moe_comm_method
2026-02-19T19:56:38.1480995Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-19T19:56:38.1495165Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.1507101Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 82, in __init__
2026-02-19T19:56:38.1517657Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-19T19:56:38.1528604Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.1539746Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 260, in _get_token_dispatcher
2026-02-19T19:56:38.1549160Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-19T19:56:38.1561941Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.1571959Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 415, in __init__
2026-02-19T19:56:38.1580497Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-19T19:56:38.1590716Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.1602440Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-19T19:56:38.1612673Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [ERROR] 2026-02-19-19:56:37 (PID:240, Device:0, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-19T19:56:38.1624440Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [PID: 240] 2026-02-19-19:56:37.149.948 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-19T19:56:38.1637473Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-19T19:56:38.1646800Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-19T19:56:38.1657933Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-19T19:56:38.1667131Z [0;36m(Worker_DP0_TP0_EP0 pid=240)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] 
2026-02-19T19:56:38.1677585Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m INFO 02-19 19:56:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-19T19:56:38.1687767Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m INFO 02-19 19:56:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-19T19:56:38.1697285Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-19T19:56:38.1707264Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-19T19:56:38.1717651Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-19T19:56:38.1727544Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-19T19:56:38.1737468Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.1747476Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-19T19:56:38.1757424Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-19T19:56:38.1768047Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 404, in load_model
2026-02-19T19:56:38.1777729Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-19T19:56:38.1788049Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2311, in load_model
2026-02-19T19:56:38.1797929Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-19T19:56:38.1808038Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.1818248Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-19T19:56:38.1828131Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return loader.load_model(
2026-02-19T19:56:38.1838748Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.1849836Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-19T19:56:38.1859212Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     model = initialize_model(
2026-02-19T19:56:38.1869302Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.1880257Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-19T19:56:38.1892086Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-19T19:56:38.1902547Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.1911633Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-19T19:56:38.1921927Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-19T19:56:38.1931686Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.1942951Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-19T19:56:38.1951397Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-19T19:56:38.1962381Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-19T19:56:38.1973657Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-19T19:56:38.1993720Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-19T19:56:38.1994545Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-19T19:56:38.2003785Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     + [
2026-02-19T19:56:38.2013766Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]       ^
2026-02-19T19:56:38.2024573Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-19T19:56:38.2035299Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-19T19:56:38.2045919Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.2056454Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-19T19:56:38.2111631Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-19T19:56:38.2112388Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.2113223Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-19T19:56:38.2113964Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-19T19:56:38.2114579Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-19T19:56:38.2118702Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-19T19:56:38.2129660Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-19T19:56:38.2139464Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.2150524Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 401, in __init__
2026-02-19T19:56:38.2160347Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-19T19:56:38.2172348Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 238, in __init__
2026-02-19T19:56:38.2183804Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-19T19:56:38.2192653Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 50, in setup_moe_comm_method
2026-02-19T19:56:38.2203487Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-19T19:56:38.2213497Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.2224648Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 82, in __init__
2026-02-19T19:56:38.2234966Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-19T19:56:38.2244946Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.2256487Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 260, in _get_token_dispatcher
2026-02-19T19:56:38.2265318Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-19T19:56:38.2274962Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.2286031Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 415, in __init__
2026-02-19T19:56:38.2295771Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-19T19:56:38.2306359Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.2318219Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-19T19:56:38.2328626Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [ERROR] 2026-02-19-19:56:37 (PID:896, Device:7, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-19T19:56:38.2340698Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [PID: 896] 2026-02-19-19:56:37.150.749 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-19T19:56:38.2353808Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-19T19:56:38.2363450Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-19T19:56:38.2376611Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-19T19:56:38.2385677Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] 
2026-02-19T19:56:38.2395427Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-19T19:56:38.2405777Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-19T19:56:38.2416818Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-19T19:56:38.2427192Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-19T19:56:38.2437505Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.2448928Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-19T19:56:38.2458616Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-19T19:56:38.2469998Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 404, in load_model
2026-02-19T19:56:38.2479192Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-19T19:56:38.2489845Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2311, in load_model
2026-02-19T19:56:38.2499195Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-19T19:56:38.2509039Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.2520263Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-19T19:56:38.2530157Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return loader.load_model(
2026-02-19T19:56:38.2540492Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.2550026Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-19T19:56:38.2560235Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     model = initialize_model(
2026-02-19T19:56:38.2570551Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.2583241Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-19T19:56:38.2590944Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-19T19:56:38.2600939Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.2611134Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-19T19:56:38.2621486Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-19T19:56:38.2630865Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.2641718Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-19T19:56:38.2651576Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-19T19:56:38.2912357Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-19T19:56:38.2913946Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-19T19:56:38.2914683Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-19T19:56:38.2915449Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-19T19:56:38.2916148Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     + [
2026-02-19T19:56:38.2916617Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]       ^
2026-02-19T19:56:38.2917291Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-19T19:56:38.2918040Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-19T19:56:38.2918741Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.2919490Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-19T19:56:38.2920460Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-19T19:56:38.2921065Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.2921936Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-19T19:56:38.2922732Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-19T19:56:38.2923254Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-19T19:56:38.2923949Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-19T19:56:38.2924727Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-19T19:56:38.2925282Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.2926006Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 401, in __init__
2026-02-19T19:56:38.2926754Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-19T19:56:38.2927494Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 238, in __init__
2026-02-19T19:56:38.2928235Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-19T19:56:38.2929090Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 50, in setup_moe_comm_method
2026-02-19T19:56:38.2929984Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-19T19:56:38.2930668Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.2931476Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 82, in __init__
2026-02-19T19:56:38.2932368Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-19T19:56:38.2933059Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.2943163Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 260, in _get_token_dispatcher
2026-02-19T19:56:38.2952084Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-19T19:56:38.2961492Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.2970731Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 415, in __init__
2026-02-19T19:56:38.2980081Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-19T19:56:38.2990015Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.3000206Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-19T19:56:38.3009205Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [ERROR] 2026-02-19-19:56:37 (PID:584, Device:4, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-19T19:56:38.3019700Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [PID: 584] 2026-02-19-19:56:37.150.243 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-19T19:56:38.3030515Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-19T19:56:38.3039483Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-19T19:56:38.3052354Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-19T19:56:38.3063059Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] 
2026-02-19T19:56:38.3072620Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m INFO 02-19 19:56:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-19T19:56:38.3081895Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m INFO 02-19 19:56:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-19T19:56:38.3091268Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-19T19:56:38.3100222Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-19T19:56:38.3109596Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-19T19:56:38.3118867Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-19T19:56:38.3128181Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.3137374Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-19T19:56:38.3146428Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-19T19:56:38.3209751Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 404, in load_model
2026-02-19T19:56:38.3210564Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-19T19:56:38.3211301Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2311, in load_model
2026-02-19T19:56:38.3212290Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-19T19:56:38.3212930Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.3213679Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-19T19:56:38.3214460Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return loader.load_model(
2026-02-19T19:56:38.3220727Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.3230106Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-19T19:56:38.3239834Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     model = initialize_model(
2026-02-19T19:56:38.3249210Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.3258463Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-19T19:56:38.3267998Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-19T19:56:38.3277611Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.3287289Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-19T19:56:38.3296273Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-19T19:56:38.3305384Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.3314563Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-19T19:56:38.3324435Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-19T19:56:38.3333552Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-19T19:56:38.3343207Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-19T19:56:38.3352998Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-19T19:56:38.3362915Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-19T19:56:38.3372669Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     + [
2026-02-19T19:56:38.3381792Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]       ^
2026-02-19T19:56:38.3392664Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-19T19:56:38.3403405Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-19T19:56:38.3413830Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.3424124Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-19T19:56:38.3434080Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-19T19:56:38.3444482Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.3456008Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-19T19:56:38.3468513Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-19T19:56:38.3479281Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-19T19:56:38.3491226Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-19T19:56:38.3502510Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-19T19:56:38.3512807Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.3523514Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 401, in __init__
2026-02-19T19:56:38.3534090Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-19T19:56:38.3544913Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 238, in __init__
2026-02-19T19:56:38.3554814Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-19T19:56:38.3565053Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 50, in setup_moe_comm_method
2026-02-19T19:56:38.3574655Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-19T19:56:38.3584140Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.3593587Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 82, in __init__
2026-02-19T19:56:38.3603723Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-19T19:56:38.3612744Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.3622695Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 260, in _get_token_dispatcher
2026-02-19T19:56:38.3631855Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-19T19:56:38.3641541Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.3651607Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 415, in __init__
2026-02-19T19:56:38.3660856Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-19T19:56:38.3670092Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.3680788Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-19T19:56:38.3691034Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [ERROR] 2026-02-19-19:56:37 (PID:691, Device:5, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-19T19:56:38.3701268Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [PID: 691] 2026-02-19-19:56:37.150.584 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-19T19:56:38.3712640Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-19T19:56:38.3720998Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-19T19:56:38.3731839Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-19T19:56:38.3740045Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] 
2026-02-19T19:56:38.3750087Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m INFO 02-19 19:56:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-19T19:56:38.3760175Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m INFO 02-19 19:56:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-19T19:56:38.3769628Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-19T19:56:38.3779378Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-19T19:56:38.3789755Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-19T19:56:38.3799909Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-19T19:56:38.3809946Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.3821213Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-19T19:56:38.3830346Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-19T19:56:38.3841582Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 404, in load_model
2026-02-19T19:56:38.3851427Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-19T19:56:38.3862191Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2311, in load_model
2026-02-19T19:56:38.3872190Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-19T19:56:38.3883019Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.3893703Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-19T19:56:38.3903519Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return loader.load_model(
2026-02-19T19:56:38.3913570Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.3924413Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-19T19:56:38.3934236Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     model = initialize_model(
2026-02-19T19:56:38.3944585Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.3954841Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-19T19:56:38.4008946Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-19T19:56:38.4009638Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.4010390Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-19T19:56:38.4011085Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-19T19:56:38.4011719Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.4021042Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-19T19:56:38.4031551Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-19T19:56:38.4041974Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-19T19:56:38.4051984Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-19T19:56:38.4061346Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-19T19:56:38.4070981Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-19T19:56:38.4092710Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     + [
2026-02-19T19:56:38.4093160Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]       ^
2026-02-19T19:56:38.4100383Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-19T19:56:38.4110368Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-19T19:56:38.4123171Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.4134693Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-19T19:56:38.4144579Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-19T19:56:38.4155022Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.4166246Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-19T19:56:38.4176003Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-19T19:56:38.4185382Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-19T19:56:38.4196254Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-19T19:56:38.4205876Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-19T19:56:38.4215990Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.4226582Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 401, in __init__
2026-02-19T19:56:38.4236987Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-19T19:56:38.4251477Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 238, in __init__
2026-02-19T19:56:38.4262581Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-19T19:56:38.4274716Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 50, in setup_moe_comm_method
2026-02-19T19:56:38.4285660Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-19T19:56:38.4296056Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.4307340Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 82, in __init__
2026-02-19T19:56:38.4317891Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-19T19:56:38.4328317Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.4339646Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 260, in _get_token_dispatcher
2026-02-19T19:56:38.4350004Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-19T19:56:38.4361150Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.4372851Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 415, in __init__
2026-02-19T19:56:38.4383011Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-19T19:56:38.4393276Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.4405822Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-19T19:56:38.4416707Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [ERROR] 2026-02-19-19:56:37 (PID:688, Device:5, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-19T19:56:38.4430554Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [PID: 688] 2026-02-19-19:56:37.150.969 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-19T19:56:38.4443570Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-19T19:56:38.4452710Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-19T19:56:38.4464375Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-19T19:56:38.4473643Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] 
2026-02-19T19:56:38.4484524Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m INFO 02-19 19:56:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-19T19:56:38.4494564Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-19T19:56:38.4504743Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-19T19:56:38.4515746Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-19T19:56:38.4526408Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-19T19:56:38.4536638Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.4549829Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-19T19:56:38.4567020Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-19T19:56:38.4579194Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 404, in load_model
2026-02-19T19:56:38.4589718Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-19T19:56:38.4601573Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2311, in load_model
2026-02-19T19:56:38.4612304Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-19T19:56:38.4622696Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.4633573Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-19T19:56:38.4644001Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return loader.load_model(
2026-02-19T19:56:38.4653997Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.4664322Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-19T19:56:38.4673751Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     model = initialize_model(
2026-02-19T19:56:38.4684585Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.4695093Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-19T19:56:38.4705175Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-19T19:56:38.4715604Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.4726666Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-19T19:56:38.4736348Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-19T19:56:38.4745906Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.4756778Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-19T19:56:38.4768429Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-19T19:56:38.4780117Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-19T19:56:38.4791469Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-19T19:56:38.4802745Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-19T19:56:38.4813964Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-19T19:56:38.4823284Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     + [
2026-02-19T19:56:38.4833510Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]       ^
2026-02-19T19:56:38.4843584Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-19T19:56:38.4854059Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-19T19:56:38.4864126Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.4874040Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-19T19:56:38.4884563Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-19T19:56:38.4894381Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.4904694Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-19T19:56:38.4914025Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-19T19:56:38.4924545Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-19T19:56:38.4934757Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-19T19:56:38.4944228Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-19T19:56:38.4954144Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.4964525Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 401, in __init__
2026-02-19T19:56:38.4974075Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-19T19:56:38.4984208Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 238, in __init__
2026-02-19T19:56:38.4993960Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-19T19:56:38.5004393Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 50, in setup_moe_comm_method
2026-02-19T19:56:38.5014592Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-19T19:56:38.5024103Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.5034552Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 82, in __init__
2026-02-19T19:56:38.5045101Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-19T19:56:38.5055493Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.5065322Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 260, in _get_token_dispatcher
2026-02-19T19:56:38.5074902Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-19T19:56:38.5084859Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.5122254Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 415, in __init__
2026-02-19T19:56:38.5123182Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-19T19:56:38.5123869Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.5128736Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-19T19:56:38.5138508Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [ERROR] 2026-02-19-19:56:37 (PID:282, Device:1, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-19T19:56:38.5149400Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [PID: 282] 2026-02-19-19:56:37.150.517 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-19T19:56:38.5162228Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-19T19:56:38.5171692Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-19T19:56:38.5183345Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-19T19:56:38.5191324Z [0;36m(Worker_DP0_TP1_EP1 pid=282)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] 
2026-02-19T19:56:38.5201425Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m INFO 02-19 19:56:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-19T19:56:38.5211496Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-19T19:56:38.5221337Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-19T19:56:38.5231591Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-19T19:56:38.5241423Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-19T19:56:38.5251779Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.5261965Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-19T19:56:38.5271405Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-19T19:56:38.5282223Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 404, in load_model
2026-02-19T19:56:38.5291929Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-19T19:56:38.5302640Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2311, in load_model
2026-02-19T19:56:38.5312474Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-19T19:56:38.5322534Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.5332669Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-19T19:56:38.5342401Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return loader.load_model(
2026-02-19T19:56:38.5351589Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.5362367Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-19T19:56:38.5371948Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     model = initialize_model(
2026-02-19T19:56:38.5382530Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.5396913Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-19T19:56:38.5407114Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-19T19:56:38.5417597Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.5429237Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-19T19:56:38.5438896Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-19T19:56:38.5451192Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.5462334Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-19T19:56:38.5472887Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-19T19:56:38.5485115Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-19T19:56:38.5498757Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-19T19:56:38.5509555Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-19T19:56:38.5523608Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-19T19:56:38.5535198Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     + [
2026-02-19T19:56:38.5548703Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]       ^
2026-02-19T19:56:38.5560444Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-19T19:56:38.5571257Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-19T19:56:38.5582488Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.5593121Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-19T19:56:38.5603849Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-19T19:56:38.5613817Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.5624882Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-19T19:56:38.5634703Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-19T19:56:38.5645083Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-19T19:56:38.5655648Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-19T19:56:38.5665474Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-19T19:56:38.5675608Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.5686914Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 401, in __init__
2026-02-19T19:56:38.5696991Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-19T19:56:38.5707667Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 238, in __init__
2026-02-19T19:56:38.5717814Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-19T19:56:38.5729118Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 50, in setup_moe_comm_method
2026-02-19T19:56:38.5739720Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-19T19:56:38.5749856Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.5760909Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 82, in __init__
2026-02-19T19:56:38.5772479Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-19T19:56:38.5783690Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.5795313Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 260, in _get_token_dispatcher
2026-02-19T19:56:38.5807813Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-19T19:56:38.5818543Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.5829468Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 415, in __init__
2026-02-19T19:56:38.5839924Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-19T19:56:38.5850631Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.5863660Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-19T19:56:38.5873570Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [ERROR] 2026-02-19-19:56:37 (PID:482, Device:3, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-19T19:56:38.5886502Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [PID: 482] 2026-02-19-19:56:37.150.754 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-19T19:56:38.5900666Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-19T19:56:38.5909606Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-19T19:56:38.5922081Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-19T19:56:38.5931858Z [0;36m(Worker_DP0_TP3_EP3 pid=482)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] 
2026-02-19T19:56:38.5942414Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m INFO 02-19 19:56:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-19T19:56:38.5952670Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-19T19:56:38.5962941Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-19T19:56:38.5974000Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-19T19:56:38.5983983Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-19T19:56:38.5994058Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.6005272Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-19T19:56:38.6015691Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-19T19:56:38.6026352Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 404, in load_model
2026-02-19T19:56:38.6036354Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-19T19:56:38.6048462Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2311, in load_model
2026-02-19T19:56:38.6058490Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-19T19:56:38.6068666Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.6079893Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-19T19:56:38.6090207Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return loader.load_model(
2026-02-19T19:56:38.6100324Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.6111235Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-19T19:56:38.6120943Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     model = initialize_model(
2026-02-19T19:56:38.6130937Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.6141686Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-19T19:56:38.6152730Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-19T19:56:38.6162951Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.6173945Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-19T19:56:38.6184026Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-19T19:56:38.6194265Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.6205563Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-19T19:56:38.6215279Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-19T19:56:38.6225803Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-19T19:56:38.6236032Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-19T19:56:38.6247757Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-19T19:56:38.6258010Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-19T19:56:38.6267398Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     + [
2026-02-19T19:56:38.6277441Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]       ^
2026-02-19T19:56:38.6288478Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-19T19:56:38.6298168Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-19T19:56:38.6307738Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.6318186Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-19T19:56:38.6327674Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-19T19:56:38.6337549Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.6347901Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-19T19:56:38.6357702Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-19T19:56:38.6367857Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-19T19:56:38.6377299Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-19T19:56:38.6386554Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-19T19:56:38.6396081Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.6406940Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 401, in __init__
2026-02-19T19:56:38.6416346Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-19T19:56:38.6425892Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 238, in __init__
2026-02-19T19:56:38.6435135Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-19T19:56:38.6445509Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 50, in setup_moe_comm_method
2026-02-19T19:56:38.6455256Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-19T19:56:38.6464302Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.6473743Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 82, in __init__
2026-02-19T19:56:38.6483176Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-19T19:56:38.6493069Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.6502933Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 260, in _get_token_dispatcher
2026-02-19T19:56:38.6511671Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-19T19:56:38.6521170Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.6531256Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 415, in __init__
2026-02-19T19:56:38.6540253Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-19T19:56:38.6549892Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.6560379Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-19T19:56:38.6569687Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [ERROR] 2026-02-19-19:56:37 (PID:480, Device:3, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-19T19:56:38.6579950Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [PID: 480] 2026-02-19-19:56:37.150.338 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-19T19:56:38.6590519Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-19T19:56:38.6598773Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-19T19:56:38.6609425Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-19T19:56:38.6617919Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] 
2026-02-19T19:56:38.6626949Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m INFO 02-19 19:56:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-19T19:56:38.6636374Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m INFO 02-19 19:56:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-19T19:56:38.6647532Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m INFO 02-19 19:56:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-19T19:56:38.6655323Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-19T19:56:38.6664750Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-19T19:56:38.6674658Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-19T19:56:38.6683931Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-19T19:56:38.6693606Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.6703665Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-19T19:56:38.6712750Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-19T19:56:38.6723407Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 404, in load_model
2026-02-19T19:56:38.6732394Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-19T19:56:38.6741751Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2311, in load_model
2026-02-19T19:56:38.6751274Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-19T19:56:38.6761252Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.6771561Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-19T19:56:38.6781317Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return loader.load_model(
2026-02-19T19:56:38.6790309Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.6800104Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-19T19:56:38.6809066Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     model = initialize_model(
2026-02-19T19:56:38.6818131Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.6827775Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-19T19:56:38.6837425Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-19T19:56:38.6846664Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.6856343Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-19T19:56:38.6865311Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-19T19:56:38.6874646Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.6884543Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-19T19:56:38.6893921Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-19T19:56:38.6903556Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-19T19:56:38.6913067Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-19T19:56:38.6922092Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-19T19:56:38.6931841Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-19T19:56:38.6940568Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     + [
2026-02-19T19:56:38.6950120Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]       ^
2026-02-19T19:56:38.6962107Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-19T19:56:38.6971708Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-19T19:56:38.6980384Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.6992267Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-19T19:56:38.7001963Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-19T19:56:38.7011724Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.7022496Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-19T19:56:38.7031637Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-19T19:56:38.7057019Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-19T19:56:38.7057720Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-19T19:56:38.7061185Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-19T19:56:38.7070210Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.7080114Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 401, in __init__
2026-02-19T19:56:38.7089734Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-19T19:56:38.7098939Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 238, in __init__
2026-02-19T19:56:38.7108469Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-19T19:56:38.7118787Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 50, in setup_moe_comm_method
2026-02-19T19:56:38.7128281Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-19T19:56:38.7137507Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.7146947Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 82, in __init__
2026-02-19T19:56:38.7157831Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-19T19:56:38.7166208Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.7175603Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 260, in _get_token_dispatcher
2026-02-19T19:56:38.7184963Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-19T19:56:38.7193701Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.7203538Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 415, in __init__
2026-02-19T19:56:38.7213124Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-19T19:56:38.7222815Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.7232843Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-19T19:56:38.7241734Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [ERROR] 2026-02-19-19:56:37 (PID:793, Device:6, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-19T19:56:38.7252448Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [PID: 793] 2026-02-19-19:56:37.150.110 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-19T19:56:38.7262856Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-19T19:56:38.7271076Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-19T19:56:38.7281244Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-19T19:56:38.7290032Z [0;36m(Worker_DP0_TP6_EP6 pid=793)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] 
2026-02-19T19:56:38.7299302Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m INFO 02-19 19:56:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-19T19:56:38.7308371Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-19T19:56:38.7317797Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-19T19:56:38.7327021Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-19T19:56:38.7336236Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-19T19:56:38.7345241Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.7354603Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-19T19:56:38.7364066Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-19T19:56:38.7374044Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 404, in load_model
2026-02-19T19:56:38.7383304Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-19T19:56:38.7392915Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2311, in load_model
2026-02-19T19:56:38.7402417Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-19T19:56:38.7410990Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.7420752Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-19T19:56:38.7429802Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return loader.load_model(
2026-02-19T19:56:38.7439191Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.7449420Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-19T19:56:38.7459759Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     model = initialize_model(
2026-02-19T19:56:38.7471745Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.7481940Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-19T19:56:38.7491609Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-19T19:56:38.7500742Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.7510595Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-19T19:56:38.7519870Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-19T19:56:38.7529275Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.7538595Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-19T19:56:38.7547524Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-19T19:56:38.7558986Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-19T19:56:38.7568792Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-19T19:56:38.7577961Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-19T19:56:38.7587273Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-19T19:56:38.7596306Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     + [
2026-02-19T19:56:38.7605695Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]       ^
2026-02-19T19:56:38.7615362Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-19T19:56:38.7624797Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-19T19:56:38.7641914Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.7653141Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-19T19:56:38.7662754Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-19T19:56:38.7671894Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.7682125Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-19T19:56:38.7691570Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-19T19:56:38.7700974Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-19T19:56:38.7710164Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-19T19:56:38.7719625Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-19T19:56:38.7728970Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.7739212Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 401, in __init__
2026-02-19T19:56:38.7748309Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-19T19:56:38.7758576Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 238, in __init__
2026-02-19T19:56:38.7769229Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-19T19:56:38.7780562Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 50, in setup_moe_comm_method
2026-02-19T19:56:38.7790781Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-19T19:56:38.7801021Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.7810840Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 82, in __init__
2026-02-19T19:56:38.7820544Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-19T19:56:38.7829369Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.7839331Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 260, in _get_token_dispatcher
2026-02-19T19:56:38.7848884Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-19T19:56:38.7858266Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.7867860Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 415, in __init__
2026-02-19T19:56:38.7876869Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-19T19:56:38.7887640Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.7898004Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-19T19:56:38.7907253Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [ERROR] 2026-02-19-19:56:37 (PID:376, Device:2, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-19T19:56:38.7917360Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [PID: 376] 2026-02-19-19:56:37.151.055 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-19T19:56:38.7929280Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-19T19:56:38.7937980Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-19T19:56:38.7947493Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-19T19:56:38.7956875Z [0;36m(Worker_DP0_TP2_EP2 pid=376)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] 
2026-02-19T19:56:38.7965914Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-19T19:56:38.7977490Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-19T19:56:38.7984256Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-19T19:56:38.7993059Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-19T19:56:38.8003414Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8012611Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-19T19:56:38.8021407Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-19T19:56:38.8031013Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 404, in load_model
2026-02-19T19:56:38.8040479Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-19T19:56:38.8050493Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2311, in load_model
2026-02-19T19:56:38.8059749Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-19T19:56:38.8068664Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8078756Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-19T19:56:38.8088533Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return loader.load_model(
2026-02-19T19:56:38.8098095Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8107955Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-19T19:56:38.8117597Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     model = initialize_model(
2026-02-19T19:56:38.8128327Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8137945Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-19T19:56:38.8147577Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-19T19:56:38.8156734Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8166819Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-19T19:56:38.8176148Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-19T19:56:38.8185570Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8195178Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-19T19:56:38.8205025Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-19T19:56:38.8214417Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-19T19:56:38.8223685Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-19T19:56:38.8232878Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-19T19:56:38.8242106Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-19T19:56:38.8251473Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     + [
2026-02-19T19:56:38.8260429Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]       ^
2026-02-19T19:56:38.8270937Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-19T19:56:38.8279536Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-19T19:56:38.8289062Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8299640Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-19T19:56:38.8308260Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-19T19:56:38.8316888Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8326860Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-19T19:56:38.8335872Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-19T19:56:38.8344697Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-19T19:56:38.8354400Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-19T19:56:38.8364021Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-19T19:56:38.8373116Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8383550Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 401, in __init__
2026-02-19T19:56:38.8391685Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-19T19:56:38.8401136Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 238, in __init__
2026-02-19T19:56:38.8410678Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-19T19:56:38.8420560Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 50, in setup_moe_comm_method
2026-02-19T19:56:38.8429314Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-19T19:56:38.8438574Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8448400Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 82, in __init__
2026-02-19T19:56:38.8457416Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-19T19:56:38.8466986Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8476804Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 260, in _get_token_dispatcher
2026-02-19T19:56:38.8486562Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-19T19:56:38.8496032Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8505186Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 415, in __init__
2026-02-19T19:56:38.8514315Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-19T19:56:38.8523502Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8533648Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-19T19:56:38.8542904Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [ERROR] 2026-02-19-19:56:37 (PID:587, Device:4, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-19T19:56:38.8553090Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [PID: 587] 2026-02-19-19:56:37.151.138 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-19T19:56:38.8565659Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-19T19:56:38.8576393Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-19T19:56:38.8588478Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-19T19:56:38.8599069Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] 
2026-02-19T19:56:38.8610390Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-19T19:56:38.8628145Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-19T19:56:38.8639253Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-19T19:56:38.8649556Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-19T19:56:38.8659626Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8670001Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-19T19:56:38.8680489Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-19T19:56:38.8690083Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 404, in load_model
2026-02-19T19:56:38.8699379Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-19T19:56:38.8709177Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2311, in load_model
2026-02-19T19:56:38.8718698Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-19T19:56:38.8728088Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8737555Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-19T19:56:38.8746587Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return loader.load_model(
2026-02-19T19:56:38.8755850Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8767715Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-19T19:56:38.8778200Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     model = initialize_model(
2026-02-19T19:56:38.8788718Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8800417Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-19T19:56:38.8810235Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-19T19:56:38.8819524Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8828999Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-19T19:56:38.8838194Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-19T19:56:38.8847638Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8857017Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-19T19:56:38.8865983Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-19T19:56:38.8875553Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-19T19:56:38.8886302Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-19T19:56:38.8895631Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-19T19:56:38.8905550Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-19T19:56:38.8914493Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     + [
2026-02-19T19:56:38.8923968Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]       ^
2026-02-19T19:56:38.8933623Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-19T19:56:38.8943091Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-19T19:56:38.8953200Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8963143Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-19T19:56:38.8973008Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-19T19:56:38.8981928Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.8991506Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-19T19:56:38.9000770Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-19T19:56:38.9010261Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-19T19:56:38.9020163Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-19T19:56:38.9028447Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-19T19:56:38.9037982Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.9047843Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 401, in __init__
2026-02-19T19:56:38.9072826Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-19T19:56:38.9073575Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 238, in __init__
2026-02-19T19:56:38.9075835Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-19T19:56:38.9086495Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 50, in setup_moe_comm_method
2026-02-19T19:56:38.9095656Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-19T19:56:38.9105024Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.9114213Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 82, in __init__
2026-02-19T19:56:38.9123908Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-19T19:56:38.9133236Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.9143267Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 260, in _get_token_dispatcher
2026-02-19T19:56:38.9152196Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-19T19:56:38.9161906Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.9171762Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 415, in __init__
2026-02-19T19:56:38.9180852Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-19T19:56:38.9189846Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.9200518Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-19T19:56:38.9210016Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [ERROR] 2026-02-19-19:56:37 (PID:241, Device:0, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-19T19:56:38.9219875Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [PID: 241] 2026-02-19-19:56:37.150.179 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-19T19:56:38.9230593Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-19T19:56:38.9239379Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-19T19:56:38.9249126Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-19T19:56:38.9257594Z [0;36m(Worker_DP1_TP0_EP8 pid=241)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] 
2026-02-19T19:56:38.9266303Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-19T19:56:38.9276137Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-19T19:56:38.9285733Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-19T19:56:38.9295128Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-19T19:56:38.9304119Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.9313760Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-19T19:56:38.9323176Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-19T19:56:38.9333253Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 404, in load_model
2026-02-19T19:56:38.9341876Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-19T19:56:38.9351383Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2311, in load_model
2026-02-19T19:56:38.9360336Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-19T19:56:38.9370047Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.9382929Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-19T19:56:38.9389396Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return loader.load_model(
2026-02-19T19:56:38.9397831Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.9408005Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-19T19:56:38.9416593Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     model = initialize_model(
2026-02-19T19:56:38.9426515Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.9436253Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-19T19:56:38.9446112Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-19T19:56:38.9454887Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.9464401Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-19T19:56:38.9473371Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-19T19:56:38.9483166Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.9492643Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-19T19:56:38.9501462Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-19T19:56:38.9510889Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-19T19:56:38.9520116Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-19T19:56:38.9530030Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-19T19:56:38.9540106Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-19T19:56:38.9548128Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     + [
2026-02-19T19:56:38.9557270Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]       ^
2026-02-19T19:56:38.9567234Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-19T19:56:38.9576723Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-19T19:56:38.9585849Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.9594866Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-19T19:56:38.9604292Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-19T19:56:38.9614233Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.9624192Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-19T19:56:38.9633492Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-19T19:56:38.9643481Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-19T19:56:38.9653280Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-19T19:56:38.9662165Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-19T19:56:38.9671074Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-19T19:56:38.9680992Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 401, in __init__
2026-02-19T19:56:38.9690167Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-19T19:56:38.9699763Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 238, in __init__
2026-02-19T19:56:38.9708500Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-19T19:56:38.9721770Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 50, in setup_moe_comm_method
2026-02-19T19:56:38.9732835Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-19T19:56:38.9742764Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.9753322Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 82, in __init__
2026-02-19T19:56:38.9764133Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-19T19:56:38.9775220Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.9786015Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 260, in _get_token_dispatcher
2026-02-19T19:56:38.9797356Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-19T19:56:38.9807594Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.9817781Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 415, in __init__
2026-02-19T19:56:38.9827754Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-19T19:56:38.9837222Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.9848425Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-19T19:56:38.9857580Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [ERROR] 2026-02-19-19:56:37 (PID:379, Device:2, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-19T19:56:38.9867648Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [PID: 379] 2026-02-19-19:56:37.150.138 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-19T19:56:38.9878546Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-19T19:56:38.9887539Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-19T19:56:38.9898263Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-19T19:56:38.9906566Z [0;36m(Worker_DP1_TP2_EP10 pid=379)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] 
2026-02-19T19:56:38.9916610Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-19T19:56:38.9926650Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-19T19:56:38.9936981Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-19T19:56:38.9945935Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-19T19:56:38.9955252Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:38.9966658Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-19T19:56:38.9976199Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-19T19:56:38.9985683Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 404, in load_model
2026-02-19T19:56:38.9994624Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-19T19:56:39.0004826Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2311, in load_model
2026-02-19T19:56:39.0014222Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-19T19:56:39.0023651Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:39.0033516Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-19T19:56:39.0043527Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return loader.load_model(
2026-02-19T19:56:39.0052962Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:39.0062690Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-19T19:56:39.0072370Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     model = initialize_model(
2026-02-19T19:56:39.0082396Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-19T19:56:39.0091737Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-19T19:56:39.0101520Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-19T19:56:39.0111337Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:39.0121517Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-19T19:56:39.0131712Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-19T19:56:39.0141165Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-19T19:56:39.0151045Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-19T19:56:39.0161731Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-19T19:56:39.0171294Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-19T19:56:39.0180947Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-19T19:56:39.0190912Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-19T19:56:39.0200986Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-19T19:56:39.0210526Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     + [
2026-02-19T19:56:39.0219830Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]       ^
2026-02-19T19:56:39.0229941Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-19T19:56:39.0239382Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-19T19:56:39.0249294Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:39.0258912Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-19T19:56:39.0268648Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-19T19:56:39.0279081Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:39.0288897Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-19T19:56:39.0298588Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-19T19:56:39.0308582Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-19T19:56:39.0319982Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-19T19:56:39.0431045Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-19T19:56:39.0454362Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-19T19:56:39.0466263Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 401, in __init__
2026-02-19T19:56:39.0473385Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-19T19:56:39.0487853Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 238, in __init__
2026-02-19T19:56:39.0498371Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-19T19:56:39.0508609Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 50, in setup_moe_comm_method
2026-02-19T19:56:39.0519534Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-19T19:56:39.0529561Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:39.0543097Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 82, in __init__
2026-02-19T19:56:39.0548801Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-19T19:56:39.0559017Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:39.0568859Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 260, in _get_token_dispatcher
2026-02-19T19:56:39.0577575Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-19T19:56:39.0587650Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:39.0597386Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 415, in __init__
2026-02-19T19:56:39.0606472Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-19T19:56:39.0615627Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:39.0625610Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-19T19:56:39.0634932Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [ERROR] 2026-02-19-19:56:37 (PID:283, Device:1, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-19T19:56:39.0645721Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] [PID: 283] 2026-02-19-19:56:37.150.570 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-19T19:56:39.0656109Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-19T19:56:39.0664735Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-19T19:56:39.0674624Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-19T19:56:39.0683736Z [0;36m(Worker_DP1_TP1_EP9 pid=283)[0;0m ERROR 02-19 19:56:38 [multiproc_executor.py:772] 
2026-02-19T19:56:42.6630424Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946] EngineCore failed to start.
2026-02-19T19:56:42.6630987Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946] Traceback (most recent call last):
2026-02-19T19:56:42.6637127Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 929, in run_engine_core
2026-02-19T19:56:42.6647466Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946]     engine_core = DPEngineCoreProc(*args, **kwargs)
2026-02-19T19:56:42.6657039Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:42.6667566Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 1279, in __init__
2026-02-19T19:56:42.6676416Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946]     super().__init__(
2026-02-19T19:56:42.6686056Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 691, in __init__
2026-02-19T19:56:42.6694977Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946]     super().__init__(
2026-02-19T19:56:42.6704269Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 105, in __init__
2026-02-19T19:56:42.6714180Z [0;36m(EngineCore_DP1 pid=180)[0;0m Process EngineCore_DP1:
2026-02-19T19:56:42.6725022Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946]     self.model_executor = executor_class(vllm_config)
2026-02-19T19:56:42.6735473Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:42.6745346Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
2026-02-19T19:56:42.6754780Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946]     super().__init__(vllm_config)
2026-02-19T19:56:42.6766458Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
2026-02-19T19:56:42.6777175Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946]     self._init_executor()
2026-02-19T19:56:42.6788659Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
2026-02-19T19:56:42.6798476Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
2026-02-19T19:56:42.6809579Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:42.6819767Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
2026-02-19T19:56:42.6830290Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946]     raise e from None
2026-02-19T19:56:42.6841441Z [0;36m(EngineCore_DP1 pid=180)[0;0m ERROR 02-19 19:56:42 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
2026-02-19T19:56:42.6852276Z [0;36m(EngineCore_DP1 pid=180)[0;0m Traceback (most recent call last):
2026-02-19T19:56:42.6862148Z [0;36m(EngineCore_DP1 pid=180)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-19T19:56:42.6872854Z [0;36m(EngineCore_DP1 pid=180)[0;0m     self.run()
2026-02-19T19:56:42.6883747Z [0;36m(EngineCore_DP1 pid=180)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-19T19:56:42.6894014Z [0;36m(EngineCore_DP1 pid=180)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-19T19:56:42.6903558Z [0;36m(EngineCore_DP1 pid=180)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 950, in run_engine_core
2026-02-19T19:56:42.6913760Z [0;36m(EngineCore_DP1 pid=180)[0;0m     raise e
2026-02-19T19:56:42.6925114Z [0;36m(EngineCore_DP1 pid=180)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 929, in run_engine_core
2026-02-19T19:56:42.6958263Z [0;36m(EngineCore_DP1 pid=180)[0;0m     engine_core = DPEngineCoreProc(*args, **kwargs)
2026-02-19T19:56:42.6967918Z [0;36m(EngineCore_DP1 pid=180)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:42.6977563Z [0;36m(EngineCore_DP1 pid=180)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 1279, in __init__
2026-02-19T19:56:42.6987091Z [0;36m(EngineCore_DP1 pid=180)[0;0m     super().__init__(
2026-02-19T19:56:42.6996965Z [0;36m(EngineCore_DP1 pid=180)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 691, in __init__
2026-02-19T19:56:42.7006409Z [0;36m(EngineCore_DP1 pid=180)[0;0m     super().__init__(
2026-02-19T19:56:42.7015791Z [0;36m(EngineCore_DP1 pid=180)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 105, in __init__
2026-02-19T19:56:42.7025527Z [0;36m(EngineCore_DP1 pid=180)[0;0m     self.model_executor = executor_class(vllm_config)
2026-02-19T19:56:42.7035032Z [0;36m(EngineCore_DP1 pid=180)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:42.7045465Z [0;36m(EngineCore_DP1 pid=180)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
2026-02-19T19:56:42.7054612Z [0;36m(EngineCore_DP1 pid=180)[0;0m     super().__init__(vllm_config)
2026-02-19T19:56:42.7064164Z [0;36m(EngineCore_DP1 pid=180)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
2026-02-19T19:56:42.7073856Z [0;36m(EngineCore_DP1 pid=180)[0;0m     self._init_executor()
2026-02-19T19:56:42.7084347Z [0;36m(EngineCore_DP1 pid=180)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
2026-02-19T19:56:42.7093020Z [0;36m(EngineCore_DP1 pid=180)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
2026-02-19T19:56:42.7101838Z [0;36m(EngineCore_DP1 pid=180)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:42.7111500Z [0;36m(EngineCore_DP1 pid=180)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
2026-02-19T19:56:42.7151510Z [0;36m(EngineCore_DP1 pid=180)[0;0m     raise e from None
2026-02-19T19:56:42.7152524Z [0;36m(EngineCore_DP1 pid=180)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
2026-02-19T19:56:42.8451892Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946] EngineCore failed to start.
2026-02-19T19:56:42.8461786Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946] Traceback (most recent call last):
2026-02-19T19:56:42.8470806Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 929, in run_engine_core
2026-02-19T19:56:42.8480238Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946]     engine_core = DPEngineCoreProc(*args, **kwargs)
2026-02-19T19:56:42.8489611Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:42.8499467Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 1279, in __init__
2026-02-19T19:56:42.8508655Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946]     super().__init__(
2026-02-19T19:56:42.8519840Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 691, in __init__
2026-02-19T19:56:42.8528483Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946]     super().__init__(
2026-02-19T19:56:42.8538810Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 105, in __init__
2026-02-19T19:56:42.8546996Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946]     self.model_executor = executor_class(vllm_config)
2026-02-19T19:56:42.8555748Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:42.8566081Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
2026-02-19T19:56:42.8575433Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946]     super().__init__(vllm_config)
2026-02-19T19:56:42.8585203Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
2026-02-19T19:56:42.8594729Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946]     self._init_executor()
2026-02-19T19:56:42.8605167Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
2026-02-19T19:56:42.8614368Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
2026-02-19T19:56:42.8623312Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:42.8633034Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
2026-02-19T19:56:42.8642362Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946]     raise e from None
2026-02-19T19:56:42.8652177Z [0;36m(EngineCore_DP0 pid=161)[0;0m ERROR 02-19 19:56:42 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
2026-02-19T19:56:42.8661252Z [0;36m(EngineCore_DP0 pid=161)[0;0m Process EngineCore_DP0:
2026-02-19T19:56:42.8670717Z [0;36m(EngineCore_DP0 pid=161)[0;0m Traceback (most recent call last):
2026-02-19T19:56:42.8680946Z [0;36m(EngineCore_DP0 pid=161)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-19T19:56:42.8690009Z [0;36m(EngineCore_DP0 pid=161)[0;0m     self.run()
2026-02-19T19:56:42.8699328Z [0;36m(EngineCore_DP0 pid=161)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-19T19:56:42.8708346Z [0;36m(EngineCore_DP0 pid=161)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-19T19:56:42.8718044Z [0;36m(EngineCore_DP0 pid=161)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 950, in run_engine_core
2026-02-19T19:56:42.8727269Z [0;36m(EngineCore_DP0 pid=161)[0;0m     raise e
2026-02-19T19:56:42.8737082Z [0;36m(EngineCore_DP0 pid=161)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 929, in run_engine_core
2026-02-19T19:56:42.8746140Z [0;36m(EngineCore_DP0 pid=161)[0;0m     engine_core = DPEngineCoreProc(*args, **kwargs)
2026-02-19T19:56:42.8756607Z [0;36m(EngineCore_DP0 pid=161)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:42.8786498Z [0;36m(EngineCore_DP0 pid=161)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 1279, in __init__
2026-02-19T19:56:42.8796445Z [0;36m(EngineCore_DP0 pid=161)[0;0m     super().__init__(
2026-02-19T19:56:42.8806611Z [0;36m(EngineCore_DP0 pid=161)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 691, in __init__
2026-02-19T19:56:42.8815360Z [0;36m(EngineCore_DP0 pid=161)[0;0m     super().__init__(
2026-02-19T19:56:42.8824707Z [0;36m(EngineCore_DP0 pid=161)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 105, in __init__
2026-02-19T19:56:42.8834131Z [0;36m(EngineCore_DP0 pid=161)[0;0m     self.model_executor = executor_class(vllm_config)
2026-02-19T19:56:42.8844071Z [0;36m(EngineCore_DP0 pid=161)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:42.8853601Z [0;36m(EngineCore_DP0 pid=161)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
2026-02-19T19:56:42.8862674Z [0;36m(EngineCore_DP0 pid=161)[0;0m     super().__init__(vllm_config)
2026-02-19T19:56:42.8871839Z [0;36m(EngineCore_DP0 pid=161)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
2026-02-19T19:56:42.8881818Z [0;36m(EngineCore_DP0 pid=161)[0;0m     self._init_executor()
2026-02-19T19:56:42.8891408Z [0;36m(EngineCore_DP0 pid=161)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
2026-02-19T19:56:42.8900589Z [0;36m(EngineCore_DP0 pid=161)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
2026-02-19T19:56:42.8909646Z [0;36m(EngineCore_DP0 pid=161)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-19T19:56:42.8919516Z [0;36m(EngineCore_DP0 pid=161)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
2026-02-19T19:56:42.8928529Z [0;36m(EngineCore_DP0 pid=161)[0;0m     raise e from None
2026-02-19T19:56:42.8938377Z [0;36m(EngineCore_DP0 pid=161)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
2026-02-19T19:56:44.4899824Z Traceback (most recent call last):
2026-02-19T19:56:44.4908190Z   File "/usr/local/python3.11.14/bin/vllm", line 7, in <module>
2026-02-19T19:56:44.4929221Z     sys.exit(main())
2026-02-19T19:56:44.4938903Z              ^^^^^^
2026-02-19T19:56:44.4948401Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/main.py", line 73, in main
2026-02-19T19:56:44.4961734Z     args.dispatch_function(args)
2026-02-19T19:56:44.4969895Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 108, in cmd
2026-02-19T19:56:44.4978407Z     run_multi_api_server(args)
2026-02-19T19:56:44.4987731Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 248, in run_multi_api_server
2026-02-19T19:56:44.4997039Z     with launch_core_engines(
2026-02-19T19:56:44.5006259Z   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 144, in __exit__
2026-02-19T19:56:44.5015274Z     next(self.gen)
2026-02-19T19:56:44.5024986Z   File "/vllm-workspace/vllm/vllm/v1/engine/utils.py", line 933, in launch_core_engines
2026-02-19T19:56:44.5034524Z     wait_for_engine_startup(
2026-02-19T19:56:44.5045065Z   File "/vllm-workspace/vllm/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
2026-02-19T19:56:44.5054821Z     raise RuntimeError(
2026-02-19T19:56:44.5064842Z RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
2026-02-19T19:56:45.3694631Z [ERROR] 2026-02-19-19:56:44 (PID:138, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
2026-02-19T19:56:45.7120191Z sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-19T19:56:47.2014631Z /usr/local/python3.11.14/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 18 leaked shared_memory objects to clean up at shutdown
2026-02-19T19:56:47.2025337Z   warnings.warn('resource_tracker: There appear to be %d '
2026-02-19T19:56:52.6871497Z FAILED
2026-02-19T19:56:52.6882430Z 
2026-02-19T19:56:52.6892879Z =================================== FAILURES ===================================
2026-02-19T19:56:52.6902855Z _______________________________ test_multi_node ________________________________
2026-02-19T19:56:52.6912224Z 
2026-02-19T19:56:52.6922308Z     @pytest.mark.asyncio
2026-02-19T19:56:52.6932095Z     async def test_multi_node() -> None:
2026-02-19T19:56:52.6940624Z         config = MultiNodeConfigLoader.from_yaml()
2026-02-19T19:56:52.6949621Z     
2026-02-19T19:56:52.6959037Z         with ProxyLauncher(
2026-02-19T19:56:52.6968496Z                 nodes=config.nodes,
2026-02-19T19:56:52.6996742Z                 disagg_cfg=config.disagg_cfg,
2026-02-19T19:56:52.6996950Z                 envs=config.envs,
2026-02-19T19:56:52.6997152Z                 proxy_port=config.proxy_port,
2026-02-19T19:56:52.7005955Z                 cur_index=config.cur_index,
2026-02-19T19:56:52.7014550Z         ) as proxy:
2026-02-19T19:56:52.7025957Z     
2026-02-19T19:56:52.7033073Z >           with RemoteOpenAIServer(
2026-02-19T19:56:52.7042489Z                     model=config.model,
2026-02-19T19:56:52.7052408Z                     vllm_serve_args=config.server_cmd,
2026-02-19T19:56:52.7060863Z                     server_port=config.server_port,
2026-02-19T19:56:52.7070581Z                     server_host=config.master_ip,
2026-02-19T19:56:52.7080128Z                     env_dict=config.envs,
2026-02-19T19:56:52.7090431Z                     auto_port=False,
2026-02-19T19:56:52.7099461Z                     proxy_port=proxy.proxy_port,
2026-02-19T19:56:52.7108663Z                     disaggregated_prefill=config.disagg_cfg,
2026-02-19T19:56:52.7118267Z                     nodes_info=config.nodes,
2026-02-19T19:56:52.7128189Z                     max_wait_seconds=2800,
2026-02-19T19:56:52.7136923Z             ) as server:
2026-02-19T19:56:52.7145907Z 
2026-02-19T19:56:52.7156190Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py:21: 
2026-02-19T19:56:52.7166041Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-19T19:56:52.7175413Z tests/e2e/conftest.py:306: in __init__
2026-02-19T19:56:52.7184612Z     self._wait_for_multiple_servers(
2026-02-19T19:56:52.7194495Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-19T19:56:52.7203839Z 
2026-02-19T19:56:52.7213245Z self = <tests.e2e.conftest.RemoteOpenAIServer object at 0xffff02ff13d0>
2026-02-19T19:56:52.7222529Z targets = [('10.0.0.219', 'http://10.0.0.219:8080/health')], timeout = 2800
2026-02-19T19:56:52.7231672Z log_interval = 30.0
2026-02-19T19:56:52.7241158Z 
2026-02-19T19:56:52.7250737Z     def _wait_for_multiple_servers(self,
2026-02-19T19:56:52.7259694Z                                    targets,
2026-02-19T19:56:52.7268545Z                                    timeout: float,
2026-02-19T19:56:52.7277634Z                                    log_interval: float = 30.0):
2026-02-19T19:56:52.7286486Z         """
2026-02-19T19:56:52.7295705Z         targets: List[(node_ip, url)]
2026-02-19T19:56:52.7304612Z         log_interval
2026-02-19T19:56:52.7313678Z         """
2026-02-19T19:56:52.7323127Z         start = time.time()
2026-02-19T19:56:52.7332589Z         client = requests
2026-02-19T19:56:52.7340897Z     
2026-02-19T19:56:52.7349255Z         ready = {node_ip: False for node_ip, _ in targets}
2026-02-19T19:56:52.7358895Z     
2026-02-19T19:56:52.7367574Z         last_log_time = 0.0
2026-02-19T19:56:52.7376416Z     
2026-02-19T19:56:52.7385789Z         while True:
2026-02-19T19:56:52.7394727Z             now = time.time()
2026-02-19T19:56:52.7404055Z             all_ready = True
2026-02-19T19:56:52.7413207Z             should_log = (now - last_log_time) >= log_interval
2026-02-19T19:56:52.7421387Z     
2026-02-19T19:56:52.7430718Z             for node_ip, url in targets:
2026-02-19T19:56:52.7439656Z                 if ready[node_ip]:
2026-02-19T19:56:52.7448433Z                     continue
2026-02-19T19:56:52.7457322Z     
2026-02-19T19:56:52.7466182Z                 try:
2026-02-19T19:56:52.7474769Z                     resp = client.get(url)
2026-02-19T19:56:52.7483973Z                     if resp.status_code == 200:
2026-02-19T19:56:52.7493523Z                         ready[node_ip] = True
2026-02-19T19:56:52.7502735Z                         logger.info(f"[READY] Node {node_ip} is ready.")
2026-02-19T19:56:52.7511781Z                 except RequestException:
2026-02-19T19:56:52.7521463Z                     all_ready = False
2026-02-19T19:56:52.7530324Z                     if should_log:
2026-02-19T19:56:52.7539257Z                         logger.debug(f"[WAIT] {url}: connection failed")
2026-02-19T19:56:52.7547977Z     
2026-02-19T19:56:52.7557198Z                     # check unexpected exit
2026-02-19T19:56:52.7566593Z                     result = self._poll()
2026-02-19T19:56:52.7575891Z                     if result is not None and result != 0:
2026-02-19T19:56:52.7584630Z >                       raise RuntimeError(
2026-02-19T19:56:52.7593779Z                             f"Server at {node_ip} exited unexpectedly."
2026-02-19T19:56:52.7602764Z                         ) from None
2026-02-19T19:56:52.7612897Z E                       RuntimeError: Server at 10.0.0.219 exited unexpectedly.
2026-02-19T19:56:52.7621161Z 
2026-02-19T19:56:52.7630612Z tests/e2e/conftest.py:399: RuntimeError
2026-02-19T19:56:52.7639596Z =============================== warnings summary ===============================
2026-02-19T19:56:52.7649265Z <frozen importlib._bootstrap>:241
2026-02-19T19:56:52.7658628Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute
2026-02-19T19:56:52.7667015Z 
2026-02-19T19:56:52.7676499Z <frozen importlib._bootstrap>:241
2026-02-19T19:56:52.7686123Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute
2026-02-19T19:56:52.7694676Z 
2026-02-19T19:56:52.7703902Z ../../usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647
2026-02-19T19:56:52.7714538Z   /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-19T19:56:52.7722884Z     warnings.warn(
2026-02-19T19:56:52.7732244Z 
2026-02-19T19:56:52.7741812Z ../../usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8
2026-02-19T19:56:52.7751471Z   /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
2026-02-19T19:56:52.7760123Z     import pkg_resources
2026-02-19T19:56:52.7769657Z 
2026-02-19T19:56:52.7780449Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2026-02-19T19:56:52.7790228Z =========================== short test summary info ============================
2026-02-19T19:56:52.7801041Z FAILED tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node
2026-02-19T19:56:52.7810621Z ================== 1 failed, 4 warnings in 301.12s (0:05:01) ===================
2026-02-19T19:56:54.4382794Z [0;31mFAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml âœ— ERROR: Some tests failed, please check the error stack above for details. If this is insufficient to pinpoint the error, please download and review the logs of all other nodes from the job's summary.[0m
2026-02-19T19:56:54.6066218Z Cleaning up background log streams...
2026-02-19T19:56:54.6764321Z ##[error]Error: failed to run script step: Error: command terminated with non-zero exit code: command terminated with exit code 1
2026-02-19T19:56:54.6803694Z ##[error]Process completed with exit code 1.
2026-02-19T19:56:54.6906441Z ##[error]Executing the custom container implementation failed. Please contact your self hosted runner administrator.
2026-02-19T19:56:54.7310867Z ##[group]Run actions/upload-artifact@v6
2026-02-19T19:56:54.7311206Z with:
2026-02-19T19:56:54.7311394Z   name: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs
2026-02-19T19:56:54.7311635Z   path: /tmp/vllm*_logs.txt
2026-02-19T19:56:54.7311812Z   retention-days: 7
2026-02-19T19:56:54.7311967Z   if-no-files-found: warn
2026-02-19T19:56:54.7312326Z   compression-level: 6
2026-02-19T19:56:54.7312482Z   overwrite: false
2026-02-19T19:56:54.7312642Z   include-hidden-files: false
2026-02-19T19:56:54.7312815Z env:
2026-02-19T19:56:54.7312992Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-19T19:56:54.7313219Z ##[endgroup]
2026-02-19T19:56:54.7336929Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-19T19:56:54.7337561Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-19T19:56:54.7337778Z ##[endgroup]
2026-02-19T19:56:55.0952905Z (node:911) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-19T19:56:55.0953594Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-19T19:56:56.0755283Z With the provided path, there will be 1 file uploaded
2026-02-19T19:56:56.0755662Z Artifact name is valid!
2026-02-19T19:56:56.0755842Z Root directory input is valid!
2026-02-19T19:56:56.9670600Z Beginning upload of artifact content to blob storage
2026-02-19T19:56:58.3193138Z Uploaded bytes 16145
2026-02-19T19:56:58.5536440Z Finished uploading artifact content to blob storage!
2026-02-19T19:56:58.5537157Z SHA256 digest of uploaded artifact zip is 8116b9a034dcad65288eaefe70c7fa1ff3d2826f6a6287f93f16efb794883b90
2026-02-19T19:56:58.5537562Z Finalizing artifact upload
2026-02-19T19:56:59.4308012Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs.zip successfully finalized. Artifact ID 5579207460
2026-02-19T19:56:59.4308702Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs has been successfully uploaded! Final size is 16145 bytes. Artifact ID is 5579207460
2026-02-19T19:56:59.4313391Z Artifact download URL: https://github.com/vllm-project/vllm-ascend/actions/runs/22190789077/artifacts/5579207460
2026-02-19T19:56:59.9349161Z ##[group]Run kubectl get pods -n "$NAMESPACE" --ignore-not-found=true
2026-02-19T19:56:59.9349544Z [36;1mkubectl get pods -n "$NAMESPACE" --ignore-not-found=true[0m
2026-02-19T19:56:59.9349872Z [36;1mkubectl delete -f ./lws.yaml --ignore-not-found=true || true[0m
2026-02-19T19:56:59.9350201Z shell: bash -el {0}
2026-02-19T19:56:59.9350354Z env:
2026-02-19T19:56:59.9350550Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-19T19:56:59.9350785Z ##[endgroup]
2026-02-19T19:56:59.9434272Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-19T19:56:59.9434907Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-19T19:56:59.9435125Z ##[endgroup]
2026-02-19T19:57:00.2926251Z (node:1023) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-19T19:57:00.2926961Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-19T19:57:00.9374631Z NAME                                             READY   STATUS    RESTARTS     AGE
2026-02-19T19:57:00.9375038Z linux-aarch64-a3-0-n4cwm-runner-bv8g4            1/1     Running   0            6m49s
2026-02-19T19:57:00.9375471Z linux-aarch64-a3-0-n4cwm-runner-bv8g4-workflow   1/1     Running   0            6m12s
2026-02-19T19:57:00.9375919Z vllm-0                                           1/1     Running   1 (6s ago)   5m46s
2026-02-19T19:57:00.9376212Z vllm-0-1                                         1/1     Running   0            5m46s
2026-02-19T19:57:01.0004374Z leaderworkerset.leaderworkerset.x-k8s.io "vllm" deleted from vllm-project namespace
2026-02-19T19:57:01.0379048Z service "vllm-leader" deleted from vllm-project namespace
2026-02-19T19:57:01.7452642Z Post job cleanup.
2026-02-19T19:57:01.7609444Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-19T19:57:01.7610587Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-19T19:57:01.7610880Z ##[endgroup]
2026-02-19T19:57:02.2092642Z (node:1147) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-19T19:57:02.2093300Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-19T19:57:02.9007467Z [command]/usr/bin/git version
2026-02-19T19:57:02.9189028Z git version 2.34.1
2026-02-19T19:57:02.9224161Z Copying '/root/.gitconfig' to '/__w/_temp/a85fcaad-a2f3-4037-ae7c-d60525cfe2f6/.gitconfig'
2026-02-19T19:57:02.9236177Z Temporarily overriding HOME='/__w/_temp/a85fcaad-a2f3-4037-ae7c-d60525cfe2f6' before making global git config changes
2026-02-19T19:57:02.9237766Z Adding repository directory to the temporary git global config as a safe directory
2026-02-19T19:57:02.9241878Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-19T19:57:02.9285601Z Removing SSH command configuration
2026-02-19T19:57:02.9291041Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-19T19:57:02.9346837Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-19T19:57:02.9828699Z Removing HTTP extra header
2026-02-19T19:57:02.9832610Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-19T19:57:02.9860873Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-19T19:57:03.0054333Z Removing includeIf entries pointing to credentials config files
2026-02-19T19:57:03.0059227Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-19T19:57:03.0079694Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-19T19:57:03.0080023Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-19T19:57:03.0080320Z includeif.gitdir:/github/workspace/.git.path
2026-02-19T19:57:03.0080591Z includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-19T19:57:03.0087617Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-19T19:57:03.0105778Z /__w/_temp/git-credentials-f99c08ad-2fad-474c-98da-8f3d672b4975.config
2026-02-19T19:57:03.0114859Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-f99c08ad-2fad-474c-98da-8f3d672b4975.config
2026-02-19T19:57:03.0148853Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-19T19:57:03.0168750Z /__w/_temp/git-credentials-f99c08ad-2fad-474c-98da-8f3d672b4975.config
2026-02-19T19:57:03.0175590Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-f99c08ad-2fad-474c-98da-8f3d672b4975.config
2026-02-19T19:57:03.0202818Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git.path
2026-02-19T19:57:03.0220607Z /github/runner_temp/git-credentials-f99c08ad-2fad-474c-98da-8f3d672b4975.config
2026-02-19T19:57:03.0227176Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-f99c08ad-2fad-474c-98da-8f3d672b4975.config
2026-02-19T19:57:03.0258384Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-19T19:57:03.0275800Z /github/runner_temp/git-credentials-f99c08ad-2fad-474c-98da-8f3d672b4975.config
2026-02-19T19:57:03.0283378Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-f99c08ad-2fad-474c-98da-8f3d672b4975.config
2026-02-19T19:57:03.0311624Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-19T19:57:03.0486811Z Removing credentials config '/__w/_temp/git-credentials-f99c08ad-2fad-474c-98da-8f3d672b4975.config'
2026-02-19T19:57:21.5022420Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-19T19:57:21.5023278Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-19T19:57:21.5023505Z ##[endgroup]
2026-02-19T19:57:21.8930048Z Cleaning up orphan processes
