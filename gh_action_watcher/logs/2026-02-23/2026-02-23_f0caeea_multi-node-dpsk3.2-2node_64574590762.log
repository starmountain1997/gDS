# Run ID: 22315540111
# Commit: f0caeeadcb37261beebd4a6e32934fa9f460db98
# Job: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
# Date: 2026-02-23
============================================================

ï»¿2026-02-23T18:39:29.3877175Z Current runner version: '2.330.0'
2026-02-23T18:39:29.3881688Z Runner name: 'linux-aarch64-a3-0-n4cwm-runner-xvq7p'
2026-02-23T18:39:29.3882646Z Runner group name: 'Default'
2026-02-23T18:39:29.3883367Z Machine name: 'linux-aarch64-a3-0-n4cwm-runner-xvq7p'
2026-02-23T18:39:29.3886867Z ##[group]GITHUB_TOKEN Permissions
2026-02-23T18:39:29.3888765Z Actions: write
2026-02-23T18:39:29.3889224Z ArtifactMetadata: write
2026-02-23T18:39:29.3889620Z Attestations: write
2026-02-23T18:39:29.3890023Z Checks: write
2026-02-23T18:39:29.3890375Z Contents: write
2026-02-23T18:39:29.3890881Z Deployments: write
2026-02-23T18:39:29.3891399Z Discussions: write
2026-02-23T18:39:29.3891758Z Issues: write
2026-02-23T18:39:29.3892342Z Metadata: read
2026-02-23T18:39:29.3892722Z Models: read
2026-02-23T18:39:29.3893070Z Packages: write
2026-02-23T18:39:29.3893466Z Pages: write
2026-02-23T18:39:29.3893817Z PullRequests: write
2026-02-23T18:39:29.3894224Z RepositoryProjects: write
2026-02-23T18:39:29.3894785Z SecurityEvents: write
2026-02-23T18:39:29.3895271Z Statuses: write
2026-02-23T18:39:29.3895616Z ##[endgroup]
2026-02-23T18:39:29.3897448Z Secret source: Actions
2026-02-23T18:39:29.3897980Z Prepare workflow directory
2026-02-23T18:39:29.4442310Z Prepare all required actions
2026-02-23T18:39:29.4473866Z Getting action download info
2026-02-23T18:39:30.7785968Z Download action repository 'actions/checkout@v6' (SHA:de0fac2e4500dabe0009e67214ff5f5447ce83dd)
2026-02-23T18:39:36.9043465Z Download action repository 'actions/upload-artifact@v6' (SHA:b7c566a772e6b6bfb58ed0dc250532a479d7789f)
2026-02-23T18:39:43.9305875Z Uses: vllm-project/vllm-ascend/.github/workflows/_e2e_nightly_multi_node.yaml@refs/heads/main (f0caeeadcb37261beebd4a6e32934fa9f460db98)
2026-02-23T18:39:43.9309008Z ##[group] Inputs
2026-02-23T18:39:43.9309352Z   soc_version: a3
2026-02-23T18:39:43.9309594Z   runner: linux-aarch64-a3-0
2026-02-23T18:39:43.9310034Z   image: swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3
2026-02-23T18:39:43.9310561Z   config_file_path: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-23T18:39:43.9310846Z   replicas: 1
2026-02-23T18:39:43.9311057Z   size: 2
2026-02-23T18:39:43.9311293Z   vllm_version: v0.15.0
2026-02-23T18:39:43.9311655Z   vllm_ascend_remote_url: https://github.com/vllm-project/vllm-ascend.git
2026-02-23T18:39:43.9312143Z   vllm_ascend_ref: main
2026-02-23T18:39:43.9312488Z ##[endgroup]
2026-02-23T18:39:43.9312957Z Complete job name: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-23T18:39:43.9796002Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-23T18:39:43.9798330Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-23T18:39:43.9798852Z ##[endgroup]
2026-02-23T18:39:59.4631141Z (node:72) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-23T18:39:59.4631965Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-23T18:40:01.9153303Z ##[group]Run # Decode and save kubeconfig
2026-02-23T18:40:01.9153785Z [36;1m# Decode and save kubeconfig[0m
2026-02-23T18:40:01.9185955Z [36;1mecho "***" | base64 -d > "$KUBECONFIG"[0m
2026-02-23T18:40:01.9186712Z shell: bash -el {0}
2026-02-23T18:40:01.9187011Z ##[endgroup]
2026-02-23T18:40:01.9437648Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-23T18:40:01.9438774Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-23T18:40:01.9439122Z ##[endgroup]
2026-02-23T18:40:02.3246202Z (node:402) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-23T18:40:02.3247084Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-23T18:40:03.2946651Z ##[group]Run actions/checkout@v6
2026-02-23T18:40:03.2946975Z with:
2026-02-23T18:40:03.2947250Z   repository: vllm-project/vllm-ascend
2026-02-23T18:40:03.2948046Z   token: ***
2026-02-23T18:40:03.2948250Z   ssh-strict: true
2026-02-23T18:40:03.2948477Z   ssh-user: git
2026-02-23T18:40:03.2948706Z   persist-credentials: true
2026-02-23T18:40:03.2948962Z   clean: true
2026-02-23T18:40:03.2949202Z   sparse-checkout-cone-mode: true
2026-02-23T18:40:03.2949480Z   fetch-depth: 1
2026-02-23T18:40:03.2949720Z   fetch-tags: false
2026-02-23T18:40:03.2950095Z   show-progress: true
2026-02-23T18:40:03.2950329Z   lfs: false
2026-02-23T18:40:03.2950549Z   submodules: false
2026-02-23T18:40:03.2950757Z   set-safe-directory: true
2026-02-23T18:40:03.2951016Z ##[endgroup]
2026-02-23T18:40:03.2991321Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-23T18:40:03.2992293Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-23T18:40:03.2992582Z ##[endgroup]
2026-02-23T18:40:03.6511390Z (node:433) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-23T18:40:03.6512334Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-23T18:40:04.4760007Z Syncing repository: vllm-project/vllm-ascend
2026-02-23T18:40:04.4761142Z ##[group]Getting Git version info
2026-02-23T18:40:04.4761414Z Working directory is '/__w/vllm-ascend/vllm-ascend'
2026-02-23T18:40:04.4761786Z [command]/usr/bin/git version
2026-02-23T18:40:04.4892338Z git version 2.34.1
2026-02-23T18:40:04.4909687Z ##[endgroup]
2026-02-23T18:40:04.4916927Z Copying '/root/.gitconfig' to '/__w/_temp/577adf76-f5a1-4337-8fb6-e5825882b40c/.gitconfig'
2026-02-23T18:40:04.4928308Z Temporarily overriding HOME='/__w/_temp/577adf76-f5a1-4337-8fb6-e5825882b40c' before making global git config changes
2026-02-23T18:40:04.4928856Z Adding repository directory to the temporary git global config as a safe directory
2026-02-23T18:40:04.4933326Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-23T18:40:04.4969889Z Deleting the contents of '/__w/vllm-ascend/vllm-ascend'
2026-02-23T18:40:04.4972489Z ##[group]Initializing the repository
2026-02-23T18:40:04.4975905Z [command]/usr/bin/git init /__w/vllm-ascend/vllm-ascend
2026-02-23T18:40:04.5093084Z hint: Using 'master' as the name for the initial branch. This default branch name
2026-02-23T18:40:04.5093468Z hint: is subject to change. To configure the initial branch name to use in all
2026-02-23T18:40:04.5093822Z hint: of your new repositories, which will suppress this warning, call:
2026-02-23T18:40:04.5094081Z hint: 
2026-02-23T18:40:04.5094309Z hint: 	git config --global init.defaultBranch <name>
2026-02-23T18:40:04.5094535Z hint: 
2026-02-23T18:40:04.5094738Z hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and
2026-02-23T18:40:04.5095106Z hint: 'development'. The just-created branch can be renamed via this command:
2026-02-23T18:40:04.5095377Z hint: 
2026-02-23T18:40:04.5095525Z hint: 	git branch -m <name>
2026-02-23T18:40:04.5104676Z Initialized empty Git repository in /__w/vllm-ascend/vllm-ascend/.git/
2026-02-23T18:40:04.5112723Z [command]/usr/bin/git remote add origin https://github.com/vllm-project/vllm-ascend
2026-02-23T18:40:04.5155160Z ##[endgroup]
2026-02-23T18:40:04.5155451Z ##[group]Disabling automatic garbage collection
2026-02-23T18:40:04.5158341Z [command]/usr/bin/git config --local gc.auto 0
2026-02-23T18:40:04.5198208Z ##[endgroup]
2026-02-23T18:40:04.5198462Z ##[group]Setting up auth
2026-02-23T18:40:04.5240539Z Removing SSH command configuration
2026-02-23T18:40:04.5318017Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-23T18:40:04.5340030Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-23T18:40:04.5762690Z Removing HTTP extra header
2026-02-23T18:40:04.5765515Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-23T18:40:04.5789831Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-23T18:40:04.5965113Z Removing includeIf entries pointing to credentials config files
2026-02-23T18:40:04.5969349Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-23T18:40:04.5998093Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-23T18:40:04.6180679Z [command]/usr/bin/git config --file /__w/_temp/git-credentials-3e1e7297-6375-40cb-ae3e-1d0b23eb2307.config http.https://github.com/.extraheader AUTHORIZATION: basic ***
2026-02-23T18:40:04.6212597Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-3e1e7297-6375-40cb-ae3e-1d0b23eb2307.config
2026-02-23T18:40:04.6237032Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-3e1e7297-6375-40cb-ae3e-1d0b23eb2307.config
2026-02-23T18:40:04.6262149Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-3e1e7297-6375-40cb-ae3e-1d0b23eb2307.config
2026-02-23T18:40:04.6287634Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-3e1e7297-6375-40cb-ae3e-1d0b23eb2307.config
2026-02-23T18:40:04.6309187Z ##[endgroup]
2026-02-23T18:40:04.6309458Z ##[group]Fetching the repository
2026-02-23T18:40:04.6317155Z [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --no-recurse-submodules --depth=1 origin +f0caeeadcb37261beebd4a6e32934fa9f460db98:refs/remotes/origin/main
2026-02-23T18:40:07.1616842Z From https://gh-proxy.test.osinfra.cn/https://github.com/vllm-project/vllm-ascend
2026-02-23T18:40:07.1617333Z  * [new ref]         f0caeeadcb37261beebd4a6e32934fa9f460db98 -> origin/main
2026-02-23T18:40:07.1637424Z [command]/usr/bin/git branch --list --remote origin/main
2026-02-23T18:40:07.1661391Z   origin/main
2026-02-23T18:40:07.1667762Z [command]/usr/bin/git rev-parse refs/remotes/origin/main
2026-02-23T18:40:07.1687767Z f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-23T18:40:07.1691583Z ##[endgroup]
2026-02-23T18:40:07.1691855Z ##[group]Determining the checkout info
2026-02-23T18:40:07.1693081Z ##[endgroup]
2026-02-23T18:40:07.1696414Z [command]/usr/bin/git sparse-checkout disable
2026-02-23T18:40:07.1730243Z [command]/usr/bin/git config --local --unset-all extensions.worktreeConfig
2026-02-23T18:40:07.1753282Z ##[group]Checking out the ref
2026-02-23T18:40:07.1756651Z [command]/usr/bin/git checkout --progress --force -B main refs/remotes/origin/main
2026-02-23T18:40:07.2632089Z Switched to a new branch 'main'
2026-02-23T18:40:07.2632367Z Branch 'main' set up to track remote branch 'main' from 'origin'.
2026-02-23T18:40:07.2642457Z ##[endgroup]
2026-02-23T18:40:07.2672585Z [command]/usr/bin/git log -1 --format=%H
2026-02-23T18:40:07.2693079Z f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-23T18:40:07.6678234Z ##[group]Run # prepare for lws entrypoint scripts
2026-02-23T18:40:07.6678578Z [36;1m# prepare for lws entrypoint scripts[0m
2026-02-23T18:40:07.6678923Z [36;1minstall -D tests/e2e/nightly/multi_node/scripts/run.sh /root/.cache/tests/run.sh[0m
2026-02-23T18:40:07.6679343Z shell: bash -el {0}
2026-02-23T18:40:07.6679501Z ##[endgroup]
2026-02-23T18:40:07.6765569Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-23T18:40:07.6766436Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-23T18:40:07.6766711Z ##[endgroup]
2026-02-23T18:40:08.0236169Z (node:474) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-23T18:40:08.0236826Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-23T18:40:08.9547100Z ##[group]Run set -euo pipefail
2026-02-23T18:40:08.9547356Z [36;1mset -euo pipefail[0m
2026-02-23T18:40:08.9547527Z [36;1m[0m
2026-02-23T18:40:08.9547784Z [36;1mCRD_NAME="${CRD_NAME:-vllm}"[0m
2026-02-23T18:40:08.9548046Z [36;1mTIMEOUT=${TIMEOUT:-120}[0m
2026-02-23T18:40:08.9548319Z [36;1mSLEEP_INTERVAL=2[0m
2026-02-23T18:40:08.9548565Z [36;1m[0m
2026-02-23T18:40:08.9548858Z [36;1mecho "Deleting leaderworkerset [$CRD_NAME] in namespace [$NAMESPACE]..."[0m
2026-02-23T18:40:08.9549348Z [36;1mkubectl delete leaderworkerset "$CRD_NAME" -n "$NAMESPACE" --ignore-not-found[0m
2026-02-23T18:40:08.9549759Z [36;1m[0m
2026-02-23T18:40:08.9550036Z [36;1mecho "Waiting for all pods starting with 'vllm' to be deleted..."[0m
2026-02-23T18:40:08.9550378Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-23T18:40:08.9550621Z [36;1m[0m
2026-02-23T18:40:08.9550816Z [36;1mwhile true; do[0m
2026-02-23T18:40:08.9551054Z [36;1m  NOW=$(date +%s)[0m
2026-02-23T18:40:08.9551381Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-23T18:40:08.9551716Z [36;1m[0m
2026-02-23T18:40:08.9551952Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-23T18:40:08.9552537Z [36;1m    echo "Timeout reached ($TIMEOUT seconds), some pods still exist:"[0m
2026-02-23T18:40:08.9552930Z [36;1m    kubectl get pods -n "$NAMESPACE" | grep '^vllm' || true[0m
2026-02-23T18:40:08.9553223Z [36;1m    exit 1[0m
2026-02-23T18:40:08.9553443Z [36;1m  fi[0m
2026-02-23T18:40:08.9553648Z [36;1m[0m
2026-02-23T18:40:08.9554062Z [36;1m  PODS_EXIST=$(kubectl get pods -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null | tr ' ' '\n' | grep '^vllm' || true)[0m
2026-02-23T18:40:08.9554542Z [36;1m[0m
2026-02-23T18:40:08.9554763Z [36;1m  if [[ -z "$PODS_EXIST" ]]; then[0m
2026-02-23T18:40:08.9555015Z [36;1m    echo "All vllm pods deleted."[0m
2026-02-23T18:40:08.9555298Z [36;1m    break[0m
2026-02-23T18:40:08.9555512Z [36;1m  else[0m
2026-02-23T18:40:08.9555844Z [36;1m    echo "Waiting for pods to be deleted: $PODS_EXIST"[0m
2026-02-23T18:40:08.9556169Z [36;1m    sleep $SLEEP_INTERVAL[0m
2026-02-23T18:40:08.9556419Z [36;1m  fi[0m
2026-02-23T18:40:08.9556603Z [36;1mdone[0m
2026-02-23T18:40:08.9557004Z shell: bash -el {0}
2026-02-23T18:40:08.9557229Z ##[endgroup]
2026-02-23T18:40:08.9642457Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-23T18:40:08.9643202Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-23T18:40:08.9643490Z ##[endgroup]
2026-02-23T18:40:09.3098897Z (node:528) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-23T18:40:09.3099713Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-23T18:40:09.8227236Z Deleting leaderworkerset [vllm] in namespace [vllm-project]...
2026-02-23T18:40:10.1285937Z Waiting for all pods starting with 'vllm' to be deleted...
2026-02-23T18:40:10.2195269Z All vllm pods deleted.
2026-02-23T18:40:10.6193878Z ##[group]Run set -e
2026-02-23T18:40:10.6194149Z [36;1mset -e[0m
2026-02-23T18:40:10.6194314Z [36;1m[0m
2026-02-23T18:40:10.6194459Z [36;1msize="2"[0m
2026-02-23T18:40:10.6194621Z [36;1mreplicas="1"[0m
2026-02-23T18:40:10.6194954Z [36;1mimage="swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3"[0m
2026-02-23T18:40:10.6195374Z [36;1mconfig_file_path="DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-23T18:40:10.6195700Z [36;1mfail_tag=FAIL_TAG_"DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-23T18:40:10.6195996Z [36;1mecho "FAIL_TAG=${fail_tag}" >> "$GITHUB_ENV"[0m
2026-02-23T18:40:10.6196214Z [36;1m[0m
2026-02-23T18:40:10.6196441Z [36;1mrequired_params=("size" "replicas" "image" "config_file_path")[0m
2026-02-23T18:40:10.6196744Z [36;1mfor param in "${required_params[@]}"; do[0m
2026-02-23T18:40:10.6196970Z [36;1m  if [ -z "${!param}" ]; then[0m
2026-02-23T18:40:10.6197430Z [36;1m    echo "Error: Parameter '$param' is required but empty"[0m
2026-02-23T18:40:10.6197678Z [36;1m    exit 1[0m
2026-02-23T18:40:10.6197840Z [36;1m  fi[0m
2026-02-23T18:40:10.6197994Z [36;1mdone[0m
2026-02-23T18:40:10.6198136Z [36;1m[0m
2026-02-23T18:40:10.6198292Z [36;1mif [ "a3" = "a3" ]; then[0m
2026-02-23T18:40:10.6198485Z [36;1m  npu_per_node=16[0m
2026-02-23T18:40:10.6198758Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws.yaml.jinja2"[0m
2026-02-23T18:40:10.6199041Z [36;1melse[0m
2026-02-23T18:40:10.6199192Z [36;1m  npu_per_node=8[0m
2026-02-23T18:40:10.6199465Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws-a2.yaml.jinja2"[0m
2026-02-23T18:40:10.6199756Z [36;1mfi[0m
2026-02-23T18:40:10.6199895Z [36;1m[0m
2026-02-23T18:40:10.6200052Z [36;1mjinja2 $TEMPLATE_FILE \[0m
2026-02-23T18:40:10.6200254Z [36;1m  -D size="$size" \[0m
2026-02-23T18:40:10.6200439Z [36;1m  -D replicas="$replicas" \[0m
2026-02-23T18:40:10.6200647Z [36;1m  -D image="$image" \[0m
2026-02-23T18:40:10.6200868Z [36;1m  -D config_file_path="$config_file_path" \[0m
2026-02-23T18:40:10.6201112Z [36;1m  -D npu_per_node="$npu_per_node" \[0m
2026-02-23T18:40:10.6201330Z [36;1m  -D fail_tag="$fail_tag" \[0m
2026-02-23T18:40:10.6201526Z [36;1m  --outfile lws.yaml[0m
2026-02-23T18:40:10.6201705Z [36;1m[0m
2026-02-23T18:40:10.6201862Z [36;1mkubectl apply -f ./lws.yaml[0m
2026-02-23T18:40:10.6202403Z shell: bash -el {0}
2026-02-23T18:40:10.6202579Z ##[endgroup]
2026-02-23T18:40:10.6274710Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-23T18:40:10.6275660Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-23T18:40:10.6275867Z ##[endgroup]
2026-02-23T18:40:10.9743633Z (node:594) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-23T18:40:10.9744286Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-23T18:40:11.9240032Z leaderworkerset.leaderworkerset.x-k8s.io/vllm created
2026-02-23T18:40:11.9428812Z service/vllm-leader created
2026-02-23T18:40:12.4218553Z ##[group]Run POD_PREFIX="${POD_PREFIX:-vllm-0}"
2026-02-23T18:40:12.4218857Z [36;1mPOD_PREFIX="${POD_PREFIX:-vllm-0}"[0m
2026-02-23T18:40:12.4219065Z [36;1mSIZE="2"[0m
2026-02-23T18:40:12.4219317Z [36;1mTIMEOUT=1200  # default timeout 20 minutes[0m
2026-02-23T18:40:12.4219522Z [36;1m[0m
2026-02-23T18:40:12.4219829Z [36;1mecho "Waiting for Pods in namespace [$NAMESPACE] to become Running and Ready (timeout ${TIMEOUT}s)..."[0m
2026-02-23T18:40:12.4220174Z [36;1m[0m
2026-02-23T18:40:12.4220311Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-23T18:40:12.4220485Z [36;1m[0m
2026-02-23T18:40:12.4220622Z [36;1mwhile true; do[0m
2026-02-23T18:40:12.4220782Z [36;1m  NOW=$(date +%s)[0m
2026-02-23T18:40:12.4220970Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-23T18:40:12.4221190Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-23T18:40:12.4221434Z [36;1m    echo "Timeout reached after ${ELAPSED}s"[0m
2026-02-23T18:40:12.4221707Z [36;1m    echo "Dumping pod status for debugging:"[0m
2026-02-23T18:40:12.4221934Z [36;1m    kubectl get pods -n "$NAMESPACE"[0m
2026-02-23T18:40:12.4222300Z [36;1m    kubectl describe pod "$LEADER_POD" -n "$NAMESPACE"[0m
2026-02-23T18:40:12.4222604Z [36;1m    exit 1[0m
2026-02-23T18:40:12.4222748Z [36;1m  fi[0m
2026-02-23T18:40:12.4222887Z [36;1m[0m
2026-02-23T18:40:12.4223028Z [36;1m  # 1) check follower pods[0m
2026-02-23T18:40:12.4223225Z [36;1m  ALL_FOLLOWERS_READY=true[0m
2026-02-23T18:40:12.4223418Z [36;1m  for ((i=1; i<SIZE; i++)); do[0m
2026-02-23T18:40:12.4223609Z [36;1m    POD="${POD_PREFIX}-${i}"[0m
2026-02-23T18:40:12.4223972Z [36;1m    PHASE=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-23T18:40:12.4224503Z [36;1m    READY=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-23T18:40:12.4225023Z [36;1m[0m
2026-02-23T18:40:12.4225216Z [36;1m    echo "Follower [$POD] phase=$PHASE ready=$READY"[0m
2026-02-23T18:40:12.4225444Z [36;1m[0m
2026-02-23T18:40:12.4225626Z [36;1m    if [[ "$PHASE" != "Running" || "$READY" != "true" ]]; then[0m
2026-02-23T18:40:12.4225901Z [36;1m      echo "Follower [$POD] not Ready yet..."[0m
2026-02-23T18:40:12.4226129Z [36;1m      ALL_FOLLOWERS_READY=false[0m
2026-02-23T18:40:12.4226311Z [36;1m      break[0m
2026-02-23T18:40:12.4226463Z [36;1m    fi[0m
2026-02-23T18:40:12.4226598Z [36;1m  done[0m
2026-02-23T18:40:12.4226733Z [36;1m[0m
2026-02-23T18:40:12.4226874Z [36;1m  # 2) check leader pod[0m
2026-02-23T18:40:12.4227249Z [36;1m  LEADER_PHASE=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-23T18:40:12.4227836Z [36;1m  LEADER_READY=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-23T18:40:12.4228222Z [36;1m[0m
2026-02-23T18:40:12.4228439Z [36;1m  echo "Leader [$LEADER_POD] phase=$LEADER_PHASE ready=$LEADER_READY"[0m
2026-02-23T18:40:12.4228699Z [36;1m[0m
2026-02-23T18:40:12.4228908Z [36;1m  if [[ "$LEADER_PHASE" != "Running" || "$LEADER_READY" != "true" ]]; then[0m
2026-02-23T18:40:12.4229179Z [36;1m    echo "Leader not Ready yet..."[0m
2026-02-23T18:40:12.4229390Z [36;1m    ALL_FOLLOWERS_READY=false[0m
2026-02-23T18:40:12.4229573Z [36;1m  fi[0m
2026-02-23T18:40:12.4229700Z [36;1m[0m
2026-02-23T18:40:12.4229865Z [36;1m  if [[ "$ALL_FOLLOWERS_READY" == "true" ]]; then[0m
2026-02-23T18:40:12.4230184Z [36;1m    echo "All follower pods and leader pod are Running and Ready â€” continuing."[0m
2026-02-23T18:40:12.4230463Z [36;1m    break[0m
2026-02-23T18:40:12.4230610Z [36;1m  fi[0m
2026-02-23T18:40:12.4230735Z [36;1m[0m
2026-02-23T18:40:12.4230868Z [36;1m  sleep 2[0m
2026-02-23T18:40:12.4231014Z [36;1mdone[0m
2026-02-23T18:40:12.4231288Z shell: bash -el {0}
2026-02-23T18:40:12.4231441Z env:
2026-02-23T18:40:12.4231766Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-23T18:40:12.4232220Z ##[endgroup]
2026-02-23T18:40:12.4307759Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-23T18:40:12.4308456Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-23T18:40:12.4308672Z ##[endgroup]
2026-02-23T18:40:12.7780194Z (node:672) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-23T18:40:12.7780872Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-23T18:40:13.3215458Z Waiting for Pods in namespace [vllm-project] to become Running and Ready (timeout 1200s)...
2026-02-23T18:40:13.4353693Z Follower [vllm-0-1] phase=Pending ready=
2026-02-23T18:40:13.4353929Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:13.5432534Z Leader [vllm-0] phase=Pending ready=
2026-02-23T18:40:13.5432846Z Leader not Ready yet...
2026-02-23T18:40:15.6567552Z Follower [vllm-0-1] phase=Pending ready=
2026-02-23T18:40:15.6567885Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:15.7688965Z Leader [vllm-0] phase=Pending ready=
2026-02-23T18:40:15.7689198Z Leader not Ready yet...
2026-02-23T18:40:17.8812436Z Follower [vllm-0-1] phase=Pending ready=
2026-02-23T18:40:17.8812754Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:17.9922736Z Leader [vllm-0] phase=Pending ready=
2026-02-23T18:40:17.9922971Z Leader not Ready yet...
2026-02-23T18:40:20.1009574Z Follower [vllm-0-1] phase=Pending ready=
2026-02-23T18:40:20.1009841Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:20.2070348Z Leader [vllm-0] phase=Pending ready=
2026-02-23T18:40:20.2070594Z Leader not Ready yet...
2026-02-23T18:40:22.3206579Z Follower [vllm-0-1] phase=Pending ready=
2026-02-23T18:40:22.3206859Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:22.4307706Z Leader [vllm-0] phase=Pending ready=
2026-02-23T18:40:22.4308286Z Leader not Ready yet...
2026-02-23T18:40:24.5449108Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:40:24.5449424Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:24.6625376Z Leader [vllm-0] phase=Pending ready=
2026-02-23T18:40:24.6625622Z Leader not Ready yet...
2026-02-23T18:40:26.7843698Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:40:26.7844009Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:26.8967790Z Leader [vllm-0] phase=Pending ready=false
2026-02-23T18:40:26.8968052Z Leader not Ready yet...
2026-02-23T18:40:29.0223800Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:40:29.0224078Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:29.1767328Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:40:31.3025830Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:40:31.3026109Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:31.4222407Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:40:33.5364536Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:40:33.5364825Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:33.6499644Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:40:35.7687232Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:40:35.7687513Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:35.8853388Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:40:38.0008383Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:40:38.0008671Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:38.1239279Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:40:40.2408276Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:40:40.2408563Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:40.3545639Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:40:42.4670355Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:40:42.4670703Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:42.5767605Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:40:44.6953711Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:40:44.6954294Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:44.8140910Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:40:46.9354385Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:40:46.9354665Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:47.0456205Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:40:49.1658736Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:40:49.1659031Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:49.2729448Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:40:51.3938027Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:40:51.3938311Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:51.5111419Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:40:53.6389326Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:40:53.6389622Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:53.7509044Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:40:55.8699884Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:40:55.8700281Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:55.9834333Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:40:58.0983523Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:40:58.0983812Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:40:58.2102595Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:41:00.3253497Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:41:00.3253890Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:41:00.4424035Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:41:02.5611121Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:41:02.5611402Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:41:02.6805927Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:41:04.8015491Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:41:04.8015773Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:41:04.9146545Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:41:07.0338863Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:41:07.0339163Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:41:07.1471478Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:41:09.2656854Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:41:09.2657137Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:41:09.3831962Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:41:11.5382258Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-23T18:41:11.5382548Z Follower [vllm-0-1] not Ready yet...
2026-02-23T18:41:11.6555082Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:41:13.7739344Z Follower [vllm-0-1] phase=Running ready=true
2026-02-23T18:41:13.8868848Z Leader [vllm-0] phase=Running ready=true
2026-02-23T18:41:13.8869508Z All follower pods and leader pod are Running and Ready â€” continuing.
2026-02-23T18:41:14.3167420Z ##[group]Run set -euo pipefail
2026-02-23T18:41:14.3167699Z [36;1mset -euo pipefail[0m
2026-02-23T18:41:14.3167863Z [36;1m[0m
2026-02-23T18:41:14.3168003Z [36;1msize="2"[0m
2026-02-23T18:41:14.3168148Z [36;1mpids=()[0m
2026-02-23T18:41:14.3168283Z [36;1m[0m
2026-02-23T18:41:14.3168420Z [36;1mcleanup() {[0m
2026-02-23T18:41:14.3168612Z [36;1m  echo "Cleaning up background log streams..."[0m
2026-02-23T18:41:14.3168852Z [36;1m  for pid in "${pids[@]}"; do[0m
2026-02-23T18:41:14.3169060Z [36;1m    kill "$pid" 2>/dev/null || true[0m
2026-02-23T18:41:14.3169330Z [36;1m  done[0m
2026-02-23T18:41:14.3169483Z [36;1m}[0m
2026-02-23T18:41:14.3169628Z [36;1mtrap cleanup EXIT[0m
2026-02-23T18:41:14.3169787Z [36;1m[0m
2026-02-23T18:41:14.3169938Z [36;1mfor i in $(seq 1 $((size - 1))); do[0m
2026-02-23T18:41:14.3170143Z [36;1m  POD="vllm-0-${i}"[0m
2026-02-23T18:41:14.3170302Z [36;1m[0m
2026-02-23T18:41:14.3170521Z [36;1m  echo "==== Collecting logs from worker pod: $POD ===="[0m
2026-02-23T18:41:14.3170789Z [36;1m  kubectl logs -f "$POD" -n "$NAMESPACE" \[0m
2026-02-23T18:41:14.3171022Z [36;1m    > "/tmp/${POD}_logs.txt" 2>&1 &[0m
2026-02-23T18:41:14.3171203Z [36;1m[0m
2026-02-23T18:41:14.3171340Z [36;1m  pids+=($!)[0m
2026-02-23T18:41:14.3171493Z [36;1mdone[0m
2026-02-23T18:41:14.3171622Z [36;1m[0m
2026-02-23T18:41:14.3171820Z [36;1mecho "==== Streaming logs from leader pod: $LEADER_POD ===="[0m
2026-02-23T18:41:14.3172213Z [36;1mecho "Looking for logs containing: $FAIL_TAG"[0m
2026-02-23T18:41:14.3172422Z [36;1m[0m
2026-02-23T18:41:14.3172661Z [36;1mkubectl logs -f "$LEADER_POD" -n "$NAMESPACE" | while IFS= read -r line; do[0m
2026-02-23T18:41:14.3172956Z [36;1m  echo "$line"[0m
2026-02-23T18:41:14.3173152Z [36;1m  if echo "$line" | grep -q "$FAIL_TAG"; then[0m
2026-02-23T18:41:14.3173369Z [36;1m    exit 1[0m
2026-02-23T18:41:14.3173515Z [36;1m  fi[0m
2026-02-23T18:41:14.3173653Z [36;1mdone[0m
2026-02-23T18:41:14.3173956Z shell: bash -el {0}
2026-02-23T18:41:14.3174099Z env:
2026-02-23T18:41:14.3174297Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-23T18:41:14.3174534Z ##[endgroup]
2026-02-23T18:41:14.3261581Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-23T18:41:14.3262405Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-23T18:41:14.3262622Z ##[endgroup]
2026-02-23T18:41:14.6798586Z (node:778) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-23T18:41:14.6799288Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-23T18:41:15.2397995Z ==== Collecting logs from worker pod: vllm-0-1 ====
2026-02-23T18:41:15.2398552Z ==== Streaming logs from leader pod: vllm-0 ====
2026-02-23T18:41:15.2398858Z Looking for logs containing: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-23T18:41:15.3213630Z /usr/local/Ascend/ascend-toolkit/set_env.sh: line 31: CMAKE_PREFIX_PATH: unbound variable
2026-02-23T18:41:15.3225320Z ====> Check NPU info
2026-02-23T18:41:15.3235806Z +------------------------------------------------------------------------------------------------+
2026-02-23T18:41:15.3246493Z | npu-smi 25.5.0                   Version: 25.5.0                                               |
2026-02-23T18:41:15.3256570Z +---------------------------+---------------+----------------------------------------------------+
2026-02-23T18:41:15.3266402Z | NPU   Name                | Health        | Power(W)    Temp(C)           Hugepages-Usage(page)|
2026-02-23T18:41:15.3276166Z | Chip  Phy-ID              | Bus-Id        | AICore(%)   Memory-Usage(MB)  HBM-Usage(MB)        |
2026-02-23T18:41:15.3286520Z +===========================+===============+====================================================+
2026-02-23T18:41:15.3296658Z | 0     Ascend910           | OK            | 167.6       36                0    / 0             |
2026-02-23T18:41:15.3305961Z | 0     0                   | 0000:9D:00.0  | 0           0    / 0          3169 / 65536         |
2026-02-23T18:41:15.3315534Z +------------------------------------------------------------------------------------------------+
2026-02-23T18:41:15.3326460Z | 0     Ascend910           | OK            | -           36                0    / 0             |
2026-02-23T18:41:15.3335731Z | 1     1                   | 0000:9F:00.0  | 0           0    / 0          2887 / 65536         |
2026-02-23T18:41:15.3345483Z +===========================+===============+====================================================+
2026-02-23T18:41:15.3355350Z | 1     Ascend910           | OK            | 168.2       35                0    / 0             |
2026-02-23T18:41:15.3365737Z | 0     2                   | 0000:99:00.0  | 0           0    / 0          3157 / 65536         |
2026-02-23T18:41:15.3375191Z +------------------------------------------------------------------------------------------------+
2026-02-23T18:41:15.3385338Z | 1     Ascend910           | OK            | -           36                0    / 0             |
2026-02-23T18:41:15.3394808Z | 1     3                   | 0000:9B:00.0  | 0           0    / 0          2891 / 65536         |
2026-02-23T18:41:15.3405321Z +===========================+===============+====================================================+
2026-02-23T18:41:15.3415006Z | 2     Ascend910           | OK            | 166.5       36                0    / 0             |
2026-02-23T18:41:15.3424672Z | 0     4                   | 0000:95:00.0  | 0           0    / 0          3164 / 65536         |
2026-02-23T18:41:15.3449370Z +------------------------------------------------------------------------------------------------+
2026-02-23T18:41:15.3469369Z | 2     Ascend910           | OK            | -           34                0    / 0             |
2026-02-23T18:41:15.3501666Z | 1     5                   | 0000:97:00.0  | 0           0    / 0          2886 / 65536         |
2026-02-23T18:41:15.3502321Z +===========================+===============+====================================================+
2026-02-23T18:41:15.3502763Z | 3     Ascend910           | OK            | 157.1       36                0    / 0             |
2026-02-23T18:41:15.3503090Z | 0     6                   | 0000:91:00.0  | 0           0    / 0          3161 / 65536         |
2026-02-23T18:41:15.3503414Z +------------------------------------------------------------------------------------------------+
2026-02-23T18:41:15.3503759Z | 3     Ascend910           | OK            | -           34                0    / 0             |
2026-02-23T18:41:15.3510376Z | 1     7                   | 0000:93:00.0  | 0           0    / 0          2887 / 65536         |
2026-02-23T18:41:15.3520664Z +===========================+===============+====================================================+
2026-02-23T18:41:15.3570323Z | 4     Ascend910           | OK            | 165.5       36                0    / 0             |
2026-02-23T18:41:15.3570675Z | 0     8                   | 0000:8D:00.0  | 0           0    / 0          3168 / 65536         |
2026-02-23T18:41:15.3571308Z +------------------------------------------------------------------------------------------------+
2026-02-23T18:41:15.3571644Z | 4     Ascend910           | OK            | -           34                0    / 0             |
2026-02-23T18:41:15.3571949Z | 1     9                   | 0000:8F:00.0  | 0           0    / 0          2888 / 65536         |
2026-02-23T18:41:15.3580182Z +===========================+===============+====================================================+
2026-02-23T18:41:15.3589167Z | 5     Ascend910           | OK            | 168.7       36                0    / 0             |
2026-02-23T18:41:15.3599777Z | 0     10                  | 0000:89:00.0  | 0           0    / 0          3151 / 65536         |
2026-02-23T18:41:15.3610054Z +------------------------------------------------------------------------------------------------+
2026-02-23T18:41:15.3619715Z | 5     Ascend910           | OK            | -           37                0    / 0             |
2026-02-23T18:41:15.3629061Z | 1     11                  | 0000:8B:00.0  | 0           0    / 0          2900 / 65536         |
2026-02-23T18:41:15.3638769Z +===========================+===============+====================================================+
2026-02-23T18:41:15.3648512Z | 6     Ascend910           | OK            | 163.9       36                0    / 0             |
2026-02-23T18:41:15.3658441Z | 0     12                  | 0000:85:00.0  | 0           0    / 0          3166 / 65536         |
2026-02-23T18:41:15.3668437Z +------------------------------------------------------------------------------------------------+
2026-02-23T18:41:15.3676873Z | 6     Ascend910           | OK            | -           35                0    / 0             |
2026-02-23T18:41:15.3686818Z | 1     13                  | 0000:87:00.0  | 0           0    / 0          2890 / 65536         |
2026-02-23T18:41:15.3696659Z +===========================+===============+====================================================+
2026-02-23T18:41:15.3705936Z | 7     Ascend910           | OK            | 165.4       34                0    / 0             |
2026-02-23T18:41:15.3715445Z | 0     14                  | 0000:81:00.0  | 0           0    / 0          3149 / 65536         |
2026-02-23T18:41:15.3727120Z +------------------------------------------------------------------------------------------------+
2026-02-23T18:41:15.3735107Z | 7     Ascend910           | OK            | -           37                0    / 0             |
2026-02-23T18:41:15.3743910Z | 1     15                  | 0000:83:00.0  | 0           0    / 0          2899 / 65536         |
2026-02-23T18:41:15.3753032Z +===========================+===============+====================================================+
2026-02-23T18:41:15.3765525Z +---------------------------+---------------+----------------------------------------------------+
2026-02-23T18:41:15.3774209Z | NPU     Chip              | Process id    | Process name             | Process memory(MB)      |
2026-02-23T18:41:15.3784361Z +===========================+===============+====================================================+
2026-02-23T18:41:15.3794456Z | No running processes found in NPU 0                                                            |
2026-02-23T18:41:15.3804980Z +===========================+===============+====================================================+
2026-02-23T18:41:15.3815770Z | No running processes found in NPU 1                                                            |
2026-02-23T18:41:15.3825026Z +===========================+===============+====================================================+
2026-02-23T18:41:15.3834473Z | No running processes found in NPU 2                                                            |
2026-02-23T18:41:15.3844314Z +===========================+===============+====================================================+
2026-02-23T18:41:15.3853874Z | No running processes found in NPU 3                                                            |
2026-02-23T18:41:15.3864345Z +===========================+===============+====================================================+
2026-02-23T18:41:15.3873876Z | No running processes found in NPU 4                                                            |
2026-02-23T18:41:15.3883902Z +===========================+===============+====================================================+
2026-02-23T18:41:15.3893580Z | No running processes found in NPU 5                                                            |
2026-02-23T18:41:15.3903086Z +===========================+===============+====================================================+
2026-02-23T18:41:15.3912848Z | No running processes found in NPU 6                                                            |
2026-02-23T18:41:15.3923533Z +===========================+===============+====================================================+
2026-02-23T18:41:15.3933110Z | No running processes found in NPU 7                                                            |
2026-02-23T18:41:15.3941778Z +===========================+===============+====================================================+
2026-02-23T18:41:15.3950920Z package_name=Ascend-cann-toolkit
2026-02-23T18:41:15.3960585Z version=8.5.0
2026-02-23T18:41:15.3970010Z innerversion=V100R001C25SPC001B232
2026-02-23T18:41:15.3979826Z compatible_version=[V100R001C15],[V100R001C18],[V100R001C19],[V100R001C20],[V100R001C21],[V100R001C23]
2026-02-23T18:41:15.3989118Z arch=aarch64
2026-02-23T18:41:15.3998711Z os=linux
2026-02-23T18:41:15.4008458Z path=/usr/local/Ascend/cann-8.5.0
2026-02-23T18:41:15.4017784Z ====> Configure mirrors and git proxy
2026-02-23T18:41:15.4026646Z Writing to /root/.config/pip/pip.conf
2026-02-23T18:41:15.4035817Z Installed vLLM-related Python packages:
2026-02-23T18:41:15.4046227Z ais_bench_benchmark               3.0.20250930                /vllm-workspace/vllm-ascend/benchmark
2026-02-23T18:41:15.4055610Z vllm                              0.15.0+empty                /vllm-workspace/vllm
2026-02-23T18:41:15.4065144Z vllm_ascend                       0.14.0rc2.dev171+gf0caeeadc /vllm-workspace/vllm-ascend
2026-02-23T18:41:15.4074275Z 
2026-02-23T18:41:15.4084759Z ============================
2026-02-23T18:41:15.4093845Z vLLM Git information
2026-02-23T18:41:15.4103739Z ============================
2026-02-23T18:41:15.4112928Z Branch:      HEAD
2026-02-23T18:41:15.4122561Z Commit hash: f176443446f659dbab5315e056e605d8984fd976
2026-02-23T18:41:15.4132063Z Author:      TJian <tunjian.tan@embeddedllm.com>
2026-02-23T18:41:15.4141159Z Date:        2026-01-29 14:45:42 +0800
2026-02-23T18:41:15.4150717Z Message:     [Release] [CI] Optim release pipeline (#33156)
2026-02-23T18:41:15.4159997Z Tags:        v0.15.0
2026-02-23T18:41:15.4170345Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm.git (fetch)
2026-02-23T18:41:15.4179304Z 
2026-02-23T18:41:15.4220497Z 
2026-02-23T18:41:15.4221098Z ============================
2026-02-23T18:41:15.4223004Z vLLM-Ascend Git information
2026-02-23T18:41:15.4223404Z ============================
2026-02-23T18:41:15.4225804Z Branch:      main
2026-02-23T18:41:15.4235167Z Commit hash: f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-23T18:41:15.4258142Z Author:      Nengjun Ma <nengjunma@outlook.com>
2026-02-23T18:41:15.4268717Z Date:        2026-02-14 18:54:04 +0800
2026-02-23T18:41:15.4268946Z Message:     [CI] unlock when load model (#6771)
2026-02-23T18:41:15.4285000Z Tags:        
2026-02-23T18:41:15.4285615Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm-ascend (fetch)
2026-02-23T18:41:15.4291540Z 
2026-02-23T18:41:15.4301028Z ====> Check triton ascend info
2026-02-23T18:41:15.4310386Z Ubuntu clang version 15.0.7
2026-02-23T18:41:15.4319717Z Target: aarch64-unknown-linux-gnu
2026-02-23T18:41:15.4329186Z Thread model: posix
2026-02-23T18:41:15.4338397Z InstalledDir: /usr/bin
2026-02-23T18:41:15.4347703Z Found candidate GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-23T18:41:15.4357437Z Selected GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-23T18:41:15.4366824Z Candidate multilib: .;@m64
2026-02-23T18:41:15.4376631Z Selected multilib: .;@m64
2026-02-23T18:41:15.4385234Z /usr/local/Ascend/cann-8.5.0/tools/bishengir/bin/bishengir-compile
2026-02-23T18:41:15.4393924Z Name: triton-ascend
2026-02-23T18:41:15.4403690Z Version: 3.2.0
2026-02-23T18:41:15.4413628Z Summary: A language and compiler for custom Deep Learning operations on Ascend hardwares
2026-02-23T18:41:15.4422584Z Home-page: https://gitcode.com/Ascend/triton-ascend/
2026-02-23T18:41:15.4431622Z Author: 
2026-02-23T18:41:15.4440914Z Author-email: 
2026-02-23T18:41:15.4450000Z License: 
2026-02-23T18:41:15.4459378Z Location: /usr/local/python3.11.14/lib/python3.11/site-packages
2026-02-23T18:41:15.4468429Z Requires: 
2026-02-23T18:41:15.4477726Z Required-by: vllm_ascend
2026-02-23T18:41:15.4488368Z INFO 02-23 18:40:52 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:41:15.4497660Z INFO 02-23 18:40:52 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:41:15.4507161Z INFO 02-23 18:40:52 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:41:15.4516433Z INFO 02-23 18:40:52 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:41:15.4527345Z ============================= test session starts ==============================
2026-02-23T18:41:15.4536609Z platform linux -- Python 3.11.14, pytest-8.4.2, pluggy-1.6.0 -- /usr/local/python3.11.14/bin/python3
2026-02-23T18:41:15.4545545Z cachedir: .pytest_cache
2026-02-23T18:41:15.4554866Z rootdir: /vllm-workspace/vllm-ascend
2026-02-23T18:41:15.4564905Z configfile: pyproject.toml
2026-02-23T18:41:15.4574186Z plugins: asyncio-1.3.0, cov-7.0.0, mock-3.15.1, anyio-4.12.1
2026-02-23T18:41:15.4583888Z asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
2026-02-23T18:41:15.4598481Z collecting ... collected 1 item
2026-02-23T18:41:15.4602103Z 
2026-02-23T18:41:15.4612546Z [2026-02-23 18:40:59] INFO multi_node_config.py:294: Loading config yaml: tests/e2e/nightly/multi_node/config/DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-23T18:41:15.4644904Z [2026-02-23 18:40:59] INFO multi_node_config.py:351: Resolving cluster IPs via DNS...
2026-02-23T18:41:15.4646323Z [2026-02-23 18:41:14] INFO multi_node_config.py:212: Node 0 envs: {'HCCL_OP_EXPANSION_MODE': 'AIV', 'VLLM_USE_MODELSCOPE': 'True', 'HCCL_BUFFSIZE': '1024', 'SERVER_PORT': '8080', 'OMP_PROC_BIND': 'False', 'OMP_NUM_THREADS': '1', 'PYTORCH_NPU_ALLOC_CONF': 'expandable_segments:True', 'VLLM_ASCEND_ENABLE_FLASHCOMM1': '1', 'ASCEND_A3_EBA_ENABLE': '1', 'HCCL_IF_IP': '10.0.0.59', 'HCCL_SOCKET_IFNAME': 'eth0', 'GLOO_SOCKET_IFNAME': 'eth0', 'TP_SOCKET_IFNAME': 'eth0', 'LOCAL_IP': '10.0.0.59', 'NIC_NAME': 'eth0', 'MASTER_IP': '10.0.0.59'}
2026-02-23T18:41:15.4647622Z [2026-02-23 18:41:14] INFO multi_node_config.py:137: Not launching proxy on non-master node
2026-02-23T18:41:15.4655059Z [2026-02-23 18:41:14] INFO conftest.py:241: Starting server with command: vllm serve vllm-ascend/DeepSeek-V3.2-W8A8 --host 0.0.0.0 --port 8080 --data-parallel-size 4 --data-parallel-size-local 2 --data-parallel-address 10.0.0.59 --data-parallel-rpc-port 13399 --tensor-parallel-size 8 --quantization ascend --seed 1024 --enable-expert-parallel --max-num-seqs 16 --max-model-len 8192 --max-num-batched-tokens 4096 --no-enable-prefix-caching --gpu-memory-utilization 0.85 --trust-remote-code --speculative-config {"num_speculative_tokens": 2, "method":"deepseek_mtp"} --compilation-config {"cudagraph_capture_sizes": [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], "cudagraph_mode": "FULL_DECODE_ONLY"} --additional-config {"layer_sharding": ["q_b_proj", "o_proj"]} --tokenizer-mode deepseek_v32 --reasoning-parser deepseek_v3
2026-02-23T18:41:19.4199234Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node INFO 02-23 18:41:19 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:41:19.4205541Z INFO 02-23 18:41:19 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:41:19.4215339Z INFO 02-23 18:41:19 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:41:19.4225493Z INFO 02-23 18:41:19 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:41:25.7280419Z 2026-02-23 18:41:25,726 - 138 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:41:25.7605044Z INFO 02-23 18:41:25 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:41:25.9028883Z INFO 02-23 18:41:25 [serve.py:100] Defaulting api_server_count to data_parallel_size (4).
2026-02-23T18:41:25.9051579Z INFO 02-23 18:41:25 [utils.py:325] 
2026-02-23T18:41:25.9063944Z INFO 02-23 18:41:25 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
2026-02-23T18:41:25.9072683Z INFO 02-23 18:41:25 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
2026-02-23T18:41:25.9083565Z INFO 02-23 18:41:25 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   vllm-ascend/DeepSeek-V3.2-W8A8
2026-02-23T18:41:25.9092652Z INFO 02-23 18:41:25 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
2026-02-23T18:41:25.9102111Z INFO 02-23 18:41:25 [utils.py:325] 
2026-02-23T18:41:25.9121266Z INFO 02-23 18:41:25 [utils.py:261] non-default args: {'model_tag': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'api_server_count': 4, 'host': '0.0.0.0', 'port': 8080, 'model': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'tokenizer_mode': 'deepseek_v32', 'trust_remote_code': True, 'seed': 1024, 'max_model_len': 8192, 'quantization': 'ascend', 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 8, 'data_parallel_size': 4, 'data_parallel_size_local': 2, 'data_parallel_address': '10.0.0.59', 'data_parallel_rpc_port': 13399, 'enable_expert_parallel': True, 'gpu_memory_utilization': 0.85, 'enable_prefix_caching': False, 'max_num_batched_tokens': 4096, 'max_num_seqs': 16, 'speculative_config': {'num_speculative_tokens': 2, 'method': 'deepseek_mtp'}, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}, 'additional_config': {'layer_sharding': ['q_b_proj', 'o_proj']}}
2026-02-23T18:41:25.9559524Z 2026-02-23 18:41:25,954 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-23T18:41:25.9628026Z INFO 02-23 18:41:25 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-23T18:41:25.9654388Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:25.9693476Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:25.9714864Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:25.9725190Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-23T18:41:25.9932990Z INFO 02-23 18:41:25 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-23T18:41:25.9942963Z INFO 02-23 18:41:25 [model.py:1561] Using max model len 8192
2026-02-23T18:41:26.2976386Z WARNING 02-23 18:41:26 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-23T18:41:26.3030898Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:26.3041749Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:26.3053836Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-23T18:41:26.3097744Z INFO 02-23 18:41:26 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-23T18:41:26.3118344Z INFO 02-23 18:41:26 [model.py:1561] Using max model len 163840
2026-02-23T18:41:26.3131517Z WARNING 02-23 18:41:26 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-23T18:41:26.3141128Z INFO 02-23 18:41:26 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-23T18:41:26.9442601Z INFO 02-23 18:41:26 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-23T18:41:26.9453048Z INFO 02-23 18:41:26 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-23T18:41:26.9467288Z WARNING 02-23 18:41:26 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-23T18:41:26.9484658Z WARNING 02-23 18:41:26 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-23T18:41:26.9491282Z INFO 02-23 18:41:26 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:41:26.9501668Z INFO 02-23 18:41:26 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:41:26.9516782Z INFO 02-23 18:41:26 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:41:26.9523201Z WARNING 02-23 18:41:26 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-23T18:41:26.9533014Z INFO 02-23 18:41:26 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-23T18:41:26.9542056Z WARNING 02-23 18:41:26 [platform.py:335] [91m
2026-02-23T18:41:26.9552652Z WARNING 02-23 18:41:26 [platform.py:335]             **********************************************************************************
2026-02-23T18:41:26.9563038Z WARNING 02-23 18:41:26 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-23T18:41:26.9588051Z WARNING 02-23 18:41:26 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-23T18:41:26.9588564Z WARNING 02-23 18:41:26 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-23T18:41:26.9593772Z WARNING 02-23 18:41:26 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-23T18:41:26.9604003Z WARNING 02-23 18:41:26 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-23T18:41:26.9620817Z WARNING 02-23 18:41:26 [platform.py:335]             * batch size for graph capture.
2026-02-23T18:41:26.9630319Z WARNING 02-23 18:41:26 [platform.py:335]             * For more details, please refer to:
2026-02-23T18:41:26.9641421Z WARNING 02-23 18:41:26 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-23T18:41:26.9645827Z WARNING 02-23 18:41:26 [platform.py:335]             **********************************************************************************[0m
2026-02-23T18:41:26.9655054Z WARNING 02-23 18:41:26 [platform.py:335]             
2026-02-23T18:41:26.9697247Z INFO 02-23 18:41:26 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-23T18:41:26.9701112Z INFO 02-23 18:41:26 [utils.py:851] Started DP Coordinator process (PID: 151)
2026-02-23T18:41:31.3212156Z INFO 02-23 18:41:31 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:41:31.3223084Z INFO 02-23 18:41:31 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:41:31.3233744Z INFO 02-23 18:41:31 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:41:31.3279797Z INFO 02-23 18:41:31 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:41:31.4261406Z INFO 02-23 18:41:31 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:41:31.4272602Z INFO 02-23 18:41:31 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:41:31.4281695Z INFO 02-23 18:41:31 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:41:31.4333428Z INFO 02-23 18:41:31 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:41:41.2702559Z INFO 02-23 18:41:41 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:41:41.2712826Z INFO 02-23 18:41:41 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:41:41.2722976Z INFO 02-23 18:41:41 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:41:41.2771445Z INFO 02-23 18:41:41 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:41:46.5667584Z INFO 02-23 18:41:46 [utils.py:218] Started 4 API server processes
2026-02-23T18:41:51.5086026Z INFO 02-23 18:41:51 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:41:51.5097196Z INFO 02-23 18:41:51 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:41:51.5107836Z INFO 02-23 18:41:51 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:41:51.5148765Z INFO 02-23 18:41:51 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:41:51.5595368Z INFO 02-23 18:41:51 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:41:51.5596917Z INFO 02-23 18:41:51 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:41:51.5608268Z INFO 02-23 18:41:51 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:41:51.5635197Z INFO 02-23 18:41:51 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:41:51.5646403Z INFO 02-23 18:41:51 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:41:51.5659961Z INFO 02-23 18:41:51 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:41:51.5683073Z INFO 02-23 18:41:51 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:41:51.5703626Z INFO 02-23 18:41:51 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:41:51.7708610Z INFO 02-23 18:41:51 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:41:51.7717286Z INFO 02-23 18:41:51 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:41:51.7723468Z INFO 02-23 18:41:51 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:41:51.7840374Z INFO 02-23 18:41:51 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:41:57.4923454Z [0;36m(ApiServer_2 pid=186)[0;0m 2026-02-23 18:41:57,490 - 186 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:41:57.5078256Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-23 18:41:57 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:41:57.5347177Z [0;36m(ApiServer_2 pid=186)[0;0m 2026-02-23 18:41:57,533 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-23T18:41:57.5402759Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-23 18:41:57 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-23T18:41:57.5666747Z [0;36m(ApiServer_0 pid=184)[0;0m 2026-02-23 18:41:57,565 - 184 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:41:57.5815327Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-23 18:41:57 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:41:57.5843042Z [0;36m(ApiServer_3 pid=187)[0;0m 2026-02-23 18:41:57,583 - 187 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:41:57.5974076Z [0;36m(ApiServer_0 pid=184)[0;0m 2026-02-23 18:41:57,596 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-23T18:41:57.5998018Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-23 18:41:57 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:41:57.6059001Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-23 18:41:57 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-23T18:41:57.6148042Z [0;36m(ApiServer_3 pid=187)[0;0m 2026-02-23 18:41:57,613 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-23T18:41:57.6200844Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-23 18:41:57 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-23T18:41:57.6368701Z [0;36m(ApiServer_2 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:57.6389878Z [0;36m(ApiServer_2 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:57.6409262Z [0;36m(ApiServer_2 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:57.6419780Z [0;36m(ApiServer_2 pid=186)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-23T18:41:57.6533356Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-23 18:41:57 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-23T18:41:57.6577646Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-23 18:41:57 [model.py:1561] Using max model len 8192
2026-02-23T18:41:57.7055359Z [0;36m(ApiServer_0 pid=184)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:57.7072593Z [0;36m(ApiServer_0 pid=184)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:57.7091890Z [0;36m(ApiServer_0 pid=184)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:57.7101565Z [0;36m(ApiServer_0 pid=184)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-23T18:41:57.7149683Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-23 18:41:57 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-23T18:41:57.7162644Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-23 18:41:57 [model.py:1561] Using max model len 8192
2026-02-23T18:41:57.7201363Z [0;36m(ApiServer_3 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:57.7219983Z [0;36m(ApiServer_3 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:57.7239986Z [0;36m(ApiServer_3 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:57.7252709Z [0;36m(ApiServer_3 pid=187)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-23T18:41:57.7309454Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-23 18:41:57 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-23T18:41:57.7317690Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-23 18:41:57 [model.py:1561] Using max model len 8192
2026-02-23T18:41:57.7553079Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-23 18:41:57 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-23T18:41:57.7624313Z [0;36m(ApiServer_2 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:57.7626565Z [0;36m(ApiServer_2 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:57.7627487Z [0;36m(ApiServer_2 pid=186)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-23T18:41:57.7677280Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-23 18:41:57 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-23T18:41:57.7687486Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-23 18:41:57 [model.py:1561] Using max model len 163840
2026-02-23T18:41:57.7696614Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-23 18:41:57 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-23T18:41:57.7706593Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-23 18:41:57 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-23T18:41:57.7750195Z [0;36m(ApiServer_1 pid=185)[0;0m 2026-02-23 18:41:57,773 - 185 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:41:57.7916549Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-23 18:41:57 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:41:57.8077361Z [0;36m(ApiServer_1 pid=185)[0;0m 2026-02-23 18:41:57,806 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-23T18:41:57.8138343Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-23 18:41:57 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-23T18:41:57.8219333Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-23 18:41:57 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-23T18:41:57.8270036Z [0;36m(ApiServer_0 pid=184)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:57.8279925Z [0;36m(ApiServer_0 pid=184)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:57.8289888Z [0;36m(ApiServer_0 pid=184)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-23T18:41:57.8317118Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-23 18:41:57 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-23T18:41:57.8328496Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-23 18:41:57 [model.py:1561] Using max model len 163840
2026-02-23T18:41:57.8339269Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-23 18:41:57 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-23T18:41:57.8357455Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-23 18:41:57 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-23T18:41:57.8365864Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-23 18:41:57 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-23T18:41:57.8390665Z [0;36m(ApiServer_3 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:57.8401168Z [0;36m(ApiServer_3 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:57.8411675Z [0;36m(ApiServer_3 pid=187)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-23T18:41:57.8448098Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-23 18:41:57 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-23T18:41:57.8475229Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-23 18:41:57 [model.py:1561] Using max model len 163840
2026-02-23T18:41:57.8486062Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-23 18:41:57 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-23T18:41:57.8494936Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-23 18:41:57 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-23T18:41:57.8850899Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-23 18:41:57 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-23T18:41:57.8876150Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-23 18:41:57 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-23T18:41:57.8887950Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-23 18:41:57 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-23T18:41:57.8896796Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-23 18:41:57 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-23T18:41:57.8905850Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-23 18:41:57 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:41:57.8920290Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-23 18:41:57 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:41:57.8931893Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-23 18:41:57 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:41:57.8942449Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-23 18:41:57 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-23T18:41:57.8953034Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-23 18:41:57 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-23T18:41:57.8963253Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-23 18:41:57 [platform.py:335] [91m
2026-02-23T18:41:57.8973845Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             **********************************************************************************
2026-02-23T18:41:57.8984306Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-23T18:41:57.8994966Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-23T18:41:57.9007668Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-23T18:41:57.9015975Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-23T18:41:57.9026309Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-23T18:41:57.9036102Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * batch size for graph capture.
2026-02-23T18:41:57.9046810Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * For more details, please refer to:
2026-02-23T18:41:57.9056469Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-23T18:41:57.9066695Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             **********************************************************************************[0m
2026-02-23T18:41:57.9076842Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             
2026-02-23T18:41:57.9087066Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-23 18:41:57 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-23T18:41:57.9135237Z [0;36m(ApiServer_1 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:57.9159538Z [0;36m(ApiServer_1 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:57.9169623Z [0;36m(ApiServer_1 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:57.9180824Z [0;36m(ApiServer_1 pid=185)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-23T18:41:57.9238725Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-23 18:41:57 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-23T18:41:57.9259399Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-23 18:41:57 [model.py:1561] Using max model len 8192
2026-02-23T18:41:57.9510383Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-23 18:41:57 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-23T18:41:57.9530242Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-23 18:41:57 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-23T18:41:57.9540001Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-23 18:41:57 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-23T18:41:57.9550453Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-23 18:41:57 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-23T18:41:57.9560299Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-23 18:41:57 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:41:57.9570199Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-23 18:41:57 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:41:57.9581740Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-23 18:41:57 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:41:57.9592421Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-23 18:41:57 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-23T18:41:57.9603290Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-23 18:41:57 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-23T18:41:57.9609688Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-23 18:41:57 [platform.py:335] [91m
2026-02-23T18:41:57.9620254Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             **********************************************************************************
2026-02-23T18:41:57.9630489Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-23T18:41:57.9640587Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-23T18:41:57.9651700Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-23T18:41:57.9661590Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-23T18:41:57.9671570Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-23T18:41:57.9682884Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * batch size for graph capture.
2026-02-23T18:41:57.9691879Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * For more details, please refer to:
2026-02-23T18:41:57.9703769Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-23T18:41:57.9713154Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             **********************************************************************************[0m
2026-02-23T18:41:57.9722500Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             
2026-02-23T18:41:57.9732785Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-23 18:41:57 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-23T18:41:57.9741912Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-23 18:41:57 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-23T18:41:57.9763047Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-23 18:41:57 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-23T18:41:57.9765959Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-23 18:41:57 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-23T18:41:57.9773835Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-23 18:41:57 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-23T18:41:57.9784513Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-23 18:41:57 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:41:57.9794835Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-23 18:41:57 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:41:57.9806691Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-23 18:41:57 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:41:57.9818481Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-23 18:41:57 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-23T18:41:57.9827374Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-23 18:41:57 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-23T18:41:57.9836830Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-23 18:41:57 [platform.py:335] [91m
2026-02-23T18:41:57.9847275Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             **********************************************************************************
2026-02-23T18:41:57.9856764Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-23T18:41:57.9867169Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-23T18:41:57.9877239Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-23T18:41:57.9890737Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-23T18:41:57.9901864Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-23T18:41:57.9908556Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * batch size for graph capture.
2026-02-23T18:41:57.9917251Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * For more details, please refer to:
2026-02-23T18:41:57.9928211Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-23T18:41:57.9937950Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             **********************************************************************************[0m
2026-02-23T18:41:57.9947924Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-23 18:41:57 [platform.py:335]             
2026-02-23T18:41:57.9957509Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-23 18:41:57 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-23T18:41:58.0311895Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-23 18:41:58 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-23T18:41:58.0332820Z [0;36m(ApiServer_1 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:58.0342338Z [0;36m(ApiServer_1 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-23T18:41:58.0351761Z [0;36m(ApiServer_1 pid=185)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-23T18:41:58.0395683Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-23 18:41:58 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-23T18:41:58.0411518Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-23 18:41:58 [model.py:1561] Using max model len 163840
2026-02-23T18:41:58.0421837Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-23 18:41:58 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-23T18:41:58.0430744Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-23 18:41:58 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-23T18:41:58.1558760Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-23 18:41:58 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-23T18:41:58.1567389Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-23 18:41:58 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-23T18:41:58.1585260Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-23 18:41:58 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-23T18:41:58.1594520Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-23 18:41:58 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-23T18:41:58.1604153Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-23 18:41:58 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:41:58.1613563Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-23 18:41:58 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:41:58.1624756Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-23 18:41:58 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:41:58.1636212Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-23 18:41:58 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-23T18:41:58.1645304Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-23 18:41:58 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-23T18:41:58.1653141Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-23 18:41:58 [platform.py:335] [91m
2026-02-23T18:41:58.1664017Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-23 18:41:58 [platform.py:335]             **********************************************************************************
2026-02-23T18:41:58.1678638Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-23 18:41:58 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-23T18:41:58.1682838Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-23 18:41:58 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-23T18:41:58.1693342Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-23 18:41:58 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-23T18:41:58.1702896Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-23 18:41:58 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-23T18:41:58.1713041Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-23 18:41:58 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-23T18:41:58.1722674Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-23 18:41:58 [platform.py:335]             * batch size for graph capture.
2026-02-23T18:41:58.1732158Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-23 18:41:58 [platform.py:335]             * For more details, please refer to:
2026-02-23T18:41:58.1742612Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-23 18:41:58 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-23T18:41:58.1752223Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-23 18:41:58 [platform.py:335]             **********************************************************************************[0m
2026-02-23T18:41:58.1763155Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-23 18:41:58 [platform.py:335]             
2026-02-23T18:41:58.1774035Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-23 18:41:58 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-23T18:42:05.0118053Z [0;36m(EngineCore_DP1 pid=173)[0;0m 2026-02-23 18:42:05,009 - 173 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:42:05.0137940Z [0;36m(EngineCore_DP0 pid=154)[0;0m 2026-02-23 18:42:05,010 - 154 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:42:05.0162215Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-23 18:42:05 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:42:05.0172281Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-23 18:42:05 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:42:05.0200270Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-23 18:42:05 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', speculative_config=SpeculativeConfig(method='mtp', model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', num_spec_tokens=2), tokenizer='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', skip_tokenizer_init=False, tokenizer_mode=deepseek_v32, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=4, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=bfloat16, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=1024, served_model_name=/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [24, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 48, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
2026-02-23T18:42:09.7386394Z INFO 02-23 18:42:09 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:42:09.7386903Z INFO 02-23 18:42:09 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:42:09.7387416Z INFO 02-23 18:42:09 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:42:09.7420835Z INFO 02-23 18:42:09 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:42:09.8533090Z INFO 02-23 18:42:09 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:42:09.8541760Z INFO 02-23 18:42:09 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:42:09.8552632Z INFO 02-23 18:42:09 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:42:09.8622366Z INFO 02-23 18:42:09 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:42:14.7291038Z 2026-02-23 18:42:14,727 - 257 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:42:14.7323962Z INFO 02-23 18:42:14 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:42:14.8016676Z 2026-02-23 18:42:14,800 - 258 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:42:14.8078526Z INFO 02-23 18:42:14 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:42:16.6612297Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-23T18:42:16.6618132Z   warnings.warn(
2026-02-23T18:42:16.6670419Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-23T18:42:16.6680750Z   warnings.warn(
2026-02-23T18:42:19.4845421Z INFO 02-23 18:42:19 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:42:19.4867324Z INFO 02-23 18:42:19 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:42:19.4877742Z INFO 02-23 18:42:19 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:42:19.4947845Z INFO 02-23 18:42:19 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:42:19.5296622Z INFO 02-23 18:42:19 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:42:19.5305456Z INFO 02-23 18:42:19 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:42:19.5315433Z INFO 02-23 18:42:19 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:42:19.5375197Z INFO 02-23 18:42:19 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:42:20.3044681Z INFO 02-23 18:42:20 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:42:20.3051714Z INFO 02-23 18:42:20 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:42:20.3062517Z INFO 02-23 18:42:20 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:42:20.3070317Z INFO 02-23 18:42:20 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:42:20.3080461Z INFO 02-23 18:42:20 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:42:20.3092171Z INFO 02-23 18:42:20 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:42:20.3545834Z INFO 02-23 18:42:20 [parallel_state.py:1212] world_size=32 rank=8 local_rank=0 distributed_init_method=tcp://10.0.0.59:42701 backend=hccl
2026-02-23T18:42:20.3585428Z INFO 02-23 18:42:20 [parallel_state.py:1212] world_size=32 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.59:42701 backend=hccl
2026-02-23T18:42:24.6680280Z 2026-02-23 18:42:24,665 - 276 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:42:24.6714808Z INFO 02-23 18:42:24 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:42:24.6938403Z 2026-02-23 18:42:24,692 - 275 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:42:24.7054971Z INFO 02-23 18:42:24 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:42:25.9944789Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-23T18:42:25.9952365Z   warnings.warn(
2026-02-23T18:42:26.0113507Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-23T18:42:26.0134020Z   warnings.warn(
2026-02-23T18:42:28.0899663Z INFO 02-23 18:42:28 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:42:28.0908797Z INFO 02-23 18:42:28 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:42:28.0918647Z INFO 02-23 18:42:28 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:42:28.1193637Z INFO 02-23 18:42:28 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:42:28.1202616Z INFO 02-23 18:42:28 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:42:28.1213945Z INFO 02-23 18:42:28 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:42:28.5017331Z INFO 02-23 18:42:28 [parallel_state.py:1212] world_size=32 rank=1 local_rank=1 distributed_init_method=tcp://10.0.0.59:42701 backend=hccl
2026-02-23T18:42:28.5351668Z INFO 02-23 18:42:28 [parallel_state.py:1212] world_size=32 rank=9 local_rank=1 distributed_init_method=tcp://10.0.0.59:42701 backend=hccl
2026-02-23T18:42:29.2555281Z INFO 02-23 18:42:29 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:42:29.2565952Z INFO 02-23 18:42:29 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:42:29.2575904Z INFO 02-23 18:42:29 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:42:29.2624776Z INFO 02-23 18:42:29 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:42:29.3906150Z INFO 02-23 18:42:29 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:42:29.3914954Z INFO 02-23 18:42:29 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:42:29.3925436Z INFO 02-23 18:42:29 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:42:29.3980291Z INFO 02-23 18:42:29 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:42:34.5807910Z 2026-02-23 18:42:34,578 - 370 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:42:34.5838421Z INFO 02-23 18:42:34 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:42:34.6878167Z 2026-02-23 18:42:34,686 - 369 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:42:34.6940607Z INFO 02-23 18:42:34 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:42:35.9064298Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-23T18:42:35.9073116Z   warnings.warn(
2026-02-23T18:42:36.0328085Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-23T18:42:36.0336692Z   warnings.warn(
2026-02-23T18:42:37.9550883Z INFO 02-23 18:42:37 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:42:37.9559038Z INFO 02-23 18:42:37 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:42:37.9569226Z INFO 02-23 18:42:37 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:42:38.1815970Z INFO 02-23 18:42:38 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:42:38.1823397Z INFO 02-23 18:42:38 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:42:38.1837460Z INFO 02-23 18:42:38 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:42:38.3866388Z INFO 02-23 18:42:38 [parallel_state.py:1212] world_size=32 rank=2 local_rank=2 distributed_init_method=tcp://10.0.0.59:42701 backend=hccl
2026-02-23T18:42:38.6118052Z INFO 02-23 18:42:38 [parallel_state.py:1212] world_size=32 rank=10 local_rank=2 distributed_init_method=tcp://10.0.0.59:42701 backend=hccl
2026-02-23T18:42:39.1948023Z INFO 02-23 18:42:39 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:42:39.1955566Z INFO 02-23 18:42:39 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:42:39.1965378Z INFO 02-23 18:42:39 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:42:39.2020371Z INFO 02-23 18:42:39 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:42:39.3644938Z INFO 02-23 18:42:39 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:42:39.3656500Z INFO 02-23 18:42:39 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:42:39.3665110Z INFO 02-23 18:42:39 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:42:39.3724194Z INFO 02-23 18:42:39 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:42:44.1478999Z 2026-02-23 18:42:44,145 - 473 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:42:44.1546554Z INFO 02-23 18:42:44 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:42:44.3046369Z 2026-02-23 18:42:44,303 - 475 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:42:44.3081441Z INFO 02-23 18:42:44 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:42:45.5565882Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-23T18:42:45.5574288Z   warnings.warn(
2026-02-23T18:42:45.6285717Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-23T18:42:45.6295208Z   warnings.warn(
2026-02-23T18:42:47.5232825Z INFO 02-23 18:42:47 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:42:47.5243115Z INFO 02-23 18:42:47 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:42:47.5254186Z INFO 02-23 18:42:47 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:42:47.7055018Z INFO 02-23 18:42:47 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:42:47.7061456Z INFO 02-23 18:42:47 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:42:47.7071438Z INFO 02-23 18:42:47 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:42:47.9564689Z INFO 02-23 18:42:47 [parallel_state.py:1212] world_size=32 rank=3 local_rank=3 distributed_init_method=tcp://10.0.0.59:42701 backend=hccl
2026-02-23T18:42:48.1292616Z INFO 02-23 18:42:48 [parallel_state.py:1212] world_size=32 rank=11 local_rank=3 distributed_init_method=tcp://10.0.0.59:42701 backend=hccl
2026-02-23T18:42:48.8127019Z INFO 02-23 18:42:48 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:42:48.8127588Z INFO 02-23 18:42:48 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:42:48.8128092Z INFO 02-23 18:42:48 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:42:48.8167426Z INFO 02-23 18:42:48 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:42:48.9886058Z INFO 02-23 18:42:48 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:42:48.9894460Z INFO 02-23 18:42:48 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:42:48.9904878Z INFO 02-23 18:42:48 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:42:48.9971040Z INFO 02-23 18:42:48 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:42:53.7850293Z 2026-02-23 18:42:53,783 - 580 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:42:53.7884376Z INFO 02-23 18:42:53 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:42:53.9399130Z 2026-02-23 18:42:53,938 - 577 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:42:53.9435517Z INFO 02-23 18:42:53 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:42:55.1076215Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-23T18:42:55.1082828Z   warnings.warn(
2026-02-23T18:42:55.3192469Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-23T18:42:55.3199701Z   warnings.warn(
2026-02-23T18:42:57.2208523Z INFO 02-23 18:42:57 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:42:57.2216590Z INFO 02-23 18:42:57 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:42:57.2225806Z INFO 02-23 18:42:57 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:42:57.5125403Z INFO 02-23 18:42:57 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:42:57.5133925Z INFO 02-23 18:42:57 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:42:57.5144312Z INFO 02-23 18:42:57 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:42:57.6412076Z INFO 02-23 18:42:57 [parallel_state.py:1212] world_size=32 rank=12 local_rank=4 distributed_init_method=tcp://10.0.0.59:42701 backend=hccl
2026-02-23T18:42:57.9354437Z INFO 02-23 18:42:57 [parallel_state.py:1212] world_size=32 rank=4 local_rank=4 distributed_init_method=tcp://10.0.0.59:42701 backend=hccl
2026-02-23T18:42:58.4645202Z INFO 02-23 18:42:58 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:42:58.4654292Z INFO 02-23 18:42:58 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:42:58.4667526Z INFO 02-23 18:42:58 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:42:58.4723315Z INFO 02-23 18:42:58 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:42:58.5224864Z INFO 02-23 18:42:58 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:42:58.5235132Z INFO 02-23 18:42:58 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:42:58.5247178Z INFO 02-23 18:42:58 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:42:58.5351235Z INFO 02-23 18:42:58 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:43:03.7196591Z 2026-02-23 18:43:03,717 - 684 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:43:03.7227989Z INFO 02-23 18:43:03 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:43:03.7354219Z 2026-02-23 18:43:03,734 - 681 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:43:03.7419402Z INFO 02-23 18:43:03 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:43:05.0297502Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-23T18:43:05.0304782Z   warnings.warn(
2026-02-23T18:43:05.0386230Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-23T18:43:05.0394198Z   warnings.warn(
2026-02-23T18:43:07.0354810Z INFO 02-23 18:43:07 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:43:07.0361775Z INFO 02-23 18:43:07 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:43:07.0374229Z INFO 02-23 18:43:07 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:43:07.1423123Z INFO 02-23 18:43:07 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:43:07.1430633Z INFO 02-23 18:43:07 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:43:07.1441174Z INFO 02-23 18:43:07 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:43:07.4671548Z INFO 02-23 18:43:07 [parallel_state.py:1212] world_size=32 rank=13 local_rank=5 distributed_init_method=tcp://10.0.0.59:42701 backend=hccl
2026-02-23T18:43:07.5754965Z INFO 02-23 18:43:07 [parallel_state.py:1212] world_size=32 rank=5 local_rank=5 distributed_init_method=tcp://10.0.0.59:42701 backend=hccl
2026-02-23T18:43:08.2913768Z INFO 02-23 18:43:08 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:43:08.2920880Z INFO 02-23 18:43:08 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:43:08.2930048Z INFO 02-23 18:43:08 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:43:08.2989042Z INFO 02-23 18:43:08 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:43:08.4129469Z INFO 02-23 18:43:08 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:43:08.4136507Z INFO 02-23 18:43:08 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:43:08.4144863Z INFO 02-23 18:43:08 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:43:08.4236197Z INFO 02-23 18:43:08 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:43:13.4126573Z 2026-02-23 18:43:13,410 - 786 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:43:13.4158245Z INFO 02-23 18:43:13 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:43:13.4181192Z 2026-02-23 18:43:13,416 - 785 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:43:13.4243597Z INFO 02-23 18:43:13 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:43:15.0753613Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-23T18:43:15.0760911Z   warnings.warn(
2026-02-23T18:43:15.1236856Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-23T18:43:15.1249906Z   warnings.warn(
2026-02-23T18:43:17.2215453Z INFO 02-23 18:43:17 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:43:17.2224752Z INFO 02-23 18:43:17 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:43:17.2237572Z INFO 02-23 18:43:17 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:43:17.2562699Z INFO 02-23 18:43:17 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:43:17.2576673Z INFO 02-23 18:43:17 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:43:17.2588314Z INFO 02-23 18:43:17 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:43:17.6535821Z INFO 02-23 18:43:17 [parallel_state.py:1212] world_size=32 rank=6 local_rank=6 distributed_init_method=tcp://10.0.0.59:42701 backend=hccl
2026-02-23T18:43:17.6718688Z INFO 02-23 18:43:17 [parallel_state.py:1212] world_size=32 rank=14 local_rank=6 distributed_init_method=tcp://10.0.0.59:42701 backend=hccl
2026-02-23T18:43:18.0247382Z INFO 02-23 18:43:18 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:43:18.0255266Z INFO 02-23 18:43:18 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:43:18.0265717Z INFO 02-23 18:43:18 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:43:18.0321516Z INFO 02-23 18:43:18 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:43:18.2665396Z INFO 02-23 18:43:18 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:43:18.2670686Z INFO 02-23 18:43:18 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:43:18.2679391Z INFO 02-23 18:43:18 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:43:18.2739669Z INFO 02-23 18:43:18 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:43:23.0440358Z 2026-02-23 18:43:23,041 - 889 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:43:23.0469415Z INFO 02-23 18:43:23 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:43:23.5151573Z 2026-02-23 18:43:23,513 - 890 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-23T18:43:23.5188837Z INFO 02-23 18:43:23 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-23T18:43:24.3288946Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-23T18:43:24.3296441Z   warnings.warn(
2026-02-23T18:43:24.8233284Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-23T18:43:24.8243042Z   warnings.warn(
2026-02-23T18:43:26.4083757Z INFO 02-23 18:43:26 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:43:26.4093911Z INFO 02-23 18:43:26 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:43:26.4106868Z INFO 02-23 18:43:26 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:43:26.8260554Z INFO 02-23 18:43:26 [parallel_state.py:1212] world_size=32 rank=15 local_rank=7 distributed_init_method=tcp://10.0.0.59:42701 backend=hccl
2026-02-23T18:43:26.9112293Z INFO 02-23 18:43:26 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:43:26.9122731Z INFO 02-23 18:43:26 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:43:26.9134133Z INFO 02-23 18:43:26 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:43:27.3563856Z INFO 02-23 18:43:27 [parallel_state.py:1212] world_size=32 rank=7 local_rank=7 distributed_init_method=tcp://10.0.0.59:42701 backend=hccl
2026-02-23T18:43:27.3970227Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.3994787Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.4510622Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.4520888Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.4533760Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.4543347Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.4552529Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.4563196Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.4572861Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.4583823Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.4639398Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.4649285Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.4807578Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.4818489Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.4828305Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.4838673Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.5382604Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-23T18:43:27.5392236Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-23T18:43:27.5401335Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-23T18:43:27.5410370Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-23T18:43:27.5420450Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-23T18:43:27.5430436Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-23T18:43:27.5440231Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-23T18:43:27.5450102Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-23T18:43:27.5461910Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-23T18:43:27.5468963Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-23T18:43:27.5479095Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-23T18:43:27.5489721Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-23T18:43:27.5499413Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-23T18:43:27.5509128Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-23T18:43:27.5519491Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-23T18:43:27.5529250Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-23T18:43:27.5539464Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5550007Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5560760Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5570068Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5579490Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5589365Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5614968Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5620934Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5621482Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5629284Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5639675Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5649300Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5658594Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5669311Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5679477Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5689206Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5698759Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5708645Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5719180Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5728002Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5738055Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5746930Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5757210Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.5768982Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6014731Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6035043Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6045467Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6056122Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6065821Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6075724Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6085562Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6095043Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6104717Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6113867Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6124541Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6133962Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6143282Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6153819Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6162934Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6174430Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6182376Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6191558Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6202073Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6211752Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6221096Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6230754Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6240224Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6249955Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-23T18:43:27.6258991Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.6269103Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.6278639Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.6288497Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.6297994Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.6307721Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.6321230Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.6331519Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.6340424Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.6351030Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.6360682Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.6370442Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.6380278Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.6389768Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.6402345Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.6419954Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.6486000Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.6509133Z INFO 02-23 18:43:27 [parallel_state.py:1423] rank 0 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
2026-02-23T18:43:27.7313354Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7336295Z INFO 02-23 18:43:27 [parallel_state.py:1423] rank 1 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
2026-02-23T18:43:27.7472411Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7537876Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7628340Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7628927Z INFO 02-23 18:43:27 [parallel_state.py:1423] rank 2 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
2026-02-23T18:43:27.7629604Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7630155Z INFO 02-23 18:43:27 [parallel_state.py:1423] rank 3 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
2026-02-23T18:43:27.7630714Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7631305Z INFO 02-23 18:43:27 [parallel_state.py:1423] rank 4 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 4, EP rank 4
2026-02-23T18:43:27.7631828Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7632403Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7632991Z INFO 02-23 18:43:27 [parallel_state.py:1423] rank 6 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 6, EP rank 6
2026-02-23T18:43:27.7636898Z INFO 02-23 18:43:27 [parallel_state.py:1423] rank 5 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 5, EP rank 5
2026-02-23T18:43:27.7647011Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7656003Z INFO 02-23 18:43:27 [parallel_state.py:1423] rank 7 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 7, EP rank 7
2026-02-23T18:43:27.7665435Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7674672Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7685738Z INFO 02-23 18:43:27 [parallel_state.py:1423] rank 8 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 0, EP rank 8
2026-02-23T18:43:27.7694950Z INFO 02-23 18:43:27 [parallel_state.py:1423] rank 9 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 1, EP rank 9
2026-02-23T18:43:27.7704571Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7714837Z INFO 02-23 18:43:27 [parallel_state.py:1423] rank 10 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 2, EP rank 10
2026-02-23T18:43:27.7725127Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7734950Z INFO 02-23 18:43:27 [parallel_state.py:1423] rank 12 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 4, EP rank 12
2026-02-23T18:43:27.7744332Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7753380Z INFO 02-23 18:43:27 [parallel_state.py:1423] rank 11 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 3, EP rank 11
2026-02-23T18:43:27.7764211Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7777738Z INFO 02-23 18:43:27 [parallel_state.py:1423] rank 14 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 6, EP rank 14
2026-02-23T18:43:27.7786432Z INFO 02-23 18:43:27 [parallel_state.py:1423] rank 15 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 7, EP rank 15
2026-02-23T18:43:27.7797585Z INFO 02-23 18:43:27 [parallel_state.py:1423] rank 13 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 5, EP rank 13
2026-02-23T18:43:27.7806966Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7817618Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7827234Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7837048Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7847011Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7857029Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7867065Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7876386Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7886732Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7896426Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7906395Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7915454Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7925914Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7935565Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7945389Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7955644Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-23T18:43:27.7966383Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.7975749Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.7985189Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.7997411Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.8009789Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.8023071Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.8032912Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.8043655Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.8053961Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.8064002Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.8075153Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.8086634Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.8096255Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.8105471Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.8117017Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.8125953Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-23T18:43:27.8891131Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9058553Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9080260Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9089930Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9100142Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9109592Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9120194Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9129640Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9139571Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9148344Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9157887Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9167594Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9178031Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9187115Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9196329Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9207009Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9413001Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9433529Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9443408Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9453026Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9462738Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9473388Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-23 18:43:27 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-23T18:43:27.9484009Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9494871Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-23 18:43:27 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-23T18:43:27.9504853Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9514936Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9524990Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9535518Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-23 18:43:27 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-23T18:43:27.9545408Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-23 18:43:27 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-23T18:43:27.9555043Z [0;36m(Worker_DP0_TP5_EP5 pid=684)[0;0m INFO 02-23 18:43:27 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-23T18:43:27.9565433Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9575397Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9586020Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-23 18:43:27 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-23T18:43:27.9597209Z [0;36m(Worker_DP1_TP6_EP14 pid=786)[0;0m INFO 02-23 18:43:27 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-23T18:43:27.9607177Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9617333Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9626809Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9637487Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9648021Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-23 18:43:27 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-23T18:43:27.9658342Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-23 18:43:27 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-23T18:43:27.9669043Z [0;36m(Worker_DP1_TP3_EP11 pid=475)[0;0m INFO 02-23 18:43:27 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-23T18:43:27.9678939Z [0;36m(Worker_DP1_TP5_EP13 pid=681)[0;0m INFO 02-23 18:43:27 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-23T18:43:27.9689086Z [0;36m(Worker_DP1_TP7_EP15 pid=889)[0;0m INFO 02-23 18:43:27 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-23T18:43:27.9746477Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m INFO 02-23 18:43:27 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-23T18:43:27.9747415Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-23 18:43:27 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-23T18:43:27.9748210Z WARNING 02-23 18:43:27 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-23T18:43:27.9807727Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-23 18:43:27 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-23T18:43:28.0325060Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m INFO 02-23 18:43:28 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-23T18:43:28.3538989Z [0;36m(Worker_DP1_TP5_EP13 pid=681)[0;0m [2026-02-23 18:43:28] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-23T18:43:28.3986924Z [0;36m(Worker_DP1_TP7_EP15 pid=889)[0;0m [2026-02-23 18:43:28] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-23T18:43:28.4062647Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m [2026-02-23 18:43:28] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-23T18:43:28.4220175Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m [2026-02-23 18:43:28] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-23T18:43:28.4563807Z [0;36m(Worker_DP1_TP6_EP14 pid=786)[0;0m [2026-02-23 18:43:28] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-23T18:43:28.5029804Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m [2026-02-23 18:43:28] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-23T18:43:28.5305133Z [0;36m(Worker_DP1_TP7_EP15 pid=889)[0;0m INFO 02-23 18:43:28 [layer.py:475] [EP Rank 15/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-23T18:43:28.5315853Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-23 18:43:28 [layer.py:475] [EP Rank 12/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-23T18:43:28.5325617Z [0;36m(Worker_DP1_TP5_EP13 pid=681)[0;0m INFO 02-23 18:43:28 [layer.py:475] [EP Rank 13/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-23T18:43:28.5335997Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-23 18:43:28 [layer.py:475] [EP Rank 9/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-23T18:43:28.5705937Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m [2026-02-23 18:43:28] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-23T18:43:28.5812728Z [0;36m(Worker_DP1_TP6_EP14 pid=786)[0;0m INFO 02-23 18:43:28 [layer.py:475] [EP Rank 14/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-23T18:43:28.6188229Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-23 18:43:28 [layer.py:475] [EP Rank 10/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-23T18:43:28.6435738Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m [2026-02-23 18:43:28] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-23T18:43:28.6520747Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m [2026-02-23 18:43:28] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-23T18:43:28.6849094Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-23 18:43:28 [layer.py:475] [EP Rank 7/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-23T18:43:28.7006184Z [0;36m(Worker_DP0_TP5_EP5 pid=684)[0;0m [2026-02-23 18:43:28] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-23T18:43:28.7206015Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m [2026-02-23 18:43:28] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-23T18:43:28.7521521Z [0;36m(Worker_DP1_TP3_EP11 pid=475)[0;0m [2026-02-23 18:43:28] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-23T18:43:28.7586087Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-23 18:43:28 [layer.py:475] [EP Rank 1/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-23T18:43:28.8117838Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m INFO 02-23 18:43:28 [layer.py:475] [EP Rank 8/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-23T18:43:28.8231602Z [0;36m(Worker_DP0_TP5_EP5 pid=684)[0;0m INFO 02-23 18:43:28 [layer.py:475] [EP Rank 5/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-23T18:43:28.8368240Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-23 18:43:28 [layer.py:475] [EP Rank 2/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-23T18:43:28.8586953Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m [2026-02-23 18:43:28] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-23T18:43:28.8664462Z [0;36m(Worker_DP1_TP3_EP11 pid=475)[0;0m INFO 02-23 18:43:28 [layer.py:475] [EP Rank 11/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-23T18:43:28.8913182Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m [2026-02-23 18:43:28] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-23T18:43:28.9696577Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m [2026-02-23 18:43:28] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-23T18:43:28.9725927Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m [2026-02-23 18:43:28] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-23T18:43:28.9752072Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-23 18:43:28 [layer.py:475] [EP Rank 6/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-23T18:43:29.0601861Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m INFO 02-23 18:43:29 [layer.py:475] [EP Rank 0/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-23T18:43:29.0786656Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-23 18:43:29 [layer.py:475] [EP Rank 4/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-23T18:43:29.0953702Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-23 18:43:29 [layer.py:475] [EP Rank 3/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-23T18:43:30.0027221Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-23T18:43:30.0034872Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m   return func(*args, **kwargs)
2026-02-23T18:43:30.0726275Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m INFO 02-23 18:43:30 [fused_moe.py:207] [EP Rank 0/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-23T18:43:30.1817422Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-23T18:43:30.1824320Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m   return func(*args, **kwargs)
2026-02-23T18:43:30.1940685Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-23T18:43:30.1949730Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m   return func(*args, **kwargs)
2026-02-23T18:43:30.1985662Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-23T18:43:30.1993781Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m   return func(*args, **kwargs)
2026-02-23T18:43:30.2188060Z [0;36m(Worker_DP1_TP7_EP15 pid=889)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-23T18:43:30.2196389Z [0;36m(Worker_DP1_TP7_EP15 pid=889)[0;0m   return func(*args, **kwargs)
2026-02-23T18:43:30.2216814Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-23T18:43:30.2226158Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m   return func(*args, **kwargs)
2026-02-23T18:43:30.2469548Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-23T18:43:30.2477068Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m   return func(*args, **kwargs)
2026-02-23T18:43:30.2499644Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-23T18:43:30.2507594Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m   return func(*args, **kwargs)
2026-02-23T18:43:30.2569905Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-23T18:43:30.2578164Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m   return func(*args, **kwargs)
2026-02-23T18:43:30.2593361Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-23 18:43:30 [fused_moe.py:207] [EP Rank 2/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-23T18:43:30.2639351Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-23T18:43:30.2641273Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m   return func(*args, **kwargs)
2026-02-23T18:43:30.2643488Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-23T18:43:30.2645403Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m   return func(*args, **kwargs)
2026-02-23T18:43:30.2647561Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-23T18:43:30.2653828Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m   return func(*args, **kwargs)
2026-02-23T18:43:30.2664042Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-23 18:43:30 [fused_moe.py:207] [EP Rank 6/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-23T18:43:30.2674736Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m INFO 02-23 18:43:30 [fused_moe.py:207] [EP Rank 8/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-23T18:43:30.2703894Z [0;36m(Worker_DP1_TP6_EP14 pid=786)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-23T18:43:30.2711580Z [0;36m(Worker_DP1_TP6_EP14 pid=786)[0;0m   return func(*args, **kwargs)
2026-02-23T18:43:30.2725026Z [0;36m(Worker_DP0_TP5_EP5 pid=684)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-23T18:43:30.2733330Z [0;36m(Worker_DP0_TP5_EP5 pid=684)[0;0m   return func(*args, **kwargs)
2026-02-23T18:43:30.2797896Z [0;36m(Worker_DP1_TP7_EP15 pid=889)[0;0m INFO 02-23 18:43:30 [fused_moe.py:207] [EP Rank 15/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-23T18:43:30.2810184Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-23 18:43:30 [fused_moe.py:207] [EP Rank 4/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-23T18:43:30.2985622Z [0;36m(Worker_DP1_TP5_EP13 pid=681)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-23T18:43:30.2994115Z [0;36m(Worker_DP1_TP5_EP13 pid=681)[0;0m   return func(*args, **kwargs)
2026-02-23T18:43:30.3066031Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-23 18:43:30 [fused_moe.py:207] [EP Rank 12/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-23T18:43:30.3103538Z [0;36m(Worker_DP1_TP3_EP11 pid=475)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-23T18:43:30.3112937Z [0;36m(Worker_DP1_TP3_EP11 pid=475)[0;0m   return func(*args, **kwargs)
2026-02-23T18:43:30.3124634Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-23 18:43:30 [fused_moe.py:207] [EP Rank 1/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-23T18:43:30.3338388Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-23 18:43:30 [fused_moe.py:207] [EP Rank 10/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-23T18:43:30.3348141Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-23 18:43:30 [fused_moe.py:207] [EP Rank 7/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-23T18:43:30.3358809Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-23 18:43:30 [fused_moe.py:207] [EP Rank 9/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-23T18:43:30.3370721Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-23 18:43:30 [fused_moe.py:207] [EP Rank 3/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-23T18:43:30.3380376Z [0;36m(Worker_DP0_TP5_EP5 pid=684)[0;0m INFO 02-23 18:43:30 [fused_moe.py:207] [EP Rank 5/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-23T18:43:30.3399539Z [0;36m(Worker_DP1_TP6_EP14 pid=786)[0;0m INFO 02-23 18:43:30 [fused_moe.py:207] [EP Rank 14/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-23T18:43:30.3621007Z [0;36m(Worker_DP1_TP5_EP13 pid=681)[0;0m INFO 02-23 18:43:30 [fused_moe.py:207] [EP Rank 13/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-23T18:43:30.3733228Z [0;36m(Worker_DP1_TP3_EP11 pid=475)[0;0m INFO 02-23 18:43:30 [fused_moe.py:207] [EP Rank 11/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-23T18:43:30.6227147Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-23 18:43:30 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-23T18:43:30.6251970Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-23 18:43:30 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-23T18:43:30.6263737Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-23 18:43:30 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-23T18:43:30.6274863Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-23 18:43:30 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-23T18:43:30.6286445Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-23 18:43:30 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-23T18:43:30.6424705Z [0;36m(Worker_DP0_TP5_EP5 pid=684)[0;0m INFO 02-23 18:43:30 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-23T18:43:30.6454079Z [0;36m(Worker_DP1_TP6_EP14 pid=786)[0;0m INFO 02-23 18:43:30 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-23T18:43:30.6591618Z [0;36m(Worker_DP1_TP5_EP13 pid=681)[0;0m INFO 02-23 18:43:30 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-23T18:43:30.6997692Z [0;36m(Worker_DP1_TP3_EP11 pid=475)[0;0m INFO 02-23 18:43:30 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-23T18:43:30.7286704Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m INFO 02-23 18:43:30 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-23T18:43:30.7839591Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-23 18:43:30 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-23T18:43:30.7877995Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-23 18:43:30 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-23T18:43:30.8021308Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m INFO 02-23 18:43:30 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-23T18:43:30.8030780Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-23 18:43:30 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-23T18:43:30.8095392Z [0;36m(Worker_DP1_TP7_EP15 pid=889)[0;0m INFO 02-23 18:43:30 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-23T18:43:30.8200674Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-23 18:43:30 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-23T18:43:33.8895698Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-23 18:43:33 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-23T18:43:33.9835011Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-23 18:43:33 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-23T18:43:34.0271231Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-23 18:43:34 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-23T18:43:34.0519492Z [0;36m(Worker_DP1_TP3_EP11 pid=475)[0;0m INFO 02-23 18:43:34 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-23T18:43:34.0795192Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-23 18:43:34 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-23T18:43:34.0795860Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-23 18:43:34 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-23T18:43:34.0973258Z [0;36m(Worker_DP0_TP5_EP5 pid=684)[0;0m INFO 02-23 18:43:34 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-23T18:43:34.1011319Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-23 18:43:34 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-23T18:43:34.1035985Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-23 18:43:34 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-23T18:43:34.1359262Z [0;36m(Worker_DP1_TP5_EP13 pid=681)[0;0m INFO 02-23 18:43:34 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-23T18:43:34.1587545Z [0;36m(Worker_DP1_TP6_EP14 pid=786)[0;0m INFO 02-23 18:43:34 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-23T18:43:34.1593412Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-23 18:43:34 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-23T18:43:34.1694576Z [0;36m(Worker_DP1_TP7_EP15 pid=889)[0;0m INFO 02-23 18:43:34 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-23T18:43:34.1873041Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-23 18:43:34 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-23T18:43:34.2806994Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m INFO 02-23 18:43:34 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-23T18:43:34.2977471Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m INFO 02-23 18:43:34 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-23T18:43:34.3548893Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:43:34.3549436Z Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
2026-02-23T18:43:35.4641526Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:43:35.4641928Z Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:01<02:59,  1.11s/it]
2026-02-23T18:43:38.4417262Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:43:38.4417713Z Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:04<05:55,  2.21s/it]
2026-02-23T18:43:39.8235198Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:43:39.8235641Z Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:05<04:52,  1.83s/it]
2026-02-23T18:43:41.1896112Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:43:41.1896936Z Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:06<04:21,  1.65s/it]
2026-02-23T18:43:44.0936895Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:43:44.0937337Z Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:09<05:31,  2.10s/it]
2026-02-23T18:43:46.7213510Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:43:46.7213959Z Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:12<05:57,  2.28s/it]
2026-02-23T18:43:49.3856791Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:43:49.3857201Z Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:15<06:15,  2.40s/it]
2026-02-23T18:43:51.7548072Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:43:51.7548529Z Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:17<06:11,  2.39s/it]
2026-02-23T18:43:53.0366799Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:43:53.0367528Z Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:18<05:15,  2.05s/it]
2026-02-23T18:43:54.3451061Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:43:54.3451573Z Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:19<04:38,  1.82s/it]
2026-02-23T18:43:55.6091491Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:43:55.6092091Z Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:21<04:10,  1.65s/it]
2026-02-23T18:43:56.9957528Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:43:56.9957999Z Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:22<03:56,  1.57s/it]
2026-02-23T18:43:58.4352937Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:43:58.4353497Z Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:24<03:49,  1.53s/it]
2026-02-23T18:44:02.1560308Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:02.1560776Z Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:27<05:26,  2.19s/it]
2026-02-23T18:44:04.6927547Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:04.6928001Z Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:30<05:39,  2.30s/it]
2026-02-23T18:44:07.3044018Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:07.3044550Z Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:32<05:51,  2.39s/it]
2026-02-23T18:44:09.8168893Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:09.8169359Z Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:35<05:54,  2.43s/it]
2026-02-23T18:44:12.3148429Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:12.3148851Z Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:37<05:55,  2.45s/it]
2026-02-23T18:44:13.6029702Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:13.6030144Z Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:39<05:02,  2.10s/it]
2026-02-23T18:44:14.9911873Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:14.9912565Z Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:40<04:29,  1.89s/it]
2026-02-23T18:44:18.3065761Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:18.3066276Z Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:43<05:28,  2.32s/it]
2026-02-23T18:44:19.5028231Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:19.5028743Z Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:45<04:39,  1.98s/it]
2026-02-23T18:44:20.9086743Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:20.9087275Z Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:46<04:13,  1.81s/it]
2026-02-23T18:44:22.3037939Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:22.3038446Z Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:47<03:54,  1.68s/it]
2026-02-23T18:44:25.9257770Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:25.9258231Z Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:51<05:12,  2.27s/it]
2026-02-23T18:44:26.9693248Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:26.9694010Z Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:52<04:20,  1.90s/it]
2026-02-23T18:44:28.3370257Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:28.3370714Z Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:53<03:56,  1.74s/it]
2026-02-23T18:44:31.1831166Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:31.1831707Z Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:56<04:39,  2.07s/it]
2026-02-23T18:44:32.8396866Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:32.8397289Z Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:58<04:20,  1.95s/it]
2026-02-23T18:44:34.3215489Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:34.3216129Z Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:59<04:00,  1.81s/it]
2026-02-23T18:44:35.7418917Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:35.7419461Z Loading safetensors checkpoint shards:  19% Completed | 31/163 [01:01<03:43,  1.69s/it]
2026-02-23T18:44:37.1510324Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:37.1510856Z Loading safetensors checkpoint shards:  20% Completed | 32/163 [01:02<03:30,  1.61s/it]
2026-02-23T18:44:38.7716613Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:38.7717136Z Loading safetensors checkpoint shards:  20% Completed | 33/163 [01:04<03:29,  1.61s/it]
2026-02-23T18:44:40.1761327Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:40.1761868Z Loading safetensors checkpoint shards:  21% Completed | 34/163 [01:05<03:19,  1.55s/it]
2026-02-23T18:44:41.5294966Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:41.5295556Z Loading safetensors checkpoint shards:  21% Completed | 35/163 [01:07<03:10,  1.49s/it]
2026-02-23T18:44:45.9676694Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:45.9677238Z Loading safetensors checkpoint shards:  22% Completed | 36/163 [01:11<05:01,  2.37s/it]
2026-02-23T18:44:46.7364535Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:46.7364945Z Loading safetensors checkpoint shards:  23% Completed | 37/163 [01:12<03:58,  1.89s/it]
2026-02-23T18:44:47.8558957Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:47.8559406Z Loading safetensors checkpoint shards:  23% Completed | 38/163 [01:13<03:27,  1.66s/it]
2026-02-23T18:44:49.2390017Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:49.2390476Z Loading safetensors checkpoint shards:  24% Completed | 39/163 [01:14<03:15,  1.58s/it]
2026-02-23T18:44:53.3086169Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:53.3086626Z Loading safetensors checkpoint shards:  25% Completed | 40/163 [01:18<04:45,  2.33s/it]
2026-02-23T18:44:54.2604527Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:54.2605015Z Loading safetensors checkpoint shards:  25% Completed | 41/163 [01:19<03:53,  1.91s/it]
2026-02-23T18:44:55.6369346Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:55.6369869Z Loading safetensors checkpoint shards:  26% Completed | 42/163 [01:21<03:32,  1.75s/it]
2026-02-23T18:44:59.2224359Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:44:59.2224890Z Loading safetensors checkpoint shards:  26% Completed | 43/163 [01:24<04:36,  2.30s/it]
2026-02-23T18:45:00.5901151Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:00.5901722Z Loading safetensors checkpoint shards:  27% Completed | 44/163 [01:26<04:00,  2.02s/it]
2026-02-23T18:45:01.9145701Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:01.9146194Z Loading safetensors checkpoint shards:  28% Completed | 45/163 [01:27<03:33,  1.81s/it]
2026-02-23T18:45:03.2133983Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:03.2134542Z Loading safetensors checkpoint shards:  28% Completed | 46/163 [01:28<03:14,  1.66s/it]
2026-02-23T18:45:09.2593822Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:09.2594381Z Loading safetensors checkpoint shards:  29% Completed | 47/163 [01:34<05:45,  2.97s/it]
2026-02-23T18:45:11.6022778Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:11.6023237Z Loading safetensors checkpoint shards:  29% Completed | 48/163 [01:37<05:20,  2.79s/it]
2026-02-23T18:45:11.7894156Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:11.7894620Z Loading safetensors checkpoint shards:  30% Completed | 49/163 [01:37<03:48,  2.01s/it]
2026-02-23T18:45:13.9753831Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:13.9754355Z Loading safetensors checkpoint shards:  31% Completed | 50/163 [01:39<03:52,  2.06s/it]
2026-02-23T18:45:16.5165972Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:16.5166481Z Loading safetensors checkpoint shards:  31% Completed | 51/163 [01:42<04:06,  2.20s/it]
2026-02-23T18:45:17.7488818Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:17.7489719Z Loading safetensors checkpoint shards:  32% Completed | 52/163 [01:43<03:32,  1.91s/it]
2026-02-23T18:45:20.6246303Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:20.6246853Z Loading safetensors checkpoint shards:  33% Completed | 53/163 [01:46<04:02,  2.20s/it]
2026-02-23T18:45:21.9003033Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:21.9003506Z Loading safetensors checkpoint shards:  33% Completed | 54/163 [01:47<03:29,  1.92s/it]
2026-02-23T18:45:23.1818584Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:23.1819033Z Loading safetensors checkpoint shards:  34% Completed | 55/163 [01:48<03:06,  1.73s/it]
2026-02-23T18:45:24.4182378Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:24.4182833Z Loading safetensors checkpoint shards:  34% Completed | 56/163 [01:50<02:49,  1.58s/it]
2026-02-23T18:45:25.7324342Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:25.7324788Z Loading safetensors checkpoint shards:  35% Completed | 57/163 [01:51<02:39,  1.50s/it]
2026-02-23T18:45:27.0746178Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:27.0746688Z Loading safetensors checkpoint shards:  36% Completed | 58/163 [01:52<02:32,  1.45s/it]
2026-02-23T18:45:31.1013976Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:31.1014563Z Loading safetensors checkpoint shards:  36% Completed | 59/163 [01:56<03:51,  2.23s/it]
2026-02-23T18:45:32.0660337Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:32.0660836Z Loading safetensors checkpoint shards:  37% Completed | 60/163 [01:57<03:10,  1.85s/it]
2026-02-23T18:45:33.4085666Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:33.4086152Z Loading safetensors checkpoint shards:  37% Completed | 61/163 [01:59<02:53,  1.70s/it]
2026-02-23T18:45:36.8885855Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:36.8886401Z Loading safetensors checkpoint shards:  38% Completed | 62/163 [02:02<03:45,  2.23s/it]
2026-02-23T18:45:38.1490095Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:38.1490679Z Loading safetensors checkpoint shards:  39% Completed | 63/163 [02:03<03:14,  1.94s/it]
2026-02-23T18:45:39.4489322Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:39.4489738Z Loading safetensors checkpoint shards:  39% Completed | 64/163 [02:05<02:53,  1.75s/it]
2026-02-23T18:45:42.6737834Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:42.6738270Z Loading safetensors checkpoint shards:  40% Completed | 65/163 [02:08<03:34,  2.19s/it]
2026-02-23T18:45:45.2194836Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:45.2195300Z Loading safetensors checkpoint shards:  40% Completed | 66/163 [02:10<03:42,  2.30s/it]
2026-02-23T18:45:46.2737882Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:46.2738294Z Loading safetensors checkpoint shards:  41% Completed | 67/163 [02:11<03:04,  1.92s/it]
2026-02-23T18:45:49.3593749Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:49.3594272Z Loading safetensors checkpoint shards:  42% Completed | 68/163 [02:15<03:35,  2.27s/it]
2026-02-23T18:45:50.4641569Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:50.4642162Z Loading safetensors checkpoint shards:  42% Completed | 69/163 [02:16<03:00,  1.92s/it]
2026-02-23T18:45:51.7160335Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:51.7160823Z Loading safetensors checkpoint shards:  43% Completed | 70/163 [02:17<02:40,  1.72s/it]
2026-02-23T18:45:54.6243324Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:54.6243853Z Loading safetensors checkpoint shards:  44% Completed | 71/163 [02:20<03:11,  2.08s/it]
2026-02-23T18:45:57.5371467Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:57.5372208Z Loading safetensors checkpoint shards:  44% Completed | 72/163 [02:23<03:31,  2.33s/it]
2026-02-23T18:45:58.8313621Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:45:58.8314305Z Loading safetensors checkpoint shards:  45% Completed | 73/163 [02:24<03:01,  2.02s/it]
2026-02-23T18:46:00.1366911Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:00.1367800Z Loading safetensors checkpoint shards:  45% Completed | 74/163 [02:25<02:40,  1.80s/it]
2026-02-23T18:46:01.5295164Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:01.5295688Z Loading safetensors checkpoint shards:  46% Completed | 75/163 [02:27<02:27,  1.68s/it]
2026-02-23T18:46:02.9817715Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:02.9818174Z Loading safetensors checkpoint shards:  47% Completed | 76/163 [02:28<02:20,  1.61s/it]
2026-02-23T18:46:07.1562484Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:07.1562906Z Loading safetensors checkpoint shards:  47% Completed | 77/163 [02:32<03:24,  2.38s/it]
2026-02-23T18:46:08.1448833Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:08.1449302Z Loading safetensors checkpoint shards:  48% Completed | 78/163 [02:33<02:46,  1.96s/it]
2026-02-23T18:46:09.4979608Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:09.4980062Z Loading safetensors checkpoint shards:  48% Completed | 79/163 [02:35<02:29,  1.78s/it]
2026-02-23T18:46:12.5600180Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:12.5600746Z Loading safetensors checkpoint shards:  49% Completed | 80/163 [02:38<02:59,  2.16s/it]
2026-02-23T18:46:15.1440635Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:15.1441244Z Loading safetensors checkpoint shards:  50% Completed | 81/163 [02:40<03:07,  2.29s/it]
2026-02-23T18:46:16.4530534Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:16.4531067Z Loading safetensors checkpoint shards:  50% Completed | 82/163 [02:42<02:41,  2.00s/it]
2026-02-23T18:46:19.3317841Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:19.3318414Z Loading safetensors checkpoint shards:  51% Completed | 83/163 [02:44<03:00,  2.26s/it]
2026-02-23T18:46:20.6671584Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:20.6672275Z Loading safetensors checkpoint shards:  52% Completed | 84/163 [02:46<02:36,  1.98s/it]
2026-02-23T18:46:23.6671371Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:23.6671960Z Loading safetensors checkpoint shards:  52% Completed | 85/163 [02:49<02:58,  2.29s/it]
2026-02-23T18:46:24.7404717Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:24.7405158Z Loading safetensors checkpoint shards:  53% Completed | 86/163 [02:50<02:28,  1.92s/it]
2026-02-23T18:46:26.6830605Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:26.6831089Z Loading safetensors checkpoint shards:  53% Completed | 87/163 [02:52<02:26,  1.93s/it]
2026-02-23T18:46:29.7034834Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:29.7035287Z Loading safetensors checkpoint shards:  54% Completed | 88/163 [02:55<02:49,  2.26s/it]
2026-02-23T18:46:30.9474996Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:30.9475483Z Loading safetensors checkpoint shards:  55% Completed | 89/163 [02:56<02:24,  1.95s/it]
2026-02-23T18:46:33.8157706Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:33.8158193Z Loading safetensors checkpoint shards:  55% Completed | 90/163 [02:59<02:42,  2.23s/it]
2026-02-23T18:46:35.0807431Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:35.0808601Z Loading safetensors checkpoint shards:  56% Completed | 91/163 [03:00<02:19,  1.94s/it]
2026-02-23T18:46:38.0427099Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:38.0427605Z Loading safetensors checkpoint shards:  56% Completed | 92/163 [03:03<02:39,  2.25s/it]
2026-02-23T18:46:39.2988073Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:39.2988604Z Loading safetensors checkpoint shards:  57% Completed | 93/163 [03:04<02:16,  1.95s/it]
2026-02-23T18:46:40.7719631Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:40.7720170Z Loading safetensors checkpoint shards:  58% Completed | 94/163 [03:06<02:04,  1.81s/it]
2026-02-23T18:46:42.3697565Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:42.3698218Z Loading safetensors checkpoint shards:  58% Completed | 95/163 [03:08<01:58,  1.74s/it]
2026-02-23T18:46:45.8688365Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:45.8688944Z Loading safetensors checkpoint shards:  59% Completed | 96/163 [03:11<02:32,  2.27s/it]
2026-02-23T18:46:46.9956061Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:46.9956558Z Loading safetensors checkpoint shards:  60% Completed | 97/163 [03:12<02:07,  1.93s/it]
2026-02-23T18:46:48.3020342Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:48.3020793Z Loading safetensors checkpoint shards:  60% Completed | 98/163 [03:13<01:53,  1.74s/it]
2026-02-23T18:46:49.5973910Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:49.5974488Z Loading safetensors checkpoint shards:  61% Completed | 99/163 [03:15<01:42,  1.61s/it]
2026-02-23T18:46:51.0168522Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:51.0168947Z Loading safetensors checkpoint shards:  61% Completed | 100/163 [03:16<01:37,  1.55s/it]
2026-02-23T18:46:52.2060473Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:52.2061039Z Loading safetensors checkpoint shards:  62% Completed | 101/163 [03:17<01:29,  1.44s/it]
2026-02-23T18:46:55.1448313Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:55.1448783Z Loading safetensors checkpoint shards:  63% Completed | 102/163 [03:20<01:55,  1.89s/it]
2026-02-23T18:46:57.6515472Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:46:57.6515935Z Loading safetensors checkpoint shards:  63% Completed | 103/163 [03:23<02:04,  2.08s/it]
2026-02-23T18:47:00.0151797Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:00.0152380Z Loading safetensors checkpoint shards:  64% Completed | 104/163 [03:25<02:07,  2.16s/it]
2026-02-23T18:47:01.2388810Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:01.2389350Z Loading safetensors checkpoint shards:  64% Completed | 105/163 [03:26<01:49,  1.88s/it]
2026-02-23T18:47:04.0965769Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:04.0966225Z Loading safetensors checkpoint shards:  65% Completed | 106/163 [03:29<02:03,  2.17s/it]
2026-02-23T18:47:05.3380662Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:05.3381108Z Loading safetensors checkpoint shards:  66% Completed | 107/163 [03:30<01:46,  1.89s/it]
2026-02-23T18:47:07.8520145Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:07.8520566Z Loading safetensors checkpoint shards:  66% Completed | 108/163 [03:33<01:54,  2.08s/it]
2026-02-23T18:47:09.0905277Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:09.0905736Z Loading safetensors checkpoint shards:  67% Completed | 109/163 [03:34<01:38,  1.83s/it]
2026-02-23T18:47:11.7900930Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:11.7901366Z Loading safetensors checkpoint shards:  67% Completed | 110/163 [03:37<01:50,  2.09s/it]
2026-02-23T18:47:14.4026028Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:14.4026491Z Loading safetensors checkpoint shards:  68% Completed | 111/163 [03:40<01:56,  2.25s/it]
2026-02-23T18:47:17.0329271Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:17.0329752Z Loading safetensors checkpoint shards:  69% Completed | 112/163 [03:42<02:00,  2.36s/it]
2026-02-23T18:47:19.6641458Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:19.6641922Z Loading safetensors checkpoint shards:  69% Completed | 113/163 [03:45<02:02,  2.44s/it]
2026-02-23T18:47:20.7533821Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:20.7534286Z Loading safetensors checkpoint shards:  70% Completed | 114/163 [03:46<01:39,  2.04s/it]
2026-02-23T18:47:22.0090781Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:22.0091235Z Loading safetensors checkpoint shards:  71% Completed | 115/163 [03:47<01:26,  1.80s/it]
2026-02-23T18:47:25.2725974Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:25.2726508Z Loading safetensors checkpoint shards:  71% Completed | 116/163 [03:50<01:45,  2.24s/it]
2026-02-23T18:47:26.5363102Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:26.5363958Z Loading safetensors checkpoint shards:  72% Completed | 117/163 [03:52<01:29,  1.95s/it]
2026-02-23T18:47:29.2805893Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:29.2806415Z Loading safetensors checkpoint shards:  72% Completed | 118/163 [03:54<01:38,  2.19s/it]
2026-02-23T18:47:30.5518582Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:30.5519129Z Loading safetensors checkpoint shards:  73% Completed | 119/163 [03:56<01:24,  1.91s/it]
2026-02-23T18:47:31.8365511Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:31.8365939Z Loading safetensors checkpoint shards:  74% Completed | 120/163 [03:57<01:14,  1.72s/it]
2026-02-23T18:47:33.1697444Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:33.1697895Z Loading safetensors checkpoint shards:  74% Completed | 121/163 [03:58<01:07,  1.61s/it]
2026-02-23T18:47:34.5397443Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:34.5397893Z Loading safetensors checkpoint shards:  75% Completed | 122/163 [04:00<01:02,  1.54s/it]
2026-02-23T18:47:38.1202437Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:38.1202922Z Loading safetensors checkpoint shards:  75% Completed | 123/163 [04:03<01:25,  2.15s/it]
2026-02-23T18:47:39.3969308Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:39.3969780Z Loading safetensors checkpoint shards:  76% Completed | 124/163 [04:05<01:13,  1.89s/it]
2026-02-23T18:47:42.2009036Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:42.2009484Z Loading safetensors checkpoint shards:  77% Completed | 125/163 [04:07<01:22,  2.16s/it]
2026-02-23T18:47:43.4907645Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:43.4908116Z Loading safetensors checkpoint shards:  77% Completed | 126/163 [04:09<01:10,  1.90s/it]
2026-02-23T18:47:44.7752276Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:44.7752837Z Loading safetensors checkpoint shards:  78% Completed | 127/163 [04:10<01:01,  1.72s/it]
2026-02-23T18:47:48.0180387Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:48.0181050Z Loading safetensors checkpoint shards:  79% Completed | 128/163 [04:13<01:16,  2.17s/it]
2026-02-23T18:47:50.8413599Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:50.8414156Z Loading safetensors checkpoint shards:  79% Completed | 129/163 [04:16<01:20,  2.37s/it]
2026-02-23T18:47:51.9040310Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:51.9040839Z Loading safetensors checkpoint shards:  80% Completed | 130/163 [04:17<01:05,  1.98s/it]
2026-02-23T18:47:53.1160451Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:53.1161034Z Loading safetensors checkpoint shards:  80% Completed | 131/163 [04:18<00:55,  1.75s/it]
2026-02-23T18:47:54.5436294Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:54.5436781Z Loading safetensors checkpoint shards:  81% Completed | 132/163 [04:20<00:51,  1.65s/it]
2026-02-23T18:47:58.6826104Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:58.6826525Z Loading safetensors checkpoint shards:  82% Completed | 133/163 [04:24<01:11,  2.40s/it]
2026-02-23T18:47:59.6821277Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:47:59.6822138Z Loading safetensors checkpoint shards:  82% Completed | 134/163 [04:25<00:57,  1.98s/it]
2026-02-23T18:48:02.7280146Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:02.7280621Z Loading safetensors checkpoint shards:  83% Completed | 135/163 [04:28<01:04,  2.30s/it]
2026-02-23T18:48:03.9070932Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:03.9071403Z Loading safetensors checkpoint shards:  83% Completed | 136/163 [04:29<00:52,  1.96s/it]
2026-02-23T18:48:05.1535056Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:05.1535602Z Loading safetensors checkpoint shards:  84% Completed | 137/163 [04:30<00:45,  1.75s/it]
2026-02-23T18:48:08.2274085Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:10.8325287Z Loading safetensors checkpoint shards:  85% Completed | 138/163 [04:33<00:53,  2.15s/it]
2026-02-23T18:48:10.8326002Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:10.8326802Z Loading safetensors checkpoint shards:  85% Completed | 139/163 [04:36<00:54,  2.28s/it]
2026-02-23T18:48:12.1553231Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:12.1553771Z Loading safetensors checkpoint shards:  86% Completed | 140/163 [04:37<00:45,  2.00s/it]
2026-02-23T18:48:13.6941727Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:13.6942401Z Loading safetensors checkpoint shards:  87% Completed | 141/163 [04:39<00:40,  1.86s/it]
2026-02-23T18:48:15.0254934Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:15.0255393Z Loading safetensors checkpoint shards:  87% Completed | 142/163 [04:40<00:35,  1.70s/it]
2026-02-23T18:48:18.2911144Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:18.2911647Z Loading safetensors checkpoint shards:  88% Completed | 143/163 [04:43<00:43,  2.17s/it]
2026-02-23T18:48:20.4006898Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:20.4007395Z Loading safetensors checkpoint shards:  88% Completed | 144/163 [04:46<00:40,  2.15s/it]
2026-02-23T18:48:22.0923139Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:22.0923597Z Loading safetensors checkpoint shards:  89% Completed | 145/163 [04:47<00:36,  2.01s/it]
2026-02-23T18:48:24.6075874Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:24.6076416Z Loading safetensors checkpoint shards:  90% Completed | 146/163 [04:50<00:36,  2.16s/it]
2026-02-23T18:48:26.5614806Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:26.5615349Z Loading safetensors checkpoint shards:  90% Completed | 147/163 [04:52<00:33,  2.10s/it]
2026-02-23T18:48:27.8390604Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:27.8391153Z Loading safetensors checkpoint shards:  91% Completed | 148/163 [04:53<00:27,  1.85s/it]
2026-02-23T18:48:29.0507690Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:29.0508230Z Loading safetensors checkpoint shards:  91% Completed | 149/163 [04:54<00:23,  1.66s/it]
2026-02-23T18:48:31.8784373Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:31.8784951Z Loading safetensors checkpoint shards:  93% Completed | 151/163 [04:57<00:18,  1.55s/it]
2026-02-23T18:48:33.1996954Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:33.1997492Z Loading safetensors checkpoint shards:  93% Completed | 152/163 [04:58<00:16,  1.49s/it]
2026-02-23T18:48:34.4451242Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:34.4451664Z Loading safetensors checkpoint shards:  94% Completed | 153/163 [05:00<00:14,  1.43s/it]
2026-02-23T18:48:36.5424679Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:36.5425145Z Loading safetensors checkpoint shards:  94% Completed | 154/163 [05:02<00:14,  1.61s/it]
2026-02-23T18:48:37.5796978Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:37.5797458Z Loading safetensors checkpoint shards:  95% Completed | 155/163 [05:03<00:11,  1.45s/it]
2026-02-23T18:48:39.2891234Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:39.2891717Z Loading safetensors checkpoint shards:  96% Completed | 156/163 [05:04<00:10,  1.52s/it]
2026-02-23T18:48:40.5705517Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:40.5706043Z Loading safetensors checkpoint shards:  96% Completed | 157/163 [05:06<00:08,  1.45s/it]
2026-02-23T18:48:44.6319703Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:44.6320139Z Loading safetensors checkpoint shards:  97% Completed | 158/163 [05:10<00:11,  2.22s/it]
2026-02-23T18:48:46.2262382Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:46.2262935Z Loading safetensors checkpoint shards:  98% Completed | 159/163 [05:11<00:08,  2.03s/it]
2026-02-23T18:48:47.6121303Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:47.6121909Z Loading safetensors checkpoint shards:  98% Completed | 160/163 [05:13<00:05,  1.84s/it]
2026-02-23T18:48:48.9922974Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:48.9923751Z Loading safetensors checkpoint shards:  99% Completed | 161/163 [05:14<00:03,  1.70s/it]
2026-02-23T18:48:52.1096067Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:52.1096676Z Loading safetensors checkpoint shards:  99% Completed | 162/163 [05:17<00:02,  2.13s/it]
2026-02-23T18:48:53.4726696Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:53.4727249Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [05:19<00:00,  1.90s/it]
2026-02-23T18:48:53.4746194Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:53.4746598Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [05:19<00:00,  1.96s/it]
2026-02-23T18:48:53.4755186Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:48:53.4860739Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m INFO 02-23 18:48:53 [default_loader.py:291] Loading weights took 319.13 seconds
2026-02-23T18:48:54.4591745Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m INFO 02-23 18:48:54 [default_loader.py:291] Loading weights took 320.12 seconds
2026-02-23T18:49:06.8045752Z INFO 02-23 18:49:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:49:06.8053831Z INFO 02-23 18:49:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:49:06.8065568Z INFO 02-23 18:49:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:49:06.8079805Z INFO 02-23 18:49:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:49:06.8091275Z INFO 02-23 18:49:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:49:06.8100954Z INFO 02-23 18:49:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:49:06.8167864Z INFO 02-23 18:49:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:49:06.8197007Z INFO 02-23 18:49:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:49:06.8206003Z INFO 02-23 18:49:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:49:06.8215731Z INFO 02-23 18:49:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:49:06.8225435Z INFO 02-23 18:49:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:49:06.8235246Z INFO 02-23 18:49:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:49:06.8246272Z INFO 02-23 18:49:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:49:06.8256264Z INFO 02-23 18:49:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:49:06.8268552Z INFO 02-23 18:49:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:49:06.8301103Z INFO 02-23 18:49:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:49:06.8867243Z INFO 02-23 18:49:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:49:06.8875379Z INFO 02-23 18:49:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:49:06.8885413Z INFO 02-23 18:49:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:49:06.8907366Z INFO 02-23 18:49:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:49:06.8917043Z INFO 02-23 18:49:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:49:06.8927236Z INFO 02-23 18:49:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:49:06.8953387Z INFO 02-23 18:49:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:49:06.8976255Z INFO 02-23 18:49:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:49:06.9793873Z INFO 02-23 18:49:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:49:06.9803710Z INFO 02-23 18:49:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:49:06.9813871Z INFO 02-23 18:49:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:49:06.9838345Z INFO 02-23 18:49:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:49:06.9847580Z INFO 02-23 18:49:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:49:06.9857311Z INFO 02-23 18:49:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:49:06.9880507Z INFO 02-23 18:49:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:49:06.9926537Z INFO 02-23 18:49:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:49:07.0002277Z INFO 02-23 18:49:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:49:07.0011487Z INFO 02-23 18:49:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:49:07.0021501Z INFO 02-23 18:49:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:49:07.0043604Z INFO 02-23 18:49:07 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:49:07.0081665Z INFO 02-23 18:49:07 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:49:07.0091531Z INFO 02-23 18:49:07 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:49:07.0100796Z INFO 02-23 18:49:07 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:49:07.0115545Z INFO 02-23 18:49:07 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:49:07.0154752Z INFO 02-23 18:49:07 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:49:07.0165871Z INFO 02-23 18:49:07 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:49:07.0175958Z INFO 02-23 18:49:07 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:49:07.0184868Z INFO 02-23 18:49:07 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:49:07.0195653Z INFO 02-23 18:49:07 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:49:07.0205570Z INFO 02-23 18:49:07 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:49:07.0242340Z INFO 02-23 18:49:07 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:49:07.0258567Z INFO 02-23 18:49:07 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:49:07.0268489Z INFO 02-23 18:49:07 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:49:07.0279454Z INFO 02-23 18:49:07 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:49:07.0288592Z INFO 02-23 18:49:07 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:49:07.0314926Z INFO 02-23 18:49:07 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:49:07.0324667Z INFO 02-23 18:49:07 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:49:07.0334190Z INFO 02-23 18:49:07 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:49:07.0344587Z INFO 02-23 18:49:07 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:49:07.0371055Z INFO 02-23 18:49:07 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:49:07.0813044Z INFO 02-23 18:49:07 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:49:07.0821408Z INFO 02-23 18:49:07 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:49:07.0831655Z INFO 02-23 18:49:07 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:49:07.0899402Z INFO 02-23 18:49:07 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:49:07.2425082Z INFO 02-23 18:49:07 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-23T18:49:07.2433304Z INFO 02-23 18:49:07 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-23T18:49:07.2444257Z INFO 02-23 18:49:07 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-23T18:49:07.2521480Z INFO 02-23 18:49:07 [__init__.py:217] Platform plugin ascend is activated
2026-02-23T18:49:29.0043525Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m WARNING 02-23 18:49:29 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-23T18:49:29.0047549Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m WARNING 02-23 18:49:29 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-23T18:49:29.0056265Z [0;36m(Worker_DP1_TP5_EP13 pid=681)[0;0m WARNING 02-23 18:49:29 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-23T18:49:29.0078512Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m WARNING 02-23 18:49:29 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-23T18:49:29.0126252Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m WARNING 02-23 18:49:29 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-23T18:49:29.0146468Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m WARNING 02-23 18:49:29 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-23T18:49:29.0168576Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m WARNING 02-23 18:49:29 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-23T18:49:29.0223102Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m WARNING 02-23 18:49:29 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-23T18:49:29.0290084Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m WARNING 02-23 18:49:29 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-23T18:49:29.0321337Z [0;36m(Worker_DP1_TP7_EP15 pid=889)[0;0m WARNING 02-23 18:49:29 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-23T18:49:29.0589704Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m WARNING 02-23 18:49:29 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-23T18:49:29.0615202Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m WARNING 02-23 18:49:29 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-23T18:49:29.0639853Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-23 18:49:29 [model_runner_v1.py:2315] Loading drafter model...
2026-02-23T18:49:29.0649434Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-23 18:49:29 [model_runner_v1.py:2315] Loading drafter model...
2026-02-23T18:49:29.0662911Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-23 18:49:29 [model_runner_v1.py:2315] Loading drafter model...
2026-02-23T18:49:29.0713669Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-23 18:49:29 [model_runner_v1.py:2315] Loading drafter model...
2026-02-23T18:49:29.0735099Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-23 18:49:29 [model_runner_v1.py:2315] Loading drafter model...
2026-02-23T18:49:29.0746521Z [0;36m(Worker_DP1_TP3_EP11 pid=475)[0;0m WARNING 02-23 18:49:29 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-23T18:49:29.0757632Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m INFO 02-23 18:49:29 [model_runner_v1.py:2315] Loading drafter model...
2026-02-23T18:49:29.0784293Z [0;36m(Worker_DP1_TP5_EP13 pid=681)[0;0m INFO 02-23 18:49:29 [model_runner_v1.py:2315] Loading drafter model...
2026-02-23T18:49:29.0892073Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-23 18:49:29 [model_runner_v1.py:2315] Loading drafter model...
2026-02-23T18:49:29.0901784Z [0;36m(Worker_DP1_TP7_EP15 pid=889)[0;0m INFO 02-23 18:49:29 [model_runner_v1.py:2315] Loading drafter model...
2026-02-23T18:49:29.0985205Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-23 18:49:29 [model_runner_v1.py:2315] Loading drafter model...
2026-02-23T18:49:29.1007574Z [0;36m(Worker_DP0_TP5_EP5 pid=684)[0;0m WARNING 02-23 18:49:29 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-23T18:49:29.1181466Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m WARNING 02-23 18:49:29 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-23T18:49:29.1190328Z [0;36m(Worker_DP1_TP6_EP14 pid=786)[0;0m WARNING 02-23 18:49:29 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-23T18:49:29.1223512Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m INFO 02-23 18:49:29 [model_runner_v1.py:2315] Loading drafter model...
2026-02-23T18:49:29.1681472Z [0;36m(Worker_DP1_TP3_EP11 pid=475)[0;0m INFO 02-23 18:49:29 [model_runner_v1.py:2315] Loading drafter model...
2026-02-23T18:49:29.1714372Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-23 18:49:29 [model_runner_v1.py:2315] Loading drafter model...
2026-02-23T18:49:29.1722095Z [0;36m(Worker_DP0_TP5_EP5 pid=684)[0;0m INFO 02-23 18:49:29 [model_runner_v1.py:2315] Loading drafter model...
2026-02-23T18:49:29.1822742Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-23 18:49:29 [model_runner_v1.py:2315] Loading drafter model...
2026-02-23T18:49:29.2171638Z [0;36m(Worker_DP1_TP6_EP14 pid=786)[0;0m INFO 02-23 18:49:29 [model_runner_v1.py:2315] Loading drafter model...
2026-02-23T18:49:29.2326159Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:29.2328240Z Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
2026-02-23T18:49:30.2731411Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:30.2731945Z Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:01<02:48,  1.04s/it]
2026-02-23T18:49:31.2985767Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:31.2986285Z Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:02<02:46,  1.03s/it]
2026-02-23T18:49:32.3874400Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:32.3875094Z Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:03<02:49,  1.06s/it]
2026-02-23T18:49:33.5190538Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:33.5191109Z Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:04<02:52,  1.09s/it]
2026-02-23T18:49:34.8088993Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:34.8089547Z Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:05<03:03,  1.16s/it]
2026-02-23T18:49:36.0650109Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:36.0650541Z Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:06<03:07,  1.19s/it]
2026-02-23T18:49:37.3358408Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:37.3359101Z Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:08<03:10,  1.22s/it]
2026-02-23T18:49:38.5336022Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:38.5336490Z Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:09<03:07,  1.21s/it]
2026-02-23T18:49:39.8058919Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:40.9980129Z Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:10<03:09,  1.23s/it]
2026-02-23T18:49:40.9980850Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:40.9981224Z Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:11<03:06,  1.22s/it]
2026-02-23T18:49:42.1486639Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:42.1487078Z Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:12<03:02,  1.20s/it]
2026-02-23T18:49:43.3821400Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:43.3822410Z Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:14<03:02,  1.21s/it]
2026-02-23T18:49:44.6617043Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:44.6617583Z Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:15<03:04,  1.23s/it]
2026-02-23T18:49:45.8813743Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:45.8814201Z Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:16<03:02,  1.23s/it]
2026-02-23T18:49:46.8571694Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:46.8572242Z Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:17<02:50,  1.15s/it]
2026-02-23T18:49:47.8268482Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:47.8268935Z Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:18<02:41,  1.10s/it]
2026-02-23T18:49:48.8196746Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:48.8197244Z Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:19<02:35,  1.07s/it]
2026-02-23T18:49:49.7675179Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:49.7675640Z Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:20<02:29,  1.03s/it]
2026-02-23T18:49:50.8310963Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:50.8311415Z Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:21<02:29,  1.04s/it]
2026-02-23T18:49:51.9590136Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:51.9590572Z Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:22<02:32,  1.06s/it]
2026-02-23T18:49:53.0400450Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:53.0400907Z Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:23<02:32,  1.07s/it]
2026-02-23T18:49:54.1012692Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:54.1013222Z Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:24<02:30,  1.07s/it]
2026-02-23T18:49:55.1357415Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:55.1357975Z Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:25<02:28,  1.06s/it]
2026-02-23T18:49:56.1947017Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:56.1947541Z Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:26<02:27,  1.06s/it]
2026-02-23T18:49:57.1695033Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:57.1695563Z Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:27<02:22,  1.03s/it]
2026-02-23T18:49:58.4335577Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:58.4336074Z Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:29<02:31,  1.10s/it]
2026-02-23T18:49:59.5601414Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:49:59.5601886Z Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:30<02:30,  1.11s/it]
2026-02-23T18:50:00.5624255Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:00.5624753Z Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:31<02:25,  1.08s/it]
2026-02-23T18:50:01.6413741Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:01.6414179Z Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:32<02:24,  1.08s/it]
2026-02-23T18:50:02.6850449Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:02.6850929Z Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:33<02:21,  1.07s/it]
2026-02-23T18:50:03.8195517Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:03.8195974Z Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:34<02:23,  1.09s/it]
2026-02-23T18:50:04.9720859Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:04.9721288Z Loading safetensors checkpoint shards:  20% Completed | 32/163 [00:35<02:25,  1.11s/it]
2026-02-23T18:50:06.0866271Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:06.0866732Z Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:36<02:24,  1.11s/it]
2026-02-23T18:50:07.2018573Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:07.2019169Z Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:37<02:23,  1.11s/it]
2026-02-23T18:50:08.2874357Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:08.2874876Z Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:39<02:21,  1.10s/it]
2026-02-23T18:50:09.2631622Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:09.2632239Z Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:40<02:15,  1.07s/it]
2026-02-23T18:50:10.3238041Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:10.3238578Z Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:41<02:14,  1.06s/it]
2026-02-23T18:50:11.3379023Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:11.3379596Z Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:42<02:11,  1.05s/it]
2026-02-23T18:50:12.4865645Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:12.4866220Z Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:43<02:13,  1.08s/it]
2026-02-23T18:50:13.5541789Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:13.5542331Z Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:44<02:12,  1.08s/it]
2026-02-23T18:50:14.6062306Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:14.6062771Z Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:45<02:10,  1.07s/it]
2026-02-23T18:50:15.7210889Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:15.7211328Z Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:46<02:10,  1.08s/it]
2026-02-23T18:50:16.8090657Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:16.8091127Z Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:47<02:10,  1.08s/it]
2026-02-23T18:50:17.8213230Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:17.8213917Z Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:48<02:06,  1.06s/it]
2026-02-23T18:50:18.8805777Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:18.8806287Z Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:49<02:05,  1.06s/it]
2026-02-23T18:50:20.0608823Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:20.0609359Z Loading safetensors checkpoint shards:  28% Completed | 46/163 [00:50<02:08,  1.10s/it]
2026-02-23T18:50:20.8026708Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:20.8027330Z Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:51<01:54,  1.01it/s]
2026-02-23T18:50:21.9700637Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:21.9701182Z Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:52<01:31,  1.25it/s]
2026-02-23T18:50:22.1006573Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:22.1007026Z Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:52<00:47,  2.35it/s]
2026-02-23T18:50:22.2145365Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:22.2145809Z Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:52<00:28,  3.78it/s]
2026-02-23T18:50:22.3225603Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:22.3225970Z Loading safetensors checkpoint shards:  36% Completed | 58/163 [00:53<00:18,  5.57it/s]
2026-02-23T18:50:22.4302498Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:22.4303012Z Loading safetensors checkpoint shards:  37% Completed | 61/163 [00:53<00:13,  7.71it/s]
2026-02-23T18:50:22.5425479Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:22.5425967Z Loading safetensors checkpoint shards:  39% Completed | 64/163 [00:53<00:09, 10.09it/s]
2026-02-23T18:50:22.6529021Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:22.6529482Z Loading safetensors checkpoint shards:  41% Completed | 67/163 [00:53<00:07, 12.66it/s]
2026-02-23T18:50:24.9551665Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:24.9552475Z Loading safetensors checkpoint shards:  43% Completed | 70/163 [00:55<00:27,  3.38it/s]
2026-02-23T18:50:27.4095040Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:27.4095516Z Loading safetensors checkpoint shards:  44% Completed | 72/163 [00:58<00:46,  1.96it/s]
2026-02-23T18:50:29.8676186Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:29.8676652Z Loading safetensors checkpoint shards:  45% Completed | 74/163 [01:00<01:01,  1.45it/s]
2026-02-23T18:50:30.6580704Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:30.6581137Z Loading safetensors checkpoint shards:  46% Completed | 75/163 [01:01<01:01,  1.42it/s]
2026-02-23T18:50:31.5134408Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:31.5134903Z Loading safetensors checkpoint shards:  47% Completed | 76/163 [01:02<01:03,  1.37it/s]
2026-02-23T18:50:32.3319731Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:32.3320219Z Loading safetensors checkpoint shards:  47% Completed | 77/163 [01:03<01:04,  1.34it/s]
2026-02-23T18:50:33.1050419Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:33.1050972Z Loading safetensors checkpoint shards:  48% Completed | 78/163 [01:03<01:04,  1.33it/s]
2026-02-23T18:50:33.9468368Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:33.9468875Z Loading safetensors checkpoint shards:  48% Completed | 79/163 [01:04<01:05,  1.29it/s]
2026-02-23T18:50:34.6389534Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:34.6389943Z Loading safetensors checkpoint shards:  49% Completed | 80/163 [01:05<01:02,  1.33it/s]
2026-02-23T18:50:35.3247634Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:35.3248055Z Loading safetensors checkpoint shards:  50% Completed | 81/163 [01:06<01:00,  1.36it/s]
2026-02-23T18:50:35.8605256Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:35.8605701Z Loading safetensors checkpoint shards:  50% Completed | 82/163 [01:06<00:55,  1.47it/s]
2026-02-23T18:50:36.3366141Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:36.3366582Z Loading safetensors checkpoint shards:  51% Completed | 83/163 [01:07<00:49,  1.61it/s]
2026-02-23T18:50:37.4204982Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:37.4205425Z Loading safetensors checkpoint shards:  52% Completed | 84/163 [01:08<00:59,  1.32it/s]
2026-02-23T18:50:38.6246727Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:38.6247323Z Loading safetensors checkpoint shards:  52% Completed | 85/163 [01:09<01:09,  1.13it/s]
2026-02-23T18:50:39.8277751Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:41.0140975Z Loading safetensors checkpoint shards:  53% Completed | 86/163 [01:10<01:15,  1.02it/s]
2026-02-23T18:50:41.0141715Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:41.0142209Z Loading safetensors checkpoint shards:  53% Completed | 87/163 [01:11<01:19,  1.04s/it]
2026-02-23T18:50:42.2685701Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:42.2686150Z Loading safetensors checkpoint shards:  54% Completed | 88/163 [01:13<01:22,  1.10s/it]
2026-02-23T18:50:43.3200443Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:43.3201204Z Loading safetensors checkpoint shards:  55% Completed | 89/163 [01:14<01:20,  1.09s/it]
2026-02-23T18:50:44.3474509Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:44.3475020Z Loading safetensors checkpoint shards:  55% Completed | 90/163 [01:15<01:18,  1.07s/it]
2026-02-23T18:50:45.4315281Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:45.4315716Z Loading safetensors checkpoint shards:  56% Completed | 91/163 [01:16<01:17,  1.07s/it]
2026-02-23T18:50:46.3923096Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:46.3923522Z Loading safetensors checkpoint shards:  56% Completed | 92/163 [01:17<01:13,  1.04s/it]
2026-02-23T18:50:47.4651517Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:47.4652096Z Loading safetensors checkpoint shards:  57% Completed | 93/163 [01:18<01:13,  1.05s/it]
2026-02-23T18:50:48.5007351Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:48.5008087Z Loading safetensors checkpoint shards:  58% Completed | 94/163 [01:19<01:12,  1.05s/it]
2026-02-23T18:50:49.5868227Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:49.5868674Z Loading safetensors checkpoint shards:  58% Completed | 95/163 [01:20<01:11,  1.06s/it]
2026-02-23T18:50:50.7113981Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:50.7114437Z Loading safetensors checkpoint shards:  59% Completed | 96/163 [01:21<01:12,  1.08s/it]
2026-02-23T18:50:51.7667868Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:51.7668286Z Loading safetensors checkpoint shards:  60% Completed | 97/163 [01:22<01:10,  1.07s/it]
2026-02-23T18:50:52.8377669Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:52.8378118Z Loading safetensors checkpoint shards:  60% Completed | 98/163 [01:23<01:09,  1.07s/it]
2026-02-23T18:50:53.9220916Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:53.9221448Z Loading safetensors checkpoint shards:  61% Completed | 99/163 [01:24<01:08,  1.08s/it]
2026-02-23T18:50:55.0198257Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:55.0198829Z Loading safetensors checkpoint shards:  61% Completed | 100/163 [01:25<01:08,  1.08s/it]
2026-02-23T18:50:56.3301821Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:56.3302472Z Loading safetensors checkpoint shards:  62% Completed | 101/163 [01:27<01:11,  1.15s/it]
2026-02-23T18:50:57.4204740Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:57.4205275Z Loading safetensors checkpoint shards:  63% Completed | 102/163 [01:28<01:09,  1.13s/it]
2026-02-23T18:50:58.4666901Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:58.4667538Z Loading safetensors checkpoint shards:  63% Completed | 103/163 [01:29<01:06,  1.11s/it]
2026-02-23T18:50:59.4354068Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:50:59.4354514Z Loading safetensors checkpoint shards:  64% Completed | 104/163 [01:30<01:02,  1.07s/it]
2026-02-23T18:51:00.4612144Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:00.4612612Z Loading safetensors checkpoint shards:  64% Completed | 105/163 [01:31<01:01,  1.05s/it]
2026-02-23T18:51:01.4241925Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:01.4242511Z Loading safetensors checkpoint shards:  65% Completed | 106/163 [01:32<00:58,  1.03s/it]
2026-02-23T18:51:03.7090320Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:03.7090756Z Loading safetensors checkpoint shards:  66% Completed | 107/163 [01:34<01:18,  1.40s/it]
2026-02-23T18:51:04.4700843Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:04.4701307Z Loading safetensors checkpoint shards:  66% Completed | 108/163 [01:35<01:06,  1.21s/it]
2026-02-23T18:51:05.5264761Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:05.5265238Z Loading safetensors checkpoint shards:  67% Completed | 109/163 [01:36<01:02,  1.16s/it]
2026-02-23T18:51:06.5820544Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:06.5821206Z Loading safetensors checkpoint shards:  67% Completed | 110/163 [01:37<00:59,  1.13s/it]
2026-02-23T18:51:07.5900336Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:07.5900899Z Loading safetensors checkpoint shards:  68% Completed | 111/163 [01:38<00:56,  1.09s/it]
2026-02-23T18:51:08.5231509Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:08.5232128Z Loading safetensors checkpoint shards:  69% Completed | 112/163 [01:39<00:53,  1.05s/it]
2026-02-23T18:51:09.4844024Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:09.4844593Z Loading safetensors checkpoint shards:  69% Completed | 113/163 [01:40<00:51,  1.02s/it]
2026-02-23T18:51:10.4527712Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:10.4528286Z Loading safetensors checkpoint shards:  70% Completed | 114/163 [01:41<00:49,  1.01s/it]
2026-02-23T18:51:11.4824681Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:11.4825228Z Loading safetensors checkpoint shards:  71% Completed | 115/163 [01:42<00:48,  1.01s/it]
2026-02-23T18:51:12.5255089Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:12.5255644Z Loading safetensors checkpoint shards:  71% Completed | 116/163 [01:43<00:48,  1.02s/it]
2026-02-23T18:51:13.5676485Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:13.5676905Z Loading safetensors checkpoint shards:  72% Completed | 117/163 [01:44<00:47,  1.03s/it]
2026-02-23T18:51:14.5206656Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:14.5207196Z Loading safetensors checkpoint shards:  72% Completed | 118/163 [01:45<00:45,  1.01s/it]
2026-02-23T18:51:15.5438321Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:15.5438817Z Loading safetensors checkpoint shards:  73% Completed | 119/163 [01:46<00:44,  1.01s/it]
2026-02-23T18:51:16.5696948Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:16.5697380Z Loading safetensors checkpoint shards:  74% Completed | 120/163 [01:47<00:43,  1.02s/it]
2026-02-23T18:51:17.6510555Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:17.6511044Z Loading safetensors checkpoint shards:  74% Completed | 121/163 [01:48<00:43,  1.04s/it]
2026-02-23T18:51:18.7653012Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:18.7653460Z Loading safetensors checkpoint shards:  75% Completed | 122/163 [01:49<00:43,  1.06s/it]
2026-02-23T18:51:19.8304543Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:19.8305004Z Loading safetensors checkpoint shards:  75% Completed | 123/163 [01:50<00:42,  1.06s/it]
2026-02-23T18:51:20.9785687Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:20.9786146Z Loading safetensors checkpoint shards:  76% Completed | 124/163 [01:51<00:42,  1.09s/it]
2026-02-23T18:51:22.0366272Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:22.0366758Z Loading safetensors checkpoint shards:  77% Completed | 125/163 [01:52<00:40,  1.08s/it]
2026-02-23T18:51:23.0976165Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:23.0976707Z Loading safetensors checkpoint shards:  77% Completed | 126/163 [01:53<00:39,  1.07s/it]
2026-02-23T18:51:24.1746213Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:24.1746676Z Loading safetensors checkpoint shards:  78% Completed | 127/163 [01:54<00:38,  1.07s/it]
2026-02-23T18:51:25.1621350Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:25.1621811Z Loading safetensors checkpoint shards:  79% Completed | 128/163 [01:55<00:36,  1.05s/it]
2026-02-23T18:51:26.2376395Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:26.2376841Z Loading safetensors checkpoint shards:  80% Completed | 130/163 [01:57<00:26,  1.23it/s]
2026-02-23T18:51:27.3612326Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:27.3612821Z Loading safetensors checkpoint shards:  80% Completed | 131/163 [01:58<00:28,  1.12it/s]
2026-02-23T18:51:28.4369045Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:28.4369604Z Loading safetensors checkpoint shards:  81% Completed | 132/163 [01:59<00:29,  1.07it/s]
2026-02-23T18:51:29.5224222Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:29.5224676Z Loading safetensors checkpoint shards:  82% Completed | 133/163 [02:00<00:29,  1.02it/s]
2026-02-23T18:51:30.5658526Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:30.5659022Z Loading safetensors checkpoint shards:  82% Completed | 134/163 [02:01<00:28,  1.00it/s]
2026-02-23T18:51:31.4806137Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:31.4806696Z Loading safetensors checkpoint shards:  83% Completed | 135/163 [02:02<00:27,  1.03it/s]
2026-02-23T18:51:32.4971709Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:32.4972296Z Loading safetensors checkpoint shards:  83% Completed | 136/163 [02:03<00:26,  1.01it/s]
2026-02-23T18:51:33.5039338Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:33.5039769Z Loading safetensors checkpoint shards:  84% Completed | 137/163 [02:04<00:25,  1.01it/s]
2026-02-23T18:51:34.5548432Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:34.5549116Z Loading safetensors checkpoint shards:  85% Completed | 138/163 [02:05<00:25,  1.01s/it]
2026-02-23T18:51:35.5322726Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:35.5323137Z Loading safetensors checkpoint shards:  85% Completed | 139/163 [02:06<00:24,  1.00s/it]
2026-02-23T18:51:36.5408965Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:36.5409428Z Loading safetensors checkpoint shards:  86% Completed | 140/163 [02:07<00:23,  1.00s/it]
2026-02-23T18:51:37.5574713Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:37.5575152Z Loading safetensors checkpoint shards:  87% Completed | 141/163 [02:08<00:22,  1.01s/it]
2026-02-23T18:51:38.6176408Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:38.6176830Z Loading safetensors checkpoint shards:  87% Completed | 142/163 [02:09<00:21,  1.02s/it]
2026-02-23T18:51:39.6128381Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:39.6128876Z Loading safetensors checkpoint shards:  88% Completed | 143/163 [02:10<00:20,  1.01s/it]
2026-02-23T18:51:40.6731174Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:40.6731621Z Loading safetensors checkpoint shards:  88% Completed | 144/163 [02:11<00:19,  1.03s/it]
2026-02-23T18:51:41.7535044Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:41.7535495Z Loading safetensors checkpoint shards:  89% Completed | 145/163 [02:12<00:18,  1.04s/it]
2026-02-23T18:51:42.7214357Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:42.7214800Z Loading safetensors checkpoint shards:  90% Completed | 146/163 [02:13<00:17,  1.02s/it]
2026-02-23T18:51:43.7266563Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:43.7267004Z Loading safetensors checkpoint shards:  90% Completed | 147/163 [02:14<00:16,  1.02s/it]
2026-02-23T18:51:44.7533048Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:44.7533564Z Loading safetensors checkpoint shards:  91% Completed | 148/163 [02:15<00:15,  1.02s/it]
2026-02-23T18:51:45.8544634Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:45.8545183Z Loading safetensors checkpoint shards:  91% Completed | 149/163 [02:16<00:14,  1.04s/it]
2026-02-23T18:51:49.1494399Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:49.1494920Z Loading safetensors checkpoint shards:  92% Completed | 150/163 [02:19<00:22,  1.72s/it]
2026-02-23T18:51:50.0850199Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:50.0850676Z Loading safetensors checkpoint shards:  93% Completed | 151/163 [02:20<00:17,  1.48s/it]
2026-02-23T18:51:51.1041587Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:51.1042197Z Loading safetensors checkpoint shards:  93% Completed | 152/163 [02:21<00:14,  1.34s/it]
2026-02-23T18:51:52.2311587Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:52.2312249Z Loading safetensors checkpoint shards:  94% Completed | 153/163 [02:22<00:12,  1.28s/it]
2026-02-23T18:51:53.2828147Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:53.2828652Z Loading safetensors checkpoint shards:  94% Completed | 154/163 [02:24<00:10,  1.21s/it]
2026-02-23T18:51:54.3194804Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:54.3195515Z Loading safetensors checkpoint shards:  95% Completed | 155/163 [02:25<00:09,  1.16s/it]
2026-02-23T18:51:55.3632252Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:55.3632778Z Loading safetensors checkpoint shards:  96% Completed | 156/163 [02:26<00:07,  1.12s/it]
2026-02-23T18:51:56.4753478Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:56.4753949Z Loading safetensors checkpoint shards:  96% Completed | 157/163 [02:27<00:06,  1.12s/it]
2026-02-23T18:51:57.4314142Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:57.4314572Z Loading safetensors checkpoint shards:  97% Completed | 158/163 [02:28<00:05,  1.07s/it]
2026-02-23T18:51:58.5411347Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:58.5411922Z Loading safetensors checkpoint shards:  98% Completed | 159/163 [02:29<00:04,  1.08s/it]
2026-02-23T18:51:58.6882461Z [0;36m(ApiServer_2 pid=186)[0;0m Process ApiServer_2:
2026-02-23T18:51:58.6936328Z [0;36m(ApiServer_0 pid=184)[0;0m Process ApiServer_0:
2026-02-23T18:51:58.6987064Z [0;36m(ApiServer_3 pid=187)[0;0m Process ApiServer_3:
2026-02-23T18:51:58.7015498Z [0;36m(ApiServer_3 pid=187)[0;0m Traceback (most recent call last):
2026-02-23T18:51:58.7025541Z [0;36m(ApiServer_0 pid=184)[0;0m Traceback (most recent call last):
2026-02-23T18:51:58.7036260Z [0;36m(ApiServer_2 pid=186)[0;0m Traceback (most recent call last):
2026-02-23T18:51:58.7048048Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-23T18:51:58.7057524Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-23T18:51:58.7067388Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-23T18:51:58.7077176Z [0;36m(ApiServer_0 pid=184)[0;0m     self.run()
2026-02-23T18:51:58.7088112Z [0;36m(ApiServer_3 pid=187)[0;0m     self.run()
2026-02-23T18:51:58.7100539Z [0;36m(ApiServer_2 pid=186)[0;0m     self.run()
2026-02-23T18:51:58.7112635Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-23T18:51:58.7124527Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-23T18:51:58.7136219Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-23T18:51:58.7145254Z [0;36m(ApiServer_3 pid=187)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-23T18:51:58.7157189Z [0;36m(ApiServer_0 pid=184)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-23T18:51:58.7166032Z [0;36m(ApiServer_2 pid=186)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-23T18:51:58.7177125Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-23T18:51:58.7188829Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-23T18:51:58.7250315Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-23T18:51:58.7250943Z [0;36m(ApiServer_3 pid=187)[0;0m     uvloop.run(
2026-02-23T18:51:58.7251263Z [0;36m(ApiServer_0 pid=184)[0;0m     uvloop.run(
2026-02-23T18:51:58.7251600Z [0;36m(ApiServer_2 pid=186)[0;0m     uvloop.run(
2026-02-23T18:51:58.7252269Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-23T18:51:58.7253030Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-23T18:51:58.7257983Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-23T18:51:58.7267791Z [0;36m(ApiServer_3 pid=187)[0;0m     return runner.run(wrapper())
2026-02-23T18:51:58.7276085Z [0;36m(ApiServer_0 pid=184)[0;0m     return runner.run(wrapper())
2026-02-23T18:51:58.7286307Z [0;36m(ApiServer_2 pid=186)[0;0m     return runner.run(wrapper())
2026-02-23T18:51:58.7295911Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.7306688Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.7315456Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.7326860Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-23T18:51:58.7336724Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-23T18:51:58.7346427Z [0;36m(ApiServer_3 pid=187)[0;0m     return self._loop.run_until_complete(task)
2026-02-23T18:51:58.7356124Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-23T18:51:58.7366049Z [0;36m(ApiServer_0 pid=184)[0;0m     return self._loop.run_until_complete(task)
2026-02-23T18:51:58.7375777Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.7386499Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.7396140Z [0;36m(ApiServer_2 pid=186)[0;0m     return self._loop.run_until_complete(task)
2026-02-23T18:51:58.7407121Z [0;36m(ApiServer_3 pid=187)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-23T18:51:58.7428850Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.7429702Z [0;36m(ApiServer_0 pid=184)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-23T18:51:58.7438430Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-23T18:51:58.7446004Z [0;36m(ApiServer_2 pid=186)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-23T18:51:58.7456076Z [0;36m(ApiServer_3 pid=187)[0;0m     return await main
2026-02-23T18:51:58.7465591Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-23T18:51:58.7474985Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^
2026-02-23T18:51:58.7484883Z [0;36m(ApiServer_0 pid=184)[0;0m     return await main
2026-02-23T18:51:58.7495404Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-23T18:51:58.7504673Z [0;36m(ApiServer_2 pid=186)[0;0m     return await main
2026-02-23T18:51:58.7526289Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^
2026-02-23T18:51:58.7526869Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-23T18:51:58.7535174Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^
2026-02-23T18:51:58.7545073Z [0;36m(ApiServer_3 pid=187)[0;0m     async with build_async_engine_client(
2026-02-23T18:51:58.7555146Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-23T18:51:58.7566144Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-23T18:51:58.7576035Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-23T18:51:58.7585931Z [0;36m(ApiServer_0 pid=184)[0;0m     async with build_async_engine_client(
2026-02-23T18:51:58.7595501Z [0;36m(ApiServer_2 pid=186)[0;0m     async with build_async_engine_client(
2026-02-23T18:51:58.7605913Z [0;36m(ApiServer_3 pid=187)[0;0m     return await anext(self.gen)
2026-02-23T18:51:58.7616277Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-23T18:51:58.7624788Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.7635579Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-23T18:51:58.7648935Z [0;36m(ApiServer_0 pid=184)[0;0m     return await anext(self.gen)
2026-02-23T18:51:58.7656897Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-23T18:51:58.7665700Z [0;36m(ApiServer_2 pid=186)[0;0m     return await anext(self.gen)
2026-02-23T18:51:58.7675494Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.7687316Z [0;36m(ApiServer_3 pid=187)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-23T18:51:58.7699641Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.7710126Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-23T18:51:58.7719641Z [0;36m(ApiServer_0 pid=184)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-23T18:51:58.7729129Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-23T18:51:58.7744545Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-23T18:51:58.7754644Z [0;36m(ApiServer_3 pid=187)[0;0m     return await anext(self.gen)
2026-02-23T18:51:58.7765631Z [0;36m(ApiServer_2 pid=186)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-23T18:51:58.7779727Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-23T18:51:58.7789576Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.7796666Z [0;36m(ApiServer_0 pid=184)[0;0m     return await anext(self.gen)
2026-02-23T18:51:58.7806381Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-23T18:51:58.7816230Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.7826414Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-23T18:51:58.7836242Z [0;36m(ApiServer_2 pid=186)[0;0m     return await anext(self.gen)
2026-02-23T18:51:58.7846292Z [0;36m(ApiServer_3 pid=187)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-23T18:51:58.7856233Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.7866431Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-23T18:51:58.7875858Z [0;36m(ApiServer_3 pid=187)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.7886176Z [0;36m(ApiServer_0 pid=184)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-23T18:51:58.7896514Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-23T18:51:58.7906344Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-23T18:51:58.7915896Z [0;36m(ApiServer_0 pid=184)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.7926507Z [0;36m(ApiServer_3 pid=187)[0;0m     return cls(
2026-02-23T18:51:58.7935599Z [0;36m(ApiServer_2 pid=186)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-23T18:51:58.7945416Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^
2026-02-23T18:51:58.7954951Z [0;36m(ApiServer_2 pid=186)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.7965833Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-23T18:51:58.7976228Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-23T18:51:58.7985875Z [0;36m(ApiServer_0 pid=184)[0;0m     return cls(
2026-02-23T18:51:58.7996583Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-23T18:51:58.8006652Z [0;36m(ApiServer_3 pid=187)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-23T18:51:58.8015406Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^
2026-02-23T18:51:58.8025243Z [0;36m(ApiServer_2 pid=186)[0;0m     return cls(
2026-02-23T18:51:58.8035025Z [0;36m(ApiServer_3 pid=187)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.8045859Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^
2026-02-23T18:51:58.8054948Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-23T18:51:58.8064074Z [0;36m(ApiServer_0 pid=184)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-23T18:51:58.8073650Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-23T18:51:58.8083980Z [0;36m(ApiServer_0 pid=184)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.8093911Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-23T18:51:58.8103648Z [0;36m(ApiServer_3 pid=187)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-23T18:51:58.8113549Z [0;36m(ApiServer_2 pid=186)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-23T18:51:58.8124321Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.8134059Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-23T18:51:58.8143075Z [0;36m(ApiServer_2 pid=186)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.8152831Z [0;36m(ApiServer_0 pid=184)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-23T18:51:58.8162770Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-23T18:51:58.8172644Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.8182831Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-23T18:51:58.8192234Z [0;36m(ApiServer_3 pid=187)[0;0m     super().__init__(
2026-02-23T18:51:58.8203102Z [0;36m(ApiServer_2 pid=186)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-23T18:51:58.8213700Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-23T18:51:58.8222917Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-23T18:51:58.8232729Z [0;36m(ApiServer_3 pid=187)[0;0m     super().__init__(
2026-02-23T18:51:58.8243043Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.8252373Z [0;36m(ApiServer_0 pid=184)[0;0m     super().__init__(
2026-02-23T18:51:58.8262514Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-23T18:51:58.8272487Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-23T18:51:58.8282425Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-23T18:51:58.8291090Z [0;36m(ApiServer_3 pid=187)[0;0m     super().__init__(
2026-02-23T18:51:58.8299819Z [0;36m(ApiServer_2 pid=186)[0;0m     super().__init__(
2026-02-23T18:51:58.8309600Z [0;36m(ApiServer_0 pid=184)[0;0m     super().__init__(
2026-02-23T18:51:58.8320840Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-23T18:51:58.8330278Z [0;36m(ApiServer_3 pid=187)[0;0m     raise TimeoutError(
2026-02-23T18:51:58.8339600Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-23T18:51:58.8349613Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-23T18:51:58.8359500Z [0;36m(ApiServer_2 pid=186)[0;0m     super().__init__(
2026-02-23T18:51:58.8369777Z [0;36m(ApiServer_0 pid=184)[0;0m     super().__init__(
2026-02-23T18:51:58.8379296Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-23T18:51:58.8389659Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-23T18:51:58.8399898Z [0;36m(ApiServer_3 pid=187)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-23T18:51:58.8409249Z [0;36m(ApiServer_2 pid=186)[0;0m     super().__init__(
2026-02-23T18:51:58.8418210Z [0;36m(ApiServer_0 pid=184)[0;0m     raise TimeoutError(
2026-02-23T18:51:58.8428071Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-23T18:51:58.8438458Z [0;36m(ApiServer_2 pid=186)[0;0m     raise TimeoutError(
2026-02-23T18:51:58.8448419Z [0;36m(ApiServer_0 pid=184)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-23T18:51:58.8457501Z [0;36m(ApiServer_2 pid=186)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-23T18:51:58.8466726Z [0;36m(ApiServer_1 pid=185)[0;0m Process ApiServer_1:
2026-02-23T18:51:58.8477031Z [0;36m(ApiServer_1 pid=185)[0;0m Traceback (most recent call last):
2026-02-23T18:51:58.8487354Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-23T18:51:58.8495934Z [0;36m(ApiServer_1 pid=185)[0;0m     self.run()
2026-02-23T18:51:58.8505978Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-23T18:51:58.8515981Z [0;36m(ApiServer_1 pid=185)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-23T18:51:58.8527083Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-23T18:51:58.8536309Z [0;36m(ApiServer_1 pid=185)[0;0m     uvloop.run(
2026-02-23T18:51:58.8545030Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-23T18:51:58.8554608Z [0;36m(ApiServer_1 pid=185)[0;0m     return runner.run(wrapper())
2026-02-23T18:51:58.8564555Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.8574596Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-23T18:51:58.8583885Z [0;36m(ApiServer_1 pid=185)[0;0m     return self._loop.run_until_complete(task)
2026-02-23T18:51:58.8592849Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.8602934Z [0;36m(ApiServer_1 pid=185)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-23T18:51:58.8613237Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-23T18:51:58.8622213Z [0;36m(ApiServer_1 pid=185)[0;0m     return await main
2026-02-23T18:51:58.8631203Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^
2026-02-23T18:51:58.8641970Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-23T18:51:58.8651651Z [0;36m(ApiServer_1 pid=185)[0;0m     async with build_async_engine_client(
2026-02-23T18:51:58.8661507Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-23T18:51:58.8671740Z [0;36m(ApiServer_1 pid=185)[0;0m     return await anext(self.gen)
2026-02-23T18:51:58.8681320Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.8691326Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-23T18:51:58.8707288Z [0;36m(ApiServer_1 pid=185)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-23T18:51:58.8709984Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-23T18:51:58.8719614Z [0;36m(ApiServer_1 pid=185)[0;0m     return await anext(self.gen)
2026-02-23T18:51:58.8736158Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.8738949Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-23T18:51:58.8748332Z [0;36m(ApiServer_1 pid=185)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-23T18:51:58.8757711Z [0;36m(ApiServer_1 pid=185)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.8768210Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-23T18:51:58.8777348Z [0;36m(ApiServer_1 pid=185)[0;0m     return cls(
2026-02-23T18:51:58.8787142Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^
2026-02-23T18:51:58.8796309Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-23T18:51:58.8806824Z [0;36m(ApiServer_1 pid=185)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-23T18:51:58.8816469Z [0;36m(ApiServer_1 pid=185)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.8826459Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-23T18:51:58.8837065Z [0;36m(ApiServer_1 pid=185)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-23T18:51:58.8847291Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-23T18:51:58.8856264Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-23T18:51:58.8865370Z [0;36m(ApiServer_1 pid=185)[0;0m     super().__init__(
2026-02-23T18:51:58.8877273Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-23T18:51:58.8886305Z [0;36m(ApiServer_1 pid=185)[0;0m     super().__init__(
2026-02-23T18:51:58.8895836Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-23T18:51:58.8904640Z [0;36m(ApiServer_1 pid=185)[0;0m     super().__init__(
2026-02-23T18:51:58.8915087Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-23T18:51:58.8925493Z [0;36m(ApiServer_1 pid=185)[0;0m     raise TimeoutError(
2026-02-23T18:51:58.8935613Z [0;36m(ApiServer_1 pid=185)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-23T18:51:59.1231804Z [0;36m(ApiServer_0 pid=184)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-23T18:51:59.1261937Z [0;36m(ApiServer_1 pid=185)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-23T18:51:59.1313235Z [0;36m(ApiServer_2 pid=186)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-23T18:51:59.1485593Z [0;36m(ApiServer_3 pid=187)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-23T18:51:59.5769871Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:51:59.5770446Z Loading safetensors checkpoint shards:  98% Completed | 160/163 [02:30<00:03,  1.07s/it]
2026-02-23T18:52:00.6446497Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:52:00.6447064Z Loading safetensors checkpoint shards:  99% Completed | 161/163 [02:31<00:02,  1.07s/it]
2026-02-23T18:52:01.7474883Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:52:01.7475437Z Loading safetensors checkpoint shards:  99% Completed | 162/163 [02:32<00:01,  1.08s/it]
2026-02-23T18:52:02.9081030Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:52:02.9081585Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [02:33<00:00,  1.10s/it]
2026-02-23T18:52:02.9101246Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:52:02.9101635Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [02:33<00:00,  1.06it/s]
2026-02-23T18:52:02.9110853Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:52:02.9746053Z [0;36m(Worker_DP1_TP7_EP15 pid=889)[0;0m INFO 02-23 18:52:02 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-23T18:52:03.1591342Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m INFO 02-23 18:52:03 [default_loader.py:291] Loading weights took 153.99 seconds
2026-02-23T18:52:03.1953435Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m INFO 02-23 18:52:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-23T18:52:03.2089743Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-23 18:52:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-23T18:52:03.2232956Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-23 18:52:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-23T18:52:03.2258092Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-23 18:52:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-23T18:52:03.2280429Z [0;36m(Worker_DP0_TP5_EP5 pid=684)[0;0m INFO 02-23 18:52:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-23T18:52:03.2536379Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-23 18:52:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-23T18:52:03.2684225Z [0;36m(Worker_DP1_TP6_EP14 pid=786)[0;0m INFO 02-23 18:52:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-23T18:52:03.9168849Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m INFO 02-23 18:52:03 [default_loader.py:291] Loading weights took 154.68 seconds
2026-02-23T18:52:03.9262488Z [0;36m(Worker_DP1_TP7_EP15 pid=889)[0;0m INFO 02-23 18:52:03 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-23T18:52:03.9650748Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m INFO 02-23 18:52:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-23T18:52:03.9668171Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-23 18:52:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-23T18:52:03.9669032Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-23 18:52:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-23T18:52:03.9734563Z [0;36m(Worker_DP1_TP5_EP13 pid=681)[0;0m INFO 02-23 18:52:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-23T18:52:03.9766576Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-23 18:52:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-23T18:52:03.9787611Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-23 18:52:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-23T18:52:03.9812168Z [0;36m(Worker_DP1_TP3_EP11 pid=475)[0;0m INFO 02-23 18:52:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-23T18:52:03.9880267Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-23 18:52:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-23T18:52:04.1271097Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m INFO 02-23 18:52:04 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-23T18:52:04.4091799Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-23 18:52:04 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-23T18:52:04.8859731Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-23 18:52:04 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-23T18:52:04.9579797Z [0;36m(Worker_DP1_TP5_EP13 pid=681)[0;0m INFO 02-23 18:52:04 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-23T18:52:05.1881254Z [0;36m(Worker_DP0_TP5_EP5 pid=684)[0;0m INFO 02-23 18:52:05 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-23T18:52:05.2855432Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-23 18:52:05 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-23T18:52:05.6078141Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-23 18:52:05 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-23T18:52:05.8122358Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-23 18:52:05 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-23T18:52:06.0259373Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-23 18:52:06 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-23T18:52:06.1071968Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-23 18:52:06 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-23T18:52:06.2180451Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-23 18:52:06 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-23T18:52:06.3154713Z [0;36m(Worker_DP1_TP3_EP11 pid=475)[0;0m INFO 02-23 18:52:06 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-23T18:52:06.3199845Z [0;36m(Worker_DP1_TP6_EP14 pid=786)[0;0m INFO 02-23 18:52:06 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-23T18:52:06.3282639Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m INFO 02-23 18:52:06 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-23T18:52:06.4949314Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-23 18:52:06 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-23T18:52:11.9182617Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m INFO 02-23 18:52:11 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/5f4e5deba5/rank_0_0/backbone for vLLM's torch.compile
2026-02-23T18:52:11.9211126Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m INFO 02-23 18:52:11 [backends.py:865] Dynamo bytecode transform time: 4.50 s
2026-02-23T18:52:12.1988142Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m INFO 02-23 18:52:12 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/5f4e5deba5/rank_0_1/backbone for vLLM's torch.compile
2026-02-23T18:52:12.2026942Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m INFO 02-23 18:52:12 [backends.py:865] Dynamo bytecode transform time: 4.78 s
2026-02-23T18:52:18.4909934Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-23T18:52:18.4917825Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m   warnings.warn(
2026-02-23T18:52:18.4942814Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-23T18:52:18.4952441Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m   warnings.warn(
2026-02-23T18:52:18.4972350Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-23T18:52:18.4982398Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m   warnings.warn(
2026-02-23T18:52:18.5063416Z [0;36m(Worker_DP1_TP5_EP13 pid=681)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-23T18:52:18.5072433Z [0;36m(Worker_DP1_TP5_EP13 pid=681)[0;0m   warnings.warn(
2026-02-23T18:52:18.5095767Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-23T18:52:18.5106149Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m   warnings.warn(
2026-02-23T18:52:18.5121123Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-23T18:52:18.5130580Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m   warnings.warn(
2026-02-23T18:52:18.5318174Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-23T18:52:18.5327094Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m   warnings.warn(
2026-02-23T18:52:18.5438155Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-23T18:52:18.5446783Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m   warnings.warn(
2026-02-23T18:52:18.5570087Z [0;36m(Worker_DP1_TP7_EP15 pid=889)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-23T18:52:18.5577916Z [0;36m(Worker_DP1_TP7_EP15 pid=889)[0;0m   warnings.warn(
2026-02-23T18:52:18.5726864Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-23T18:52:18.5734594Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m   warnings.warn(
2026-02-23T18:52:18.5961611Z [0;36m(Worker_DP1_TP3_EP11 pid=475)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-23T18:52:18.5970858Z [0;36m(Worker_DP1_TP3_EP11 pid=475)[0;0m   warnings.warn(
2026-02-23T18:52:18.6001008Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-23T18:52:18.6010826Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m   warnings.warn(
2026-02-23T18:52:18.6120884Z [0;36m(Worker_DP1_TP6_EP14 pid=786)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-23T18:52:18.6131191Z [0;36m(Worker_DP1_TP6_EP14 pid=786)[0;0m   warnings.warn(
2026-02-23T18:52:18.6172132Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-23T18:52:18.6180805Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m   warnings.warn(
2026-02-23T18:52:18.6385086Z [0;36m(Worker_DP0_TP5_EP5 pid=684)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-23T18:52:18.6395029Z [0;36m(Worker_DP0_TP5_EP5 pid=684)[0;0m   warnings.warn(
2026-02-23T18:52:18.6709391Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-23T18:52:18.6717375Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m   warnings.warn(
2026-02-23T18:52:31.8902857Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m INFO 02-23 18:52:31 [backends.py:319] Compiling a graph for compile range (1, 4096) takes 13.40 s
2026-02-23T18:52:31.8910423Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m INFO 02-23 18:52:31 [monitor.py:34] torch.compile takes 17.90 s in total
2026-02-23T18:52:32.3780800Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m INFO 02-23 18:52:32 [backends.py:319] Compiling a graph for compile range (1, 4096) takes 13.76 s
2026-02-23T18:52:32.3805160Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m INFO 02-23 18:52:32 [monitor.py:34] torch.compile takes 18.54 s in total
2026-02-23T18:52:39.8218230Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-23 18:52:39 [worker.py:338] Available memory: 18258174464, total memory: 65787658240
2026-02-23T18:52:39.8723685Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-23 18:52:39 [worker.py:338] Available memory: 18556175052, total memory: 65796046848
2026-02-23T18:52:39.8860214Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-23 18:52:39 [worker.py:338] Available memory: 18254305792, total memory: 65787658240
2026-02-23T18:52:39.9068282Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-23 18:52:39 [worker.py:338] Available memory: 18269615616, total memory: 65787658240
2026-02-23T18:52:39.9312914Z [0;36m(Worker_DP1_TP5_EP13 pid=681)[0;0m INFO 02-23 18:52:39 [worker.py:338] Available memory: 18545609420, total memory: 65796046848
2026-02-23T18:52:39.9340246Z [0;36m(Worker_DP1_TP7_EP15 pid=889)[0;0m INFO 02-23 18:52:39 [worker.py:338] Available memory: 18552090316, total memory: 65796046848
2026-02-23T18:52:39.9560074Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-23 18:52:39 [worker.py:338] Available memory: 18258321920, total memory: 65787658240
2026-02-23T18:52:39.9779319Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-23 18:52:39 [worker.py:338] Available memory: 18535404236, total memory: 65796046848
2026-02-23T18:52:40.0061955Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m INFO 02-23 18:52:40 [worker.py:338] Available memory: 17857280512, total memory: 65787658240
2026-02-23T18:52:40.0315524Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-23 18:52:40 [worker.py:338] Available memory: 18556302028, total memory: 65796046848
2026-02-23T18:52:40.3907780Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m INFO 02-23 18:52:40 [worker.py:338] Available memory: 17856264704, total memory: 65787658240
2026-02-23T18:52:40.8191044Z [0;36m(Worker_DP0_TP5_EP5 pid=684)[0;0m INFO 02-23 18:52:40 [worker.py:338] Available memory: 18547436236, total memory: 65796046848
2026-02-23T18:52:40.8336422Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-23 18:52:40 [worker.py:338] Available memory: 18560518860, total memory: 65796046848
2026-02-23T18:52:41.3794002Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-23 18:52:41 [worker.py:338] Available memory: 18243379712, total memory: 65787658240
2026-02-23T18:52:41.3817108Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-23 18:52:41 [kv_cache_utils.py:1307] GPU KV cache size: 204,544 tokens
2026-02-23T18:52:41.3827474Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-23 18:52:41 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 24.97x
2026-02-23T18:52:41.4300080Z [0;36m(Worker_DP1_TP6_EP14 pid=786)[0;0m INFO 02-23 18:52:41 [worker.py:338] Available memory: 18263287296, total memory: 65787658240
2026-02-23T18:52:41.5449599Z [0;36m(Worker_DP1_TP3_EP11 pid=475)[0;0m INFO 02-23 18:52:41 [worker.py:338] Available memory: 18547219148, total memory: 65796046848
2026-02-23T18:52:41.5473900Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-23 18:52:41 [kv_cache_utils.py:1307] GPU KV cache size: 204,544 tokens
2026-02-23T18:52:41.5482803Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-23 18:52:41 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 24.97x
2026-02-23T18:52:58.3972327Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m 
2026-02-23T18:52:58.3974786Z Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s][rank12]:[W223 18:52:58.321946053 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-23T18:52:58.3983905Z [rank10]:[W223 18:52:58.322008414 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-23T18:52:58.4000144Z [rank13]:[W223 18:52:58.322439357 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-23T18:52:58.4013027Z [rank9]:[W223 18:52:58.322498017 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-23T18:52:58.4024450Z [rank11]:[W223 18:52:58.322521647 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-23T18:52:58.4038021Z [rank14]:[W223 18:52:58.322649078 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-23T18:52:58.4049326Z [rank15]:[W223 18:52:58.322649308 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-23T18:52:58.4059953Z [rank8]:[W223 18:52:58.322671719 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-23T18:52:58.4085700Z [rank3]:[W223 18:52:58.334564379 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-23T18:52:58.4097016Z [rank5]:[W223 18:52:58.334724240 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-23T18:52:58.4106951Z [rank2]:[W223 18:52:58.334729530 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-23T18:52:58.4117520Z [rank4]:[W223 18:52:58.334768530 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-23T18:52:58.4143175Z [rank7]:[W223 18:52:58.338416048 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-23T18:52:58.4153213Z [rank6]:[W223 18:52:58.338436628 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-23T18:52:58.4163307Z [rank1]:[W223 18:52:58.338907142 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-23T18:52:58.4173993Z [rank0]:[W223 18:52:58.340902167 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-23T18:53:00.3043994Z [rank4]:[W223 18:53:00.229115666 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-23T18:53:00.3061145Z [rank6]:[W223 18:53:00.230187104 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-23T18:53:00.3070787Z [rank5]:[W223 18:53:00.230255525 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-23T18:53:00.3084112Z [rank7]:[W223 18:53:00.230731409 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-23T18:53:00.3091783Z [rank2]:[W223 18:53:00.231436054 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-23T18:53:00.3102290Z [rank0]:[W223 18:53:00.231461654 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-23T18:53:00.3114524Z [rank3]:[W223 18:53:00.231982088 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-23T18:53:00.3123292Z [rank1]:[W223 18:53:00.232731954 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-23T18:53:00.5034991Z [rank13]:[W223 18:53:00.429093351 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-23T18:53:00.5053570Z [rank10]:[W223 18:53:00.430604462 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-23T18:53:00.5063800Z [rank9]:[W223 18:53:00.431118986 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-23T18:53:00.5073901Z [rank11]:[W223 18:53:00.431568590 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-23T18:53:00.5084567Z [rank12]:[W223 18:53:00.431939812 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-23T18:53:00.5094409Z [rank8]:[W223 18:53:00.432287415 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-23T18:53:00.5104555Z [rank14]:[W223 18:53:00.432470636 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-23T18:53:00.5115536Z [rank15]:[W223 18:53:00.433018830 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-23T18:53:03.5058189Z 
2026-02-23T18:53:03.5059218Z Capturing CUDA graphs (decode, FULL):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:16<00:16, 16.69s/it]
2026-02-23T18:53:03.5059765Z Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:19<00:00,  8.63s/it]
2026-02-23T18:53:03.5060280Z Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:19<00:00,  9.84s/it]
2026-02-23T18:53:03.9833375Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m INFO 02-23 18:53:03 [gpu_model_runner.py:5051] Graph capturing finished in 21 secs, took 0.27 GiB
2026-02-23T18:53:05.0161062Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-23 18:53:05 [core.py:272] init engine (profile, create kv cache, warmup model) took 58.52 seconds
2026-02-23T18:53:05.0556746Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m INFO 02-23 18:53:05 [gpu_model_runner.py:5051] Graph capturing finished in 22 secs, took 0.27 GiB
2026-02-23T18:53:05.0665770Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-23 18:53:05 [core.py:272] init engine (profile, create kv cache, warmup model) took 58.69 seconds
2026-02-23T18:53:05.9168551Z INFO 02-23 18:53:05 [coordinator.py:200] All engine subscriptions received by DP coordinator
2026-02-23T18:53:05.9177118Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-23 18:53:05 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-23T18:53:05.9186431Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-23 18:53:05 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-23T18:53:05.9196448Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-23 18:53:05 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:53:05.9207735Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-23 18:53:05 [ascend_config.py:412] Dynamic EPLB is False
2026-02-23T18:53:05.9217607Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-23 18:53:05 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:53:05.9227484Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-23 18:53:05 [ascend_config.py:413] The number of redundant experts is 0
2026-02-23T18:53:05.9238814Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-23 18:53:05 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:53:05.9249649Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-23 18:53:05 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-23T18:53:05.9258708Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-23 18:53:05 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-23T18:53:05.9270287Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-23 18:53:05 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-23T18:53:05.9278530Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-23 18:53:05 [platform.py:335] [91m
2026-02-23T18:53:05.9295424Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             **********************************************************************************
2026-02-23T18:53:05.9299541Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-23T18:53:05.9309702Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-23T18:53:05.9319421Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-23T18:53:05.9331762Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-23T18:53:05.9339605Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-23T18:53:05.9349460Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             * batch size for graph capture.
2026-02-23T18:53:05.9360148Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             * For more details, please refer to:
2026-02-23T18:53:05.9370596Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-23T18:53:05.9381373Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             **********************************************************************************[0m
2026-02-23T18:53:05.9392785Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             
2026-02-23T18:53:05.9401525Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-23 18:53:05 [platform.py:335] [91m
2026-02-23T18:53:05.9412258Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             **********************************************************************************
2026-02-23T18:53:05.9421785Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-23T18:53:05.9431725Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-23T18:53:05.9442552Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-23T18:53:05.9452069Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-23T18:53:05.9462258Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-23T18:53:05.9472554Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             * batch size for graph capture.
2026-02-23T18:53:05.9483873Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             * For more details, please refer to:
2026-02-23T18:53:05.9554758Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-23T18:53:05.9555566Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             **********************************************************************************[0m
2026-02-23T18:53:05.9556350Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-23 18:53:05 [platform.py:335]             
2026-02-23T18:53:05.9556962Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-23 18:53:05 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-23T18:53:05.9557619Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-23 18:53:05 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-23T18:53:05.9558898Z INFO 02-23 18:53:05 [utils.py:249] Waiting for API servers to complete ...
2026-02-23T18:53:05.9559418Z ERROR 02-23 18:53:05 [utils.py:290] Exception occurred while running API servers: Process ApiServer_0 (PID: 184) died with exit code 1
2026-02-23T18:53:05.9563241Z ERROR 02-23 18:53:05 [utils.py:290] Traceback (most recent call last):
2026-02-23T18:53:05.9573139Z ERROR 02-23 18:53:05 [utils.py:290]   File "/vllm-workspace/vllm/vllm/v1/utils.py", line 277, in wait_for_completion_or_failure
2026-02-23T18:53:05.9581918Z ERROR 02-23 18:53:05 [utils.py:290]     raise RuntimeError(
2026-02-23T18:53:05.9591514Z ERROR 02-23 18:53:05 [utils.py:290] RuntimeError: Process ApiServer_0 (PID: 184) died with exit code 1
2026-02-23T18:53:05.9602319Z INFO 02-23 18:53:05 [utils.py:293] Terminating remaining processes ...
2026-02-23T18:53:06.1914988Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-23T18:53:06.1923230Z [0;36m(Worker_DP0_TP5_EP5 pid=684)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-23T18:53:06.1933714Z [0;36m(Worker_DP1_TP5_EP13 pid=681)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-23T18:53:06.1943179Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-23T18:53:06.1954205Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-23T18:53:06.1964478Z [0;36m(Worker_DP1_TP7_EP15 pid=889)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-23T18:53:06.1974507Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-23T18:53:06.1984087Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-23T18:53:06.1993658Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-23T18:53:06.2005064Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-23T18:53:06.2015252Z [0;36m(Worker_DP1_TP3_EP11 pid=475)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-23T18:53:06.2025547Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-23T18:53:06.2035759Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-23T18:53:06.2046361Z [0;36m(Worker_DP1_TP6_EP14 pid=786)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-23T18:53:06.2056175Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-23T18:53:06.2065363Z [0;36m(Worker_DP1_TP5_EP13 pid=681)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-23T18:53:06.2079210Z [0;36m(Worker_DP1_TP7_EP15 pid=889)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-23T18:53:06.2086550Z [0;36m(Worker_DP0_TP5_EP5 pid=684)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-23T18:53:06.2096415Z [0;36m(Worker_DP0_TP0_EP0 pid=258)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-23T18:53:06.2106000Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-23T18:53:06.2116514Z [0;36m(Worker_DP1_TP0_EP8 pid=257)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-23T18:53:06.2126887Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-23T18:53:06.2136862Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-23T18:53:06.2147266Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-23T18:53:06.2157251Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-23T18:53:06.2167080Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-23T18:53:06.2177065Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-23T18:53:06.2191150Z [0;36m(Worker_DP1_TP3_EP11 pid=475)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-23T18:53:06.2203598Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-23T18:53:06.2213187Z [0;36m(Worker_DP1_TP6_EP14 pid=786)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-23T18:53:06.2222893Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-23T18:53:06.2232101Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-23 18:53:06 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-23T18:53:11.2288279Z Traceback (most recent call last):
2026-02-23T18:53:11.2345513Z   File "/usr/local/python3.11.14/bin/vllm", line 7, in <module>
2026-02-23T18:53:11.2345866Z     sys.exit(main())
2026-02-23T18:53:11.2346078Z              ^^^^^^
2026-02-23T18:53:11.2346432Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/main.py", line 73, in main
2026-02-23T18:53:11.2347788Z     args.dispatch_function(args)
2026-02-23T18:53:11.2358884Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 108, in cmd
2026-02-23T18:53:11.2368371Z     run_multi_api_server(args)
2026-02-23T18:53:11.2377675Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 282, in run_multi_api_server
2026-02-23T18:53:11.2387508Z     wait_for_completion_or_failure(
2026-02-23T18:53:11.2398146Z   File "/vllm-workspace/vllm/vllm/v1/utils.py", line 277, in wait_for_completion_or_failure
2026-02-23T18:53:11.2407958Z     raise RuntimeError(
2026-02-23T18:53:11.2417639Z RuntimeError: Process ApiServer_0 (PID: 184) died with exit code 1
2026-02-23T18:53:11.3019233Z [ERROR] 2026-02-23-18:53:11 (PID:138, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
2026-02-23T18:53:11.6166541Z sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-23T18:53:13.3246263Z /usr/local/python3.11.14/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked shared_memory objects to clean up at shutdown
2026-02-23T18:53:13.3256076Z   warnings.warn('resource_tracker: There appear to be %d '
2026-02-23T18:53:16.8472362Z FAILED
2026-02-23T18:53:16.8478304Z 
2026-02-23T18:53:16.8489847Z =================================== FAILURES ===================================
2026-02-23T18:53:16.8569971Z _______________________________ test_multi_node ________________________________
2026-02-23T18:53:16.8570377Z 
2026-02-23T18:53:16.8570480Z     @pytest.mark.asyncio
2026-02-23T18:53:16.8571947Z     async def test_multi_node() -> None:
2026-02-23T18:53:16.8572674Z         config = MultiNodeConfigLoader.from_yaml()
2026-02-23T18:53:16.8572997Z     
2026-02-23T18:53:16.8573199Z         with ProxyLauncher(
2026-02-23T18:53:16.8573447Z                 nodes=config.nodes,
2026-02-23T18:53:16.8578839Z                 disagg_cfg=config.disagg_cfg,
2026-02-23T18:53:16.8589103Z                 envs=config.envs,
2026-02-23T18:53:16.8604812Z                 proxy_port=config.proxy_port,
2026-02-23T18:53:16.8612366Z                 cur_index=config.cur_index,
2026-02-23T18:53:16.8619632Z         ) as proxy:
2026-02-23T18:53:16.8640535Z     
2026-02-23T18:53:16.8640809Z >           with RemoteOpenAIServer(
2026-02-23T18:53:16.8651291Z                     model=config.model,
2026-02-23T18:53:16.8661430Z                     vllm_serve_args=config.server_cmd,
2026-02-23T18:53:16.8693063Z                     server_port=config.server_port,
2026-02-23T18:53:16.8693462Z                     server_host=config.master_ip,
2026-02-23T18:53:16.8693741Z                     env_dict=config.envs,
2026-02-23T18:53:16.8700484Z                     auto_port=False,
2026-02-23T18:53:16.8709807Z                     proxy_port=proxy.proxy_port,
2026-02-23T18:53:16.8721518Z                     disaggregated_prefill=config.disagg_cfg,
2026-02-23T18:53:16.8732491Z                     nodes_info=config.nodes,
2026-02-23T18:53:16.8747205Z                     max_wait_seconds=2800,
2026-02-23T18:53:16.8751344Z             ) as server:
2026-02-23T18:53:16.8761946Z 
2026-02-23T18:53:16.8772997Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py:21: 
2026-02-23T18:53:16.8783868Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-23T18:53:16.8794693Z tests/e2e/conftest.py:306: in __init__
2026-02-23T18:53:16.8805558Z     self._wait_for_multiple_servers(
2026-02-23T18:53:16.8816050Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-23T18:53:16.8826138Z 
2026-02-23T18:53:16.8837129Z self = <tests.e2e.conftest.RemoteOpenAIServer object at 0xffff062fe0d0>
2026-02-23T18:53:16.8847226Z targets = [('10.0.0.59', 'http://10.0.0.59:8080/health')], timeout = 2800
2026-02-23T18:53:16.8856834Z log_interval = 30.0
2026-02-23T18:53:16.8867793Z 
2026-02-23T18:53:16.8876232Z     def _wait_for_multiple_servers(self,
2026-02-23T18:53:16.8886636Z                                    targets,
2026-02-23T18:53:16.8895952Z                                    timeout: float,
2026-02-23T18:53:16.8906366Z                                    log_interval: float = 30.0):
2026-02-23T18:53:16.8917237Z         """
2026-02-23T18:53:16.8926437Z         targets: List[(node_ip, url)]
2026-02-23T18:53:16.8935791Z         log_interval
2026-02-23T18:53:16.8945558Z         """
2026-02-23T18:53:16.8955625Z         start = time.time()
2026-02-23T18:53:16.8965913Z         client = requests
2026-02-23T18:53:16.8978424Z     
2026-02-23T18:53:16.8985284Z         ready = {node_ip: False for node_ip, _ in targets}
2026-02-23T18:53:16.8994861Z     
2026-02-23T18:53:16.9004886Z         last_log_time = 0.0
2026-02-23T18:53:16.9014883Z     
2026-02-23T18:53:16.9023520Z         while True:
2026-02-23T18:53:16.9033601Z             now = time.time()
2026-02-23T18:53:16.9043776Z             all_ready = True
2026-02-23T18:53:16.9052992Z             should_log = (now - last_log_time) >= log_interval
2026-02-23T18:53:16.9062771Z     
2026-02-23T18:53:16.9072566Z             for node_ip, url in targets:
2026-02-23T18:53:16.9082393Z                 if ready[node_ip]:
2026-02-23T18:53:16.9092276Z                     continue
2026-02-23T18:53:16.9101962Z     
2026-02-23T18:53:16.9112146Z                 try:
2026-02-23T18:53:16.9121439Z                     resp = client.get(url)
2026-02-23T18:53:16.9131160Z                     if resp.status_code == 200:
2026-02-23T18:53:16.9140916Z                         ready[node_ip] = True
2026-02-23T18:53:16.9150611Z                         logger.info(f"[READY] Node {node_ip} is ready.")
2026-02-23T18:53:16.9160940Z                 except RequestException:
2026-02-23T18:53:16.9170820Z                     all_ready = False
2026-02-23T18:53:16.9180382Z                     if should_log:
2026-02-23T18:53:16.9190290Z                         logger.debug(f"[WAIT] {url}: connection failed")
2026-02-23T18:53:16.9200378Z     
2026-02-23T18:53:16.9210793Z                     # check unexpected exit
2026-02-23T18:53:16.9221142Z                     result = self._poll()
2026-02-23T18:53:16.9228130Z                     if result is not None and result != 0:
2026-02-23T18:53:16.9238009Z >                       raise RuntimeError(
2026-02-23T18:53:16.9247931Z                             f"Server at {node_ip} exited unexpectedly."
2026-02-23T18:53:16.9262219Z                         ) from None
2026-02-23T18:53:16.9267341Z E                       RuntimeError: Server at 10.0.0.59 exited unexpectedly.
2026-02-23T18:53:16.9277092Z 
2026-02-23T18:53:16.9287348Z tests/e2e/conftest.py:399: RuntimeError
2026-02-23T18:53:16.9297154Z =============================== warnings summary ===============================
2026-02-23T18:53:16.9305849Z <frozen importlib._bootstrap>:241
2026-02-23T18:53:16.9316357Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute
2026-02-23T18:53:16.9326231Z 
2026-02-23T18:53:16.9336588Z <frozen importlib._bootstrap>:241
2026-02-23T18:53:16.9346960Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute
2026-02-23T18:53:16.9356709Z 
2026-02-23T18:53:16.9368860Z ../../usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647
2026-02-23T18:53:16.9378059Z   /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-23T18:53:16.9386887Z     warnings.warn(
2026-02-23T18:53:16.9396686Z 
2026-02-23T18:53:16.9457814Z ../../usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8
2026-02-23T18:53:16.9458753Z   /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
2026-02-23T18:53:16.9459428Z     import pkg_resources
2026-02-23T18:53:16.9459584Z 
2026-02-23T18:53:16.9459764Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2026-02-23T18:53:16.9460329Z =========================== short test summary info ============================
2026-02-23T18:53:16.9470514Z FAILED tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node
2026-02-23T18:53:16.9481223Z ================== 1 failed, 4 warnings in 736.81s (0:12:16) ===================
2026-02-23T18:53:18.6099616Z [0;31mFAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml âœ— ERROR: Some tests failed, please check the error stack above for details. If this is insufficient to pinpoint the error, please download and review the logs of all other nodes from the job's summary.[0m
2026-02-23T18:53:18.8079135Z Cleaning up background log streams...
2026-02-23T18:53:18.8743268Z ##[error]Error: failed to run script step: Error: command terminated with non-zero exit code: command terminated with exit code 1
2026-02-23T18:53:18.8782547Z ##[error]Process completed with exit code 1.
2026-02-23T18:53:18.8876863Z ##[error]Executing the custom container implementation failed. Please contact your self hosted runner administrator.
2026-02-23T18:53:18.9558449Z ##[group]Run actions/upload-artifact@v6
2026-02-23T18:53:18.9558663Z with:
2026-02-23T18:53:18.9558844Z   name: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs
2026-02-23T18:53:18.9559082Z   path: /tmp/vllm*_logs.txt
2026-02-23T18:53:18.9559308Z   retention-days: 7
2026-02-23T18:53:18.9559458Z   if-no-files-found: warn
2026-02-23T18:53:18.9559628Z   compression-level: 6
2026-02-23T18:53:18.9559783Z   overwrite: false
2026-02-23T18:53:18.9559941Z   include-hidden-files: false
2026-02-23T18:53:18.9560124Z env:
2026-02-23T18:53:18.9560301Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-23T18:53:18.9560530Z ##[endgroup]
2026-02-23T18:53:18.9584107Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-23T18:53:18.9584732Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-23T18:53:18.9584943Z ##[endgroup]
2026-02-23T18:53:19.3092278Z (node:926) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-23T18:53:19.3092944Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-23T18:53:20.3149980Z With the provided path, there will be 1 file uploaded
2026-02-23T18:53:20.3155238Z Artifact name is valid!
2026-02-23T18:53:20.3155636Z Root directory input is valid!
2026-02-23T18:53:21.5082407Z Beginning upload of artifact content to blob storage
2026-02-23T18:53:23.2743777Z Uploaded bytes 12551
2026-02-23T18:53:23.5433760Z Finished uploading artifact content to blob storage!
2026-02-23T18:53:23.5434223Z SHA256 digest of uploaded artifact zip is 05285685734fd1e67e13e285ad0f64bfe907b4b769836127637d3b558435f547
2026-02-23T18:53:23.5434581Z Finalizing artifact upload
2026-02-23T18:53:24.3587603Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs.zip successfully finalized. Artifact ID 5622596107
2026-02-23T18:53:24.3588294Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs has been successfully uploaded! Final size is 12551 bytes. Artifact ID is 5622596107
2026-02-23T18:53:24.3589838Z Artifact download URL: https://github.com/vllm-project/vllm-ascend/actions/runs/22315540111/artifacts/5622596107
2026-02-23T18:53:26.0125932Z ##[group]Run kubectl get pods -n "$NAMESPACE" --ignore-not-found=true
2026-02-23T18:53:26.0126341Z [36;1mkubectl get pods -n "$NAMESPACE" --ignore-not-found=true[0m
2026-02-23T18:53:26.0126671Z [36;1mkubectl delete -f ./lws.yaml --ignore-not-found=true || true[0m
2026-02-23T18:53:26.0127014Z shell: bash -el {0}
2026-02-23T18:53:26.0127249Z env:
2026-02-23T18:53:26.0127444Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-23T18:53:26.0127684Z ##[endgroup]
2026-02-23T18:53:26.0207241Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-23T18:53:26.0207895Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-23T18:53:26.0208113Z ##[endgroup]
2026-02-23T18:53:26.3741259Z (node:1088) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-23T18:53:26.3741921Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-23T18:53:27.0329183Z NAME                                             READY   STATUS    RESTARTS     AGE
2026-02-23T18:53:27.0329574Z linux-aarch64-a3-0-n4cwm-runner-xvq7p            1/1     Running   0            14m
2026-02-23T18:53:27.0329988Z linux-aarch64-a3-0-n4cwm-runner-xvq7p-workflow   1/1     Running   0            13m
2026-02-23T18:53:27.0330594Z vllm-0                                           1/1     Running   1 (9s ago)   13m
2026-02-23T18:53:27.0330884Z vllm-0-1                                         1/1     Running   0            13m
2026-02-23T18:53:27.0956650Z leaderworkerset.leaderworkerset.x-k8s.io "vllm" deleted from vllm-project namespace
2026-02-23T18:53:27.1095871Z service "vllm-leader" deleted from vllm-project namespace
2026-02-23T18:53:27.5747603Z Post job cleanup.
2026-02-23T18:53:27.5768224Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-23T18:53:27.5768885Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-23T18:53:27.5769112Z ##[endgroup]
2026-02-23T18:53:27.9707377Z (node:1212) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-23T18:53:27.9708486Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-23T18:53:28.6109083Z [command]/usr/bin/git version
2026-02-23T18:53:28.6284987Z git version 2.34.1
2026-02-23T18:53:28.6312767Z Copying '/root/.gitconfig' to '/__w/_temp/55844be1-fd4b-49c6-895c-88b7eaa355b5/.gitconfig'
2026-02-23T18:53:28.6320754Z Temporarily overriding HOME='/__w/_temp/55844be1-fd4b-49c6-895c-88b7eaa355b5' before making global git config changes
2026-02-23T18:53:28.6321299Z Adding repository directory to the temporary git global config as a safe directory
2026-02-23T18:53:28.6324815Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-23T18:53:28.6359341Z Removing SSH command configuration
2026-02-23T18:53:28.6364099Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-23T18:53:28.6416448Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-23T18:53:28.6927951Z Removing HTTP extra header
2026-02-23T18:53:28.6931246Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-23T18:53:28.6958788Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-23T18:53:28.7141751Z Removing includeIf entries pointing to credentials config files
2026-02-23T18:53:28.7144526Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-23T18:53:28.7163380Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-23T18:53:28.7163853Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-23T18:53:28.7164290Z includeif.gitdir:/github/workspace/.git.path
2026-02-23T18:53:28.7164662Z includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-23T18:53:28.7170335Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-23T18:53:28.7186912Z /__w/_temp/git-credentials-3e1e7297-6375-40cb-ae3e-1d0b23eb2307.config
2026-02-23T18:53:28.7195812Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-3e1e7297-6375-40cb-ae3e-1d0b23eb2307.config
2026-02-23T18:53:28.7222974Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-23T18:53:28.7241867Z /__w/_temp/git-credentials-3e1e7297-6375-40cb-ae3e-1d0b23eb2307.config
2026-02-23T18:53:28.7248865Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-3e1e7297-6375-40cb-ae3e-1d0b23eb2307.config
2026-02-23T18:53:28.7276073Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git.path
2026-02-23T18:53:28.7291470Z /github/runner_temp/git-credentials-3e1e7297-6375-40cb-ae3e-1d0b23eb2307.config
2026-02-23T18:53:28.7298374Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-3e1e7297-6375-40cb-ae3e-1d0b23eb2307.config
2026-02-23T18:53:28.7328287Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-23T18:53:28.7346269Z /github/runner_temp/git-credentials-3e1e7297-6375-40cb-ae3e-1d0b23eb2307.config
2026-02-23T18:53:28.7352755Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-3e1e7297-6375-40cb-ae3e-1d0b23eb2307.config
2026-02-23T18:53:28.7379917Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-23T18:53:28.7621312Z Removing credentials config '/__w/_temp/git-credentials-3e1e7297-6375-40cb-ae3e-1d0b23eb2307.config'
2026-02-23T18:53:47.2611355Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-23T18:53:47.2612426Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-23T18:53:47.2612667Z ##[endgroup]
2026-02-23T18:53:47.7424834Z Cleaning up orphan processes
