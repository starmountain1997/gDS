# Run ID: 22148925734
# Commit: f0caeeadcb37261beebd4a6e32934fa9f460db98
# Job: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
# Date: 2026-02-18
============================================================

ï»¿2026-02-18T17:33:36.3623752Z Current runner version: '2.330.0'
2026-02-18T17:33:36.3628410Z Runner name: 'linux-aarch64-a3-0-n4cwm-runner-rxxwk'
2026-02-18T17:33:36.3629140Z Runner group name: 'Default'
2026-02-18T17:33:36.3629888Z Machine name: 'linux-aarch64-a3-0-n4cwm-runner-rxxwk'
2026-02-18T17:33:36.3633553Z ##[group]GITHUB_TOKEN Permissions
2026-02-18T17:33:36.3635486Z Actions: write
2026-02-18T17:33:36.3635926Z ArtifactMetadata: write
2026-02-18T17:33:36.3636364Z Attestations: write
2026-02-18T17:33:36.3636739Z Checks: write
2026-02-18T17:33:36.3637211Z Contents: write
2026-02-18T17:33:36.3637608Z Deployments: write
2026-02-18T17:33:36.3637967Z Discussions: write
2026-02-18T17:33:36.3638343Z Issues: write
2026-02-18T17:33:36.3638696Z Metadata: read
2026-02-18T17:33:36.3639070Z Models: read
2026-02-18T17:33:36.3639441Z Packages: write
2026-02-18T17:33:36.3639809Z Pages: write
2026-02-18T17:33:36.3640186Z PullRequests: write
2026-02-18T17:33:36.3640590Z RepositoryProjects: write
2026-02-18T17:33:36.3641139Z SecurityEvents: write
2026-02-18T17:33:36.3641708Z Statuses: write
2026-02-18T17:33:36.3642388Z ##[endgroup]
2026-02-18T17:33:36.3644144Z Secret source: Actions
2026-02-18T17:33:36.3644631Z Prepare workflow directory
2026-02-18T17:33:36.4188893Z Prepare all required actions
2026-02-18T17:33:36.4219753Z Getting action download info
2026-02-18T17:33:37.6547139Z Download action repository 'actions/checkout@v6' (SHA:de0fac2e4500dabe0009e67214ff5f5447ce83dd)
2026-02-18T17:33:42.3722497Z Download action repository 'actions/upload-artifact@v6' (SHA:b7c566a772e6b6bfb58ed0dc250532a479d7789f)
2026-02-18T17:33:49.7318350Z Uses: vllm-project/vllm-ascend/.github/workflows/_e2e_nightly_multi_node.yaml@refs/heads/main (f0caeeadcb37261beebd4a6e32934fa9f460db98)
2026-02-18T17:33:49.7321571Z ##[group] Inputs
2026-02-18T17:33:49.7321857Z   soc_version: a3
2026-02-18T17:33:49.7322301Z   runner: linux-aarch64-a3-0
2026-02-18T17:33:49.7322700Z   image: swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3
2026-02-18T17:33:49.7323161Z   config_file_path: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-18T17:33:49.7323499Z   replicas: 1
2026-02-18T17:33:49.7323689Z   size: 2
2026-02-18T17:33:49.7323898Z   vllm_version: v0.15.0
2026-02-18T17:33:49.7324296Z   vllm_ascend_remote_url: https://github.com/vllm-project/vllm-ascend.git
2026-02-18T17:33:49.7324617Z   vllm_ascend_ref: main
2026-02-18T17:33:49.7324920Z ##[endgroup]
2026-02-18T17:33:49.7325507Z Complete job name: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-18T17:33:49.7887275Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-18T17:33:49.7889478Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-18T17:33:49.7889961Z ##[endgroup]
2026-02-18T17:34:05.2956231Z (node:70) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-18T17:34:05.2957064Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-18T17:34:07.1577948Z ##[group]Run # Decode and save kubeconfig
2026-02-18T17:34:07.1578358Z [36;1m# Decode and save kubeconfig[0m
2026-02-18T17:34:07.1610915Z [36;1mecho "***" | base64 -d > "$KUBECONFIG"[0m
2026-02-18T17:34:07.1611491Z shell: bash -el {0}
2026-02-18T17:34:07.1611821Z ##[endgroup]
2026-02-18T17:34:07.1726996Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-18T17:34:07.1727910Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-18T17:34:07.1728216Z ##[endgroup]
2026-02-18T17:34:07.5234273Z (node:401) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-18T17:34:07.5235107Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-18T17:34:08.3931151Z ##[group]Run actions/checkout@v6
2026-02-18T17:34:08.3931503Z with:
2026-02-18T17:34:08.3931745Z   repository: vllm-project/vllm-ascend
2026-02-18T17:34:08.3932563Z   token: ***
2026-02-18T17:34:08.3932801Z   ssh-strict: true
2026-02-18T17:34:08.3933009Z   ssh-user: git
2026-02-18T17:34:08.3933238Z   persist-credentials: true
2026-02-18T17:34:08.3933511Z   clean: true
2026-02-18T17:34:08.3933720Z   sparse-checkout-cone-mode: true
2026-02-18T17:34:08.3933982Z   fetch-depth: 1
2026-02-18T17:34:08.3934193Z   fetch-tags: false
2026-02-18T17:34:08.3934435Z   show-progress: true
2026-02-18T17:34:08.3934659Z   lfs: false
2026-02-18T17:34:08.3934858Z   submodules: false
2026-02-18T17:34:08.3935083Z   set-safe-directory: true
2026-02-18T17:34:08.3935405Z ##[endgroup]
2026-02-18T17:34:08.3976136Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-18T17:34:08.3976990Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-18T17:34:08.3977292Z ##[endgroup]
2026-02-18T17:34:08.7519338Z (node:432) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-18T17:34:08.7520633Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-18T17:34:09.3128300Z Syncing repository: vllm-project/vllm-ascend
2026-02-18T17:34:09.3129570Z ##[group]Getting Git version info
2026-02-18T17:34:09.3129967Z Working directory is '/__w/vllm-ascend/vllm-ascend'
2026-02-18T17:34:09.3130433Z [command]/usr/bin/git version
2026-02-18T17:34:09.3130781Z git version 2.34.1
2026-02-18T17:34:09.3132348Z ##[endgroup]
2026-02-18T17:34:09.3135283Z Copying '/root/.gitconfig' to '/__w/_temp/5422846d-3a64-41d2-8078-4337da411e77/.gitconfig'
2026-02-18T17:34:09.3138043Z Temporarily overriding HOME='/__w/_temp/5422846d-3a64-41d2-8078-4337da411e77' before making global git config changes
2026-02-18T17:34:09.3138696Z Adding repository directory to the temporary git global config as a safe directory
2026-02-18T17:34:09.3142243Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-18T17:34:09.3171657Z Deleting the contents of '/__w/vllm-ascend/vllm-ascend'
2026-02-18T17:34:09.3173772Z ##[group]Initializing the repository
2026-02-18T17:34:09.3177362Z [command]/usr/bin/git init /__w/vllm-ascend/vllm-ascend
2026-02-18T17:34:09.3280495Z hint: Using 'master' as the name for the initial branch. This default branch name
2026-02-18T17:34:09.3281046Z hint: is subject to change. To configure the initial branch name to use in all
2026-02-18T17:34:09.3281514Z hint: of your new repositories, which will suppress this warning, call:
2026-02-18T17:34:09.3281845Z hint: 
2026-02-18T17:34:09.3282228Z hint: 	git config --global init.defaultBranch <name>
2026-02-18T17:34:09.3282570Z hint: 
2026-02-18T17:34:09.3282865Z hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and
2026-02-18T17:34:09.3283269Z hint: 'development'. The just-created branch can be renamed via this command:
2026-02-18T17:34:09.3283642Z hint: 
2026-02-18T17:34:09.3283861Z hint: 	git branch -m <name>
2026-02-18T17:34:09.3287019Z Initialized empty Git repository in /__w/vllm-ascend/vllm-ascend/.git/
2026-02-18T17:34:09.3294909Z [command]/usr/bin/git remote add origin https://github.com/vllm-project/vllm-ascend
2026-02-18T17:34:09.3332433Z ##[endgroup]
2026-02-18T17:34:09.3332806Z ##[group]Disabling automatic garbage collection
2026-02-18T17:34:09.3335174Z [command]/usr/bin/git config --local gc.auto 0
2026-02-18T17:34:09.3359437Z ##[endgroup]
2026-02-18T17:34:09.3359785Z ##[group]Setting up auth
2026-02-18T17:34:09.3360381Z Removing SSH command configuration
2026-02-18T17:34:09.3366698Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-18T17:34:09.3391338Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-18T17:34:09.3572181Z Removing HTTP extra header
2026-02-18T17:34:09.3575202Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-18T17:34:09.3599643Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-18T17:34:09.3778483Z Removing includeIf entries pointing to credentials config files
2026-02-18T17:34:09.3782529Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-18T17:34:09.3809232Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-18T17:34:09.3997085Z [command]/usr/bin/git config --file /__w/_temp/git-credentials-3b318f8c-c51a-4b9a-94c5-ffda996b378b.config http.https://github.com/.extraheader AUTHORIZATION: basic ***
2026-02-18T17:34:09.4029293Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-3b318f8c-c51a-4b9a-94c5-ffda996b378b.config
2026-02-18T17:34:09.4058634Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-3b318f8c-c51a-4b9a-94c5-ffda996b378b.config
2026-02-18T17:34:09.4084466Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-3b318f8c-c51a-4b9a-94c5-ffda996b378b.config
2026-02-18T17:34:09.4112667Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-3b318f8c-c51a-4b9a-94c5-ffda996b378b.config
2026-02-18T17:34:09.4136426Z ##[endgroup]
2026-02-18T17:34:09.4136822Z ##[group]Fetching the repository
2026-02-18T17:34:09.4143225Z [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --no-recurse-submodules --depth=1 origin +f0caeeadcb37261beebd4a6e32934fa9f460db98:refs/remotes/origin/main
2026-02-18T17:34:11.9543891Z From https://gh-proxy.test.osinfra.cn/https://github.com/vllm-project/vllm-ascend
2026-02-18T17:34:11.9544462Z  * [new ref]         f0caeeadcb37261beebd4a6e32934fa9f460db98 -> origin/main
2026-02-18T17:34:11.9564042Z [command]/usr/bin/git branch --list --remote origin/main
2026-02-18T17:34:11.9590263Z   origin/main
2026-02-18T17:34:11.9597864Z [command]/usr/bin/git rev-parse refs/remotes/origin/main
2026-02-18T17:34:11.9616221Z f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-18T17:34:11.9620266Z ##[endgroup]
2026-02-18T17:34:11.9620683Z ##[group]Determining the checkout info
2026-02-18T17:34:11.9622373Z ##[endgroup]
2026-02-18T17:34:11.9625207Z [command]/usr/bin/git sparse-checkout disable
2026-02-18T17:34:11.9663159Z [command]/usr/bin/git config --local --unset-all extensions.worktreeConfig
2026-02-18T17:34:11.9686152Z ##[group]Checking out the ref
2026-02-18T17:34:11.9689467Z [command]/usr/bin/git checkout --progress --force -B main refs/remotes/origin/main
2026-02-18T17:34:12.0550686Z Switched to a new branch 'main'
2026-02-18T17:34:12.0551039Z Branch 'main' set up to track remote branch 'main' from 'origin'.
2026-02-18T17:34:12.0562877Z ##[endgroup]
2026-02-18T17:34:12.0592457Z [command]/usr/bin/git log -1 --format=%H
2026-02-18T17:34:12.0614993Z f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-18T17:34:12.4561445Z ##[group]Run # prepare for lws entrypoint scripts
2026-02-18T17:34:12.4561939Z [36;1m# prepare for lws entrypoint scripts[0m
2026-02-18T17:34:12.4562464Z [36;1minstall -D tests/e2e/nightly/multi_node/scripts/run.sh /root/.cache/tests/run.sh[0m
2026-02-18T17:34:12.4562982Z shell: bash -el {0}
2026-02-18T17:34:12.4563182Z ##[endgroup]
2026-02-18T17:34:12.4643492Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-18T17:34:12.4644325Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-18T17:34:12.4644587Z ##[endgroup]
2026-02-18T17:34:12.8167185Z (node:473) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-18T17:34:12.8167943Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-18T17:34:13.7972604Z ##[group]Run set -euo pipefail
2026-02-18T17:34:13.7972970Z [36;1mset -euo pipefail[0m
2026-02-18T17:34:13.7973226Z [36;1m[0m
2026-02-18T17:34:13.7973438Z [36;1mCRD_NAME="${CRD_NAME:-vllm}"[0m
2026-02-18T17:34:13.7973707Z [36;1mTIMEOUT=${TIMEOUT:-120}[0m
2026-02-18T17:34:13.7973985Z [36;1mSLEEP_INTERVAL=2[0m
2026-02-18T17:34:13.7974198Z [36;1m[0m
2026-02-18T17:34:13.7974754Z [36;1mecho "Deleting leaderworkerset [$CRD_NAME] in namespace [$NAMESPACE]..."[0m
2026-02-18T17:34:13.7975253Z [36;1mkubectl delete leaderworkerset "$CRD_NAME" -n "$NAMESPACE" --ignore-not-found[0m
2026-02-18T17:34:13.7975622Z [36;1m[0m
2026-02-18T17:34:13.7975902Z [36;1mecho "Waiting for all pods starting with 'vllm' to be deleted..."[0m
2026-02-18T17:34:13.7976259Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-18T17:34:13.7976503Z [36;1m[0m
2026-02-18T17:34:13.7976702Z [36;1mwhile true; do[0m
2026-02-18T17:34:13.7976941Z [36;1m  NOW=$(date +%s)[0m
2026-02-18T17:34:13.7977289Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-18T17:34:13.7977555Z [36;1m[0m
2026-02-18T17:34:13.7977769Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-18T17:34:13.7978120Z [36;1m    echo "Timeout reached ($TIMEOUT seconds), some pods still exist:"[0m
2026-02-18T17:34:13.7978534Z [36;1m    kubectl get pods -n "$NAMESPACE" | grep '^vllm' || true[0m
2026-02-18T17:34:13.7978849Z [36;1m    exit 1[0m
2026-02-18T17:34:13.7979031Z [36;1m  fi[0m
2026-02-18T17:34:13.7979252Z [36;1m[0m
2026-02-18T17:34:13.7979749Z [36;1m  PODS_EXIST=$(kubectl get pods -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null | tr ' ' '\n' | grep '^vllm' || true)[0m
2026-02-18T17:34:13.7980196Z [36;1m[0m
2026-02-18T17:34:13.7980430Z [36;1m  if [[ -z "$PODS_EXIST" ]]; then[0m
2026-02-18T17:34:13.7980690Z [36;1m    echo "All vllm pods deleted."[0m
2026-02-18T17:34:13.7980941Z [36;1m    break[0m
2026-02-18T17:34:13.7981176Z [36;1m  else[0m
2026-02-18T17:34:13.7981429Z [36;1m    echo "Waiting for pods to be deleted: $PODS_EXIST"[0m
2026-02-18T17:34:13.7981738Z [36;1m    sleep $SLEEP_INTERVAL[0m
2026-02-18T17:34:13.7982199Z [36;1m  fi[0m
2026-02-18T17:34:13.7982391Z [36;1mdone[0m
2026-02-18T17:34:13.7982754Z shell: bash -el {0}
2026-02-18T17:34:13.7982999Z ##[endgroup]
2026-02-18T17:34:13.8064341Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-18T17:34:13.8065093Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-18T17:34:13.8065398Z ##[endgroup]
2026-02-18T17:34:14.1549918Z (node:527) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-18T17:34:14.1550581Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-18T17:34:14.6550116Z Deleting leaderworkerset [vllm] in namespace [vllm-project]...
2026-02-18T17:34:14.7396864Z Waiting for all pods starting with 'vllm' to be deleted...
2026-02-18T17:34:14.8081705Z All vllm pods deleted.
2026-02-18T17:34:15.2076618Z ##[group]Run set -e
2026-02-18T17:34:15.2076859Z [36;1mset -e[0m
2026-02-18T17:34:15.2077013Z [36;1m[0m
2026-02-18T17:34:15.2077144Z [36;1msize="2"[0m
2026-02-18T17:34:15.2077296Z [36;1mreplicas="1"[0m
2026-02-18T17:34:15.2077617Z [36;1mimage="swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3"[0m
2026-02-18T17:34:15.2078045Z [36;1mconfig_file_path="DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-18T17:34:15.2078357Z [36;1mfail_tag=FAIL_TAG_"DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-18T17:34:15.2078623Z [36;1mecho "FAIL_TAG=${fail_tag}" >> "$GITHUB_ENV"[0m
2026-02-18T17:34:15.2078830Z [36;1m[0m
2026-02-18T17:34:15.2079042Z [36;1mrequired_params=("size" "replicas" "image" "config_file_path")[0m
2026-02-18T17:34:15.2079320Z [36;1mfor param in "${required_params[@]}"; do[0m
2026-02-18T17:34:15.2079539Z [36;1m  if [ -z "${!param}" ]; then[0m
2026-02-18T17:34:15.2080006Z [36;1m    echo "Error: Parameter '$param' is required but empty"[0m
2026-02-18T17:34:15.2080247Z [36;1m    exit 1[0m
2026-02-18T17:34:15.2080387Z [36;1m  fi[0m
2026-02-18T17:34:15.2080523Z [36;1mdone[0m
2026-02-18T17:34:15.2080652Z [36;1m[0m
2026-02-18T17:34:15.2080793Z [36;1mif [ "a3" = "a3" ]; then[0m
2026-02-18T17:34:15.2080976Z [36;1m  npu_per_node=16[0m
2026-02-18T17:34:15.2081230Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws.yaml.jinja2"[0m
2026-02-18T17:34:15.2081500Z [36;1melse[0m
2026-02-18T17:34:15.2081646Z [36;1m  npu_per_node=8[0m
2026-02-18T17:34:15.2081902Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws-a2.yaml.jinja2"[0m
2026-02-18T17:34:15.2082394Z [36;1mfi[0m
2026-02-18T17:34:15.2082524Z [36;1m[0m
2026-02-18T17:34:15.2082671Z [36;1mjinja2 $TEMPLATE_FILE \[0m
2026-02-18T17:34:15.2082865Z [36;1m  -D size="$size" \[0m
2026-02-18T17:34:15.2083046Z [36;1m  -D replicas="$replicas" \[0m
2026-02-18T17:34:15.2083242Z [36;1m  -D image="$image" \[0m
2026-02-18T17:34:15.2083464Z [36;1m  -D config_file_path="$config_file_path" \[0m
2026-02-18T17:34:15.2083695Z [36;1m  -D npu_per_node="$npu_per_node" \[0m
2026-02-18T17:34:15.2083901Z [36;1m  -D fail_tag="$fail_tag" \[0m
2026-02-18T17:34:15.2084092Z [36;1m  --outfile lws.yaml[0m
2026-02-18T17:34:15.2084256Z [36;1m[0m
2026-02-18T17:34:15.2084399Z [36;1mkubectl apply -f ./lws.yaml[0m
2026-02-18T17:34:15.2084701Z shell: bash -el {0}
2026-02-18T17:34:15.2084855Z ##[endgroup]
2026-02-18T17:34:15.2160885Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-18T17:34:15.2161756Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-18T17:34:15.2162182Z ##[endgroup]
2026-02-18T17:34:15.5945946Z (node:593) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-18T17:34:15.5946654Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-18T17:34:16.5234301Z leaderworkerset.leaderworkerset.x-k8s.io/vllm created
2026-02-18T17:34:16.5424875Z service/vllm-leader created
2026-02-18T17:34:17.0199222Z ##[group]Run POD_PREFIX="${POD_PREFIX:-vllm-0}"
2026-02-18T17:34:17.0199540Z [36;1mPOD_PREFIX="${POD_PREFIX:-vllm-0}"[0m
2026-02-18T17:34:17.0199795Z [36;1mSIZE="2"[0m
2026-02-18T17:34:17.0199978Z [36;1mTIMEOUT=1200  # default timeout 20 minutes[0m
2026-02-18T17:34:17.0200189Z [36;1m[0m
2026-02-18T17:34:17.0200490Z [36;1mecho "Waiting for Pods in namespace [$NAMESPACE] to become Running and Ready (timeout ${TIMEOUT}s)..."[0m
2026-02-18T17:34:17.0200835Z [36;1m[0m
2026-02-18T17:34:17.0200980Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-18T17:34:17.0201146Z [36;1m[0m
2026-02-18T17:34:17.0201282Z [36;1mwhile true; do[0m
2026-02-18T17:34:17.0201444Z [36;1m  NOW=$(date +%s)[0m
2026-02-18T17:34:17.0201631Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-18T17:34:17.0201846Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-18T17:34:17.0202305Z [36;1m    echo "Timeout reached after ${ELAPSED}s"[0m
2026-02-18T17:34:17.0202607Z [36;1m    echo "Dumping pod status for debugging:"[0m
2026-02-18T17:34:17.0202913Z [36;1m    kubectl get pods -n "$NAMESPACE"[0m
2026-02-18T17:34:17.0203176Z [36;1m    kubectl describe pod "$LEADER_POD" -n "$NAMESPACE"[0m
2026-02-18T17:34:17.0203408Z [36;1m    exit 1[0m
2026-02-18T17:34:17.0203550Z [36;1m  fi[0m
2026-02-18T17:34:17.0203687Z [36;1m[0m
2026-02-18T17:34:17.0203835Z [36;1m  # 1) check follower pods[0m
2026-02-18T17:34:17.0204021Z [36;1m  ALL_FOLLOWERS_READY=true[0m
2026-02-18T17:34:17.0204261Z [36;1m  for ((i=1; i<SIZE; i++)); do[0m
2026-02-18T17:34:17.0204455Z [36;1m    POD="${POD_PREFIX}-${i}"[0m
2026-02-18T17:34:17.0204822Z [36;1m    PHASE=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-18T17:34:17.0205353Z [36;1m    READY=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-18T17:34:17.0205855Z [36;1m[0m
2026-02-18T17:34:17.0206038Z [36;1m    echo "Follower [$POD] phase=$PHASE ready=$READY"[0m
2026-02-18T17:34:17.0206264Z [36;1m[0m
2026-02-18T17:34:17.0206440Z [36;1m    if [[ "$PHASE" != "Running" || "$READY" != "true" ]]; then[0m
2026-02-18T17:34:17.0206712Z [36;1m      echo "Follower [$POD] not Ready yet..."[0m
2026-02-18T17:34:17.0206937Z [36;1m      ALL_FOLLOWERS_READY=false[0m
2026-02-18T17:34:17.0207119Z [36;1m      break[0m
2026-02-18T17:34:17.0207273Z [36;1m    fi[0m
2026-02-18T17:34:17.0207415Z [36;1m  done[0m
2026-02-18T17:34:17.0207545Z [36;1m[0m
2026-02-18T17:34:17.0207696Z [36;1m  # 2) check leader pod[0m
2026-02-18T17:34:17.0208076Z [36;1m  LEADER_PHASE=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-18T17:34:17.0208672Z [36;1m  LEADER_READY=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-18T17:34:17.0218529Z [36;1m[0m
2026-02-18T17:34:17.0219034Z [36;1m  echo "Leader [$LEADER_POD] phase=$LEADER_PHASE ready=$LEADER_READY"[0m
2026-02-18T17:34:17.0219369Z [36;1m[0m
2026-02-18T17:34:17.0219696Z [36;1m  if [[ "$LEADER_PHASE" != "Running" || "$LEADER_READY" != "true" ]]; then[0m
2026-02-18T17:34:17.0220032Z [36;1m    echo "Leader not Ready yet..."[0m
2026-02-18T17:34:17.0220312Z [36;1m    ALL_FOLLOWERS_READY=false[0m
2026-02-18T17:34:17.0220554Z [36;1m  fi[0m
2026-02-18T17:34:17.0220764Z [36;1m[0m
2026-02-18T17:34:17.0221002Z [36;1m  if [[ "$ALL_FOLLOWERS_READY" == "true" ]]; then[0m
2026-02-18T17:34:17.0221383Z [36;1m    echo "All follower pods and leader pod are Running and Ready â€” continuing."[0m
2026-02-18T17:34:17.0221746Z [36;1m    break[0m
2026-02-18T17:34:17.0221955Z [36;1m  fi[0m
2026-02-18T17:34:17.0228432Z [36;1m[0m
2026-02-18T17:34:17.0228635Z [36;1m  sleep 2[0m
2026-02-18T17:34:17.0228842Z [36;1mdone[0m
2026-02-18T17:34:17.0229168Z shell: bash -el {0}
2026-02-18T17:34:17.0229381Z env:
2026-02-18T17:34:17.0229758Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-18T17:34:17.0230069Z ##[endgroup]
2026-02-18T17:34:17.0370140Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-18T17:34:17.0370909Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-18T17:34:17.0371184Z ##[endgroup]
2026-02-18T17:34:17.4241039Z (node:671) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-18T17:34:17.4241790Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-18T17:34:17.9533709Z Waiting for Pods in namespace [vllm-project] to become Running and Ready (timeout 1200s)...
2026-02-18T17:34:18.0843912Z Follower [vllm-0-1] phase=Pending ready=
2026-02-18T17:34:18.0844238Z Follower [vllm-0-1] not Ready yet...
2026-02-18T17:34:18.1936606Z Leader [vllm-0] phase=Pending ready=
2026-02-18T17:34:18.1937049Z Leader not Ready yet...
2026-02-18T17:34:20.3088734Z Follower [vllm-0-1] phase=Pending ready=
2026-02-18T17:34:20.3089159Z Follower [vllm-0-1] not Ready yet...
2026-02-18T17:34:20.4199441Z Leader [vllm-0] phase=Pending ready=
2026-02-18T17:34:20.4199783Z Leader not Ready yet...
2026-02-18T17:34:22.5527156Z Follower [vllm-0-1] phase=Pending ready=
2026-02-18T17:34:22.5527505Z Follower [vllm-0-1] not Ready yet...
2026-02-18T17:34:22.6692479Z Leader [vllm-0] phase=Pending ready=
2026-02-18T17:34:22.6692807Z Leader not Ready yet...
2026-02-18T17:34:24.7861265Z Follower [vllm-0-1] phase=Pending ready=
2026-02-18T17:34:24.7861663Z Follower [vllm-0-1] not Ready yet...
2026-02-18T17:34:24.8997991Z Leader [vllm-0] phase=Pending ready=
2026-02-18T17:34:24.8998393Z Leader not Ready yet...
2026-02-18T17:34:27.0180338Z Follower [vllm-0-1] phase=Pending ready=
2026-02-18T17:34:27.0180804Z Follower [vllm-0-1] not Ready yet...
2026-02-18T17:34:27.1423378Z Leader [vllm-0] phase=Pending ready=
2026-02-18T17:34:27.1424085Z Leader not Ready yet...
2026-02-18T17:34:29.2687810Z Follower [vllm-0-1] phase=Pending ready=
2026-02-18T17:34:29.2688218Z Follower [vllm-0-1] not Ready yet...
2026-02-18T17:34:29.3865005Z Leader [vllm-0] phase=Pending ready=false
2026-02-18T17:34:29.3865242Z Leader not Ready yet...
2026-02-18T17:34:31.5109261Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-18T17:34:31.5109542Z Follower [vllm-0-1] not Ready yet...
2026-02-18T17:34:31.6280513Z Leader [vllm-0] phase=Running ready=true
2026-02-18T17:34:33.7436655Z Follower [vllm-0-1] phase=Running ready=true
2026-02-18T17:34:33.8580101Z Leader [vllm-0] phase=Running ready=true
2026-02-18T17:34:33.8580834Z All follower pods and leader pod are Running and Ready â€” continuing.
2026-02-18T17:34:34.2921046Z ##[group]Run set -euo pipefail
2026-02-18T17:34:34.2921323Z [36;1mset -euo pipefail[0m
2026-02-18T17:34:34.2921488Z [36;1m[0m
2026-02-18T17:34:34.2921624Z [36;1msize="2"[0m
2026-02-18T17:34:34.2921764Z [36;1mpids=()[0m
2026-02-18T17:34:34.2921925Z [36;1m[0m
2026-02-18T17:34:34.2922194Z [36;1mcleanup() {[0m
2026-02-18T17:34:34.2922393Z [36;1m  echo "Cleaning up background log streams..."[0m
2026-02-18T17:34:34.2922634Z [36;1m  for pid in "${pids[@]}"; do[0m
2026-02-18T17:34:34.2922839Z [36;1m    kill "$pid" 2>/dev/null || true[0m
2026-02-18T17:34:34.2923026Z [36;1m  done[0m
2026-02-18T17:34:34.2923168Z [36;1m}[0m
2026-02-18T17:34:34.2923303Z [36;1mtrap cleanup EXIT[0m
2026-02-18T17:34:34.2923465Z [36;1m[0m
2026-02-18T17:34:34.2923614Z [36;1mfor i in $(seq 1 $((size - 1))); do[0m
2026-02-18T17:34:34.2923807Z [36;1m  POD="vllm-0-${i}"[0m
2026-02-18T17:34:34.2923975Z [36;1m[0m
2026-02-18T17:34:34.2924240Z [36;1m  echo "==== Collecting logs from worker pod: $POD ===="[0m
2026-02-18T17:34:34.2924521Z [36;1m  kubectl logs -f "$POD" -n "$NAMESPACE" \[0m
2026-02-18T17:34:34.2924746Z [36;1m    > "/tmp/${POD}_logs.txt" 2>&1 &[0m
2026-02-18T17:34:34.2924932Z [36;1m[0m
2026-02-18T17:34:34.2925061Z [36;1m  pids+=($!)[0m
2026-02-18T17:34:34.2925223Z [36;1mdone[0m
2026-02-18T17:34:34.2925362Z [36;1m[0m
2026-02-18T17:34:34.2925555Z [36;1mecho "==== Streaming logs from leader pod: $LEADER_POD ===="[0m
2026-02-18T17:34:34.2925839Z [36;1mecho "Looking for logs containing: $FAIL_TAG"[0m
2026-02-18T17:34:34.2926048Z [36;1m[0m
2026-02-18T17:34:34.2926275Z [36;1mkubectl logs -f "$LEADER_POD" -n "$NAMESPACE" | while IFS= read -r line; do[0m
2026-02-18T17:34:34.2926563Z [36;1m  echo "$line"[0m
2026-02-18T17:34:34.2926756Z [36;1m  if echo "$line" | grep -q "$FAIL_TAG"; then[0m
2026-02-18T17:34:34.2926958Z [36;1m    exit 1[0m
2026-02-18T17:34:34.2927104Z [36;1m  fi[0m
2026-02-18T17:34:34.2927235Z [36;1mdone[0m
2026-02-18T17:34:34.2927606Z shell: bash -el {0}
2026-02-18T17:34:34.2927761Z env:
2026-02-18T17:34:34.2927959Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-18T17:34:34.2928190Z ##[endgroup]
2026-02-18T17:34:34.3004958Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-18T17:34:34.3005792Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-18T17:34:34.3006008Z ##[endgroup]
2026-02-18T17:34:34.6519742Z (node:763) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-18T17:34:34.6520415Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-18T17:34:35.3874067Z ==== Collecting logs from worker pod: vllm-0-1 ====
2026-02-18T17:34:35.3874459Z ==== Streaming logs from leader pod: vllm-0 ====
2026-02-18T17:34:35.3874768Z Looking for logs containing: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-18T17:34:35.4695169Z /usr/local/Ascend/ascend-toolkit/set_env.sh: line 31: CMAKE_PREFIX_PATH: unbound variable
2026-02-18T17:34:35.4705734Z ====> Check NPU info
2026-02-18T17:34:35.4716800Z +------------------------------------------------------------------------------------------------+
2026-02-18T17:34:35.4727181Z | npu-smi 25.5.0                   Version: 25.5.0                                               |
2026-02-18T17:34:35.4736659Z +---------------------------+---------------+----------------------------------------------------+
2026-02-18T17:34:35.4746484Z | NPU   Name                | Health        | Power(W)    Temp(C)           Hugepages-Usage(page)|
2026-02-18T17:34:35.4756189Z | Chip  Phy-ID              | Bus-Id        | AICore(%)   Memory-Usage(MB)  HBM-Usage(MB)        |
2026-02-18T17:34:35.4766579Z +===========================+===============+====================================================+
2026-02-18T17:34:35.4777237Z | 0     Ascend910           | OK            | 160.8       36                0    / 0             |
2026-02-18T17:34:35.4787353Z | 0     0                   | 0000:9D:00.0  | 0           0    / 0          3150 / 65536         |
2026-02-18T17:34:35.4798778Z +------------------------------------------------------------------------------------------------+
2026-02-18T17:34:35.4809321Z | 0     Ascend910           | OK            | -           35                0    / 0             |
2026-02-18T17:34:35.4818320Z | 1     1                   | 0000:9F:00.0  | 0           0    / 0          2899 / 65536         |
2026-02-18T17:34:35.4828023Z +===========================+===============+====================================================+
2026-02-18T17:34:35.4868739Z | 1     Ascend910           | OK            | 163.6       36                0    / 0             |
2026-02-18T17:34:35.4869066Z | 0     2                   | 0000:99:00.0  | 0           0    / 0          3160 / 65536         |
2026-02-18T17:34:35.4869386Z +------------------------------------------------------------------------------------------------+
2026-02-18T17:34:35.4869730Z | 1     Ascend910           | OK            | -           36                0    / 0             |
2026-02-18T17:34:35.4876968Z | 1     3                   | 0000:9B:00.0  | 0           0    / 0          2886 / 65536         |
2026-02-18T17:34:35.4887400Z +===========================+===============+====================================================+
2026-02-18T17:34:35.4897418Z | 2     Ascend910           | OK            | 163.7       36                0    / 0             |
2026-02-18T17:34:35.4906984Z | 0     4                   | 0000:95:00.0  | 0           0    / 0          3151 / 65536         |
2026-02-18T17:34:35.4916219Z +------------------------------------------------------------------------------------------------+
2026-02-18T17:34:35.4926065Z | 2     Ascend910           | OK            | -           37                0    / 0             |
2026-02-18T17:34:35.4935716Z | 1     5                   | 0000:97:00.0  | 0           0    / 0          2898 / 65536         |
2026-02-18T17:34:35.4944999Z +===========================+===============+====================================================+
2026-02-18T17:34:35.4954990Z | 3     Ascend910           | OK            | 169.6       37                0    / 0             |
2026-02-18T17:34:35.4965082Z | 0     6                   | 0000:91:00.0  | 0           0    / 0          3157 / 65536         |
2026-02-18T17:34:35.4975203Z +------------------------------------------------------------------------------------------------+
2026-02-18T17:34:35.4984224Z | 3     Ascend910           | OK            | -           36                0    / 0             |
2026-02-18T17:34:35.4993477Z | 1     7                   | 0000:93:00.0  | 0           0    / 0          2889 / 65536         |
2026-02-18T17:34:35.5003012Z +===========================+===============+====================================================+
2026-02-18T17:34:35.5013092Z | 4     Ascend910           | OK            | 166.4       36                0    / 0             |
2026-02-18T17:34:35.5022324Z | 0     8                   | 0000:8D:00.0  | 0           0    / 0          3151 / 65536         |
2026-02-18T17:34:35.5031666Z +------------------------------------------------------------------------------------------------+
2026-02-18T17:34:35.5042989Z | 4     Ascend910           | OK            | -           35                0    / 0             |
2026-02-18T17:34:35.5053094Z | 1     9                   | 0000:8F:00.0  | 0           0    / 0          2897 / 65536         |
2026-02-18T17:34:35.5062132Z +===========================+===============+====================================================+
2026-02-18T17:34:35.5070604Z | 5     Ascend910           | OK            | 162.0       34                0    / 0             |
2026-02-18T17:34:35.5079788Z | 0     10                  | 0000:89:00.0  | 0           0    / 0          3143 / 65536         |
2026-02-18T17:34:35.5089650Z +------------------------------------------------------------------------------------------------+
2026-02-18T17:34:35.5098841Z | 5     Ascend910           | OK            | -           36                0    / 0             |
2026-02-18T17:34:35.5107979Z | 1     11                  | 0000:8B:00.0  | 0           0    / 0          2899 / 65536         |
2026-02-18T17:34:35.5117406Z +===========================+===============+====================================================+
2026-02-18T17:34:35.5127316Z | 6     Ascend910           | OK            | 160.5       37                0    / 0             |
2026-02-18T17:34:35.5136815Z | 0     12                  | 0000:85:00.0  | 0           0    / 0          3151 / 65536         |
2026-02-18T17:34:35.5146252Z +------------------------------------------------------------------------------------------------+
2026-02-18T17:34:35.5155300Z | 6     Ascend910           | OK            | -           35                0    / 0             |
2026-02-18T17:34:35.5165643Z | 1     13                  | 0000:87:00.0  | 0           0    / 0          2898 / 65536         |
2026-02-18T17:34:35.5175238Z +===========================+===============+====================================================+
2026-02-18T17:34:35.5184639Z | 7     Ascend910           | OK            | 164.6       36                0    / 0             |
2026-02-18T17:34:35.5194352Z | 0     14                  | 0000:81:00.0  | 0           0    / 0          3159 / 65536         |
2026-02-18T17:34:35.5204720Z +------------------------------------------------------------------------------------------------+
2026-02-18T17:34:35.5214689Z | 7     Ascend910           | OK            | -           36                0    / 0             |
2026-02-18T17:34:35.5224898Z | 1     15                  | 0000:83:00.0  | 0           0    / 0          2887 / 65536         |
2026-02-18T17:34:35.5233101Z +===========================+===============+====================================================+
2026-02-18T17:34:35.5242813Z +---------------------------+---------------+----------------------------------------------------+
2026-02-18T17:34:35.5252168Z | NPU     Chip              | Process id    | Process name             | Process memory(MB)      |
2026-02-18T17:34:35.5261424Z +===========================+===============+====================================================+
2026-02-18T17:34:35.5271337Z | No running processes found in NPU 0                                                            |
2026-02-18T17:34:35.5281257Z +===========================+===============+====================================================+
2026-02-18T17:34:35.5290998Z | No running processes found in NPU 1                                                            |
2026-02-18T17:34:35.5300631Z +===========================+===============+====================================================+
2026-02-18T17:34:35.5309749Z | No running processes found in NPU 2                                                            |
2026-02-18T17:34:35.5319578Z +===========================+===============+====================================================+
2026-02-18T17:34:35.5329435Z | No running processes found in NPU 3                                                            |
2026-02-18T17:34:35.5339425Z +===========================+===============+====================================================+
2026-02-18T17:34:35.5348848Z | No running processes found in NPU 4                                                            |
2026-02-18T17:34:35.5358564Z +===========================+===============+====================================================+
2026-02-18T17:34:35.5368330Z | No running processes found in NPU 5                                                            |
2026-02-18T17:34:35.5378057Z +===========================+===============+====================================================+
2026-02-18T17:34:35.5387470Z | No running processes found in NPU 6                                                            |
2026-02-18T17:34:35.5397024Z +===========================+===============+====================================================+
2026-02-18T17:34:35.5407589Z | No running processes found in NPU 7                                                            |
2026-02-18T17:34:35.5416870Z +===========================+===============+====================================================+
2026-02-18T17:34:35.5426117Z package_name=Ascend-cann-toolkit
2026-02-18T17:34:35.5435387Z version=8.5.0
2026-02-18T17:34:35.5444894Z innerversion=V100R001C25SPC001B232
2026-02-18T17:34:35.5454635Z compatible_version=[V100R001C15],[V100R001C18],[V100R001C19],[V100R001C20],[V100R001C21],[V100R001C23]
2026-02-18T17:34:35.5463387Z arch=aarch64
2026-02-18T17:34:35.5472821Z os=linux
2026-02-18T17:34:35.5482303Z path=/usr/local/Ascend/cann-8.5.0
2026-02-18T17:34:35.5491785Z ====> Configure mirrors and git proxy
2026-02-18T17:34:35.5501129Z Writing to /root/.config/pip/pip.conf
2026-02-18T17:34:35.5510287Z Installed vLLM-related Python packages:
2026-02-18T17:34:35.5519930Z ais_bench_benchmark               3.0.20250930                /vllm-workspace/vllm-ascend/benchmark
2026-02-18T17:34:35.5530214Z vllm                              0.15.0+empty                /vllm-workspace/vllm
2026-02-18T17:34:35.5539605Z vllm_ascend                       0.14.0rc2.dev171+gf0caeeadc /vllm-workspace/vllm-ascend
2026-02-18T17:34:35.5547951Z 
2026-02-18T17:34:35.5557086Z ============================
2026-02-18T17:34:35.5566808Z vLLM Git information
2026-02-18T17:34:35.5575818Z ============================
2026-02-18T17:34:35.5585012Z Branch:      HEAD
2026-02-18T17:34:35.5595623Z Commit hash: f176443446f659dbab5315e056e605d8984fd976
2026-02-18T17:34:35.5604300Z Author:      TJian <tunjian.tan@embeddedllm.com>
2026-02-18T17:34:35.5613186Z Date:        2026-01-29 14:45:42 +0800
2026-02-18T17:34:35.5622292Z Message:     [Release] [CI] Optim release pipeline (#33156)
2026-02-18T17:34:35.5631374Z Tags:        v0.15.0
2026-02-18T17:34:35.5640786Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm.git (fetch)
2026-02-18T17:34:35.5649950Z 
2026-02-18T17:34:35.5658660Z 
2026-02-18T17:34:35.5668032Z ============================
2026-02-18T17:34:35.5677374Z vLLM-Ascend Git information
2026-02-18T17:34:35.5687178Z ============================
2026-02-18T17:34:35.5696528Z Branch:      main
2026-02-18T17:34:35.5705859Z Commit hash: f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-18T17:34:35.5715181Z Author:      Nengjun Ma <nengjunma@outlook.com>
2026-02-18T17:34:35.5724756Z Date:        2026-02-14 18:54:04 +0800
2026-02-18T17:34:35.5734101Z Message:     [CI] unlock when load model (#6771)
2026-02-18T17:34:35.5743401Z Tags:        
2026-02-18T17:34:35.5753191Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm-ascend (fetch)
2026-02-18T17:34:35.5762687Z 
2026-02-18T17:34:35.5773378Z ====> Check triton ascend info
2026-02-18T17:34:35.5783781Z Ubuntu clang version 15.0.7
2026-02-18T17:34:35.5793859Z Target: aarch64-unknown-linux-gnu
2026-02-18T17:34:35.5803938Z Thread model: posix
2026-02-18T17:34:35.5813127Z InstalledDir: /usr/bin
2026-02-18T17:34:35.5822359Z Found candidate GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-18T17:34:35.5831792Z Selected GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-18T17:34:35.5841315Z Candidate multilib: .;@m64
2026-02-18T17:34:35.5851003Z Selected multilib: .;@m64
2026-02-18T17:34:35.5860553Z /usr/local/Ascend/cann-8.5.0/tools/bishengir/bin/bishengir-compile
2026-02-18T17:34:35.5869256Z Name: triton-ascend
2026-02-18T17:34:35.5878858Z Version: 3.2.0
2026-02-18T17:34:35.5889405Z Summary: A language and compiler for custom Deep Learning operations on Ascend hardwares
2026-02-18T17:34:35.5899062Z Home-page: https://gitcode.com/Ascend/triton-ascend/
2026-02-18T17:34:35.5908041Z Author: 
2026-02-18T17:34:35.5917738Z Author-email: 
2026-02-18T17:34:35.5928046Z License: 
2026-02-18T17:34:35.5937491Z Location: /usr/local/python3.11.14/lib/python3.11/site-packages
2026-02-18T17:34:35.5946973Z Requires: 
2026-02-18T17:34:35.5956401Z Required-by: vllm_ascend
2026-02-18T17:34:52.7047756Z INFO 02-18 17:34:52 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:34:52.7059477Z INFO 02-18 17:34:52 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:34:52.7072072Z INFO 02-18 17:34:52 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:34:52.9186825Z INFO 02-18 17:34:52 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:34:58.9069358Z ============================= test session starts ==============================
2026-02-18T17:34:58.9080685Z platform linux -- Python 3.11.14, pytest-8.4.2, pluggy-1.6.0 -- /usr/local/python3.11.14/bin/python3
2026-02-18T17:34:58.9093146Z cachedir: .pytest_cache
2026-02-18T17:34:58.9104370Z rootdir: /vllm-workspace/vllm-ascend
2026-02-18T17:34:58.9116034Z configfile: pyproject.toml
2026-02-18T17:34:58.9128501Z plugins: asyncio-1.3.0, mock-3.15.1, cov-7.0.0, anyio-4.12.1
2026-02-18T17:34:58.9140269Z asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
2026-02-18T17:34:59.6506943Z collecting ... collected 1 item
2026-02-18T17:34:59.6514092Z 
2026-02-18T17:34:59.6529701Z [2026-02-18 17:34:59] INFO multi_node_config.py:294: Loading config yaml: tests/e2e/nightly/multi_node/config/DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-18T17:34:59.6576161Z [2026-02-18 17:34:59] INFO multi_node_config.py:351: Resolving cluster IPs via DNS...
2026-02-18T17:34:59.6622737Z [2026-02-18 17:34:59] INFO multi_node_config.py:212: Node 0 envs: {'HCCL_OP_EXPANSION_MODE': 'AIV', 'VLLM_USE_MODELSCOPE': 'True', 'HCCL_BUFFSIZE': '1024', 'SERVER_PORT': '8080', 'OMP_PROC_BIND': 'False', 'OMP_NUM_THREADS': '1', 'PYTORCH_NPU_ALLOC_CONF': 'expandable_segments:True', 'VLLM_ASCEND_ENABLE_FLASHCOMM1': '1', 'ASCEND_A3_EBA_ENABLE': '1', 'HCCL_IF_IP': '10.0.0.179', 'HCCL_SOCKET_IFNAME': 'eth0', 'GLOO_SOCKET_IFNAME': 'eth0', 'TP_SOCKET_IFNAME': 'eth0', 'LOCAL_IP': '10.0.0.179', 'NIC_NAME': 'eth0', 'MASTER_IP': '10.0.0.179'}
2026-02-18T17:34:59.6644609Z [2026-02-18 17:34:59] INFO multi_node_config.py:137: Not launching proxy on non-master node
2026-02-18T17:34:59.6656685Z [2026-02-18 17:34:59] INFO conftest.py:241: Starting server with command: vllm serve vllm-ascend/DeepSeek-V3.2-W8A8 --host 0.0.0.0 --port 8080 --data-parallel-size 4 --data-parallel-size-local 2 --data-parallel-address 10.0.0.179 --data-parallel-rpc-port 13399 --tensor-parallel-size 8 --quantization ascend --seed 1024 --enable-expert-parallel --max-num-seqs 16 --max-model-len 8192 --max-num-batched-tokens 4096 --no-enable-prefix-caching --gpu-memory-utilization 0.85 --trust-remote-code --speculative-config {"num_speculative_tokens": 2, "method":"deepseek_mtp"} --compilation-config {"cudagraph_capture_sizes": [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], "cudagraph_mode": "FULL_DECODE_ONLY"} --additional-config {"layer_sharding": ["q_b_proj", "o_proj"]} --tokenizer-mode deepseek_v32 --reasoning-parser deepseek_v3
2026-02-18T17:35:06.0700390Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node INFO 02-18 17:35:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:35:06.0859122Z INFO 02-18 17:35:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:35:06.0870894Z INFO 02-18 17:35:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:35:06.0881862Z INFO 02-18 17:35:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:35:14.7613785Z 2026-02-18 17:35:14,758 - 138 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:35:14.7933273Z INFO 02-18 17:35:14 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:35:14.9821802Z INFO 02-18 17:35:14 [serve.py:100] Defaulting api_server_count to data_parallel_size (4).
2026-02-18T17:35:14.9851817Z INFO 02-18 17:35:14 [utils.py:325] 
2026-02-18T17:35:14.9860511Z INFO 02-18 17:35:14 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
2026-02-18T17:35:14.9870523Z INFO 02-18 17:35:14 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
2026-02-18T17:35:14.9880821Z INFO 02-18 17:35:14 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   vllm-ascend/DeepSeek-V3.2-W8A8
2026-02-18T17:35:14.9890815Z INFO 02-18 17:35:14 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
2026-02-18T17:35:14.9901139Z INFO 02-18 17:35:14 [utils.py:325] 
2026-02-18T17:35:14.9920590Z INFO 02-18 17:35:14 [utils.py:261] non-default args: {'model_tag': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'api_server_count': 4, 'host': '0.0.0.0', 'port': 8080, 'model': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'tokenizer_mode': 'deepseek_v32', 'trust_remote_code': True, 'seed': 1024, 'max_model_len': 8192, 'quantization': 'ascend', 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 8, 'data_parallel_size': 4, 'data_parallel_size_local': 2, 'data_parallel_address': '10.0.0.179', 'data_parallel_rpc_port': 13399, 'enable_expert_parallel': True, 'gpu_memory_utilization': 0.85, 'enable_prefix_caching': False, 'max_num_batched_tokens': 4096, 'max_num_seqs': 16, 'speculative_config': {'num_speculative_tokens': 2, 'method': 'deepseek_mtp'}, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}, 'additional_config': {'layer_sharding': ['q_b_proj', 'o_proj']}}
2026-02-18T17:35:15.0341766Z 2026-02-18 17:35:15,032 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-18T17:35:15.0404375Z INFO 02-18 17:35:15 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-18T17:35:15.0427674Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:15.0468005Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:15.0492934Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:15.0502985Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-18T17:35:15.0705686Z INFO 02-18 17:35:15 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-18T17:35:15.0714511Z INFO 02-18 17:35:15 [model.py:1561] Using max model len 8192
2026-02-18T17:35:15.3440924Z WARNING 02-18 17:35:15 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-18T17:35:15.3461472Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:15.3470672Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:15.3481508Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-18T17:35:25.8591311Z INFO 02-18 17:35:25 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-18T17:35:25.8645804Z INFO 02-18 17:35:25 [model.py:1561] Using max model len 163840
2026-02-18T17:35:25.8706697Z WARNING 02-18 17:35:25 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-18T17:35:25.8708682Z INFO 02-18 17:35:25 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-18T17:35:26.1943393Z INFO 02-18 17:35:26 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-18T17:35:26.1948625Z INFO 02-18 17:35:26 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-18T17:35:26.1993559Z WARNING 02-18 17:35:26 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-18T17:35:26.2004032Z WARNING 02-18 17:35:26 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-18T17:35:26.2012804Z INFO 02-18 17:35:26 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:35:26.2023021Z INFO 02-18 17:35:26 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:35:26.2032869Z INFO 02-18 17:35:26 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:35:26.2042748Z WARNING 02-18 17:35:26 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-18T17:35:26.2052669Z INFO 02-18 17:35:26 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-18T17:35:26.2062626Z WARNING 02-18 17:35:26 [platform.py:335] [91m
2026-02-18T17:35:26.2072344Z WARNING 02-18 17:35:26 [platform.py:335]             **********************************************************************************
2026-02-18T17:35:26.2082394Z WARNING 02-18 17:35:26 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-18T17:35:26.2091870Z WARNING 02-18 17:35:26 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-18T17:35:26.2102702Z WARNING 02-18 17:35:26 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-18T17:35:26.2112046Z WARNING 02-18 17:35:26 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-18T17:35:26.2122046Z WARNING 02-18 17:35:26 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-18T17:35:26.2131532Z WARNING 02-18 17:35:26 [platform.py:335]             * batch size for graph capture.
2026-02-18T17:35:26.2140620Z WARNING 02-18 17:35:26 [platform.py:335]             * For more details, please refer to:
2026-02-18T17:35:26.2151188Z WARNING 02-18 17:35:26 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-18T17:35:26.2161147Z WARNING 02-18 17:35:26 [platform.py:335]             **********************************************************************************[0m
2026-02-18T17:35:26.2170090Z WARNING 02-18 17:35:26 [platform.py:335]             
2026-02-18T17:35:26.2180539Z INFO 02-18 17:35:26 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-18T17:35:26.2189395Z INFO 02-18 17:35:26 [utils.py:851] Started DP Coordinator process (PID: 158)
2026-02-18T17:35:30.8772424Z INFO 02-18 17:35:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:35:30.8781272Z INFO 02-18 17:35:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:35:30.8793137Z INFO 02-18 17:35:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:35:30.8840018Z INFO 02-18 17:35:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:35:31.0348577Z INFO 02-18 17:35:31 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:35:31.0358005Z INFO 02-18 17:35:31 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:35:31.0367662Z INFO 02-18 17:35:31 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:35:31.0430216Z INFO 02-18 17:35:31 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:35:40.7189146Z INFO 02-18 17:35:40 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:35:40.7199282Z INFO 02-18 17:35:40 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:35:40.7211392Z INFO 02-18 17:35:40 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:35:40.7256385Z INFO 02-18 17:35:40 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:35:45.8385130Z INFO 02-18 17:35:45 [utils.py:218] Started 4 API server processes
2026-02-18T17:35:46.3200797Z [0;36m(EngineCore_DP1 pid=180)[0;0m 2026-02-18 17:35:46,317 - 180 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:35:46.3228222Z [0;36m(EngineCore_DP0 pid=161)[0;0m 2026-02-18 17:35:46,318 - 161 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:35:46.3251654Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-18 17:35:46 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:35:46.3261278Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-18 17:35:46 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:35:46.3288351Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-18 17:35:46 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', speculative_config=SpeculativeConfig(method='mtp', model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', num_spec_tokens=2), tokenizer='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', skip_tokenizer_init=False, tokenizer_mode=deepseek_v32, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=4, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=bfloat16, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=1024, served_model_name=/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [24, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 48, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
2026-02-18T17:35:50.9330288Z INFO 02-18 17:35:50 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:35:50.9355345Z INFO 02-18 17:35:50 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:35:50.9365431Z INFO 02-18 17:35:50 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:35:50.9425338Z INFO 02-18 17:35:50 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:35:50.9880589Z INFO 02-18 17:35:50 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:35:50.9889867Z INFO 02-18 17:35:50 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:35:50.9899782Z INFO 02-18 17:35:50 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:35:50.9960122Z INFO 02-18 17:35:50 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:35:51.1873828Z INFO 02-18 17:35:51 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:35:51.1880997Z INFO 02-18 17:35:51 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:35:51.1889948Z INFO 02-18 17:35:51 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:35:51.1974162Z INFO 02-18 17:35:51 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:35:51.2050314Z INFO 02-18 17:35:51 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:35:51.2057382Z INFO 02-18 17:35:51 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:35:51.2066313Z INFO 02-18 17:35:51 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:35:51.2120976Z INFO 02-18 17:35:51 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:35:51.2418878Z INFO 02-18 17:35:51 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:35:51.2427032Z INFO 02-18 17:35:51 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:35:51.2436202Z INFO 02-18 17:35:51 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:35:51.2512440Z INFO 02-18 17:35:51 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:35:51.3831088Z INFO 02-18 17:35:51 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:35:51.3839406Z INFO 02-18 17:35:51 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:35:51.3849684Z INFO 02-18 17:35:51 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:35:51.3903950Z INFO 02-18 17:35:51 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:35:56.4158221Z 2026-02-18 17:35:56,413 - 208 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:35:56.4215034Z INFO 02-18 17:35:56 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:35:56.5223438Z 2026-02-18 17:35:56,520 - 209 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:35:56.5283840Z INFO 02-18 17:35:56 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:35:56.9093504Z [0;36m(ApiServer_3 pid=194)[0;0m 2026-02-18 17:35:56,907 - 194 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:35:56.9240417Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-18 17:35:56 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:35:56.9413745Z [0;36m(ApiServer_1 pid=192)[0;0m 2026-02-18 17:35:56,939 - 192 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:35:56.9497078Z [0;36m(ApiServer_3 pid=194)[0;0m 2026-02-18 17:35:56,947 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-18T17:35:56.9550417Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-18 17:35:56 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-18T17:35:56.9595358Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-18 17:35:56 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:35:56.9707665Z [0;36m(ApiServer_1 pid=192)[0;0m 2026-02-18 17:35:56,968 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-18T17:35:56.9764233Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-18 17:35:56 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-18T17:35:57.0537384Z [0;36m(ApiServer_3 pid=194)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:57.0558514Z [0;36m(ApiServer_3 pid=194)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:57.0602177Z [0;36m(ApiServer_3 pid=194)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:57.0617558Z [0;36m(ApiServer_3 pid=194)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-18T17:35:57.0681186Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-18 17:35:57 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-18T17:35:57.0693473Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-18 17:35:57 [model.py:1561] Using max model len 8192
2026-02-18T17:35:57.0866762Z [0;36m(ApiServer_1 pid=192)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:57.0887164Z [0;36m(ApiServer_1 pid=192)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:57.0907805Z [0;36m(ApiServer_1 pid=192)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:57.0919147Z [0;36m(ApiServer_1 pid=192)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-18T17:35:57.0965559Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-18 17:35:57 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-18T17:35:57.0988728Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-18 17:35:57 [model.py:1561] Using max model len 8192
2026-02-18T17:35:57.1750392Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-18 17:35:57 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-18T17:35:57.1775087Z [0;36m(ApiServer_3 pid=194)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:57.1775800Z [0;36m(ApiServer_3 pid=194)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:57.1786261Z [0;36m(ApiServer_3 pid=194)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-18T17:35:57.1846568Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-18 17:35:57 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-18T17:35:57.1869580Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-18 17:35:57 [model.py:1561] Using max model len 163840
2026-02-18T17:35:57.1879525Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-18 17:35:57 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-18T17:35:57.1890019Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-18 17:35:57 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-18T17:35:57.2125726Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-18 17:35:57 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-18T17:35:57.2148193Z [0;36m(ApiServer_1 pid=192)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:57.2157232Z [0;36m(ApiServer_1 pid=192)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:57.2167189Z [0;36m(ApiServer_1 pid=192)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-18T17:35:57.2208540Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-18 17:35:57 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-18T17:35:57.2228822Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-18 17:35:57 [model.py:1561] Using max model len 163840
2026-02-18T17:35:57.2238565Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-18 17:35:57 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-18T17:35:57.2248257Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-18 17:35:57 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-18T17:35:57.3028628Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-18 17:35:57 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-18T17:35:57.3038276Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-18 17:35:57 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-18T17:35:57.3050292Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-18 17:35:57 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-18T17:35:57.3061537Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-18 17:35:57 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-18T17:35:57.3070707Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-18 17:35:57 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:35:57.3080809Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-18 17:35:57 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:35:57.3092507Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-18 17:35:57 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:35:57.3102269Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-18 17:35:57 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-18T17:35:57.3112770Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-18 17:35:57 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-18T17:35:57.3122081Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-18 17:35:57 [platform.py:335] [91m
2026-02-18T17:35:57.3132190Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             **********************************************************************************
2026-02-18T17:35:57.3143511Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-18T17:35:57.3155087Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-18T17:35:57.3168289Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-18T17:35:57.3178005Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-18T17:35:57.3188066Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-18T17:35:57.3197552Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * batch size for graph capture.
2026-02-18T17:35:57.3208957Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * For more details, please refer to:
2026-02-18T17:35:57.3218692Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-18T17:35:57.3228642Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             **********************************************************************************[0m
2026-02-18T17:35:57.3237591Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             
2026-02-18T17:35:57.3248415Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-18 17:35:57 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-18T17:35:57.3412114Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-18 17:35:57 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-18T17:35:57.3421298Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-18 17:35:57 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-18T17:35:57.3433876Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-18 17:35:57 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-18T17:35:57.3444789Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-18 17:35:57 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-18T17:35:57.3454325Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-18 17:35:57 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:35:57.3463722Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-18 17:35:57 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:35:57.3474730Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-18 17:35:57 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:35:57.3485363Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-18 17:35:57 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-18T17:35:57.3495626Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-18 17:35:57 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-18T17:35:57.3504923Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-18 17:35:57 [platform.py:335] [91m
2026-02-18T17:35:57.3515480Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             **********************************************************************************
2026-02-18T17:35:57.3524862Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-18T17:35:57.3535182Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-18T17:35:57.3545285Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-18T17:35:57.3554745Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-18T17:35:57.3565248Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-18T17:35:57.3575028Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * batch size for graph capture.
2026-02-18T17:35:57.3584337Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * For more details, please refer to:
2026-02-18T17:35:57.3593789Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-18T17:35:57.3604305Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             **********************************************************************************[0m
2026-02-18T17:35:57.3614592Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             
2026-02-18T17:35:57.3627680Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-18 17:35:57 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-18T17:35:57.4310071Z [0;36m(ApiServer_2 pid=193)[0;0m 2026-02-18 17:35:57,429 - 193 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:35:57.4467547Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-18 17:35:57 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:35:57.4497585Z [0;36m(ApiServer_0 pid=191)[0;0m 2026-02-18 17:35:57,448 - 191 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:35:57.4617897Z [0;36m(ApiServer_2 pid=193)[0;0m 2026-02-18 17:35:57,459 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-18T17:35:57.4651387Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-18 17:35:57 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:35:57.4682766Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-18 17:35:57 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-18T17:35:57.4797956Z [0;36m(ApiServer_0 pid=191)[0;0m 2026-02-18 17:35:57,477 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-18T17:35:57.4848223Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-18 17:35:57 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-18T17:35:57.5660990Z [0;36m(ApiServer_2 pid=193)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:57.5680553Z [0;36m(ApiServer_2 pid=193)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:57.5689478Z [0;36m(ApiServer_2 pid=193)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:57.5699495Z [0;36m(ApiServer_2 pid=193)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-18T17:35:57.5752761Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-18 17:35:57 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-18T17:35:57.5770251Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-18 17:35:57 [model.py:1561] Using max model len 8192
2026-02-18T17:35:57.5850524Z [0;36m(ApiServer_0 pid=191)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:57.5869032Z [0;36m(ApiServer_0 pid=191)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:57.5887932Z [0;36m(ApiServer_0 pid=191)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:57.5898765Z [0;36m(ApiServer_0 pid=191)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-18T17:35:57.5943029Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-18 17:35:57 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-18T17:35:57.5991579Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-18 17:35:57 [model.py:1561] Using max model len 8192
2026-02-18T17:35:57.6907911Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-18 17:35:57 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-18T17:35:57.6930469Z [0;36m(ApiServer_2 pid=193)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:57.6939317Z [0;36m(ApiServer_2 pid=193)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:57.6948846Z [0;36m(ApiServer_2 pid=193)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-18T17:35:57.6988535Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-18 17:35:57 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-18T17:35:57.7011027Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-18 17:35:57 [model.py:1561] Using max model len 163840
2026-02-18T17:35:57.7021206Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-18 17:35:57 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-18T17:35:57.7031066Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-18 17:35:57 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-18T17:35:57.7060141Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-18 17:35:57 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-18T17:35:57.7081268Z [0;36m(ApiServer_0 pid=191)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:57.7090100Z [0;36m(ApiServer_0 pid=191)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-18T17:35:57.7099858Z [0;36m(ApiServer_0 pid=191)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-18T17:35:57.7141455Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-18 17:35:57 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-18T17:35:57.7163061Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-18 17:35:57 [model.py:1561] Using max model len 163840
2026-02-18T17:35:57.7172598Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-18 17:35:57 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-18T17:35:57.7182155Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-18 17:35:57 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-18T17:35:57.8221695Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-18 17:35:57 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-18T17:35:57.8231166Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-18 17:35:57 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-18T17:35:57.8243535Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-18 17:35:57 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-18T17:35:57.8253699Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-18 17:35:57 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-18T17:35:57.8263324Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-18 17:35:57 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:35:57.8272920Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-18 17:35:57 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:35:57.8284114Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-18 17:35:57 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:35:57.8293992Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-18 17:35:57 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-18T17:35:57.8303907Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-18 17:35:57 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-18T17:35:57.8312893Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-18 17:35:57 [platform.py:335] [91m
2026-02-18T17:35:57.8323213Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             **********************************************************************************
2026-02-18T17:35:57.8333260Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-18T17:35:57.8342538Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-18T17:35:57.8352380Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-18T17:35:57.8363055Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-18T17:35:57.8372866Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-18T17:35:57.8383347Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * batch size for graph capture.
2026-02-18T17:35:57.8393143Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * For more details, please refer to:
2026-02-18T17:35:57.8403133Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-18T17:35:57.8413073Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             **********************************************************************************[0m
2026-02-18T17:35:57.8422613Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             
2026-02-18T17:35:57.8432487Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-18 17:35:57 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-18T17:35:57.8442301Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-18 17:35:57 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-18T17:35:57.8452173Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-18 17:35:57 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-18T17:35:57.8462421Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-18 17:35:57 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-18T17:35:57.8472957Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-18 17:35:57 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-18T17:35:57.8483059Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-18 17:35:57 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:35:57.8492677Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-18 17:35:57 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:35:57.8505062Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-18 17:35:57 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:35:57.8514176Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-18 17:35:57 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-18T17:35:57.8523946Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-18 17:35:57 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-18T17:35:57.8534144Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-18 17:35:57 [platform.py:335] [91m
2026-02-18T17:35:57.8544321Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             **********************************************************************************
2026-02-18T17:35:57.8553623Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-18T17:35:57.8564229Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-18T17:35:57.8576390Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-18T17:35:57.8588360Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-18T17:35:57.8598045Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-18T17:35:57.8608355Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * batch size for graph capture.
2026-02-18T17:35:57.8617524Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * For more details, please refer to:
2026-02-18T17:35:57.8627466Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-18T17:35:57.8636970Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             **********************************************************************************[0m
2026-02-18T17:35:57.8647141Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-18 17:35:57 [platform.py:335]             
2026-02-18T17:35:57.8656350Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-18 17:35:57 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-18T17:35:58.4448954Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-18T17:35:58.4456244Z   warnings.warn(
2026-02-18T17:35:58.4473070Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-18T17:35:58.4481804Z   warnings.warn(
2026-02-18T17:36:01.2885525Z INFO 02-18 17:36:01 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:36:01.2894282Z INFO 02-18 17:36:01 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:36:01.2905167Z INFO 02-18 17:36:01 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:36:01.2925889Z INFO 02-18 17:36:01 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:36:01.2935344Z INFO 02-18 17:36:01 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:36:01.2947622Z INFO 02-18 17:36:01 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:36:01.2980470Z INFO 02-18 17:36:01 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:36:01.3002868Z INFO 02-18 17:36:01 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:36:02.1788753Z INFO 02-18 17:36:02 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:36:02.1797018Z INFO 02-18 17:36:02 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:36:02.1807991Z INFO 02-18 17:36:02 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:36:02.1867932Z INFO 02-18 17:36:02 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:36:02.1876783Z INFO 02-18 17:36:02 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:36:02.1893007Z INFO 02-18 17:36:02 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:36:02.2308868Z INFO 02-18 17:36:02 [parallel_state.py:1212] world_size=32 rank=8 local_rank=0 distributed_init_method=tcp://10.0.0.179:44307 backend=hccl
2026-02-18T17:36:02.2336101Z INFO 02-18 17:36:02 [parallel_state.py:1212] world_size=32 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.179:44307 backend=hccl
2026-02-18T17:36:06.3701532Z 2026-02-18 17:36:06,367 - 258 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:36:06.3760687Z INFO 02-18 17:36:06 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:36:06.4278110Z 2026-02-18 17:36:06,425 - 259 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:36:06.4338709Z INFO 02-18 17:36:06 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:36:07.6941864Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-18T17:36:07.6948452Z   warnings.warn(
2026-02-18T17:36:07.8765543Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-18T17:36:07.8775465Z   warnings.warn(
2026-02-18T17:36:09.7957902Z INFO 02-18 17:36:09 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:36:09.7965832Z INFO 02-18 17:36:09 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:36:09.7975949Z INFO 02-18 17:36:09 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:36:10.1249049Z INFO 02-18 17:36:10 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:36:10.1256640Z INFO 02-18 17:36:10 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:36:10.1267247Z INFO 02-18 17:36:10 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:36:10.2242334Z INFO 02-18 17:36:10 [parallel_state.py:1212] world_size=32 rank=9 local_rank=1 distributed_init_method=tcp://10.0.0.179:44307 backend=hccl
2026-02-18T17:36:10.5488677Z INFO 02-18 17:36:10 [parallel_state.py:1212] world_size=32 rank=1 local_rank=1 distributed_init_method=tcp://10.0.0.179:44307 backend=hccl
2026-02-18T17:36:10.9516857Z INFO 02-18 17:36:10 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:36:10.9526386Z INFO 02-18 17:36:10 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:36:10.9536210Z INFO 02-18 17:36:10 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:36:10.9589470Z INFO 02-18 17:36:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:36:11.1218508Z INFO 02-18 17:36:11 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:36:11.1227055Z INFO 02-18 17:36:11 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:36:11.1238059Z INFO 02-18 17:36:11 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:36:11.1307033Z INFO 02-18 17:36:11 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:36:15.9895326Z 2026-02-18 17:36:15,987 - 376 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:36:15.9952796Z INFO 02-18 17:36:15 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:36:16.4688201Z 2026-02-18 17:36:16,466 - 378 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:36:16.4795336Z INFO 02-18 17:36:16 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:36:17.3300424Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-18T17:36:17.3306781Z   warnings.warn(
2026-02-18T17:36:17.8105274Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-18T17:36:17.8111683Z   warnings.warn(
2026-02-18T17:36:19.4639427Z INFO 02-18 17:36:19 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:36:19.4648560Z INFO 02-18 17:36:19 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:36:19.4659888Z INFO 02-18 17:36:19 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:36:19.9082471Z INFO 02-18 17:36:19 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:36:19.9090292Z INFO 02-18 17:36:19 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:36:19.9100136Z INFO 02-18 17:36:19 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:36:19.9131590Z INFO 02-18 17:36:19 [parallel_state.py:1212] world_size=32 rank=10 local_rank=2 distributed_init_method=tcp://10.0.0.179:44307 backend=hccl
2026-02-18T17:36:20.3330616Z INFO 02-18 17:36:20 [parallel_state.py:1212] world_size=32 rank=2 local_rank=2 distributed_init_method=tcp://10.0.0.179:44307 backend=hccl
2026-02-18T17:36:20.5299736Z INFO 02-18 17:36:20 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:36:20.5308138Z INFO 02-18 17:36:20 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:36:20.5316544Z INFO 02-18 17:36:20 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:36:20.5374144Z INFO 02-18 17:36:20 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:36:20.9053747Z INFO 02-18 17:36:20 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:36:20.9061618Z INFO 02-18 17:36:20 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:36:20.9070424Z INFO 02-18 17:36:20 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:36:20.9127420Z INFO 02-18 17:36:20 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:36:25.4968839Z 2026-02-18 17:36:25,494 - 480 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:36:25.5024651Z INFO 02-18 17:36:25 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:36:26.0811871Z 2026-02-18 17:36:26,078 - 483 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:36:26.0882092Z INFO 02-18 17:36:26 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:36:26.7952952Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-18T17:36:26.7959145Z   warnings.warn(
2026-02-18T17:36:27.3857103Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-18T17:36:27.3864810Z   warnings.warn(
2026-02-18T17:36:28.8589727Z INFO 02-18 17:36:28 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:36:28.8597209Z INFO 02-18 17:36:28 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:36:28.8608420Z INFO 02-18 17:36:28 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:36:29.2825921Z INFO 02-18 17:36:29 [parallel_state.py:1212] world_size=32 rank=11 local_rank=3 distributed_init_method=tcp://10.0.0.179:44307 backend=hccl
2026-02-18T17:36:29.4333527Z INFO 02-18 17:36:29 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:36:29.4342338Z INFO 02-18 17:36:29 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:36:29.4352399Z INFO 02-18 17:36:29 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:36:29.8209411Z INFO 02-18 17:36:29 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:36:29.8216976Z INFO 02-18 17:36:29 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:36:29.8225335Z INFO 02-18 17:36:29 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:36:29.8282933Z INFO 02-18 17:36:29 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:36:29.8839204Z INFO 02-18 17:36:29 [parallel_state.py:1212] world_size=32 rank=3 local_rank=3 distributed_init_method=tcp://10.0.0.179:44307 backend=hccl
2026-02-18T17:36:30.5147605Z INFO 02-18 17:36:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:36:30.5156155Z INFO 02-18 17:36:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:36:30.5168647Z INFO 02-18 17:36:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:36:30.5224678Z INFO 02-18 17:36:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:36:34.7682606Z 2026-02-18 17:36:34,764 - 584 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:36:34.7726987Z INFO 02-18 17:36:34 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:36:35.6250066Z 2026-02-18 17:36:35,622 - 587 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:36:35.6308401Z INFO 02-18 17:36:35 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:36:36.0775499Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-18T17:36:36.0789450Z   warnings.warn(
2026-02-18T17:36:36.9298472Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-18T17:36:36.9305071Z   warnings.warn(
2026-02-18T17:36:38.1485194Z INFO 02-18 17:36:38 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:36:38.1493332Z INFO 02-18 17:36:38 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:36:38.1504121Z INFO 02-18 17:36:38 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:36:38.5815679Z INFO 02-18 17:36:38 [parallel_state.py:1212] world_size=32 rank=12 local_rank=4 distributed_init_method=tcp://10.0.0.179:44307 backend=hccl
2026-02-18T17:36:39.0193827Z INFO 02-18 17:36:39 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:36:39.0202590Z INFO 02-18 17:36:39 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:36:39.0212686Z INFO 02-18 17:36:39 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:36:39.3559893Z INFO 02-18 17:36:39 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:36:39.3572929Z INFO 02-18 17:36:39 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:36:39.3583975Z INFO 02-18 17:36:39 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:36:39.3634534Z INFO 02-18 17:36:39 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:36:39.4477423Z INFO 02-18 17:36:39 [parallel_state.py:1212] world_size=32 rank=4 local_rank=4 distributed_init_method=tcp://10.0.0.179:44307 backend=hccl
2026-02-18T17:36:40.1108043Z INFO 02-18 17:36:40 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:36:40.1116666Z INFO 02-18 17:36:40 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:36:40.1126289Z INFO 02-18 17:36:40 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:36:40.1183437Z INFO 02-18 17:36:40 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:36:44.5846214Z 2026-02-18 17:36:44,582 - 688 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:36:44.5903409Z INFO 02-18 17:36:44 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:36:45.2799987Z 2026-02-18 17:36:45,277 - 691 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:36:45.2856506Z INFO 02-18 17:36:45 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:36:45.9082583Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-18T17:36:45.9090663Z   warnings.warn(
2026-02-18T17:36:47.0225821Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-18T17:36:47.0231734Z   warnings.warn(
2026-02-18T17:36:48.0337831Z INFO 02-18 17:36:48 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:36:48.0345775Z INFO 02-18 17:36:48 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:36:48.0356437Z INFO 02-18 17:36:48 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:36:48.4760812Z INFO 02-18 17:36:48 [parallel_state.py:1212] world_size=32 rank=13 local_rank=5 distributed_init_method=tcp://10.0.0.179:44307 backend=hccl
2026-02-18T17:36:49.0312744Z INFO 02-18 17:36:49 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:36:49.0320326Z INFO 02-18 17:36:49 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:36:49.0329777Z INFO 02-18 17:36:49 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:36:49.0384217Z INFO 02-18 17:36:49 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:36:49.2105011Z INFO 02-18 17:36:49 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:36:49.2113528Z INFO 02-18 17:36:49 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:36:49.2122500Z INFO 02-18 17:36:49 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:36:49.6311648Z INFO 02-18 17:36:49 [parallel_state.py:1212] world_size=32 rank=5 local_rank=5 distributed_init_method=tcp://10.0.0.179:44307 backend=hccl
2026-02-18T17:36:49.8019227Z INFO 02-18 17:36:49 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:36:49.8026701Z INFO 02-18 17:36:49 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:36:49.8036826Z INFO 02-18 17:36:49 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:36:49.8094290Z INFO 02-18 17:36:49 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:36:53.9680709Z 2026-02-18 17:36:53,965 - 792 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:36:53.9737334Z INFO 02-18 17:36:53 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:36:55.2088776Z 2026-02-18 17:36:55,206 - 795 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:36:55.2146829Z INFO 02-18 17:36:55 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:36:55.2688018Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-18T17:36:55.2697284Z   warnings.warn(
2026-02-18T17:36:56.5385134Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-18T17:36:56.5392565Z   warnings.warn(
2026-02-18T17:36:57.3339897Z INFO 02-18 17:36:57 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:36:57.3351411Z INFO 02-18 17:36:57 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:36:57.3364307Z INFO 02-18 17:36:57 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:36:57.7548378Z INFO 02-18 17:36:57 [parallel_state.py:1212] world_size=32 rank=14 local_rank=6 distributed_init_method=tcp://10.0.0.179:44307 backend=hccl
2026-02-18T17:36:58.6106938Z INFO 02-18 17:36:58 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:36:58.6117823Z INFO 02-18 17:36:58 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:36:58.6126412Z INFO 02-18 17:36:58 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:36:58.6180505Z INFO 02-18 17:36:58 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:36:58.6629463Z INFO 02-18 17:36:58 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:36:58.6638444Z INFO 02-18 17:36:58 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:36:58.6649808Z INFO 02-18 17:36:58 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:36:59.0974174Z INFO 02-18 17:36:59 [parallel_state.py:1212] world_size=32 rank=6 local_rank=6 distributed_init_method=tcp://10.0.0.179:44307 backend=hccl
2026-02-18T17:36:59.6423017Z INFO 02-18 17:36:59 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:36:59.6432599Z INFO 02-18 17:36:59 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:36:59.6441857Z INFO 02-18 17:36:59 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:36:59.6493925Z INFO 02-18 17:36:59 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:37:03.5681729Z 2026-02-18 17:37:03,565 - 896 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:37:03.5731568Z INFO 02-18 17:37:03 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:37:04.5578227Z 2026-02-18 17:37:04,555 - 900 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-18T17:37:04.5648836Z INFO 02-18 17:37:04 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-18T17:37:04.8515466Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-18T17:37:04.8520467Z   warnings.warn(
2026-02-18T17:37:05.8356195Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-18T17:37:05.8364161Z   warnings.warn(
2026-02-18T17:37:06.9427710Z INFO 02-18 17:37:06 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:37:06.9436923Z INFO 02-18 17:37:06 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:37:06.9448009Z INFO 02-18 17:37:06 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:37:07.3645854Z INFO 02-18 17:37:07 [parallel_state.py:1212] world_size=32 rank=15 local_rank=7 distributed_init_method=tcp://10.0.0.179:44307 backend=hccl
2026-02-18T17:37:07.8640760Z INFO 02-18 17:37:07 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:37:07.8649831Z INFO 02-18 17:37:07 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:37:07.8660670Z INFO 02-18 17:37:07 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:37:08.2898081Z INFO 02-18 17:37:08 [parallel_state.py:1212] world_size=32 rank=7 local_rank=7 distributed_init_method=tcp://10.0.0.179:44307 backend=hccl
2026-02-18T17:37:08.3344200Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.3362355Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.3372092Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.3381043Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.3391396Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.3803057Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.3843313Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.3863580Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.3873211Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.3900018Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.3909990Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.3968424Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.4090134Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.4101236Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.4115635Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.4125105Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.4820505Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-18T17:37:08.4842807Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-18T17:37:08.4854226Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-18T17:37:08.4863420Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-18T17:37:08.4873592Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-18T17:37:08.4883617Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-18T17:37:08.4894386Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-18T17:37:08.4904562Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-18T17:37:08.4914768Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-18T17:37:08.4924444Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-18T17:37:08.4934356Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-18T17:37:08.4944415Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-18T17:37:08.4954417Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-18T17:37:08.4964729Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-18T17:37:08.4975272Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-18T17:37:08.4986201Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-18T17:37:08.4996627Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5007111Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5017023Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5027291Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5036463Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5046956Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5056953Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5066629Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5076357Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5089246Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5097008Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5106864Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5116918Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5128171Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5137836Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5148440Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5157699Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5167904Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5178308Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5188005Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5197517Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5208274Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5218346Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5267439Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5534860Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5549091Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5560762Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5570898Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5581212Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5592099Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5602325Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5611727Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5622302Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5631021Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5640979Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5650925Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5662285Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5670420Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5681219Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5690969Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5700400Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5710102Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5719167Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5728511Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5738098Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5748043Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5759208Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5769824Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-18T17:37:08.5779563Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.5789423Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.5798891Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.5809456Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.5818563Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.5829398Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.5839931Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.5850054Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.5859864Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.5869585Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.5880820Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.5890611Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.5904733Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.6017617Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.6058326Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.6064609Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.6073555Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.6091152Z INFO 02-18 17:37:08 [parallel_state.py:1423] rank 0 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
2026-02-18T17:37:08.6730835Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.6749114Z INFO 02-18 17:37:08 [parallel_state.py:1423] rank 1 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
2026-02-18T17:37:08.6813787Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.6836047Z INFO 02-18 17:37:08 [parallel_state.py:1423] rank 5 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 5, EP rank 5
2026-02-18T17:37:08.6845977Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.6855931Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.6866102Z INFO 02-18 17:37:08 [parallel_state.py:1423] rank 7 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 7, EP rank 7
2026-02-18T17:37:08.6874768Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.6885130Z INFO 02-18 17:37:08 [parallel_state.py:1423] rank 9 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 1, EP rank 9
2026-02-18T17:37:08.6895830Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.6905331Z INFO 02-18 17:37:08 [parallel_state.py:1423] rank 10 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 2, EP rank 10
2026-02-18T17:37:08.6914837Z INFO 02-18 17:37:08 [parallel_state.py:1423] rank 11 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 3, EP rank 11
2026-02-18T17:37:08.6924798Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.6937140Z INFO 02-18 17:37:08 [parallel_state.py:1423] rank 15 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 7, EP rank 15
2026-02-18T17:37:08.7329760Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7348904Z INFO 02-18 17:37:08 [parallel_state.py:1423] rank 6 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 6, EP rank 6
2026-02-18T17:37:08.7442912Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7461676Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7471685Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7480851Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7490619Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7499423Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7509392Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7518740Z INFO 02-18 17:37:08 [parallel_state.py:1423] rank 8 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 0, EP rank 8
2026-02-18T17:37:08.7529513Z INFO 02-18 17:37:08 [parallel_state.py:1423] rank 3 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
2026-02-18T17:37:08.7538810Z INFO 02-18 17:37:08 [parallel_state.py:1423] rank 4 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 4, EP rank 4
2026-02-18T17:37:08.7548274Z INFO 02-18 17:37:08 [parallel_state.py:1423] rank 13 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 5, EP rank 13
2026-02-18T17:37:08.7558153Z INFO 02-18 17:37:08 [parallel_state.py:1423] rank 2 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
2026-02-18T17:37:08.7567789Z INFO 02-18 17:37:08 [parallel_state.py:1423] rank 14 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 6, EP rank 14
2026-02-18T17:37:08.7577041Z INFO 02-18 17:37:08 [parallel_state.py:1423] rank 12 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 4, EP rank 12
2026-02-18T17:37:08.7771739Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7783320Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7794138Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7803977Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7814623Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7823629Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7833118Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7842953Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7853128Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7863244Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7872876Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7882433Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7893008Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7901856Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7911606Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7921798Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-18T17:37:08.7953410Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.7963253Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.7972734Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.7982859Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.7992840Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.8002221Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.8012189Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.8022422Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.8032924Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.8042405Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.8051260Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.8061683Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.8071221Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.8081001Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.8090746Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.8100194Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-18T17:37:08.8528595Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.8549929Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.8562980Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.8573705Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.8583226Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.8592646Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.8602676Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.8613175Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.8633718Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.8643615Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.8653912Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.8663599Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.8673141Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.8683433Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.8693738Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.8724684Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.9045356Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.9056196Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.9064867Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.9074653Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.9083928Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.9094347Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.9103831Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.9113632Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.9124286Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m INFO 02-18 17:37:08 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-18T17:37:08.9134562Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m INFO 02-18 17:37:08 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-18T17:37:08.9143999Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m INFO 02-18 17:37:08 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-18T17:37:08.9153705Z [0;36m(Worker_DP0_TP3_EP3 pid=483)[0;0m INFO 02-18 17:37:08 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-18T17:37:08.9163248Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.9173620Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.9187047Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.9197455Z [0;36m(Worker_DP0_TP6_EP6 pid=795)[0;0m INFO 02-18 17:37:08 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-18T17:37:08.9207490Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.9218168Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m INFO 02-18 17:37:08 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-18T17:37:08.9228025Z [0;36m(Worker_DP0_TP2_EP2 pid=378)[0;0m INFO 02-18 17:37:08 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-18T17:37:08.9237248Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.9247612Z [0;36m(Worker_DP0_TP1_EP1 pid=259)[0;0m INFO 02-18 17:37:08 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-18T17:37:08.9256720Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m INFO 02-18 17:37:08 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-18T17:37:08.9266368Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-18 17:37:08 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-18T17:37:08.9279351Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.9289762Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-18 17:37:08 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-18T17:37:08.9299444Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-18 17:37:08 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-18T17:37:08.9309169Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.9318482Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m INFO 02-18 17:37:08 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-18T17:37:08.9328646Z WARNING 02-18 17:37:08 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-18T17:37:08.9339193Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m INFO 02-18 17:37:08 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-18T17:37:08.9348974Z [0;36m(Worker_DP0_TP7_EP7 pid=900)[0;0m INFO 02-18 17:37:08 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-18T17:37:09.2519008Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-18 17:37:09 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-18T17:37:09.2610123Z [0;36m(Worker_DP0_TP2_EP2 pid=378)[0;0m [2026-02-18 17:37:09] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-18T17:37:09.2726443Z [0;36m(Worker_DP0_TP3_EP3 pid=483)[0;0m [2026-02-18 17:37:09] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-18T17:37:09.2956961Z [0;36m(Worker_DP0_TP6_EP6 pid=795)[0;0m [2026-02-18 17:37:09] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-18T17:37:09.3001662Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m [2026-02-18 17:37:09] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-18T17:37:09.3334253Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m [2026-02-18 17:37:09] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-18T17:37:09.3443918Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m [2026-02-18 17:37:09] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-18T17:37:09.3538067Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m [2026-02-18 17:37:09] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-18T17:37:09.3633779Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m [2026-02-18 17:37:09] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-18T17:37:09.4004320Z [0;36m(Worker_DP0_TP1_EP1 pid=259)[0;0m [2026-02-18 17:37:09] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-18T17:37:09.4468575Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m [2026-02-18 17:37:09] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-18T17:37:09.4526157Z [0;36m(Worker_DP0_TP2_EP2 pid=378)[0;0m INFO 02-18 17:37:09 [layer.py:475] [EP Rank 2/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-18T17:37:09.4535693Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-18 17:37:09 [layer.py:475] [EP Rank 9/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-18T17:37:09.4545069Z [0;36m(Worker_DP0_TP6_EP6 pid=795)[0;0m INFO 02-18 17:37:09 [layer.py:475] [EP Rank 6/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-18T17:37:09.4555552Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m INFO 02-18 17:37:09 [layer.py:475] [EP Rank 13/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-18T17:37:09.4565478Z [0;36m(Worker_DP0_TP3_EP3 pid=483)[0;0m INFO 02-18 17:37:09 [layer.py:475] [EP Rank 3/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-18T17:37:09.4576087Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m INFO 02-18 17:37:09 [layer.py:475] [EP Rank 12/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-18T17:37:09.4660698Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m INFO 02-18 17:37:09 [layer.py:475] [EP Rank 14/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-18T17:37:09.4777490Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m INFO 02-18 17:37:09 [layer.py:475] [EP Rank 11/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-18T17:37:09.5158264Z [0;36m(Worker_DP0_TP1_EP1 pid=259)[0;0m INFO 02-18 17:37:09 [layer.py:475] [EP Rank 1/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-18T17:37:09.5568295Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-18 17:37:09 [layer.py:475] [EP Rank 10/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-18T17:37:09.6013492Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m [2026-02-18 17:37:09] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-18T17:37:09.6149256Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m [2026-02-18 17:37:09] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-18T17:37:09.6555485Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m [2026-02-18 17:37:09] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-18T17:37:09.7130285Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m INFO 02-18 17:37:09 [layer.py:475] [EP Rank 15/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-18T17:37:09.7357312Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m INFO 02-18 17:37:09 [layer.py:475] [EP Rank 5/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-18T17:37:09.8219913Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-18 17:37:09 [layer.py:475] [EP Rank 8/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-18T17:37:09.8379202Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m [2026-02-18 17:37:09] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-18T17:37:09.8427333Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m [2026-02-18 17:37:09] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-18T17:37:09.9640889Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m INFO 02-18 17:37:09 [layer.py:475] [EP Rank 4/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-18T17:37:09.9998571Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-18 17:37:09 [layer.py:475] [EP Rank 0/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-18T17:37:10.1827858Z [0;36m(Worker_DP0_TP7_EP7 pid=900)[0;0m [2026-02-18 17:37:10] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-18T17:37:10.2904370Z [0;36m(Worker_DP0_TP7_EP7 pid=900)[0;0m INFO 02-18 17:37:10 [layer.py:475] [EP Rank 7/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-18T17:37:10.7447185Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-18T17:37:10.7453873Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m   return func(*args, **kwargs)
2026-02-18T17:37:10.8149712Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-18 17:37:10 [fused_moe.py:207] [EP Rank 0/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-18T17:37:10.8644489Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-18T17:37:10.8651518Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m   return func(*args, **kwargs)
2026-02-18T17:37:10.9285816Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-18 17:37:10 [fused_moe.py:207] [EP Rank 8/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-18T17:37:10.9505891Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-18T17:37:10.9513507Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m   return func(*args, **kwargs)
2026-02-18T17:37:10.9573967Z [0;36m(Worker_DP0_TP2_EP2 pid=378)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-18T17:37:10.9581714Z [0;36m(Worker_DP0_TP2_EP2 pid=378)[0;0m   return func(*args, **kwargs)
2026-02-18T17:37:10.9779162Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-18T17:37:10.9788822Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m   return func(*args, **kwargs)
2026-02-18T17:37:10.9804781Z [0;36m(Worker_DP0_TP6_EP6 pid=795)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-18T17:37:10.9814613Z [0;36m(Worker_DP0_TP6_EP6 pid=795)[0;0m   return func(*args, **kwargs)
2026-02-18T17:37:10.9909276Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-18T17:37:10.9917026Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m   return func(*args, **kwargs)
2026-02-18T17:37:11.0021318Z [0;36m(Worker_DP0_TP1_EP1 pid=259)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-18T17:37:11.0029122Z [0;36m(Worker_DP0_TP1_EP1 pid=259)[0;0m   return func(*args, **kwargs)
2026-02-18T17:37:11.0041828Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-18T17:37:11.0050858Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m   return func(*args, **kwargs)
2026-02-18T17:37:11.0062121Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-18T17:37:11.0071257Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m   return func(*args, **kwargs)
2026-02-18T17:37:11.0141708Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m INFO 02-18 17:37:11 [fused_moe.py:207] [EP Rank 11/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-18T17:37:11.0161930Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-18T17:37:11.0170855Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m   return func(*args, **kwargs)
2026-02-18T17:37:11.0249722Z [0;36m(Worker_DP0_TP2_EP2 pid=378)[0;0m INFO 02-18 17:37:11 [fused_moe.py:207] [EP Rank 2/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-18T17:37:11.0269866Z [0;36m(Worker_DP0_TP3_EP3 pid=483)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-18T17:37:11.0278583Z [0;36m(Worker_DP0_TP3_EP3 pid=483)[0;0m   return func(*args, **kwargs)
2026-02-18T17:37:11.0297864Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-18T17:37:11.0306499Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m   return func(*args, **kwargs)
2026-02-18T17:37:11.0320157Z [0;36m(Worker_DP0_TP7_EP7 pid=900)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-18T17:37:11.0329240Z [0;36m(Worker_DP0_TP7_EP7 pid=900)[0;0m   return func(*args, **kwargs)
2026-02-18T17:37:11.0373367Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m INFO 02-18 17:37:11 [fused_moe.py:207] [EP Rank 15/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-18T17:37:11.0399465Z [0;36m(Worker_DP0_TP6_EP6 pid=795)[0;0m INFO 02-18 17:37:11 [fused_moe.py:207] [EP Rank 6/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-18T17:37:11.0428573Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-18T17:37:11.0437071Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m   return func(*args, **kwargs)
2026-02-18T17:37:11.0507009Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m INFO 02-18 17:37:11 [fused_moe.py:207] [EP Rank 13/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-18T17:37:11.0601688Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-18T17:37:11.0610776Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m   return func(*args, **kwargs)
2026-02-18T17:37:11.0622121Z [0;36m(Worker_DP0_TP1_EP1 pid=259)[0;0m INFO 02-18 17:37:11 [fused_moe.py:207] [EP Rank 1/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-18T17:37:11.0642486Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-18 17:37:11 [fused_moe.py:207] [EP Rank 9/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-18T17:37:11.0669291Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m INFO 02-18 17:37:11 [fused_moe.py:207] [EP Rank 4/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-18T17:37:11.0728492Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-18 17:37:11 [fused_moe.py:207] [EP Rank 10/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-18T17:37:11.0928810Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m INFO 02-18 17:37:11 [fused_moe.py:207] [EP Rank 14/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-18T17:37:11.0947701Z [0;36m(Worker_DP0_TP3_EP3 pid=483)[0;0m INFO 02-18 17:37:11 [fused_moe.py:207] [EP Rank 3/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-18T17:37:11.0993595Z [0;36m(Worker_DP0_TP7_EP7 pid=900)[0;0m INFO 02-18 17:37:11 [fused_moe.py:207] [EP Rank 7/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-18T17:37:11.1012421Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m INFO 02-18 17:37:11 [fused_moe.py:207] [EP Rank 12/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-18T17:37:11.1193352Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m INFO 02-18 17:37:11 [fused_moe.py:207] [EP Rank 5/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-18T17:37:11.3080516Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m INFO 02-18 17:37:11 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-18T17:37:11.3307869Z [0;36m(Worker_DP0_TP2_EP2 pid=378)[0;0m INFO 02-18 17:37:11 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-18T17:37:11.3488031Z [0;36m(Worker_DP0_TP6_EP6 pid=795)[0;0m INFO 02-18 17:37:11 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-18T17:37:11.3510878Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m INFO 02-18 17:37:11 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-18T17:37:11.3520453Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m INFO 02-18 17:37:11 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-18T17:37:11.3680226Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-18 17:37:11 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-18T17:37:11.3720728Z [0;36m(Worker_DP0_TP1_EP1 pid=259)[0;0m INFO 02-18 17:37:11 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-18T17:37:11.3748062Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m INFO 02-18 17:37:11 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-18T17:37:11.3852915Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-18 17:37:11 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-18T17:37:11.4003428Z [0;36m(Worker_DP0_TP3_EP3 pid=483)[0;0m INFO 02-18 17:37:11 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-18T17:37:11.4196799Z [0;36m(Worker_DP0_TP7_EP7 pid=900)[0;0m INFO 02-18 17:37:11 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-18T17:37:11.4246231Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m INFO 02-18 17:37:11 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-18T17:37:11.4268543Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m INFO 02-18 17:37:11 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-18T17:37:11.4457264Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-18 17:37:11 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-18T17:37:11.4482391Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m INFO 02-18 17:37:11 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-18T17:37:11.4670577Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-18 17:37:11 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-18T17:37:14.6983537Z [0;36m(Worker_DP0_TP1_EP1 pid=259)[0;0m INFO 02-18 17:37:14 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-18T17:37:14.7126142Z [0;36m(Worker_DP0_TP6_EP6 pid=795)[0;0m INFO 02-18 17:37:14 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-18T17:37:14.7693931Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m INFO 02-18 17:37:14 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-18T17:37:14.7720156Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m INFO 02-18 17:37:14 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-18T17:37:14.7760943Z [0;36m(Worker_DP0_TP2_EP2 pid=378)[0;0m INFO 02-18 17:37:14 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-18T17:37:14.8140302Z [0;36m(Worker_DP0_TP7_EP7 pid=900)[0;0m INFO 02-18 17:37:14 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-18T17:37:14.8391195Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-18 17:37:14 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-18T17:37:14.8458700Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m INFO 02-18 17:37:14 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-18T17:37:14.8663360Z [0;36m(Worker_DP0_TP3_EP3 pid=483)[0;0m INFO 02-18 17:37:14 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-18T17:37:14.9002294Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-18 17:37:14 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-18T17:37:14.9063817Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-18 17:37:14 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-18T17:37:14.9106010Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m INFO 02-18 17:37:14 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-18T17:37:14.9267556Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m INFO 02-18 17:37:14 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-18T17:37:14.9621033Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m INFO 02-18 17:37:14 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-18T17:37:14.9661121Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:37:14.9661423Z Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
2026-02-18T17:37:14.9671842Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-18 17:37:14 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-18T17:37:14.9757256Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m INFO 02-18 17:37:14 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-18T17:37:16.2515439Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:37:16.2515801Z Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:01<03:28,  1.29s/it]
2026-02-18T17:37:20.2982910Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:37:20.2983277Z Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:05<07:48,  2.91s/it]
2026-02-18T17:37:21.3882997Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:37:21.3883365Z Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:06<05:32,  2.08s/it]
2026-02-18T17:37:23.0233400Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:37:23.0233779Z Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:08<05:02,  1.90s/it]
2026-02-18T17:37:25.9825998Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:37:25.9835934Z Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:11<06:00,  2.28s/it]
2026-02-18T17:37:29.6296044Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:37:29.6296414Z Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:14<07:11,  2.75s/it]
2026-02-18T17:37:32.6818329Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:37:32.6818695Z Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:17<07:24,  2.85s/it]
2026-02-18T17:37:34.9296845Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:37:34.9297204Z Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:19<06:51,  2.66s/it]
2026-02-18T17:37:36.4837654Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:37:36.4838446Z Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:21<05:56,  2.31s/it]
2026-02-18T17:37:38.3697696Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:37:38.3698080Z Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:23<05:33,  2.18s/it]
2026-02-18T17:37:40.2622149Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:37:40.2622524Z Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:25<05:18,  2.09s/it]
2026-02-18T17:37:42.5161923Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:37:42.5162401Z Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:27<05:23,  2.14s/it]
2026-02-18T17:37:44.2821262Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:37:44.2821661Z Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:29<05:04,  2.03s/it]
2026-02-18T17:37:48.5124445Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:37:48.5124840Z Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:33<06:41,  2.69s/it]
2026-02-18T17:37:52.1721407Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:37:52.1721780Z Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:37<07:21,  2.98s/it]
2026-02-18T17:37:54.3674345Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:37:54.3674749Z Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:39<06:43,  2.75s/it]
2026-02-18T17:37:56.9577396Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:37:56.9577769Z Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:41<06:34,  2.70s/it]
2026-02-18T17:37:59.9108169Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:37:59.9108537Z Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:44<06:42,  2.78s/it]
2026-02-18T17:38:01.2114083Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:01.2114475Z Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:46<05:35,  2.33s/it]
2026-02-18T17:38:02.9781440Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:02.9781814Z Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:48<05:09,  2.16s/it]
2026-02-18T17:38:06.9485466Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:06.9485863Z Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:51<06:24,  2.71s/it]
2026-02-18T17:38:08.3187518Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:08.3187895Z Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:53<05:24,  2.30s/it]
2026-02-18T17:38:09.8437950Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:09.8438295Z Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:54<04:49,  2.07s/it]
2026-02-18T17:38:11.5585108Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:11.5585487Z Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:56<04:32,  1.96s/it]
2026-02-18T17:38:16.0373140Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:16.0373586Z Loading safetensors checkpoint shards:  15% Completed | 25/163 [01:01<06:15,  2.72s/it]
2026-02-18T17:38:16.7551738Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:16.7552325Z Loading safetensors checkpoint shards:  16% Completed | 26/163 [01:01<04:50,  2.12s/it]
2026-02-18T17:38:18.1508939Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:18.1509397Z Loading safetensors checkpoint shards:  17% Completed | 27/163 [01:03<04:18,  1.90s/it]
2026-02-18T17:38:21.8042484Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:21.8042986Z Loading safetensors checkpoint shards:  17% Completed | 28/163 [01:06<05:27,  2.43s/it]
2026-02-18T17:38:23.3838102Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:23.3838653Z Loading safetensors checkpoint shards:  18% Completed | 29/163 [01:08<04:51,  2.17s/it]
2026-02-18T17:38:24.7739310Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:24.7739846Z Loading safetensors checkpoint shards:  18% Completed | 30/163 [01:09<04:17,  1.94s/it]
2026-02-18T17:38:26.1541442Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:26.1541917Z Loading safetensors checkpoint shards:  19% Completed | 31/163 [01:11<03:53,  1.77s/it]
2026-02-18T17:38:28.0109944Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:28.0110547Z Loading safetensors checkpoint shards:  20% Completed | 32/163 [01:13<03:55,  1.80s/it]
2026-02-18T17:38:30.0518902Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:30.0519356Z Loading safetensors checkpoint shards:  20% Completed | 33/163 [01:15<04:03,  1.87s/it]
2026-02-18T17:38:32.6110584Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:32.6111009Z Loading safetensors checkpoint shards:  21% Completed | 34/163 [01:17<04:27,  2.08s/it]
2026-02-18T17:38:34.4065686Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:34.4066145Z Loading safetensors checkpoint shards:  21% Completed | 35/163 [01:19<04:13,  1.98s/it]
2026-02-18T17:38:40.2090240Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:40.2090788Z Loading safetensors checkpoint shards:  22% Completed | 36/163 [01:25<06:38,  3.14s/it]
2026-02-18T17:38:41.8917103Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:41.8917657Z Loading safetensors checkpoint shards:  23% Completed | 37/163 [01:26<05:40,  2.70s/it]
2026-02-18T17:38:43.9018319Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:43.9018807Z Loading safetensors checkpoint shards:  23% Completed | 38/163 [01:28<05:11,  2.49s/it]
2026-02-18T17:38:45.9720666Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:45.9721176Z Loading safetensors checkpoint shards:  24% Completed | 39/163 [01:31<04:53,  2.37s/it]
2026-02-18T17:38:52.1763659Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:52.1764171Z Loading safetensors checkpoint shards:  25% Completed | 40/163 [01:37<07:12,  3.52s/it]
2026-02-18T17:38:53.1555980Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:53.1556522Z Loading safetensors checkpoint shards:  25% Completed | 41/163 [01:38<05:36,  2.76s/it]
2026-02-18T17:38:54.0955833Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:54.0956271Z Loading safetensors checkpoint shards:  26% Completed | 42/163 [01:39<04:27,  2.21s/it]
2026-02-18T17:38:58.7918629Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:38:58.7919278Z Loading safetensors checkpoint shards:  26% Completed | 43/163 [01:43<05:54,  2.96s/it]
2026-02-18T17:39:00.1973495Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:00.1973969Z Loading safetensors checkpoint shards:  27% Completed | 44/163 [01:45<04:56,  2.49s/it]
2026-02-18T17:39:01.6636372Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:01.6636866Z Loading safetensors checkpoint shards:  28% Completed | 45/163 [01:46<04:17,  2.18s/it]
2026-02-18T17:39:02.9680617Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:02.9681063Z Loading safetensors checkpoint shards:  28% Completed | 46/163 [01:48<03:44,  1.92s/it]
2026-02-18T17:39:12.8149163Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:12.8149658Z Loading safetensors checkpoint shards:  29% Completed | 47/163 [01:57<08:18,  4.30s/it]
2026-02-18T17:39:16.1055492Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:16.1056010Z Loading safetensors checkpoint shards:  29% Completed | 48/163 [02:01<07:39,  4.00s/it]
2026-02-18T17:39:16.3551514Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:16.3551950Z Loading safetensors checkpoint shards:  30% Completed | 49/163 [02:01<05:27,  2.87s/it]
2026-02-18T17:39:18.4094309Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:18.4094716Z Loading safetensors checkpoint shards:  31% Completed | 50/163 [02:03<04:56,  2.63s/it]
2026-02-18T17:39:21.4266134Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:21.4266551Z Loading safetensors checkpoint shards:  31% Completed | 51/163 [02:06<05:07,  2.74s/it]
2026-02-18T17:39:22.4565480Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:22.4566189Z Loading safetensors checkpoint shards:  32% Completed | 52/163 [02:07<04:07,  2.23s/it]
2026-02-18T17:39:26.8770971Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:26.8771457Z Loading safetensors checkpoint shards:  33% Completed | 53/163 [02:11<05:17,  2.89s/it]
2026-02-18T17:39:28.0673743Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:28.0674183Z Loading safetensors checkpoint shards:  33% Completed | 54/163 [02:13<04:19,  2.38s/it]
2026-02-18T17:39:29.0433235Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:29.0433711Z Loading safetensors checkpoint shards:  34% Completed | 55/163 [02:14<03:31,  1.96s/it]
2026-02-18T17:39:30.7133787Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:30.7134328Z Loading safetensors checkpoint shards:  34% Completed | 56/163 [02:15<03:20,  1.87s/it]
2026-02-18T17:39:32.4958534Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:32.4959150Z Loading safetensors checkpoint shards:  35% Completed | 57/163 [02:17<03:15,  1.84s/it]
2026-02-18T17:39:34.4418007Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:34.4418615Z Loading safetensors checkpoint shards:  36% Completed | 58/163 [02:19<03:16,  1.88s/it]
2026-02-18T17:39:40.2586524Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:40.2586965Z Loading safetensors checkpoint shards:  36% Completed | 59/163 [02:25<05:17,  3.06s/it]
2026-02-18T17:39:41.4297049Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:41.4297546Z Loading safetensors checkpoint shards:  37% Completed | 60/163 [02:26<04:16,  2.49s/it]
2026-02-18T17:39:42.5763122Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:42.5763581Z Loading safetensors checkpoint shards:  37% Completed | 61/163 [02:27<03:32,  2.09s/it]
2026-02-18T17:39:45.9421618Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:45.9422170Z Loading safetensors checkpoint shards:  38% Completed | 62/163 [02:30<04:09,  2.47s/it]
2026-02-18T17:39:46.7551736Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:46.7552327Z Loading safetensors checkpoint shards:  39% Completed | 63/163 [02:31<03:17,  1.97s/it]
2026-02-18T17:39:48.1350810Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:48.1351225Z Loading safetensors checkpoint shards:  39% Completed | 64/163 [02:33<02:57,  1.80s/it]
2026-02-18T17:39:54.1414569Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:54.1415031Z Loading safetensors checkpoint shards:  40% Completed | 65/163 [02:39<04:59,  3.06s/it]
2026-02-18T17:39:56.8225028Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:56.8225519Z Loading safetensors checkpoint shards:  40% Completed | 66/163 [02:41<04:45,  2.95s/it]
2026-02-18T17:39:58.1626417Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:39:58.1626876Z Loading safetensors checkpoint shards:  41% Completed | 67/163 [02:43<03:56,  2.46s/it]
2026-02-18T17:40:01.5050469Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:01.5051024Z Loading safetensors checkpoint shards:  42% Completed | 68/163 [02:46<04:19,  2.73s/it]
2026-02-18T17:40:02.3451431Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:02.3451865Z Loading safetensors checkpoint shards:  42% Completed | 69/163 [02:47<03:23,  2.16s/it]
2026-02-18T17:40:03.3649428Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:03.3650176Z Loading safetensors checkpoint shards:  43% Completed | 70/163 [02:48<02:49,  1.82s/it]
2026-02-18T17:40:05.9095512Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:05.9095914Z Loading safetensors checkpoint shards:  44% Completed | 71/163 [02:50<03:07,  2.04s/it]
2026-02-18T17:40:09.8369544Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:09.8370008Z Loading safetensors checkpoint shards:  44% Completed | 72/163 [02:54<03:56,  2.60s/it]
2026-02-18T17:40:11.1051621Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:11.1052199Z Loading safetensors checkpoint shards:  45% Completed | 73/163 [02:56<03:18,  2.20s/it]
2026-02-18T17:40:12.9151234Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:12.9151931Z Loading safetensors checkpoint shards:  45% Completed | 74/163 [02:57<03:05,  2.09s/it]
2026-02-18T17:40:15.0232838Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:15.0233265Z Loading safetensors checkpoint shards:  46% Completed | 75/163 [03:00<03:04,  2.09s/it]
2026-02-18T17:40:16.7488564Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:16.7489023Z Loading safetensors checkpoint shards:  47% Completed | 76/163 [03:01<02:52,  1.98s/it]
2026-02-18T17:40:20.8468333Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:20.8468816Z Loading safetensors checkpoint shards:  47% Completed | 77/163 [03:05<03:45,  2.62s/it]
2026-02-18T17:40:22.6357019Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:22.6357571Z Loading safetensors checkpoint shards:  48% Completed | 78/163 [03:07<03:21,  2.37s/it]
2026-02-18T17:40:24.1498450Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:24.1499005Z Loading safetensors checkpoint shards:  48% Completed | 79/163 [03:09<02:57,  2.11s/it]
2026-02-18T17:40:27.7788089Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:27.7788594Z Loading safetensors checkpoint shards:  49% Completed | 80/163 [03:12<03:33,  2.57s/it]
2026-02-18T17:40:31.9068830Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:31.9069290Z Loading safetensors checkpoint shards:  50% Completed | 81/163 [03:16<04:08,  3.04s/it]
2026-02-18T17:40:33.2964861Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:33.2965371Z Loading safetensors checkpoint shards:  50% Completed | 82/163 [03:18<03:25,  2.54s/it]
2026-02-18T17:40:35.7164575Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:35.7165113Z Loading safetensors checkpoint shards:  51% Completed | 83/163 [03:20<03:20,  2.51s/it]
2026-02-18T17:40:37.3508766Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:37.3509214Z Loading safetensors checkpoint shards:  52% Completed | 84/163 [03:22<02:57,  2.24s/it]
2026-02-18T17:40:39.9787429Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:39.9787921Z Loading safetensors checkpoint shards:  52% Completed | 85/163 [03:25<03:04,  2.36s/it]
2026-02-18T17:40:41.3530872Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:41.3531432Z Loading safetensors checkpoint shards:  53% Completed | 86/163 [03:26<02:38,  2.06s/it]
2026-02-18T17:40:43.0248332Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:43.0248879Z Loading safetensors checkpoint shards:  53% Completed | 87/163 [03:28<02:27,  1.95s/it]
2026-02-18T17:40:47.6061790Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:47.6062445Z Loading safetensors checkpoint shards:  54% Completed | 88/163 [03:32<03:25,  2.74s/it]
2026-02-18T17:40:49.2553420Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:49.2553927Z Loading safetensors checkpoint shards:  55% Completed | 89/163 [03:34<02:58,  2.41s/it]
2026-02-18T17:40:54.0592263Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:54.0592718Z Loading safetensors checkpoint shards:  55% Completed | 90/163 [03:39<03:48,  3.13s/it]
2026-02-18T17:40:55.8858712Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:40:55.8859153Z Loading safetensors checkpoint shards:  56% Completed | 91/163 [03:40<03:17,  2.74s/it]
2026-02-18T17:41:00.7371226Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:01.9380507Z Loading safetensors checkpoint shards:  56% Completed | 92/163 [03:45<03:59,  3.37s/it]
2026-02-18T17:41:01.9381184Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:01.9381595Z Loading safetensors checkpoint shards:  57% Completed | 93/163 [03:46<03:10,  2.72s/it]
2026-02-18T17:41:03.3905419Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:03.3905865Z Loading safetensors checkpoint shards:  58% Completed | 94/163 [03:48<02:41,  2.34s/it]
2026-02-18T17:41:05.0976401Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:05.0976860Z Loading safetensors checkpoint shards:  58% Completed | 95/163 [03:50<02:26,  2.15s/it]
2026-02-18T17:41:09.3936562Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:09.3937005Z Loading safetensors checkpoint shards:  59% Completed | 96/163 [03:54<03:07,  2.79s/it]
2026-02-18T17:41:11.0547126Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:11.0547659Z Loading safetensors checkpoint shards:  60% Completed | 97/163 [03:56<02:41,  2.45s/it]
2026-02-18T17:41:12.6115505Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:12.6116045Z Loading safetensors checkpoint shards:  60% Completed | 98/163 [03:57<02:22,  2.19s/it]
2026-02-18T17:41:14.4073971Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:14.4074547Z Loading safetensors checkpoint shards:  61% Completed | 99/163 [03:59<02:12,  2.07s/it]
2026-02-18T17:41:16.4941453Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:16.4942068Z Loading safetensors checkpoint shards:  61% Completed | 100/163 [04:01<02:10,  2.07s/it]
2026-02-18T17:41:18.1129041Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:18.1129615Z Loading safetensors checkpoint shards:  62% Completed | 101/163 [04:03<02:00,  1.94s/it]
2026-02-18T17:41:21.9612952Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:21.9613398Z Loading safetensors checkpoint shards:  63% Completed | 102/163 [04:06<02:33,  2.51s/it]
2026-02-18T17:41:25.2146358Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:25.2146801Z Loading safetensors checkpoint shards:  63% Completed | 103/163 [04:10<02:44,  2.73s/it]
2026-02-18T17:41:28.4211177Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:28.4211648Z Loading safetensors checkpoint shards:  64% Completed | 104/163 [04:13<02:49,  2.88s/it]
2026-02-18T17:41:29.2565229Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:29.2565682Z Loading safetensors checkpoint shards:  64% Completed | 105/163 [04:14<02:11,  2.26s/it]
2026-02-18T17:41:33.0160388Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:33.0160840Z Loading safetensors checkpoint shards:  65% Completed | 106/163 [04:18<02:34,  2.71s/it]
2026-02-18T17:41:34.2869901Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:34.2870374Z Loading safetensors checkpoint shards:  66% Completed | 107/163 [04:19<02:07,  2.28s/it]
2026-02-18T17:41:37.3964501Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:37.3964980Z Loading safetensors checkpoint shards:  66% Completed | 108/163 [04:22<02:19,  2.53s/it]
2026-02-18T17:41:38.6277200Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:38.6277717Z Loading safetensors checkpoint shards:  67% Completed | 109/163 [04:23<01:55,  2.14s/it]
2026-02-18T17:41:42.0771035Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:42.0771565Z Loading safetensors checkpoint shards:  67% Completed | 110/163 [04:27<02:14,  2.53s/it]
2026-02-18T17:41:45.7080105Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:45.7080623Z Loading safetensors checkpoint shards:  68% Completed | 111/163 [04:30<02:28,  2.86s/it]
2026-02-18T17:41:48.0718678Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:48.0719227Z Loading safetensors checkpoint shards:  69% Completed | 112/163 [04:33<02:18,  2.71s/it]
2026-02-18T17:41:51.2033433Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:51.2033888Z Loading safetensors checkpoint shards:  69% Completed | 113/163 [04:36<02:21,  2.84s/it]
2026-02-18T17:41:52.4605236Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:52.4605730Z Loading safetensors checkpoint shards:  70% Completed | 114/163 [04:37<01:55,  2.36s/it]
2026-02-18T17:41:53.9190368Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:53.9190841Z Loading safetensors checkpoint shards:  71% Completed | 115/163 [04:38<01:40,  2.09s/it]
2026-02-18T17:41:58.5305736Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:41:58.5306164Z Loading safetensors checkpoint shards:  71% Completed | 116/163 [04:43<02:13,  2.85s/it]
2026-02-18T17:42:00.1096952Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:00.1097988Z Loading safetensors checkpoint shards:  72% Completed | 117/163 [04:45<01:53,  2.47s/it]
2026-02-18T17:42:03.7895200Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:03.7895702Z Loading safetensors checkpoint shards:  72% Completed | 118/163 [04:48<02:07,  2.83s/it]
2026-02-18T17:42:04.8043891Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:04.8044446Z Loading safetensors checkpoint shards:  73% Completed | 119/163 [04:49<01:40,  2.29s/it]
2026-02-18T17:42:06.6196317Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:06.6196791Z Loading safetensors checkpoint shards:  74% Completed | 120/163 [04:51<01:32,  2.15s/it]
2026-02-18T17:42:08.3306631Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:08.3307170Z Loading safetensors checkpoint shards:  74% Completed | 121/163 [04:53<01:24,  2.01s/it]
2026-02-18T17:42:10.1821239Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:10.1821678Z Loading safetensors checkpoint shards:  75% Completed | 122/163 [04:55<01:20,  1.97s/it]
2026-02-18T17:42:14.0525212Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:14.0525660Z Loading safetensors checkpoint shards:  75% Completed | 123/163 [04:59<01:41,  2.54s/it]
2026-02-18T17:42:15.3943113Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:15.3943682Z Loading safetensors checkpoint shards:  76% Completed | 124/163 [05:00<01:24,  2.18s/it]
2026-02-18T17:42:18.9978346Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:18.9978800Z Loading safetensors checkpoint shards:  77% Completed | 125/163 [05:04<01:39,  2.61s/it]
2026-02-18T17:42:20.4563693Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:20.4564178Z Loading safetensors checkpoint shards:  77% Completed | 126/163 [05:05<01:23,  2.26s/it]
2026-02-18T17:42:21.9945009Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:21.9945522Z Loading safetensors checkpoint shards:  78% Completed | 127/163 [05:07<01:13,  2.04s/it]
2026-02-18T17:42:25.9541531Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:25.9541974Z Loading safetensors checkpoint shards:  79% Completed | 128/163 [05:10<01:31,  2.62s/it]
2026-02-18T17:42:29.4708740Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:29.4709203Z Loading safetensors checkpoint shards:  79% Completed | 129/163 [05:14<01:38,  2.89s/it]
2026-02-18T17:42:30.8541112Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:30.8541678Z Loading safetensors checkpoint shards:  80% Completed | 130/163 [05:15<01:20,  2.44s/it]
2026-02-18T17:42:32.4155714Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:32.4156333Z Loading safetensors checkpoint shards:  80% Completed | 131/163 [05:17<01:09,  2.17s/it]
2026-02-18T17:42:34.0985468Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:34.0986020Z Loading safetensors checkpoint shards:  81% Completed | 132/163 [05:19<01:02,  2.03s/it]
2026-02-18T17:42:37.8574443Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:37.8574991Z Loading safetensors checkpoint shards:  82% Completed | 133/163 [05:22<01:16,  2.55s/it]
2026-02-18T17:42:39.6449815Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:39.6450356Z Loading safetensors checkpoint shards:  82% Completed | 134/163 [05:24<01:07,  2.32s/it]
2026-02-18T17:42:44.3470793Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:44.3471570Z Loading safetensors checkpoint shards:  83% Completed | 135/163 [05:29<01:24,  3.03s/it]
2026-02-18T17:42:45.6093163Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:45.6093587Z Loading safetensors checkpoint shards:  83% Completed | 136/163 [05:30<01:07,  2.50s/it]
2026-02-18T17:42:47.4722657Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:47.4723113Z Loading safetensors checkpoint shards:  84% Completed | 137/163 [05:32<01:00,  2.31s/it]
2026-02-18T17:42:52.4359491Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:52.4359955Z Loading safetensors checkpoint shards:  85% Completed | 138/163 [05:37<01:17,  3.11s/it]
2026-02-18T17:42:55.0408873Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:55.0409335Z Loading safetensors checkpoint shards:  85% Completed | 139/163 [05:40<01:10,  2.96s/it]
2026-02-18T17:42:55.8284476Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:55.8284928Z Loading safetensors checkpoint shards:  86% Completed | 140/163 [05:40<00:53,  2.31s/it]
2026-02-18T17:42:57.6637469Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:57.6637906Z Loading safetensors checkpoint shards:  87% Completed | 141/163 [05:42<00:47,  2.16s/it]
2026-02-18T17:42:59.3717997Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:42:59.3718546Z Loading safetensors checkpoint shards:  87% Completed | 142/163 [05:44<00:42,  2.03s/it]
2026-02-18T17:43:03.0487323Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:03.0487928Z Loading safetensors checkpoint shards:  88% Completed | 143/163 [05:48<00:50,  2.52s/it]
2026-02-18T17:43:06.2647851Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:06.2648382Z Loading safetensors checkpoint shards:  88% Completed | 144/163 [05:51<00:51,  2.73s/it]
2026-02-18T17:43:08.8873937Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:08.8874436Z Loading safetensors checkpoint shards:  89% Completed | 145/163 [05:53<00:48,  2.70s/it]
2026-02-18T17:43:10.9065695Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:10.9066195Z Loading safetensors checkpoint shards:  90% Completed | 146/163 [05:55<00:42,  2.49s/it]
2026-02-18T17:43:12.7728161Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:12.7728585Z Loading safetensors checkpoint shards:  90% Completed | 147/163 [05:57<00:36,  2.31s/it]
2026-02-18T17:43:14.0899229Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:14.0899694Z Loading safetensors checkpoint shards:  91% Completed | 148/163 [05:59<00:30,  2.01s/it]
2026-02-18T17:43:15.3349280Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:15.3349765Z Loading safetensors checkpoint shards:  91% Completed | 149/163 [06:00<00:24,  1.78s/it]
2026-02-18T17:43:18.9598258Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:18.9598800Z Loading safetensors checkpoint shards:  93% Completed | 151/163 [06:03<00:21,  1.79s/it]
2026-02-18T17:43:20.6959547Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:20.6960186Z Loading safetensors checkpoint shards:  93% Completed | 152/163 [06:05<00:19,  1.78s/it]
2026-02-18T17:43:22.2716123Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:22.2716663Z Loading safetensors checkpoint shards:  94% Completed | 153/163 [06:07<00:17,  1.73s/it]
2026-02-18T17:43:24.8051515Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:24.8052122Z Loading safetensors checkpoint shards:  94% Completed | 154/163 [06:09<00:17,  1.95s/it]
2026-02-18T17:43:26.1633182Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:26.1633706Z Loading safetensors checkpoint shards:  95% Completed | 155/163 [06:11<00:14,  1.78s/it]
2026-02-18T17:43:27.9384537Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:27.9385108Z Loading safetensors checkpoint shards:  96% Completed | 156/163 [06:12<00:12,  1.78s/it]
2026-02-18T17:43:29.7304044Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:29.7304590Z Loading safetensors checkpoint shards:  96% Completed | 157/163 [06:14<00:10,  1.78s/it]
2026-02-18T17:43:34.3089748Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:34.3090223Z Loading safetensors checkpoint shards:  97% Completed | 158/163 [06:19<00:13,  2.60s/it]
2026-02-18T17:43:35.3662625Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:35.3663159Z Loading safetensors checkpoint shards:  98% Completed | 159/163 [06:20<00:08,  2.15s/it]
2026-02-18T17:43:37.1643381Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:37.1643867Z Loading safetensors checkpoint shards:  98% Completed | 160/163 [06:22<00:06,  2.04s/it]
2026-02-18T17:43:38.6051890Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:38.6052721Z Loading safetensors checkpoint shards:  99% Completed | 161/163 [06:23<00:03,  1.86s/it]
2026-02-18T17:43:42.0816815Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:42.0817364Z Loading safetensors checkpoint shards:  99% Completed | 162/163 [06:27<00:02,  2.34s/it]
2026-02-18T17:43:43.5493192Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:43.5493654Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [06:28<00:00,  2.08s/it]
2026-02-18T17:43:43.5516190Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:43.5516599Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [06:28<00:00,  2.38s/it]
2026-02-18T17:43:43.5526063Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:43:43.5726649Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-18 17:43:43 [default_loader.py:291] Loading weights took 388.60 seconds
2026-02-18T17:43:44.5340275Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-18 17:43:44 [default_loader.py:291] Loading weights took 389.58 seconds
2026-02-18T17:43:56.8730284Z INFO 02-18 17:43:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:43:56.8740526Z INFO 02-18 17:43:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:43:56.8756263Z INFO 02-18 17:43:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:43:56.8776188Z INFO 02-18 17:43:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:43:56.8786737Z INFO 02-18 17:43:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:43:56.8796860Z INFO 02-18 17:43:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:43:56.8823611Z INFO 02-18 17:43:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:43:56.8833469Z INFO 02-18 17:43:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:43:56.8843744Z INFO 02-18 17:43:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:43:56.8853953Z INFO 02-18 17:43:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:43:56.8864042Z INFO 02-18 17:43:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:43:56.8873669Z INFO 02-18 17:43:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:43:56.8884625Z INFO 02-18 17:43:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:43:56.8893882Z INFO 02-18 17:43:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:43:56.8904352Z INFO 02-18 17:43:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:43:56.8913722Z INFO 02-18 17:43:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:43:56.8923670Z INFO 02-18 17:43:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:43:56.8933284Z INFO 02-18 17:43:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:43:56.8943046Z INFO 02-18 17:43:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:43:56.8961441Z INFO 02-18 17:43:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:43:56.8963705Z INFO 02-18 17:43:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:43:56.8972870Z INFO 02-18 17:43:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:43:56.8982588Z INFO 02-18 17:43:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:43:56.8992487Z INFO 02-18 17:43:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:43:56.9002047Z INFO 02-18 17:43:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:43:56.9012424Z INFO 02-18 17:43:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:43:56.9021033Z INFO 02-18 17:43:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:43:56.9030859Z INFO 02-18 17:43:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:43:56.9040833Z INFO 02-18 17:43:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:43:56.9050545Z INFO 02-18 17:43:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:43:56.9060469Z INFO 02-18 17:43:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:43:56.9070554Z INFO 02-18 17:43:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:43:56.9080546Z INFO 02-18 17:43:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:43:56.9090468Z INFO 02-18 17:43:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:43:56.9099810Z INFO 02-18 17:43:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:43:56.9109973Z INFO 02-18 17:43:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:43:56.9119889Z INFO 02-18 17:43:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:43:56.9136797Z INFO 02-18 17:43:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:43:56.9138873Z INFO 02-18 17:43:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:43:56.9149148Z INFO 02-18 17:43:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:43:56.9626383Z INFO 02-18 17:43:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:43:56.9634661Z INFO 02-18 17:43:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:43:56.9644011Z INFO 02-18 17:43:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:43:56.9653879Z INFO 02-18 17:43:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:43:56.9662807Z INFO 02-18 17:43:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:43:56.9672394Z INFO 02-18 17:43:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:43:56.9681676Z INFO 02-18 17:43:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:43:56.9691417Z INFO 02-18 17:43:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:43:56.9701302Z INFO 02-18 17:43:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:43:56.9710195Z INFO 02-18 17:43:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:43:56.9726550Z INFO 02-18 17:43:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:43:56.9736207Z INFO 02-18 17:43:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:43:56.9820687Z INFO 02-18 17:43:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:43:56.9830202Z INFO 02-18 17:43:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:43:56.9839784Z INFO 02-18 17:43:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:43:56.9881670Z INFO 02-18 17:43:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:43:56.9891514Z INFO 02-18 17:43:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:43:56.9901333Z INFO 02-18 17:43:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:43:56.9910690Z INFO 02-18 17:43:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-18T17:43:56.9919772Z INFO 02-18 17:43:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-18T17:43:56.9929846Z INFO 02-18 17:43:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-18T17:43:56.9939678Z INFO 02-18 17:43:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:43:57.0019144Z INFO 02-18 17:43:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:43:57.0028325Z INFO 02-18 17:43:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-18T17:44:19.4454942Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m WARNING 02-18 17:44:19 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-18T17:44:19.4591582Z [0;36m(Worker_DP0_TP2_EP2 pid=378)[0;0m WARNING 02-18 17:44:19 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-18T17:44:19.4667673Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m WARNING 02-18 17:44:19 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-18T17:44:19.4687781Z [0;36m(Worker_DP0_TP3_EP3 pid=483)[0;0m WARNING 02-18 17:44:19 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-18T17:44:19.4717447Z [0;36m(Worker_DP0_TP1_EP1 pid=259)[0;0m WARNING 02-18 17:44:19 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-18T17:44:19.4787853Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m WARNING 02-18 17:44:19 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-18T17:44:19.4894547Z [0;36m(Worker_DP0_TP6_EP6 pid=795)[0;0m WARNING 02-18 17:44:19 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-18T17:44:19.4945880Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m WARNING 02-18 17:44:19 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-18T17:44:19.5031879Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-18 17:44:19 [model_runner_v1.py:2315] Loading drafter model...
2026-02-18T17:44:19.5128427Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m WARNING 02-18 17:44:19 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-18T17:44:19.5150632Z [0;36m(Worker_DP0_TP2_EP2 pid=378)[0;0m INFO 02-18 17:44:19 [model_runner_v1.py:2315] Loading drafter model...
2026-02-18T17:44:19.5253981Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m INFO 02-18 17:44:19 [model_runner_v1.py:2315] Loading drafter model...
2026-02-18T17:44:19.5263184Z [0;36m(Worker_DP0_TP3_EP3 pid=483)[0;0m INFO 02-18 17:44:19 [model_runner_v1.py:2315] Loading drafter model...
2026-02-18T17:44:19.5281370Z [0;36m(Worker_DP0_TP1_EP1 pid=259)[0;0m INFO 02-18 17:44:19 [model_runner_v1.py:2315] Loading drafter model...
2026-02-18T17:44:19.5306644Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m WARNING 02-18 17:44:19 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-18T17:44:19.5342160Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m WARNING 02-18 17:44:19 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-18T17:44:19.5399004Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m WARNING 02-18 17:44:19 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-18T17:44:19.5443639Z [0;36m(Worker_DP0_TP6_EP6 pid=795)[0;0m INFO 02-18 17:44:19 [model_runner_v1.py:2315] Loading drafter model...
2026-02-18T17:44:19.5466854Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-18 17:44:19 [model_runner_v1.py:2315] Loading drafter model...
2026-02-18T17:44:19.5489471Z [0;36m(Worker_DP0_TP7_EP7 pid=900)[0;0m WARNING 02-18 17:44:19 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-18T17:44:19.5499975Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-18 17:44:19 [model_runner_v1.py:2315] Loading drafter model...
2026-02-18T17:44:19.5511553Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m WARNING 02-18 17:44:19 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-18T17:44:19.5559098Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m WARNING 02-18 17:44:19 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-18T17:44:19.5721968Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m INFO 02-18 17:44:19 [model_runner_v1.py:2315] Loading drafter model...
2026-02-18T17:44:19.5838978Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m WARNING 02-18 17:44:19 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-18T17:44:19.5908107Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m INFO 02-18 17:44:19 [model_runner_v1.py:2315] Loading drafter model...
2026-02-18T17:44:19.6086337Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m INFO 02-18 17:44:19 [model_runner_v1.py:2315] Loading drafter model...
2026-02-18T17:44:19.6116722Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m INFO 02-18 17:44:19 [model_runner_v1.py:2315] Loading drafter model...
2026-02-18T17:44:19.6187688Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m INFO 02-18 17:44:19 [model_runner_v1.py:2315] Loading drafter model...
2026-02-18T17:44:19.6408233Z [0;36m(Worker_DP0_TP7_EP7 pid=900)[0;0m INFO 02-18 17:44:19 [model_runner_v1.py:2315] Loading drafter model...
2026-02-18T17:44:19.6747861Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:19.6750047Z Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
2026-02-18T17:44:19.6780350Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m INFO 02-18 17:44:19 [model_runner_v1.py:2315] Loading drafter model...
2026-02-18T17:44:19.6918748Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-18 17:44:19 [model_runner_v1.py:2315] Loading drafter model...
2026-02-18T17:44:20.8266260Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:20.8266808Z Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:01<03:06,  1.15s/it]
2026-02-18T17:44:21.9555317Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:21.9555940Z Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:02<03:03,  1.14s/it]
2026-02-18T17:44:23.3001402Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:23.3001840Z Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:03<03:17,  1.23s/it]
2026-02-18T17:44:24.6230198Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:24.6230600Z Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:04<03:21,  1.27s/it]
2026-02-18T17:44:25.9152623Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:25.9153043Z Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:06<03:21,  1.28s/it]
2026-02-18T17:44:27.1368611Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:27.1369080Z Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:07<03:17,  1.26s/it]
2026-02-18T17:44:28.4162351Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:28.4162773Z Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:08<03:17,  1.27s/it]
2026-02-18T17:44:29.4832881Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:29.4833652Z Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:09<03:06,  1.20s/it]
2026-02-18T17:44:30.6648569Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:30.6649128Z Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:10<03:04,  1.20s/it]
2026-02-18T17:44:31.8322744Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:31.8323189Z Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:12<03:01,  1.19s/it]
2026-02-18T17:44:32.9827856Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:32.9828349Z Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:13<02:58,  1.18s/it]
2026-02-18T17:44:34.2160168Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:34.2160745Z Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:14<03:00,  1.19s/it]
2026-02-18T17:44:35.4791439Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:35.4791934Z Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:15<03:02,  1.21s/it]
2026-02-18T17:44:36.7007820Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:36.7008343Z Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:17<03:01,  1.22s/it]
2026-02-18T17:44:37.7883247Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:37.7883771Z Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:18<02:54,  1.18s/it]
2026-02-18T17:44:38.8105573Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:38.8106057Z Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:19<02:46,  1.13s/it]
2026-02-18T17:44:39.8294258Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:39.8294701Z Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:20<02:40,  1.10s/it]
2026-02-18T17:44:40.8294406Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:40.8294867Z Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:21<02:34,  1.07s/it]
2026-02-18T17:44:41.9670655Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:41.9671162Z Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:22<02:36,  1.09s/it]
2026-02-18T17:44:43.0848472Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:43.0848932Z Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:23<02:36,  1.10s/it]
2026-02-18T17:44:44.2306218Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:44.2306695Z Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:24<02:37,  1.11s/it]
2026-02-18T17:44:45.3740720Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:45.3741159Z Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:25<02:38,  1.12s/it]
2026-02-18T17:44:46.4929422Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:46.4929889Z Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:26<02:36,  1.12s/it]
2026-02-18T17:44:47.6008821Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:47.6009337Z Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:27<02:35,  1.12s/it]
2026-02-18T17:44:48.6854434Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:48.6854857Z Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:29<02:32,  1.11s/it]
2026-02-18T17:44:50.2011158Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:50.2011673Z Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:30<02:48,  1.23s/it]
2026-02-18T17:44:51.5443264Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:51.5443859Z Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:31<02:51,  1.26s/it]
2026-02-18T17:44:52.6119051Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:52.6119546Z Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:32<02:42,  1.20s/it]
2026-02-18T17:44:53.7409064Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:53.7409507Z Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:34<02:38,  1.18s/it]
2026-02-18T17:44:54.8309386Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:54.8309837Z Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:35<02:33,  1.15s/it]
2026-02-18T17:44:56.0388203Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:56.0388626Z Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:36<02:34,  1.17s/it]
2026-02-18T17:44:57.1988072Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:57.1988507Z Loading safetensors checkpoint shards:  20% Completed | 32/163 [00:37<02:32,  1.17s/it]
2026-02-18T17:44:58.3675961Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:58.3676379Z Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:38<02:31,  1.17s/it]
2026-02-18T17:44:59.4572274Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:44:59.4572786Z Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:39<02:27,  1.14s/it]
2026-02-18T17:45:00.5378917Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:00.5379426Z Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:40<02:24,  1.13s/it]
2026-02-18T17:45:01.5846645Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:01.5847195Z Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:41<02:19,  1.10s/it]
2026-02-18T17:45:02.7573031Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:02.7573542Z Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:43<02:21,  1.12s/it]
2026-02-18T17:45:03.8810810Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:03.8811355Z Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:44<02:20,  1.12s/it]
2026-02-18T17:45:05.0895628Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:05.0896170Z Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:45<02:22,  1.15s/it]
2026-02-18T17:45:06.1862124Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:06.1862637Z Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:46<02:19,  1.13s/it]
2026-02-18T17:45:07.4021850Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:07.4022345Z Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:47<02:21,  1.16s/it]
2026-02-18T17:45:08.6185424Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:08.6185862Z Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:48<02:22,  1.18s/it]
2026-02-18T17:45:09.6757531Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:09.6757967Z Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:49<02:16,  1.14s/it]
2026-02-18T17:45:10.8289793Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:10.8290340Z Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:51<02:16,  1.14s/it]
2026-02-18T17:45:11.9981043Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:11.9981462Z Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:52<02:15,  1.15s/it]
2026-02-18T17:45:13.2079460Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:13.2079983Z Loading safetensors checkpoint shards:  28% Completed | 46/163 [00:53<02:16,  1.17s/it]
2026-02-18T17:45:13.9329523Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:13.9329972Z Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:54<02:00,  1.04s/it]
2026-02-18T17:45:15.1229084Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:15.1229518Z Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:55<01:34,  1.20it/s]
2026-02-18T17:45:16.2799838Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:16.2807076Z Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:56<01:43,  1.10it/s]
2026-02-18T17:45:17.4146388Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:17.4146938Z Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:57<01:48,  1.03it/s]
2026-02-18T17:45:18.6177852Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:18.6178391Z Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:58<01:54,  1.03s/it]
2026-02-18T17:45:19.6727828Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:19.6728331Z Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:59<01:54,  1.04s/it]
2026-02-18T17:45:20.8844596Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:20.8845062Z Loading safetensors checkpoint shards:  33% Completed | 54/163 [01:01<01:58,  1.09s/it]
2026-02-18T17:45:22.0951631Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:22.0952254Z Loading safetensors checkpoint shards:  34% Completed | 55/163 [01:02<02:01,  1.12s/it]
2026-02-18T17:45:23.3143967Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:23.3144731Z Loading safetensors checkpoint shards:  34% Completed | 56/163 [01:03<02:03,  1.15s/it]
2026-02-18T17:45:24.5227979Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:24.5228434Z Loading safetensors checkpoint shards:  35% Completed | 57/163 [01:04<02:03,  1.17s/it]
2026-02-18T17:45:25.7098174Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:25.7098624Z Loading safetensors checkpoint shards:  36% Completed | 58/163 [01:06<02:03,  1.17s/it]
2026-02-18T17:45:26.8959250Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:26.8959672Z Loading safetensors checkpoint shards:  36% Completed | 59/163 [01:07<02:02,  1.18s/it]
2026-02-18T17:45:28.1278172Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:28.1278656Z Loading safetensors checkpoint shards:  37% Completed | 60/163 [01:08<02:02,  1.19s/it]
2026-02-18T17:45:29.3737492Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:29.3737974Z Loading safetensors checkpoint shards:  37% Completed | 61/163 [01:09<02:03,  1.21s/it]
2026-02-18T17:45:30.5514270Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:30.5514821Z Loading safetensors checkpoint shards:  38% Completed | 62/163 [01:10<02:01,  1.20s/it]
2026-02-18T17:45:31.6678452Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:31.6678997Z Loading safetensors checkpoint shards:  39% Completed | 63/163 [01:11<01:57,  1.17s/it]
2026-02-18T17:45:32.8512529Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:32.8513011Z Loading safetensors checkpoint shards:  39% Completed | 64/163 [01:13<01:56,  1.18s/it]
2026-02-18T17:45:34.0063942Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:34.0064469Z Loading safetensors checkpoint shards:  40% Completed | 65/163 [01:14<01:54,  1.17s/it]
2026-02-18T17:45:35.1114834Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:35.1115330Z Loading safetensors checkpoint shards:  40% Completed | 66/163 [01:15<01:51,  1.15s/it]
2026-02-18T17:45:36.2073128Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:36.2073570Z Loading safetensors checkpoint shards:  41% Completed | 67/163 [01:16<01:48,  1.13s/it]
2026-02-18T17:45:37.3348920Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:37.3349362Z Loading safetensors checkpoint shards:  42% Completed | 68/163 [01:17<01:47,  1.13s/it]
2026-02-18T17:45:38.5272122Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:38.5272604Z Loading safetensors checkpoint shards:  42% Completed | 69/163 [01:18<01:48,  1.15s/it]
2026-02-18T17:45:39.7450282Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:39.7450716Z Loading safetensors checkpoint shards:  43% Completed | 70/163 [01:20<01:48,  1.17s/it]
2026-02-18T17:45:40.8743877Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:40.8744401Z Loading safetensors checkpoint shards:  44% Completed | 71/163 [01:21<01:46,  1.16s/it]
2026-02-18T17:45:41.9640991Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:41.9641533Z Loading safetensors checkpoint shards:  44% Completed | 72/163 [01:22<01:43,  1.14s/it]
2026-02-18T17:45:43.0606678Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:43.0607215Z Loading safetensors checkpoint shards:  45% Completed | 73/163 [01:23<01:41,  1.13s/it]
2026-02-18T17:45:44.2144821Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:44.2145657Z Loading safetensors checkpoint shards:  45% Completed | 74/163 [01:24<01:40,  1.13s/it]
2026-02-18T17:45:45.3729285Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:45.3729807Z Loading safetensors checkpoint shards:  46% Completed | 75/163 [01:25<01:40,  1.14s/it]
2026-02-18T17:45:46.4331379Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:46.4331881Z Loading safetensors checkpoint shards:  47% Completed | 76/163 [01:26<01:37,  1.12s/it]
2026-02-18T17:45:47.5581931Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:47.5582497Z Loading safetensors checkpoint shards:  47% Completed | 77/163 [01:27<01:36,  1.12s/it]
2026-02-18T17:45:48.7182580Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:48.7183080Z Loading safetensors checkpoint shards:  48% Completed | 78/163 [01:29<01:36,  1.13s/it]
2026-02-18T17:45:49.9167464Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:49.9167911Z Loading safetensors checkpoint shards:  48% Completed | 79/163 [01:30<01:36,  1.15s/it]
2026-02-18T17:45:51.0547854Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:51.0548290Z Loading safetensors checkpoint shards:  49% Completed | 80/163 [01:31<01:35,  1.15s/it]
2026-02-18T17:45:52.2352281Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:52.2352734Z Loading safetensors checkpoint shards:  50% Completed | 81/163 [01:32<01:34,  1.16s/it]
2026-02-18T17:45:53.4675631Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:53.4676105Z Loading safetensors checkpoint shards:  50% Completed | 82/163 [01:33<01:35,  1.18s/it]
2026-02-18T17:45:54.5697343Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:54.5697854Z Loading safetensors checkpoint shards:  51% Completed | 83/163 [01:34<01:32,  1.16s/it]
2026-02-18T17:45:55.7236762Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:55.7237205Z Loading safetensors checkpoint shards:  52% Completed | 84/163 [01:36<01:31,  1.16s/it]
2026-02-18T17:45:56.8203389Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:56.8203818Z Loading safetensors checkpoint shards:  52% Completed | 85/163 [01:37<01:28,  1.14s/it]
2026-02-18T17:45:57.9170930Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:57.9171597Z Loading safetensors checkpoint shards:  53% Completed | 86/163 [01:38<01:26,  1.13s/it]
2026-02-18T17:45:58.0733857Z [0;36m(ApiServer_1 pid=192)[0;0m Process ApiServer_1:
2026-02-18T17:45:58.0872719Z [0;36m(ApiServer_1 pid=192)[0;0m Traceback (most recent call last):
2026-02-18T17:45:58.0891436Z [0;36m(ApiServer_1 pid=192)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-18T17:45:58.0902626Z [0;36m(ApiServer_1 pid=192)[0;0m     self.run()
2026-02-18T17:45:58.0915704Z [0;36m(ApiServer_1 pid=192)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-18T17:45:58.0926475Z [0;36m(ApiServer_1 pid=192)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-18T17:45:58.0937460Z [0;36m(ApiServer_1 pid=192)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-18T17:45:58.0947339Z [0;36m(ApiServer_1 pid=192)[0;0m     uvloop.run(
2026-02-18T17:45:58.0957415Z [0;36m(ApiServer_1 pid=192)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-18T17:45:58.0968025Z [0;36m(ApiServer_1 pid=192)[0;0m     return runner.run(wrapper())
2026-02-18T17:45:58.0977619Z [0;36m(ApiServer_1 pid=192)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.0987663Z [0;36m(ApiServer_1 pid=192)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-18T17:45:58.0999711Z [0;36m(ApiServer_1 pid=192)[0;0m     return self._loop.run_until_complete(task)
2026-02-18T17:45:58.1013720Z [0;36m(ApiServer_1 pid=192)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.1023645Z [0;36m(ApiServer_1 pid=192)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-18T17:45:58.1034698Z [0;36m(ApiServer_1 pid=192)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-18T17:45:58.1044834Z [0;36m(ApiServer_1 pid=192)[0;0m     return await main
2026-02-18T17:45:58.1054887Z [0;36m(ApiServer_1 pid=192)[0;0m            ^^^^^^^^^^
2026-02-18T17:45:58.1065140Z [0;36m(ApiServer_1 pid=192)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-18T17:45:58.1074621Z [0;36m(ApiServer_1 pid=192)[0;0m     async with build_async_engine_client(
2026-02-18T17:45:58.1084867Z [0;36m(ApiServer_1 pid=192)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-18T17:45:58.1094112Z [0;36m(ApiServer_1 pid=192)[0;0m     return await anext(self.gen)
2026-02-18T17:45:58.1104119Z [0;36m(ApiServer_1 pid=192)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.1114712Z [0;36m(ApiServer_1 pid=192)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-18T17:45:58.1124520Z [0;36m(ApiServer_1 pid=192)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-18T17:45:58.1135167Z [0;36m(ApiServer_1 pid=192)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-18T17:45:58.1144143Z [0;36m(ApiServer_1 pid=192)[0;0m     return await anext(self.gen)
2026-02-18T17:45:58.1153592Z [0;36m(ApiServer_1 pid=192)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.1164292Z [0;36m(ApiServer_1 pid=192)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-18T17:45:58.1173278Z [0;36m(ApiServer_1 pid=192)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-18T17:45:58.1182626Z [0;36m(ApiServer_1 pid=192)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.1192197Z [0;36m(ApiServer_1 pid=192)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-18T17:45:58.1202623Z [0;36m(ApiServer_1 pid=192)[0;0m     return cls(
2026-02-18T17:45:58.1212387Z [0;36m(ApiServer_1 pid=192)[0;0m            ^^^^
2026-02-18T17:45:58.1222587Z [0;36m(ApiServer_1 pid=192)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-18T17:45:58.1232077Z [0;36m(ApiServer_1 pid=192)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-18T17:45:58.1241822Z [0;36m(ApiServer_1 pid=192)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.1251932Z [0;36m(ApiServer_1 pid=192)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-18T17:45:58.1262426Z [0;36m(ApiServer_1 pid=192)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-18T17:45:58.1272068Z [0;36m(ApiServer_1 pid=192)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.1281896Z [0;36m(ApiServer_1 pid=192)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-18T17:45:58.1292359Z [0;36m(ApiServer_1 pid=192)[0;0m     super().__init__(
2026-02-18T17:45:58.1301369Z [0;36m(ApiServer_1 pid=192)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-18T17:45:58.1310138Z [0;36m(ApiServer_1 pid=192)[0;0m     super().__init__(
2026-02-18T17:45:58.1319697Z [0;36m(ApiServer_1 pid=192)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-18T17:45:58.1329407Z [0;36m(ApiServer_1 pid=192)[0;0m     super().__init__(
2026-02-18T17:45:58.1339157Z [0;36m(ApiServer_1 pid=192)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-18T17:45:58.1348450Z [0;36m(ApiServer_1 pid=192)[0;0m     raise TimeoutError(
2026-02-18T17:45:58.1358794Z [0;36m(ApiServer_1 pid=192)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-18T17:45:58.1368290Z [0;36m(ApiServer_3 pid=194)[0;0m Process ApiServer_3:
2026-02-18T17:45:58.1377300Z [0;36m(ApiServer_3 pid=194)[0;0m Traceback (most recent call last):
2026-02-18T17:45:58.1387159Z [0;36m(ApiServer_3 pid=194)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-18T17:45:58.1396499Z [0;36m(ApiServer_3 pid=194)[0;0m     self.run()
2026-02-18T17:45:58.1406943Z [0;36m(ApiServer_3 pid=194)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-18T17:45:58.1416475Z [0;36m(ApiServer_3 pid=194)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-18T17:45:58.1426112Z [0;36m(ApiServer_3 pid=194)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-18T17:45:58.1435377Z [0;36m(ApiServer_3 pid=194)[0;0m     uvloop.run(
2026-02-18T17:45:58.1445832Z [0;36m(ApiServer_3 pid=194)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-18T17:45:58.1455510Z [0;36m(ApiServer_3 pid=194)[0;0m     return runner.run(wrapper())
2026-02-18T17:45:58.1464777Z [0;36m(ApiServer_3 pid=194)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.1474619Z [0;36m(ApiServer_3 pid=194)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-18T17:45:58.1484165Z [0;36m(ApiServer_3 pid=194)[0;0m     return self._loop.run_until_complete(task)
2026-02-18T17:45:58.1494325Z [0;36m(ApiServer_3 pid=194)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.1503394Z [0;36m(ApiServer_3 pid=194)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-18T17:45:58.1513089Z [0;36m(ApiServer_3 pid=194)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-18T17:45:58.1521477Z [0;36m(ApiServer_3 pid=194)[0;0m     return await main
2026-02-18T17:45:58.1530799Z [0;36m(ApiServer_3 pid=194)[0;0m            ^^^^^^^^^^
2026-02-18T17:45:58.1540734Z [0;36m(ApiServer_3 pid=194)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-18T17:45:58.1549754Z [0;36m(ApiServer_3 pid=194)[0;0m     async with build_async_engine_client(
2026-02-18T17:45:58.1559366Z [0;36m(ApiServer_3 pid=194)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-18T17:45:58.1569724Z [0;36m(ApiServer_3 pid=194)[0;0m     return await anext(self.gen)
2026-02-18T17:45:58.1579037Z [0;36m(ApiServer_3 pid=194)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.1589036Z [0;36m(ApiServer_3 pid=194)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-18T17:45:58.1598232Z [0;36m(ApiServer_3 pid=194)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-18T17:45:58.1608574Z [0;36m(ApiServer_3 pid=194)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-18T17:45:58.1617545Z [0;36m(ApiServer_3 pid=194)[0;0m     return await anext(self.gen)
2026-02-18T17:45:58.1626870Z [0;36m(ApiServer_3 pid=194)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.1636854Z [0;36m(ApiServer_3 pid=194)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-18T17:45:58.1646486Z [0;36m(ApiServer_3 pid=194)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-18T17:45:58.1656371Z [0;36m(ApiServer_3 pid=194)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.1665680Z [0;36m(ApiServer_3 pid=194)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-18T17:45:58.1674456Z [0;36m(ApiServer_3 pid=194)[0;0m     return cls(
2026-02-18T17:45:58.1683543Z [0;36m(ApiServer_3 pid=194)[0;0m            ^^^^
2026-02-18T17:45:58.1693556Z [0;36m(ApiServer_3 pid=194)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-18T17:45:58.1702896Z [0;36m(ApiServer_3 pid=194)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-18T17:45:58.1712347Z [0;36m(ApiServer_3 pid=194)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.1722234Z [0;36m(ApiServer_3 pid=194)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-18T17:45:58.1731749Z [0;36m(ApiServer_3 pid=194)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-18T17:45:58.1741593Z [0;36m(ApiServer_3 pid=194)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.1751306Z [0;36m(ApiServer_3 pid=194)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-18T17:45:58.1761125Z [0;36m(ApiServer_3 pid=194)[0;0m     super().__init__(
2026-02-18T17:45:58.1772800Z [0;36m(ApiServer_3 pid=194)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-18T17:45:58.1784173Z [0;36m(ApiServer_3 pid=194)[0;0m     super().__init__(
2026-02-18T17:45:58.1795265Z [0;36m(ApiServer_3 pid=194)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-18T17:45:58.1805312Z [0;36m(ApiServer_3 pid=194)[0;0m     super().__init__(
2026-02-18T17:45:58.1815300Z [0;36m(ApiServer_3 pid=194)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-18T17:45:58.1825056Z [0;36m(ApiServer_3 pid=194)[0;0m     raise TimeoutError(
2026-02-18T17:45:58.1834892Z [0;36m(ApiServer_3 pid=194)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-18T17:45:58.3859971Z [0;36m(ApiServer_2 pid=193)[0;0m Process ApiServer_2:
2026-02-18T17:45:58.3874694Z [0;36m(ApiServer_2 pid=193)[0;0m Traceback (most recent call last):
2026-02-18T17:45:58.3892318Z [0;36m(ApiServer_2 pid=193)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-18T17:45:58.3904772Z [0;36m(ApiServer_2 pid=193)[0;0m     self.run()
2026-02-18T17:45:58.3914632Z [0;36m(ApiServer_2 pid=193)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-18T17:45:58.3924131Z [0;36m(ApiServer_2 pid=193)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-18T17:45:58.3933962Z [0;36m(ApiServer_2 pid=193)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-18T17:45:58.3943189Z [0;36m(ApiServer_2 pid=193)[0;0m     uvloop.run(
2026-02-18T17:45:58.3952511Z [0;36m(ApiServer_2 pid=193)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-18T17:45:58.3961801Z [0;36m(ApiServer_2 pid=193)[0;0m     return runner.run(wrapper())
2026-02-18T17:45:58.3971855Z [0;36m(ApiServer_2 pid=193)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.3981534Z [0;36m(ApiServer_2 pid=193)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-18T17:45:58.3991211Z [0;36m(ApiServer_2 pid=193)[0;0m     return self._loop.run_until_complete(task)
2026-02-18T17:45:58.4000296Z [0;36m(ApiServer_2 pid=193)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.4010377Z [0;36m(ApiServer_2 pid=193)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-18T17:45:58.4019969Z [0;36m(ApiServer_2 pid=193)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-18T17:45:58.4028606Z [0;36m(ApiServer_2 pid=193)[0;0m     return await main
2026-02-18T17:45:58.4038281Z [0;36m(ApiServer_2 pid=193)[0;0m            ^^^^^^^^^^
2026-02-18T17:45:58.4048633Z [0;36m(ApiServer_2 pid=193)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-18T17:45:58.4057490Z [0;36m(ApiServer_2 pid=193)[0;0m     async with build_async_engine_client(
2026-02-18T17:45:58.4067342Z [0;36m(ApiServer_2 pid=193)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-18T17:45:58.4077630Z [0;36m(ApiServer_2 pid=193)[0;0m     return await anext(self.gen)
2026-02-18T17:45:58.4091335Z [0;36m(ApiServer_2 pid=193)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.4101489Z [0;36m(ApiServer_2 pid=193)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-18T17:45:58.4110651Z [0;36m(ApiServer_2 pid=193)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-18T17:45:58.4120925Z [0;36m(ApiServer_2 pid=193)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-18T17:45:58.4130474Z [0;36m(ApiServer_2 pid=193)[0;0m     return await anext(self.gen)
2026-02-18T17:45:58.4139796Z [0;36m(ApiServer_2 pid=193)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.4149516Z [0;36m(ApiServer_2 pid=193)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-18T17:45:58.4159334Z [0;36m(ApiServer_2 pid=193)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-18T17:45:58.4169657Z [0;36m(ApiServer_2 pid=193)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.4179621Z [0;36m(ApiServer_2 pid=193)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-18T17:45:58.4188723Z [0;36m(ApiServer_2 pid=193)[0;0m     return cls(
2026-02-18T17:45:58.4198168Z [0;36m(ApiServer_2 pid=193)[0;0m            ^^^^
2026-02-18T17:45:58.4208524Z [0;36m(ApiServer_2 pid=193)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-18T17:45:58.4218236Z [0;36m(ApiServer_2 pid=193)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-18T17:45:58.4228057Z [0;36m(ApiServer_2 pid=193)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.4238106Z [0;36m(ApiServer_2 pid=193)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-18T17:45:58.4248503Z [0;36m(ApiServer_2 pid=193)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-18T17:45:58.4262996Z [0;36m(ApiServer_2 pid=193)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.4272726Z [0;36m(ApiServer_2 pid=193)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-18T17:45:58.4282385Z [0;36m(ApiServer_2 pid=193)[0;0m     super().__init__(
2026-02-18T17:45:58.4292475Z [0;36m(ApiServer_2 pid=193)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-18T17:45:58.4301494Z [0;36m(ApiServer_2 pid=193)[0;0m     super().__init__(
2026-02-18T17:45:58.4311321Z [0;36m(ApiServer_2 pid=193)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-18T17:45:58.4320959Z [0;36m(ApiServer_2 pid=193)[0;0m     super().__init__(
2026-02-18T17:45:58.4330951Z [0;36m(ApiServer_2 pid=193)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-18T17:45:58.4340322Z [0;36m(ApiServer_2 pid=193)[0;0m     raise TimeoutError(
2026-02-18T17:45:58.4350037Z [0;36m(ApiServer_2 pid=193)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-18T17:45:58.4360443Z [0;36m(ApiServer_0 pid=191)[0;0m Process ApiServer_0:
2026-02-18T17:45:58.4369338Z [0;36m(ApiServer_0 pid=191)[0;0m Traceback (most recent call last):
2026-02-18T17:45:58.4379079Z [0;36m(ApiServer_0 pid=191)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-18T17:45:58.4388558Z [0;36m(ApiServer_0 pid=191)[0;0m     self.run()
2026-02-18T17:45:58.4397535Z [0;36m(ApiServer_0 pid=191)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-18T17:45:58.4406876Z [0;36m(ApiServer_0 pid=191)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-18T17:45:58.4416825Z [0;36m(ApiServer_0 pid=191)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-18T17:45:58.4426136Z [0;36m(ApiServer_0 pid=191)[0;0m     uvloop.run(
2026-02-18T17:45:58.4436113Z [0;36m(ApiServer_0 pid=191)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-18T17:45:58.4445223Z [0;36m(ApiServer_0 pid=191)[0;0m     return runner.run(wrapper())
2026-02-18T17:45:58.4454909Z [0;36m(ApiServer_0 pid=191)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.4464329Z [0;36m(ApiServer_0 pid=191)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-18T17:45:58.4473720Z [0;36m(ApiServer_0 pid=191)[0;0m     return self._loop.run_until_complete(task)
2026-02-18T17:45:58.4483387Z [0;36m(ApiServer_0 pid=191)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.4493442Z [0;36m(ApiServer_0 pid=191)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-18T17:45:58.4502721Z [0;36m(ApiServer_0 pid=191)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-18T17:45:58.4512287Z [0;36m(ApiServer_0 pid=191)[0;0m     return await main
2026-02-18T17:45:58.4521777Z [0;36m(ApiServer_0 pid=191)[0;0m            ^^^^^^^^^^
2026-02-18T17:45:58.4531572Z [0;36m(ApiServer_0 pid=191)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-18T17:45:58.4540958Z [0;36m(ApiServer_0 pid=191)[0;0m     async with build_async_engine_client(
2026-02-18T17:45:58.4550695Z [0;36m(ApiServer_0 pid=191)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-18T17:45:58.4559947Z [0;36m(ApiServer_0 pid=191)[0;0m     return await anext(self.gen)
2026-02-18T17:45:58.4569628Z [0;36m(ApiServer_0 pid=191)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.4579596Z [0;36m(ApiServer_0 pid=191)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-18T17:45:58.4589133Z [0;36m(ApiServer_0 pid=191)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-18T17:45:58.4598228Z [0;36m(ApiServer_0 pid=191)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-18T17:45:58.4607725Z [0;36m(ApiServer_0 pid=191)[0;0m     return await anext(self.gen)
2026-02-18T17:45:58.4616793Z [0;36m(ApiServer_0 pid=191)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.4626556Z [0;36m(ApiServer_0 pid=191)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-18T17:45:58.4635923Z [0;36m(ApiServer_0 pid=191)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-18T17:45:58.4645276Z [0;36m(ApiServer_0 pid=191)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.4655360Z [0;36m(ApiServer_0 pid=191)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-18T17:45:58.4664639Z [0;36m(ApiServer_0 pid=191)[0;0m     return cls(
2026-02-18T17:45:58.4673877Z [0;36m(ApiServer_0 pid=191)[0;0m            ^^^^
2026-02-18T17:45:58.4684010Z [0;36m(ApiServer_0 pid=191)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-18T17:45:58.4693715Z [0;36m(ApiServer_0 pid=191)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-18T17:45:58.4703245Z [0;36m(ApiServer_0 pid=191)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.4712977Z [0;36m(ApiServer_0 pid=191)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-18T17:45:58.4722149Z [0;36m(ApiServer_0 pid=191)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-18T17:45:58.4731602Z [0;36m(ApiServer_0 pid=191)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-18T17:45:58.4741296Z [0;36m(ApiServer_0 pid=191)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-18T17:45:58.4750605Z [0;36m(ApiServer_0 pid=191)[0;0m     super().__init__(
2026-02-18T17:45:58.4760669Z [0;36m(ApiServer_0 pid=191)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-18T17:45:58.4771498Z [0;36m(ApiServer_0 pid=191)[0;0m     super().__init__(
2026-02-18T17:45:58.4782712Z [0;36m(ApiServer_0 pid=191)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-18T17:45:58.4792570Z [0;36m(ApiServer_0 pid=191)[0;0m     super().__init__(
2026-02-18T17:45:58.4803238Z [0;36m(ApiServer_0 pid=191)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-18T17:45:58.4813504Z [0;36m(ApiServer_0 pid=191)[0;0m     raise TimeoutError(
2026-02-18T17:45:58.4824356Z [0;36m(ApiServer_0 pid=191)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-18T17:45:58.4834385Z [0;36m(ApiServer_3 pid=194)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-18T17:45:58.7749877Z [0;36m(ApiServer_2 pid=193)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-18T17:45:58.7805206Z [0;36m(ApiServer_0 pid=191)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-18T17:45:59.0645082Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:45:59.0645547Z Loading safetensors checkpoint shards:  53% Completed | 87/163 [01:39<01:26,  1.13s/it]
2026-02-18T17:45:59.0742764Z [0;36m(ApiServer_1 pid=192)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-18T17:46:00.1888764Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:00.1889245Z Loading safetensors checkpoint shards:  54% Completed | 88/163 [01:40<01:24,  1.13s/it]
2026-02-18T17:46:01.3759360Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:01.3759889Z Loading safetensors checkpoint shards:  55% Completed | 89/163 [01:41<01:24,  1.15s/it]
2026-02-18T17:46:02.5528931Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:02.5529385Z Loading safetensors checkpoint shards:  55% Completed | 90/163 [01:42<01:24,  1.16s/it]
2026-02-18T17:46:03.8040809Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:03.8041256Z Loading safetensors checkpoint shards:  56% Completed | 91/163 [01:44<01:25,  1.18s/it]
2026-02-18T17:46:05.0461853Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:05.0462440Z Loading safetensors checkpoint shards:  56% Completed | 92/163 [01:45<01:25,  1.20s/it]
2026-02-18T17:46:06.3441232Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:06.3441635Z Loading safetensors checkpoint shards:  57% Completed | 93/163 [01:46<01:26,  1.23s/it]
2026-02-18T17:46:07.5680197Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:07.5680646Z Loading safetensors checkpoint shards:  58% Completed | 94/163 [01:47<01:24,  1.23s/it]
2026-02-18T17:46:08.7262876Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:08.7263305Z Loading safetensors checkpoint shards:  58% Completed | 95/163 [01:49<01:22,  1.21s/it]
2026-02-18T17:46:09.8488769Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:09.8489516Z Loading safetensors checkpoint shards:  59% Completed | 96/163 [01:50<01:19,  1.18s/it]
2026-02-18T17:46:11.0552990Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:11.0553435Z Loading safetensors checkpoint shards:  60% Completed | 97/163 [01:51<01:18,  1.19s/it]
2026-02-18T17:46:12.2444642Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:12.2445214Z Loading safetensors checkpoint shards:  60% Completed | 98/163 [01:52<01:17,  1.19s/it]
2026-02-18T17:46:13.3368092Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:13.3368658Z Loading safetensors checkpoint shards:  61% Completed | 99/163 [01:53<01:14,  1.16s/it]
2026-02-18T17:46:14.4784485Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:14.4785257Z Loading safetensors checkpoint shards:  61% Completed | 100/163 [01:54<01:12,  1.15s/it]
2026-02-18T17:46:15.7983120Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:15.7983681Z Loading safetensors checkpoint shards:  62% Completed | 101/163 [01:56<01:14,  1.20s/it]
2026-02-18T17:46:16.8279059Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:16.8279471Z Loading safetensors checkpoint shards:  63% Completed | 102/163 [01:57<01:10,  1.15s/it]
2026-02-18T17:46:17.8736186Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:17.8736756Z Loading safetensors checkpoint shards:  63% Completed | 103/163 [01:58<01:07,  1.12s/it]
2026-02-18T17:46:18.8672504Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:18.8672933Z Loading safetensors checkpoint shards:  64% Completed | 104/163 [01:59<01:03,  1.08s/it]
2026-02-18T17:46:19.9327669Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:19.9328125Z Loading safetensors checkpoint shards:  64% Completed | 105/163 [02:00<01:02,  1.08s/it]
2026-02-18T17:46:20.9324180Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:20.9324615Z Loading safetensors checkpoint shards:  65% Completed | 106/163 [02:01<01:00,  1.05s/it]
2026-02-18T17:46:23.7381763Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:23.7382366Z Loading safetensors checkpoint shards:  66% Completed | 107/163 [02:04<01:28,  1.58s/it]
2026-02-18T17:46:24.1481499Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:24.1481965Z Loading safetensors checkpoint shards:  66% Completed | 108/163 [02:04<01:07,  1.23s/it]
2026-02-18T17:46:25.2361252Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:25.2361714Z Loading safetensors checkpoint shards:  67% Completed | 109/163 [02:05<01:04,  1.19s/it]
2026-02-18T17:46:26.3639145Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:26.3639652Z Loading safetensors checkpoint shards:  67% Completed | 110/163 [02:06<01:01,  1.17s/it]
2026-02-18T17:46:27.3823802Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:27.3824228Z Loading safetensors checkpoint shards:  68% Completed | 111/163 [02:07<00:58,  1.12s/it]
2026-02-18T17:46:28.4089224Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:28.4089831Z Loading safetensors checkpoint shards:  69% Completed | 112/163 [02:08<00:55,  1.09s/it]
2026-02-18T17:46:29.5081347Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:29.5081890Z Loading safetensors checkpoint shards:  69% Completed | 113/163 [02:09<00:54,  1.10s/it]
2026-02-18T17:46:30.6137976Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:30.6138466Z Loading safetensors checkpoint shards:  70% Completed | 114/163 [02:10<00:53,  1.10s/it]
2026-02-18T17:46:31.7602519Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:31.7603100Z Loading safetensors checkpoint shards:  71% Completed | 115/163 [02:12<00:53,  1.11s/it]
2026-02-18T17:46:32.8850310Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:32.8850892Z Loading safetensors checkpoint shards:  71% Completed | 116/163 [02:13<00:52,  1.12s/it]
2026-02-18T17:46:34.0409968Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:34.0410385Z Loading safetensors checkpoint shards:  72% Completed | 117/163 [02:14<00:51,  1.13s/it]
2026-02-18T17:46:35.0851124Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:35.0851680Z Loading safetensors checkpoint shards:  72% Completed | 118/163 [02:15<00:49,  1.10s/it]
2026-02-18T17:46:36.1348145Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:36.1348587Z Loading safetensors checkpoint shards:  73% Completed | 119/163 [02:16<00:47,  1.09s/it]
2026-02-18T17:46:37.2765894Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:37.2766358Z Loading safetensors checkpoint shards:  74% Completed | 120/163 [02:17<00:47,  1.10s/it]
2026-02-18T17:46:38.4314128Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:38.4314576Z Loading safetensors checkpoint shards:  74% Completed | 121/163 [02:18<00:47,  1.12s/it]
2026-02-18T17:46:39.6257707Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:39.6258309Z Loading safetensors checkpoint shards:  75% Completed | 122/163 [02:19<00:46,  1.14s/it]
2026-02-18T17:46:40.7557604Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:40.7558131Z Loading safetensors checkpoint shards:  75% Completed | 123/163 [02:21<00:45,  1.14s/it]
2026-02-18T17:46:41.8726738Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:41.8727139Z Loading safetensors checkpoint shards:  76% Completed | 124/163 [02:22<00:44,  1.13s/it]
2026-02-18T17:46:42.9857940Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:42.9858371Z Loading safetensors checkpoint shards:  77% Completed | 125/163 [02:23<00:42,  1.13s/it]
2026-02-18T17:46:44.0754066Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:44.0754526Z Loading safetensors checkpoint shards:  77% Completed | 126/163 [02:24<00:41,  1.12s/it]
2026-02-18T17:46:45.1917111Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:45.1917570Z Loading safetensors checkpoint shards:  78% Completed | 127/163 [02:25<00:40,  1.12s/it]
2026-02-18T17:46:46.3040672Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:46.3041153Z Loading safetensors checkpoint shards:  79% Completed | 128/163 [02:26<00:39,  1.11s/it]
2026-02-18T17:46:47.4300679Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:47.4301097Z Loading safetensors checkpoint shards:  80% Completed | 130/163 [02:27<00:28,  1.16it/s]
2026-02-18T17:46:48.5885524Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:48.5885984Z Loading safetensors checkpoint shards:  80% Completed | 131/163 [02:28<00:29,  1.07it/s]
2026-02-18T17:46:49.7364973Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:49.7365480Z Loading safetensors checkpoint shards:  81% Completed | 132/163 [02:30<00:30,  1.01it/s]
2026-02-18T17:46:50.9040232Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:50.9040779Z Loading safetensors checkpoint shards:  82% Completed | 133/163 [02:31<00:31,  1.04s/it]
2026-02-18T17:46:52.1025110Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:52.1025640Z Loading safetensors checkpoint shards:  82% Completed | 134/163 [02:32<00:31,  1.08s/it]
2026-02-18T17:46:53.2265474Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:53.2266002Z Loading safetensors checkpoint shards:  83% Completed | 135/163 [02:33<00:30,  1.09s/it]
2026-02-18T17:46:54.4489339Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:54.4489861Z Loading safetensors checkpoint shards:  83% Completed | 136/163 [02:34<00:30,  1.13s/it]
2026-02-18T17:46:55.5900614Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:55.5901137Z Loading safetensors checkpoint shards:  84% Completed | 137/163 [02:35<00:29,  1.13s/it]
2026-02-18T17:46:56.6122150Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:56.6122713Z Loading safetensors checkpoint shards:  85% Completed | 138/163 [02:36<00:27,  1.10s/it]
2026-02-18T17:46:57.5747201Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:57.5747828Z Loading safetensors checkpoint shards:  85% Completed | 139/163 [02:37<00:25,  1.06s/it]
2026-02-18T17:46:58.6108733Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:58.6109174Z Loading safetensors checkpoint shards:  86% Completed | 140/163 [02:38<00:24,  1.05s/it]
2026-02-18T17:46:59.6798118Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:46:59.6798603Z Loading safetensors checkpoint shards:  87% Completed | 141/163 [02:40<00:23,  1.06s/it]
2026-02-18T17:47:00.7568331Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:00.7568760Z Loading safetensors checkpoint shards:  87% Completed | 142/163 [02:41<00:22,  1.06s/it]
2026-02-18T17:47:01.8713334Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:01.8713816Z Loading safetensors checkpoint shards:  88% Completed | 143/163 [02:42<00:21,  1.08s/it]
2026-02-18T17:47:02.9303324Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:02.9304113Z Loading safetensors checkpoint shards:  88% Completed | 144/163 [02:43<00:20,  1.07s/it]
2026-02-18T17:47:03.9952897Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:03.9953384Z Loading safetensors checkpoint shards:  89% Completed | 145/163 [02:44<00:19,  1.07s/it]
2026-02-18T17:47:04.9462898Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:04.9463548Z Loading safetensors checkpoint shards:  90% Completed | 146/163 [02:45<00:17,  1.03s/it]
2026-02-18T17:47:05.9557271Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:05.9557878Z Loading safetensors checkpoint shards:  90% Completed | 147/163 [02:46<00:16,  1.03s/it]
2026-02-18T17:47:07.0228084Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:07.0228511Z Loading safetensors checkpoint shards:  91% Completed | 148/163 [02:47<00:15,  1.04s/it]
2026-02-18T17:47:08.1310534Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:08.1311005Z Loading safetensors checkpoint shards:  91% Completed | 149/163 [02:48<00:14,  1.06s/it]
2026-02-18T17:47:12.7747455Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:12.7747974Z Loading safetensors checkpoint shards:  92% Completed | 150/163 [02:53<00:27,  2.13s/it]
2026-02-18T17:47:13.7653287Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:13.7653837Z Loading safetensors checkpoint shards:  93% Completed | 151/163 [02:54<00:21,  1.79s/it]
2026-02-18T17:47:14.8180845Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:14.8181284Z Loading safetensors checkpoint shards:  93% Completed | 152/163 [02:55<00:17,  1.57s/it]
2026-02-18T17:47:15.9177850Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:15.9178319Z Loading safetensors checkpoint shards:  94% Completed | 153/163 [02:56<00:14,  1.43s/it]
2026-02-18T17:47:16.9876181Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:16.9876642Z Loading safetensors checkpoint shards:  94% Completed | 154/163 [02:57<00:11,  1.32s/it]
2026-02-18T17:47:18.0287673Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:18.0288146Z Loading safetensors checkpoint shards:  95% Completed | 155/163 [02:58<00:09,  1.24s/it]
2026-02-18T17:47:19.1112739Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:19.1113269Z Loading safetensors checkpoint shards:  96% Completed | 156/163 [02:59<00:08,  1.19s/it]
2026-02-18T17:47:20.2866541Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:20.2867089Z Loading safetensors checkpoint shards:  96% Completed | 157/163 [03:00<00:07,  1.19s/it]
2026-02-18T17:47:21.2857995Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:21.2858530Z Loading safetensors checkpoint shards:  97% Completed | 158/163 [03:01<00:05,  1.13s/it]
2026-02-18T17:47:22.3197944Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:22.3198503Z Loading safetensors checkpoint shards:  98% Completed | 159/163 [03:02<00:04,  1.10s/it]
2026-02-18T17:47:23.4684860Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:23.4685443Z Loading safetensors checkpoint shards:  98% Completed | 160/163 [03:03<00:03,  1.12s/it]
2026-02-18T17:47:24.5743029Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:24.5743565Z Loading safetensors checkpoint shards:  99% Completed | 161/163 [03:04<00:02,  1.11s/it]
2026-02-18T17:47:25.7052874Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:25.7053410Z Loading safetensors checkpoint shards:  99% Completed | 162/163 [03:06<00:01,  1.12s/it]
2026-02-18T17:47:26.8832296Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:26.8832856Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [03:07<00:00,  1.14s/it]
2026-02-18T17:47:26.8851844Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:26.8852321Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [03:07<00:00,  1.15s/it]
2026-02-18T17:47:26.8863133Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:47:26.9539778Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-18 17:47:26 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-18T17:47:27.1113498Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-18 17:47:27 [default_loader.py:291] Loading weights took 187.52 seconds
2026-02-18T17:47:27.1471806Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-18 17:47:27 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-18T17:47:27.1481234Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m INFO 02-18 17:47:27 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-18T17:47:27.1517519Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m INFO 02-18 17:47:27 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-18T17:47:27.1543930Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m INFO 02-18 17:47:27 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-18T17:47:27.1565485Z [0;36m(Worker_DP0_TP2_EP2 pid=378)[0;0m INFO 02-18 17:47:27 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-18T17:47:27.1583136Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m INFO 02-18 17:47:27 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-18T17:47:27.1641008Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m INFO 02-18 17:47:27 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-18T17:47:27.8269324Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-18 17:47:27 [default_loader.py:291] Loading weights took 188.15 seconds
2026-02-18T17:47:27.8466126Z [0;36m(Worker_DP0_TP7_EP7 pid=900)[0;0m INFO 02-18 17:47:27 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-18T17:47:27.8644627Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-18 17:47:27 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-18T17:47:27.8666584Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m INFO 02-18 17:47:27 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-18T17:47:27.8708298Z [0;36m(Worker_DP0_TP6_EP6 pid=795)[0;0m INFO 02-18 17:47:27 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-18T17:47:27.8726101Z [0;36m(Worker_DP0_TP3_EP3 pid=483)[0;0m INFO 02-18 17:47:27 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-18T17:47:27.8758268Z [0;36m(Worker_DP0_TP1_EP1 pid=259)[0;0m INFO 02-18 17:47:27 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-18T17:47:27.8799431Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-18 17:47:27 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-18T17:47:27.8860921Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m INFO 02-18 17:47:27 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-18T17:47:28.0276654Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m INFO 02-18 17:47:28 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-18T17:47:28.1548594Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-18 17:47:28 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-18T17:47:28.1998918Z [0;36m(Worker_DP0_TP2_EP2 pid=378)[0;0m INFO 02-18 17:47:28 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-18T17:47:28.7914681Z [0;36m(Worker_DP0_TP6_EP6 pid=795)[0;0m INFO 02-18 17:47:28 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-18T17:47:28.9629976Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-18 17:47:28 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-18T17:47:28.9983642Z [0;36m(Worker_DP0_TP3_EP3 pid=483)[0;0m INFO 02-18 17:47:28 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-18T17:47:29.0397187Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m INFO 02-18 17:47:29 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-18T17:47:29.1091898Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m INFO 02-18 17:47:29 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-18T17:47:29.1395399Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m INFO 02-18 17:47:29 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-18T17:47:29.1881728Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m INFO 02-18 17:47:29 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-18T17:47:29.4464953Z [0;36m(Worker_DP0_TP7_EP7 pid=900)[0;0m INFO 02-18 17:47:29 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-18T17:47:29.8320693Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m INFO 02-18 17:47:29 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-18T17:47:30.0042198Z [0;36m(Worker_DP0_TP1_EP1 pid=259)[0;0m INFO 02-18 17:47:30 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-18T17:47:30.0095517Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-18 17:47:30 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-18T17:47:30.0463088Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m INFO 02-18 17:47:30 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-18T17:47:30.5247328Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-18 17:47:30 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-18T17:47:36.0362622Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-18 17:47:36 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/5f4e5deba5/rank_0_1/backbone for vLLM's torch.compile
2026-02-18T17:47:36.0397208Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-18 17:47:36 [backends.py:865] Dynamo bytecode transform time: 4.99 s
2026-02-18T17:47:36.0758263Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-18 17:47:36 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/5f4e5deba5/rank_0_0/backbone for vLLM's torch.compile
2026-02-18T17:47:36.0768747Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-18 17:47:36 [backends.py:865] Dynamo bytecode transform time: 5.03 s
2026-02-18T17:47:42.5202663Z [0;36m(Worker_DP0_TP3_EP3 pid=483)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-18T17:47:42.5212820Z [0;36m(Worker_DP0_TP3_EP3 pid=483)[0;0m   warnings.warn(
2026-02-18T17:47:42.5395949Z [0;36m(Worker_DP0_TP6_EP6 pid=795)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-18T17:47:42.5404125Z [0;36m(Worker_DP0_TP6_EP6 pid=795)[0;0m   warnings.warn(
2026-02-18T17:47:42.5584330Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-18T17:47:42.5593216Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m   warnings.warn(
2026-02-18T17:47:42.5736043Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-18T17:47:42.5744713Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m   warnings.warn(
2026-02-18T17:47:42.5766811Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-18T17:47:42.5777837Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m   warnings.warn(
2026-02-18T17:47:42.5796531Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-18T17:47:42.5807594Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m   warnings.warn(
2026-02-18T17:47:42.5891825Z [0;36m(Worker_DP0_TP7_EP7 pid=900)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-18T17:47:42.5899973Z [0;36m(Worker_DP0_TP7_EP7 pid=900)[0;0m   warnings.warn(
2026-02-18T17:47:42.5916082Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-18T17:47:42.5924856Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m   warnings.warn(
2026-02-18T17:47:42.5941622Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-18T17:47:42.5950983Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m   warnings.warn(
2026-02-18T17:47:42.6116167Z [0;36m(Worker_DP0_TP1_EP1 pid=259)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-18T17:47:42.6124124Z [0;36m(Worker_DP0_TP1_EP1 pid=259)[0;0m   warnings.warn(
2026-02-18T17:47:42.6140662Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-18T17:47:42.6149317Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m   warnings.warn(
2026-02-18T17:47:42.6164752Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-18T17:47:42.6179524Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m   warnings.warn(
2026-02-18T17:47:42.6250657Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-18T17:47:42.6261815Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m   warnings.warn(
2026-02-18T17:47:42.6414616Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-18T17:47:42.6426548Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m   warnings.warn(
2026-02-18T17:47:42.6589666Z [0;36m(Worker_DP0_TP2_EP2 pid=378)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-18T17:47:42.6601041Z [0;36m(Worker_DP0_TP2_EP2 pid=378)[0;0m   warnings.warn(
2026-02-18T17:47:42.6748086Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-18T17:47:42.6757452Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m   warnings.warn(
2026-02-18T17:47:55.8774890Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-18 17:47:55 [backends.py:319] Compiling a graph for compile range (1, 4096) takes 13.29 s
2026-02-18T17:47:55.8796130Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-18 17:47:55 [monitor.py:34] torch.compile takes 18.28 s in total
2026-02-18T17:47:56.3184577Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-18 17:47:56 [backends.py:319] Compiling a graph for compile range (1, 4096) takes 13.76 s
2026-02-18T17:47:56.3193152Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-18 17:47:56 [monitor.py:34] torch.compile takes 18.79 s in total
2026-02-18T17:48:01.4007706Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-18 17:48:01 [worker.py:338] Available memory: 18264625664, total memory: 65787658240
2026-02-18T17:48:01.4081377Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m INFO 02-18 17:48:01 [worker.py:338] Available memory: 18535295692, total memory: 65796046848
2026-02-18T17:48:01.4281483Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-18 17:48:01 [worker.py:338] Available memory: 18543436492, total memory: 65796046848
2026-02-18T17:48:01.4378066Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m INFO 02-18 17:48:01 [worker.py:338] Available memory: 18258309632, total memory: 65787658240
2026-02-18T17:48:01.5094935Z [0;36m(Worker_DP0_TP6_EP6 pid=795)[0;0m INFO 02-18 17:48:01 [worker.py:338] Available memory: 18257738240, total memory: 65787658240
2026-02-18T17:48:01.5177315Z [0;36m(Worker_DP0_TP2_EP2 pid=378)[0;0m INFO 02-18 17:48:01 [worker.py:338] Available memory: 18264299008, total memory: 65787658240
2026-02-18T17:48:01.5303657Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m INFO 02-18 17:48:01 [worker.py:338] Available memory: 18535598796, total memory: 65796046848
2026-02-18T17:48:01.5854277Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m INFO 02-18 17:48:01 [worker.py:338] Available memory: 18563178188, total memory: 65796046848
2026-02-18T17:48:01.5876473Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-18 17:48:01 [worker.py:338] Available memory: 17874508288, total memory: 65787658240
2026-02-18T17:48:01.7329837Z [0;36m(Worker_DP0_TP3_EP3 pid=483)[0;0m INFO 02-18 17:48:01 [worker.py:338] Available memory: 18540794572, total memory: 65796046848
2026-02-18T17:48:01.8216625Z [0;36m(Worker_DP0_TP1_EP1 pid=259)[0;0m INFO 02-18 17:48:01 [worker.py:338] Available memory: 18549222092, total memory: 65796046848
2026-02-18T17:48:01.8519080Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m INFO 02-18 17:48:01 [worker.py:338] Available memory: 18273759744, total memory: 65787658240
2026-02-18T17:48:01.8572231Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m INFO 02-18 17:48:01 [worker.py:338] Available memory: 18546129612, total memory: 65796046848
2026-02-18T17:48:02.0272147Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m INFO 02-18 17:48:02 [worker.py:338] Available memory: 18254607872, total memory: 65787658240
2026-02-18T17:48:02.0289949Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-18 17:48:02 [kv_cache_utils.py:1307] GPU KV cache size: 204,672 tokens
2026-02-18T17:48:02.0300409Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-18 17:48:02 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 24.98x
2026-02-18T17:48:02.1673572Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-18 17:48:02 [worker.py:338] Available memory: 17878071808, total memory: 65787658240
2026-02-18T17:48:02.1777513Z [0;36m(Worker_DP0_TP7_EP7 pid=900)[0;0m INFO 02-18 17:48:02 [worker.py:338] Available memory: 18555790028, total memory: 65796046848
2026-02-18T17:48:02.1797754Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-18 17:48:02 [kv_cache_utils.py:1307] GPU KV cache size: 204,672 tokens
2026-02-18T17:48:02.1811341Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-18 17:48:02 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 24.98x
2026-02-18T17:48:19.7800020Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-18T17:48:19.7800905Z Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s][rank3]:[W218 17:48:19.231402268 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-18T17:48:19.7810634Z [rank6]:[W218 17:48:19.231402248 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-18T17:48:19.7821096Z [rank13]:[W218 17:48:19.231449149 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-18T17:48:19.7830717Z [rank8]:[W218 17:48:19.232023273 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-18T17:48:19.7841079Z [rank10]:[W218 17:48:19.232084414 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-18T17:48:19.7851465Z [rank14]:[W218 17:48:19.232128594 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-18T17:48:19.7861316Z [rank15]:[W218 17:48:19.232524367 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-18T17:48:19.7872417Z [rank9]:[W218 17:48:19.232631658 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-18T17:48:19.7882292Z [rank11]:[W218 17:48:19.232949740 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-18T17:48:19.7891792Z [rank5]:[W218 17:48:19.233959378 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-18T17:48:19.7901910Z [rank2]:[W218 17:48:19.234112669 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-18T17:48:19.7911623Z [rank7]:[W218 17:48:19.234114959 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-18T17:48:19.7922655Z [rank4]:[W218 17:48:19.234163970 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-18T17:48:19.7932925Z [rank1]:[W218 17:48:19.234328681 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-18T17:48:19.7942865Z [rank12]:[W218 17:48:19.234753774 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-18T17:48:19.7953256Z [rank0]:[W218 17:48:19.236271606 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-18T17:48:22.2450242Z [rank4]:[W218 17:48:22.696760934 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-18T17:48:22.2462496Z [rank2]:[W218 17:48:22.696764304 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-18T17:48:22.2475013Z [rank6]:[W218 17:48:22.696780554 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-18T17:48:22.2486078Z [rank3]:[W218 17:48:22.696813444 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-18T17:48:22.2496011Z [rank1]:[W218 17:48:22.696881415 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-18T17:48:22.2506624Z [rank7]:[W218 17:48:22.697164237 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-18T17:48:22.2517537Z [rank0]:[W218 17:48:22.697308078 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-18T17:48:22.2528313Z [rank5]:[W218 17:48:22.698492167 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-18T17:48:22.3789418Z [rank15]:[W218 17:48:22.831067670 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-18T17:48:22.3807884Z [rank13]:[W218 17:48:22.831571724 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-18T17:48:22.3817420Z [rank9]:[W218 17:48:22.832101468 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-18T17:48:22.3827306Z [rank8]:[W218 17:48:22.832483221 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-18T17:48:22.3836926Z [rank11]:[W218 17:48:22.832908505 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-18T17:48:22.3847574Z [rank10]:[W218 17:48:22.832917335 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-18T17:48:22.3857155Z [rank14]:[W218 17:48:22.833075056 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-18T17:48:22.3867493Z [rank12]:[W218 17:48:22.833848562 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-18T17:48:25.3774405Z 
2026-02-18T17:48:25.3775977Z Capturing CUDA graphs (decode, FULL):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:17<00:17, 17.46s/it]
2026-02-18T17:48:25.3776825Z Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:20<00:00,  8.90s/it]
2026-02-18T17:48:25.3777573Z Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:20<00:00, 10.18s/it]
2026-02-18T17:48:25.9217605Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-18 17:48:25 [gpu_model_runner.py:5051] Graph capturing finished in 22 secs, took 0.27 GiB
2026-02-18T17:48:26.0956986Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-18 17:48:26 [gpu_model_runner.py:5051] Graph capturing finished in 22 secs, took 0.27 GiB
2026-02-18T17:48:26.4884755Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-18 17:48:26 [core.py:272] init engine (profile, create kv cache, warmup model) took 56.41 seconds
2026-02-18T17:48:26.5675757Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-18 17:48:26 [core.py:272] init engine (profile, create kv cache, warmup model) took 56.05 seconds
2026-02-18T17:48:28.0320310Z INFO 02-18 17:48:28 [coordinator.py:200] All engine subscriptions received by DP coordinator
2026-02-18T17:48:28.0339941Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-18 17:48:28 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-18T17:48:28.0350854Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-18 17:48:28 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-18T17:48:28.0361371Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-18 17:48:28 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:48:28.0372063Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-18 17:48:28 [ascend_config.py:412] Dynamic EPLB is False
2026-02-18T17:48:28.0382665Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-18 17:48:28 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:48:28.0392878Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-18 17:48:28 [ascend_config.py:413] The number of redundant experts is 0
2026-02-18T17:48:28.0406257Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-18 17:48:28 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:48:28.0415077Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-18 17:48:28 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-18T17:48:28.0423780Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-18 17:48:28 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-18T17:48:28.0433574Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-18 17:48:28 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-18T17:48:28.0443104Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-18 17:48:28 [platform.py:335] [91m
2026-02-18T17:48:28.0452963Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             **********************************************************************************
2026-02-18T17:48:28.0462812Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-18T17:48:28.0472810Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-18T17:48:28.0484234Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-18T17:48:28.0494077Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-18T17:48:28.0505841Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-18T17:48:28.0518615Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             * batch size for graph capture.
2026-02-18T17:48:28.0528949Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             * For more details, please refer to:
2026-02-18T17:48:28.0539113Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-18T17:48:28.0548975Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             **********************************************************************************[0m
2026-02-18T17:48:28.0558118Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             
2026-02-18T17:48:28.0568405Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-18 17:48:28 [platform.py:335] [91m
2026-02-18T17:48:28.0579033Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             **********************************************************************************
2026-02-18T17:48:28.0588691Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-18T17:48:28.0598203Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-18T17:48:28.0609853Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-18T17:48:28.0619482Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-18T17:48:28.0629328Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-18T17:48:28.0638215Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             * batch size for graph capture.
2026-02-18T17:48:28.0648955Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             * For more details, please refer to:
2026-02-18T17:48:28.0658535Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-18T17:48:28.0668301Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             **********************************************************************************[0m
2026-02-18T17:48:28.0677938Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-18 17:48:28 [platform.py:335]             
2026-02-18T17:48:28.0687128Z INFO 02-18 17:48:28 [utils.py:249] Waiting for API servers to complete ...
2026-02-18T17:48:28.0697132Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-18 17:48:28 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-18T17:48:28.0706568Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-18 17:48:28 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-18T17:48:28.0716587Z ERROR 02-18 17:48:28 [utils.py:290] Exception occurred while running API servers: Process ApiServer_0 (PID: 191) died with exit code 1
2026-02-18T17:48:28.0726203Z ERROR 02-18 17:48:28 [utils.py:290] Traceback (most recent call last):
2026-02-18T17:48:28.0736633Z ERROR 02-18 17:48:28 [utils.py:290]   File "/vllm-workspace/vllm/vllm/v1/utils.py", line 277, in wait_for_completion_or_failure
2026-02-18T17:48:28.0745280Z ERROR 02-18 17:48:28 [utils.py:290]     raise RuntimeError(
2026-02-18T17:48:28.0754344Z ERROR 02-18 17:48:28 [utils.py:290] RuntimeError: Process ApiServer_0 (PID: 191) died with exit code 1
2026-02-18T17:48:28.0765170Z INFO 02-18 17:48:28 [utils.py:293] Terminating remaining processes ...
2026-02-18T17:48:28.3184784Z [0;36m(Worker_DP0_TP1_EP1 pid=259)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-18T17:48:28.3193347Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-18T17:48:28.3203866Z [0;36m(Worker_DP0_TP6_EP6 pid=795)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-18T17:48:28.3213810Z [0;36m(Worker_DP0_TP3_EP3 pid=483)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-18T17:48:28.3224200Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-18T17:48:28.3234602Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-18T17:48:28.3245110Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-18T17:48:28.3254994Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-18T17:48:28.3265415Z [0;36m(Worker_DP0_TP2_EP2 pid=378)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-18T17:48:28.3275239Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-18T17:48:28.3285266Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-18T17:48:28.3295107Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-18T17:48:28.3304208Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-18T17:48:28.3313735Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-18T17:48:28.3323050Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-18T17:48:28.3332470Z [0;36m(Worker_DP0_TP1_EP1 pid=259)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-18T17:48:28.3341835Z [0;36m(Worker_DP0_TP6_EP6 pid=795)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-18T17:48:28.3351394Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-18T17:48:28.3360583Z [0;36m(Worker_DP0_TP2_EP2 pid=378)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-18T17:48:28.3370515Z [0;36m(Worker_DP0_TP3_EP3 pid=483)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-18T17:48:28.3380204Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-18T17:48:28.3390161Z [0;36m(Worker_DP0_TP5_EP5 pid=691)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-18T17:48:28.3399023Z [0;36m(Worker_DP1_TP6_EP14 pid=792)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-18T17:48:28.3409736Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-18T17:48:28.3418759Z [0;36m(Worker_DP0_TP7_EP7 pid=900)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-18T17:48:28.3428671Z [0;36m(Worker_DP1_TP3_EP11 pid=480)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-18T17:48:28.3437658Z [0;36m(Worker_DP1_TP5_EP13 pid=688)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-18T17:48:28.3447190Z [0;36m(Worker_DP1_TP4_EP12 pid=584)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-18T17:48:28.3456351Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-18T17:48:28.3465801Z [0;36m(Worker_DP1_TP7_EP15 pid=896)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-18T17:48:28.3475754Z [0;36m(Worker_DP0_TP4_EP4 pid=587)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-18T17:48:28.3485558Z [0;36m(Worker_DP0_TP7_EP7 pid=900)[0;0m INFO 02-18 17:48:28 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-18T17:48:33.3542306Z Traceback (most recent call last):
2026-02-18T17:48:33.3551695Z   File "/usr/local/python3.11.14/bin/vllm", line 7, in <module>
2026-02-18T17:48:33.3565542Z     sys.exit(main())
2026-02-18T17:48:33.3577716Z              ^^^^^^
2026-02-18T17:48:33.3587962Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/main.py", line 73, in main
2026-02-18T17:48:33.3597747Z     args.dispatch_function(args)
2026-02-18T17:48:33.3608643Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 108, in cmd
2026-02-18T17:48:33.3618486Z     run_multi_api_server(args)
2026-02-18T17:48:33.3628422Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 282, in run_multi_api_server
2026-02-18T17:48:33.3638436Z     wait_for_completion_or_failure(
2026-02-18T17:48:33.3648359Z   File "/vllm-workspace/vllm/vllm/v1/utils.py", line 277, in wait_for_completion_or_failure
2026-02-18T17:48:33.3657704Z     raise RuntimeError(
2026-02-18T17:48:33.3666825Z RuntimeError: Process ApiServer_0 (PID: 191) died with exit code 1
2026-02-18T17:48:33.4326755Z [ERROR] 2026-02-18-17:48:33 (PID:138, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
2026-02-18T17:48:33.7921164Z sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-18T17:48:35.5388802Z /usr/local/python3.11.14/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked shared_memory objects to clean up at shutdown
2026-02-18T17:48:35.5395702Z   warnings.warn('resource_tracker: There appear to be %d '
2026-02-18T17:48:41.5170846Z FAILED
2026-02-18T17:48:41.5181399Z 
2026-02-18T17:48:41.5193181Z =================================== FAILURES ===================================
2026-02-18T17:48:41.5205643Z _______________________________ test_multi_node ________________________________
2026-02-18T17:48:41.5215276Z 
2026-02-18T17:48:41.5225347Z     @pytest.mark.asyncio
2026-02-18T17:48:41.5235556Z     async def test_multi_node() -> None:
2026-02-18T17:48:41.5246997Z         config = MultiNodeConfigLoader.from_yaml()
2026-02-18T17:48:41.5264401Z     
2026-02-18T17:48:41.5275966Z         with ProxyLauncher(
2026-02-18T17:48:41.5286137Z                 nodes=config.nodes,
2026-02-18T17:48:41.5296171Z                 disagg_cfg=config.disagg_cfg,
2026-02-18T17:48:41.5306147Z                 envs=config.envs,
2026-02-18T17:48:41.5316194Z                 proxy_port=config.proxy_port,
2026-02-18T17:48:41.5325840Z                 cur_index=config.cur_index,
2026-02-18T17:48:41.5335101Z         ) as proxy:
2026-02-18T17:48:41.5346407Z     
2026-02-18T17:48:41.5364587Z >           with RemoteOpenAIServer(
2026-02-18T17:48:41.5375461Z                     model=config.model,
2026-02-18T17:48:41.5385084Z                     vllm_serve_args=config.server_cmd,
2026-02-18T17:48:41.5395587Z                     server_port=config.server_port,
2026-02-18T17:48:41.5405548Z                     server_host=config.master_ip,
2026-02-18T17:48:41.5417548Z                     env_dict=config.envs,
2026-02-18T17:48:41.5428524Z                     auto_port=False,
2026-02-18T17:48:41.5438515Z                     proxy_port=proxy.proxy_port,
2026-02-18T17:48:41.5449296Z                     disaggregated_prefill=config.disagg_cfg,
2026-02-18T17:48:41.5458984Z                     nodes_info=config.nodes,
2026-02-18T17:48:41.5468607Z                     max_wait_seconds=2800,
2026-02-18T17:48:41.5478591Z             ) as server:
2026-02-18T17:48:41.5488558Z 
2026-02-18T17:48:41.5497684Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py:21: 
2026-02-18T17:48:41.5507280Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-18T17:48:41.5517160Z tests/e2e/conftest.py:306: in __init__
2026-02-18T17:48:41.5527576Z     self._wait_for_multiple_servers(
2026-02-18T17:48:41.5536937Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-18T17:48:41.5546234Z 
2026-02-18T17:48:41.5556284Z self = <tests.e2e.conftest.RemoteOpenAIServer object at 0xfffefd317f50>
2026-02-18T17:48:41.5565978Z targets = [('10.0.0.179', 'http://10.0.0.179:8080/health')], timeout = 2800
2026-02-18T17:48:41.5575812Z log_interval = 30.0
2026-02-18T17:48:41.5584915Z 
2026-02-18T17:48:41.5594460Z     def _wait_for_multiple_servers(self,
2026-02-18T17:48:41.5604300Z                                    targets,
2026-02-18T17:48:41.5614242Z                                    timeout: float,
2026-02-18T17:48:41.5623841Z                                    log_interval: float = 30.0):
2026-02-18T17:48:41.5632918Z         """
2026-02-18T17:48:41.5642930Z         targets: List[(node_ip, url)]
2026-02-18T17:48:41.5653880Z         log_interval
2026-02-18T17:48:41.5662119Z         """
2026-02-18T17:48:41.5671117Z         start = time.time()
2026-02-18T17:48:41.5681367Z         client = requests
2026-02-18T17:48:41.5690778Z     
2026-02-18T17:48:41.5701228Z         ready = {node_ip: False for node_ip, _ in targets}
2026-02-18T17:48:41.5709505Z     
2026-02-18T17:48:41.5727905Z         last_log_time = 0.0
2026-02-18T17:48:41.5728923Z     
2026-02-18T17:48:41.5738449Z         while True:
2026-02-18T17:48:41.5747430Z             now = time.time()
2026-02-18T17:48:41.5756974Z             all_ready = True
2026-02-18T17:48:41.5767394Z             should_log = (now - last_log_time) >= log_interval
2026-02-18T17:48:41.5776829Z     
2026-02-18T17:48:41.5786494Z             for node_ip, url in targets:
2026-02-18T17:48:41.5796183Z                 if ready[node_ip]:
2026-02-18T17:48:41.5806765Z                     continue
2026-02-18T17:48:41.5816595Z     
2026-02-18T17:48:41.5825772Z                 try:
2026-02-18T17:48:41.5835108Z                     resp = client.get(url)
2026-02-18T17:48:41.5844990Z                     if resp.status_code == 200:
2026-02-18T17:48:41.5855151Z                         ready[node_ip] = True
2026-02-18T17:48:41.5865391Z                         logger.info(f"[READY] Node {node_ip} is ready.")
2026-02-18T17:48:41.5875590Z                 except RequestException:
2026-02-18T17:48:41.5885547Z                     all_ready = False
2026-02-18T17:48:41.5895512Z                     if should_log:
2026-02-18T17:48:41.5905354Z                         logger.debug(f"[WAIT] {url}: connection failed")
2026-02-18T17:48:41.5914744Z     
2026-02-18T17:48:41.5924999Z                     # check unexpected exit
2026-02-18T17:48:41.5934659Z                     result = self._poll()
2026-02-18T17:48:41.5944120Z                     if result is not None and result != 0:
2026-02-18T17:48:41.5953437Z >                       raise RuntimeError(
2026-02-18T17:48:41.5963090Z                             f"Server at {node_ip} exited unexpectedly."
2026-02-18T17:48:41.5972323Z                         ) from None
2026-02-18T17:48:41.5981724Z E                       RuntimeError: Server at 10.0.0.179 exited unexpectedly.
2026-02-18T17:48:41.5990732Z 
2026-02-18T17:48:41.6000298Z tests/e2e/conftest.py:399: RuntimeError
2026-02-18T17:48:41.6010247Z =============================== warnings summary ===============================
2026-02-18T17:48:41.6019687Z <frozen importlib._bootstrap>:241
2026-02-18T17:48:41.6030991Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute
2026-02-18T17:48:41.6042810Z 
2026-02-18T17:48:41.6052579Z <frozen importlib._bootstrap>:241
2026-02-18T17:48:41.6062272Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute
2026-02-18T17:48:41.6071516Z 
2026-02-18T17:48:41.6081073Z ../../usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647
2026-02-18T17:48:41.6091878Z   /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-18T17:48:41.6100280Z     warnings.warn(
2026-02-18T17:48:41.6109903Z 
2026-02-18T17:48:41.6119627Z ../../usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8
2026-02-18T17:48:41.6130518Z   /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
2026-02-18T17:48:41.6139058Z     import pkg_resources
2026-02-18T17:48:41.6147538Z 
2026-02-18T17:48:41.6156724Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2026-02-18T17:48:41.6166452Z =========================== short test summary info ============================
2026-02-18T17:48:41.6175811Z FAILED tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node
2026-02-18T17:48:41.6185180Z ================== 1 failed, 4 warnings in 821.32s (0:13:41) ===================
2026-02-18T17:48:43.2330439Z [0;31mFAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml âœ— ERROR: Some tests failed, please check the error stack above for details. If this is insufficient to pinpoint the error, please download and review the logs of all other nodes from the job's summary.[0m
2026-02-18T17:48:43.4545789Z Cleaning up background log streams...
2026-02-18T17:48:43.6595888Z ##[error]Error: failed to run script step: Error: command terminated with non-zero exit code: command terminated with exit code 1
2026-02-18T17:48:43.6634612Z ##[error]Process completed with exit code 1.
2026-02-18T17:48:43.6729478Z ##[error]Executing the custom container implementation failed. Please contact your self hosted runner administrator.
2026-02-18T17:48:43.7107484Z ##[group]Run actions/upload-artifact@v6
2026-02-18T17:48:43.7107768Z with:
2026-02-18T17:48:43.7108058Z   name: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs
2026-02-18T17:48:43.7108345Z   path: /tmp/vllm*_logs.txt
2026-02-18T17:48:43.7108580Z   retention-days: 7
2026-02-18T17:48:43.7108909Z   if-no-files-found: warn
2026-02-18T17:48:43.7109125Z   compression-level: 6
2026-02-18T17:48:43.7109342Z   overwrite: false
2026-02-18T17:48:43.7109567Z   include-hidden-files: false
2026-02-18T17:48:43.7109809Z env:
2026-02-18T17:48:43.7110053Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-18T17:48:43.7110343Z ##[endgroup]
2026-02-18T17:48:43.7137290Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-18T17:48:43.7138231Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-18T17:48:43.7138524Z ##[endgroup]
2026-02-18T17:48:44.0656587Z (node:915) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-18T17:48:44.0657370Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-18T17:48:45.0830330Z With the provided path, there will be 1 file uploaded
2026-02-18T17:48:45.0835183Z Artifact name is valid!
2026-02-18T17:48:45.0835500Z Root directory input is valid!
2026-02-18T17:48:46.0715340Z Beginning upload of artifact content to blob storage
2026-02-18T17:48:47.0338415Z Uploaded bytes 12634
2026-02-18T17:48:47.2720053Z Finished uploading artifact content to blob storage!
2026-02-18T17:48:47.2720786Z SHA256 digest of uploaded artifact zip is fa1591ccc5c7cfcfd951e8692ffece0ab488aa96c69eb0005963ac13fcaf68cc
2026-02-18T17:48:47.2721272Z Finalizing artifact upload
2026-02-18T17:48:48.5138291Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs.zip successfully finalized. Artifact ID 5560073585
2026-02-18T17:48:48.5139168Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs has been successfully uploaded! Final size is 12634 bytes. Artifact ID is 5560073585
2026-02-18T17:48:48.5143668Z Artifact download URL: https://github.com/vllm-project/vllm-ascend/actions/runs/22148925734/artifacts/5560073585
2026-02-18T17:48:48.9737696Z ##[group]Run kubectl get pods -n "$NAMESPACE" --ignore-not-found=true
2026-02-18T17:48:48.9738196Z [36;1mkubectl get pods -n "$NAMESPACE" --ignore-not-found=true[0m
2026-02-18T17:48:48.9738603Z [36;1mkubectl delete -f ./lws.yaml --ignore-not-found=true || true[0m
2026-02-18T17:48:48.9739012Z shell: bash -el {0}
2026-02-18T17:48:48.9739247Z env:
2026-02-18T17:48:48.9739506Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-18T17:48:48.9739837Z ##[endgroup]
2026-02-18T17:48:48.9816374Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-18T17:48:48.9817321Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-18T17:48:48.9817623Z ##[endgroup]
2026-02-18T17:48:49.3301607Z (node:1029) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-18T17:48:49.3302544Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-18T17:48:50.0090567Z NAME                                             READY   STATUS    RESTARTS     AGE
2026-02-18T17:48:50.0091081Z linux-aarch64-a3-0-n4cwm-runner-rxxwk            1/1     Running   0            15m
2026-02-18T17:48:50.0091607Z linux-aarch64-a3-0-n4cwm-runner-rxxwk-workflow   1/1     Running   0            15m
2026-02-18T17:48:50.0092132Z vllm-0                                           1/1     Running   1 (7s ago)   14m
2026-02-18T17:48:50.0295324Z vllm-0-1                                         1/1     Running   0            14m
2026-02-18T17:48:50.0804508Z leaderworkerset.leaderworkerset.x-k8s.io "vllm" deleted from vllm-project namespace
2026-02-18T17:48:50.1018763Z service "vllm-leader" deleted from vllm-project namespace
2026-02-18T17:48:50.5676936Z Post job cleanup.
2026-02-18T17:48:50.5699159Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-18T17:48:50.5699997Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-18T17:48:50.5700280Z ##[endgroup]
2026-02-18T17:48:50.9247232Z (node:1155) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-18T17:48:50.9248361Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-18T17:48:51.5893625Z [command]/usr/bin/git version
2026-02-18T17:48:51.6099440Z git version 2.34.1
2026-02-18T17:48:51.6130766Z Copying '/root/.gitconfig' to '/__w/_temp/05002542-5acd-41fc-8148-705ce449c728/.gitconfig'
2026-02-18T17:48:51.6137635Z Temporarily overriding HOME='/__w/_temp/05002542-5acd-41fc-8148-705ce449c728' before making global git config changes
2026-02-18T17:48:51.6138255Z Adding repository directory to the temporary git global config as a safe directory
2026-02-18T17:48:51.6142823Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-18T17:48:51.6178609Z Removing SSH command configuration
2026-02-18T17:48:51.6183177Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-18T17:48:51.6236595Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-18T17:48:51.6717279Z Removing HTTP extra header
2026-02-18T17:48:51.6718380Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-18T17:48:51.6744285Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-18T17:48:51.6928571Z Removing includeIf entries pointing to credentials config files
2026-02-18T17:48:51.6933436Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-18T17:48:51.6951611Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-18T17:48:51.6951934Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-18T17:48:51.6952337Z includeif.gitdir:/github/workspace/.git.path
2026-02-18T17:48:51.6952603Z includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-18T17:48:51.6959259Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-18T17:48:51.6980102Z /__w/_temp/git-credentials-3b318f8c-c51a-4b9a-94c5-ffda996b378b.config
2026-02-18T17:48:51.6985402Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-3b318f8c-c51a-4b9a-94c5-ffda996b378b.config
2026-02-18T17:48:51.7013631Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-18T17:48:51.7030753Z /__w/_temp/git-credentials-3b318f8c-c51a-4b9a-94c5-ffda996b378b.config
2026-02-18T17:48:51.7038282Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-3b318f8c-c51a-4b9a-94c5-ffda996b378b.config
2026-02-18T17:48:51.7063274Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git.path
2026-02-18T17:48:51.7080624Z /github/runner_temp/git-credentials-3b318f8c-c51a-4b9a-94c5-ffda996b378b.config
2026-02-18T17:48:51.7087381Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-3b318f8c-c51a-4b9a-94c5-ffda996b378b.config
2026-02-18T17:48:51.7112362Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-18T17:48:51.7129359Z /github/runner_temp/git-credentials-3b318f8c-c51a-4b9a-94c5-ffda996b378b.config
2026-02-18T17:48:51.7135691Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-3b318f8c-c51a-4b9a-94c5-ffda996b378b.config
2026-02-18T17:48:51.7164860Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-18T17:48:51.7340551Z Removing credentials config '/__w/_temp/git-credentials-3b318f8c-c51a-4b9a-94c5-ffda996b378b.config'
2026-02-18T17:49:10.1409897Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-18T17:49:10.1410657Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-18T17:49:10.1410876Z ##[endgroup]
2026-02-18T17:49:10.6023600Z Cleaning up orphan processes
