# Run ID: 22280686910
# Commit: f0caeeadcb37261beebd4a6e32934fa9f460db98
# Job: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
# Date: 2026-02-22
============================================================

ï»¿2026-02-22T18:13:21.2490259Z Current runner version: '2.330.0'
2026-02-22T18:13:21.2495432Z Runner name: 'linux-aarch64-a3-0-n4cwm-runner-tgpp7'
2026-02-22T18:13:21.2496208Z Runner group name: 'Default'
2026-02-22T18:13:21.2496942Z Machine name: 'linux-aarch64-a3-0-n4cwm-runner-tgpp7'
2026-02-22T18:13:21.2500274Z ##[group]GITHUB_TOKEN Permissions
2026-02-22T18:13:21.2502427Z Actions: write
2026-02-22T18:13:21.2502895Z ArtifactMetadata: write
2026-02-22T18:13:21.2503332Z Attestations: write
2026-02-22T18:13:21.2503700Z Checks: write
2026-02-22T18:13:21.2504080Z Contents: write
2026-02-22T18:13:21.2504432Z Deployments: write
2026-02-22T18:13:21.2504919Z Discussions: write
2026-02-22T18:13:21.2505301Z Issues: write
2026-02-22T18:13:21.2505630Z Metadata: read
2026-02-22T18:13:21.2506012Z Models: read
2026-02-22T18:13:21.2506351Z Packages: write
2026-02-22T18:13:21.2506713Z Pages: write
2026-02-22T18:13:21.2507100Z PullRequests: write
2026-02-22T18:13:21.2507472Z RepositoryProjects: write
2026-02-22T18:13:21.2508089Z SecurityEvents: write
2026-02-22T18:13:21.2508527Z Statuses: write
2026-02-22T18:13:21.2509001Z ##[endgroup]
2026-02-22T18:13:21.2510719Z Secret source: Actions
2026-02-22T18:13:21.2511215Z Prepare workflow directory
2026-02-22T18:13:21.3074233Z Prepare all required actions
2026-02-22T18:13:21.3105215Z Getting action download info
2026-02-22T18:13:23.2330922Z Download action repository 'actions/checkout@v6' (SHA:de0fac2e4500dabe0009e67214ff5f5447ce83dd)
2026-02-22T18:13:27.7327409Z Download action repository 'actions/upload-artifact@v6' (SHA:b7c566a772e6b6bfb58ed0dc250532a479d7789f)
2026-02-22T18:13:35.3286659Z Uses: vllm-project/vllm-ascend/.github/workflows/_e2e_nightly_multi_node.yaml@refs/heads/main (f0caeeadcb37261beebd4a6e32934fa9f460db98)
2026-02-22T18:13:35.3289792Z ##[group] Inputs
2026-02-22T18:13:35.3290064Z   soc_version: a3
2026-02-22T18:13:35.3290315Z   runner: linux-aarch64-a3-0
2026-02-22T18:13:35.3290804Z   image: swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3
2026-02-22T18:13:35.3291270Z   config_file_path: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-22T18:13:35.3291566Z   replicas: 1
2026-02-22T18:13:35.3291780Z   size: 2
2026-02-22T18:13:35.3292137Z   vllm_version: v0.15.0
2026-02-22T18:13:35.3292529Z   vllm_ascend_remote_url: https://github.com/vllm-project/vllm-ascend.git
2026-02-22T18:13:35.3292838Z   vllm_ascend_ref: main
2026-02-22T18:13:35.3293115Z ##[endgroup]
2026-02-22T18:13:35.3293607Z Complete job name: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-22T18:13:35.3781058Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-22T18:13:35.3783750Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-22T18:13:35.3784202Z ##[endgroup]
2026-02-22T18:13:50.9556837Z (node:69) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-22T18:13:50.9558273Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-22T18:13:52.7420185Z ##[group]Run # Decode and save kubeconfig
2026-02-22T18:13:52.7420680Z [36;1m# Decode and save kubeconfig[0m
2026-02-22T18:13:52.7453378Z [36;1mecho "***" | base64 -d > "$KUBECONFIG"[0m
2026-02-22T18:13:52.7453988Z shell: bash -el {0}
2026-02-22T18:13:52.7454313Z ##[endgroup]
2026-02-22T18:13:52.7569291Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-22T18:13:52.7570443Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-22T18:13:52.7570934Z ##[endgroup]
2026-02-22T18:13:53.1071495Z (node:399) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-22T18:13:53.1072456Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-22T18:13:53.9855728Z ##[group]Run actions/checkout@v6
2026-02-22T18:13:53.9856058Z with:
2026-02-22T18:13:53.9856362Z   repository: vllm-project/vllm-ascend
2026-02-22T18:13:53.9857115Z   token: ***
2026-02-22T18:13:53.9857345Z   ssh-strict: true
2026-02-22T18:13:53.9857571Z   ssh-user: git
2026-02-22T18:13:53.9857811Z   persist-credentials: true
2026-02-22T18:13:53.9858057Z   clean: true
2026-02-22T18:13:53.9858273Z   sparse-checkout-cone-mode: true
2026-02-22T18:13:53.9858531Z   fetch-depth: 1
2026-02-22T18:13:53.9858732Z   fetch-tags: false
2026-02-22T18:13:53.9858967Z   show-progress: true
2026-02-22T18:13:53.9859191Z   lfs: false
2026-02-22T18:13:53.9859466Z   submodules: false
2026-02-22T18:13:53.9859702Z   set-safe-directory: true
2026-02-22T18:13:53.9859955Z ##[endgroup]
2026-02-22T18:13:53.9900742Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-22T18:13:53.9901730Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-22T18:13:53.9902192Z ##[endgroup]
2026-02-22T18:13:54.3447011Z (node:430) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-22T18:13:54.3447940Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-22T18:13:54.8996418Z Syncing repository: vllm-project/vllm-ascend
2026-02-22T18:13:54.8997511Z ##[group]Getting Git version info
2026-02-22T18:13:54.8997895Z Working directory is '/__w/vllm-ascend/vllm-ascend'
2026-02-22T18:13:54.8998376Z [command]/usr/bin/git version
2026-02-22T18:13:54.8998642Z git version 2.34.1
2026-02-22T18:13:54.9000006Z ##[endgroup]
2026-02-22T18:13:54.9007608Z Copying '/root/.gitconfig' to '/__w/_temp/acda3a64-627d-4e10-aaf1-134ebfa27d10/.gitconfig'
2026-02-22T18:13:54.9018556Z Temporarily overriding HOME='/__w/_temp/acda3a64-627d-4e10-aaf1-134ebfa27d10' before making global git config changes
2026-02-22T18:13:54.9019183Z Adding repository directory to the temporary git global config as a safe directory
2026-02-22T18:13:54.9022338Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-22T18:13:54.9056682Z Deleting the contents of '/__w/vllm-ascend/vllm-ascend'
2026-02-22T18:13:54.9059360Z ##[group]Initializing the repository
2026-02-22T18:13:54.9063171Z [command]/usr/bin/git init /__w/vllm-ascend/vllm-ascend
2026-02-22T18:13:54.9176875Z hint: Using 'master' as the name for the initial branch. This default branch name
2026-02-22T18:13:54.9177353Z hint: is subject to change. To configure the initial branch name to use in all
2026-02-22T18:13:54.9177774Z hint: of your new repositories, which will suppress this warning, call:
2026-02-22T18:13:54.9178115Z hint: 
2026-02-22T18:13:54.9178465Z hint: 	git config --global init.defaultBranch <name>
2026-02-22T18:13:54.9178736Z hint: 
2026-02-22T18:13:54.9179028Z hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and
2026-02-22T18:13:54.9179463Z hint: 'development'. The just-created branch can be renamed via this command:
2026-02-22T18:13:54.9179788Z hint: 
2026-02-22T18:13:54.9180005Z hint: 	git branch -m <name>
2026-02-22T18:13:54.9184552Z Initialized empty Git repository in /__w/vllm-ascend/vllm-ascend/.git/
2026-02-22T18:13:54.9193172Z [command]/usr/bin/git remote add origin https://github.com/vllm-project/vllm-ascend
2026-02-22T18:13:54.9231584Z ##[endgroup]
2026-02-22T18:13:54.9231966Z ##[group]Disabling automatic garbage collection
2026-02-22T18:13:54.9234639Z [command]/usr/bin/git config --local gc.auto 0
2026-02-22T18:13:54.9263450Z ##[endgroup]
2026-02-22T18:13:54.9263798Z ##[group]Setting up auth
2026-02-22T18:13:54.9264794Z Removing SSH command configuration
2026-02-22T18:13:54.9269559Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-22T18:13:54.9298034Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-22T18:13:54.9483415Z Removing HTTP extra header
2026-02-22T18:13:54.9487052Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-22T18:13:54.9511636Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-22T18:13:54.9686309Z Removing includeIf entries pointing to credentials config files
2026-02-22T18:13:54.9690819Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-22T18:13:54.9716186Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-22T18:13:54.9901006Z [command]/usr/bin/git config --file /__w/_temp/git-credentials-9a4e7775-8d2d-4d1d-92ce-2621dc1395c4.config http.https://github.com/.extraheader AUTHORIZATION: basic ***
2026-02-22T18:13:54.9937901Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-9a4e7775-8d2d-4d1d-92ce-2621dc1395c4.config
2026-02-22T18:13:54.9963677Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-9a4e7775-8d2d-4d1d-92ce-2621dc1395c4.config
2026-02-22T18:13:54.9988438Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-9a4e7775-8d2d-4d1d-92ce-2621dc1395c4.config
2026-02-22T18:13:55.0014462Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-9a4e7775-8d2d-4d1d-92ce-2621dc1395c4.config
2026-02-22T18:13:55.0037176Z ##[endgroup]
2026-02-22T18:13:55.0037561Z ##[group]Fetching the repository
2026-02-22T18:13:55.0044794Z [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --no-recurse-submodules --depth=1 origin +f0caeeadcb37261beebd4a6e32934fa9f460db98:refs/remotes/origin/main
2026-02-22T18:13:56.6803981Z From https://gh-proxy.test.osinfra.cn/https://github.com/vllm-project/vllm-ascend
2026-02-22T18:13:56.6804709Z  * [new ref]         f0caeeadcb37261beebd4a6e32934fa9f460db98 -> origin/main
2026-02-22T18:13:56.6824087Z [command]/usr/bin/git branch --list --remote origin/main
2026-02-22T18:13:56.6848932Z   origin/main
2026-02-22T18:13:56.6856248Z [command]/usr/bin/git rev-parse refs/remotes/origin/main
2026-02-22T18:13:56.6875227Z f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-22T18:13:56.6878507Z ##[endgroup]
2026-02-22T18:13:56.6878884Z ##[group]Determining the checkout info
2026-02-22T18:13:56.6880999Z ##[endgroup]
2026-02-22T18:13:56.6884338Z [command]/usr/bin/git sparse-checkout disable
2026-02-22T18:13:56.6916830Z [command]/usr/bin/git config --local --unset-all extensions.worktreeConfig
2026-02-22T18:13:56.6941234Z ##[group]Checking out the ref
2026-02-22T18:13:56.6943841Z [command]/usr/bin/git checkout --progress --force -B main refs/remotes/origin/main
2026-02-22T18:13:56.7819335Z Switched to a new branch 'main'
2026-02-22T18:13:56.7819693Z Branch 'main' set up to track remote branch 'main' from 'origin'.
2026-02-22T18:13:56.7830350Z ##[endgroup]
2026-02-22T18:13:56.7863266Z [command]/usr/bin/git log -1 --format=%H
2026-02-22T18:13:56.7883597Z f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-22T18:13:57.2018281Z ##[group]Run # prepare for lws entrypoint scripts
2026-02-22T18:13:57.2018675Z [36;1m# prepare for lws entrypoint scripts[0m
2026-02-22T18:13:57.2019101Z [36;1minstall -D tests/e2e/nightly/multi_node/scripts/run.sh /root/.cache/tests/run.sh[0m
2026-02-22T18:13:57.2019593Z shell: bash -el {0}
2026-02-22T18:13:57.2019820Z ##[endgroup]
2026-02-22T18:13:57.2127270Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-22T18:13:57.2128229Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-22T18:13:57.2128537Z ##[endgroup]
2026-02-22T18:13:57.5637609Z (node:471) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-22T18:13:57.5638408Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-22T18:13:58.4796494Z ##[group]Run set -euo pipefail
2026-02-22T18:13:58.4796859Z [36;1mset -euo pipefail[0m
2026-02-22T18:13:58.4797295Z [36;1m[0m
2026-02-22T18:13:58.4797618Z [36;1mCRD_NAME="${CRD_NAME:-vllm}"[0m
2026-02-22T18:13:58.4797888Z [36;1mTIMEOUT=${TIMEOUT:-120}[0m
2026-02-22T18:13:58.4798109Z [36;1mSLEEP_INTERVAL=2[0m
2026-02-22T18:13:58.4798353Z [36;1m[0m
2026-02-22T18:13:58.4798633Z [36;1mecho "Deleting leaderworkerset [$CRD_NAME] in namespace [$NAMESPACE]..."[0m
2026-02-22T18:13:58.4799117Z [36;1mkubectl delete leaderworkerset "$CRD_NAME" -n "$NAMESPACE" --ignore-not-found[0m
2026-02-22T18:13:58.4799511Z [36;1m[0m
2026-02-22T18:13:58.4799764Z [36;1mecho "Waiting for all pods starting with 'vllm' to be deleted..."[0m
2026-02-22T18:13:58.4800101Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-22T18:13:58.4800359Z [36;1m[0m
2026-02-22T18:13:58.4800539Z [36;1mwhile true; do[0m
2026-02-22T18:13:58.4800836Z [36;1m  NOW=$(date +%s)[0m
2026-02-22T18:13:58.4801205Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-22T18:13:58.4801476Z [36;1m[0m
2026-02-22T18:13:58.4801706Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-22T18:13:58.4802203Z [36;1m    echo "Timeout reached ($TIMEOUT seconds), some pods still exist:"[0m
2026-02-22T18:13:58.4802679Z [36;1m    kubectl get pods -n "$NAMESPACE" | grep '^vllm' || true[0m
2026-02-22T18:13:58.4802996Z [36;1m    exit 1[0m
2026-02-22T18:13:58.4803188Z [36;1m  fi[0m
2026-02-22T18:13:58.4803399Z [36;1m[0m
2026-02-22T18:13:58.4803853Z [36;1m  PODS_EXIST=$(kubectl get pods -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null | tr ' ' '\n' | grep '^vllm' || true)[0m
2026-02-22T18:13:58.4804322Z [36;1m[0m
2026-02-22T18:13:58.4804608Z [36;1m  if [[ -z "$PODS_EXIST" ]]; then[0m
2026-02-22T18:13:58.4804889Z [36;1m    echo "All vllm pods deleted."[0m
2026-02-22T18:13:58.4805124Z [36;1m    break[0m
2026-02-22T18:13:58.4805334Z [36;1m  else[0m
2026-02-22T18:13:58.4805619Z [36;1m    echo "Waiting for pods to be deleted: $PODS_EXIST"[0m
2026-02-22T18:13:58.4805901Z [36;1m    sleep $SLEEP_INTERVAL[0m
2026-02-22T18:13:58.4806147Z [36;1m  fi[0m
2026-02-22T18:13:58.4806362Z [36;1mdone[0m
2026-02-22T18:13:58.4806674Z shell: bash -el {0}
2026-02-22T18:13:58.4806892Z ##[endgroup]
2026-02-22T18:13:58.4894064Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-22T18:13:58.4894989Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-22T18:13:58.4895346Z ##[endgroup]
2026-02-22T18:13:58.8415876Z (node:525) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-22T18:13:58.8416654Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-22T18:13:59.3928421Z Deleting leaderworkerset [vllm] in namespace [vllm-project]...
2026-02-22T18:13:59.4776025Z Waiting for all pods starting with 'vllm' to be deleted...
2026-02-22T18:13:59.5496785Z All vllm pods deleted.
2026-02-22T18:13:59.9508346Z ##[group]Run set -e
2026-02-22T18:13:59.9508591Z [36;1mset -e[0m
2026-02-22T18:13:59.9508737Z [36;1m[0m
2026-02-22T18:13:59.9508866Z [36;1msize="2"[0m
2026-02-22T18:13:59.9509017Z [36;1mreplicas="1"[0m
2026-02-22T18:13:59.9509348Z [36;1mimage="swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3"[0m
2026-02-22T18:13:59.9509776Z [36;1mconfig_file_path="DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-22T18:13:59.9510103Z [36;1mfail_tag=FAIL_TAG_"DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-22T18:13:59.9510391Z [36;1mecho "FAIL_TAG=${fail_tag}" >> "$GITHUB_ENV"[0m
2026-02-22T18:13:59.9510594Z [36;1m[0m
2026-02-22T18:13:59.9510813Z [36;1mrequired_params=("size" "replicas" "image" "config_file_path")[0m
2026-02-22T18:13:59.9511115Z [36;1mfor param in "${required_params[@]}"; do[0m
2026-02-22T18:13:59.9511336Z [36;1m  if [ -z "${!param}" ]; then[0m
2026-02-22T18:13:59.9511792Z [36;1m    echo "Error: Parameter '$param' is required but empty"[0m
2026-02-22T18:13:59.9512148Z [36;1m    exit 1[0m
2026-02-22T18:13:59.9512300Z [36;1m  fi[0m
2026-02-22T18:13:59.9512435Z [36;1mdone[0m
2026-02-22T18:13:59.9512560Z [36;1m[0m
2026-02-22T18:13:59.9512762Z [36;1mif [ "a3" = "a3" ]; then[0m
2026-02-22T18:13:59.9512952Z [36;1m  npu_per_node=16[0m
2026-02-22T18:13:59.9513224Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws.yaml.jinja2"[0m
2026-02-22T18:13:59.9513510Z [36;1melse[0m
2026-02-22T18:13:59.9513650Z [36;1m  npu_per_node=8[0m
2026-02-22T18:13:59.9513921Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws-a2.yaml.jinja2"[0m
2026-02-22T18:13:59.9514204Z [36;1mfi[0m
2026-02-22T18:13:59.9514327Z [36;1m[0m
2026-02-22T18:13:59.9514480Z [36;1mjinja2 $TEMPLATE_FILE \[0m
2026-02-22T18:13:59.9514670Z [36;1m  -D size="$size" \[0m
2026-02-22T18:13:59.9514847Z [36;1m  -D replicas="$replicas" \[0m
2026-02-22T18:13:59.9515031Z [36;1m  -D image="$image" \[0m
2026-02-22T18:13:59.9515251Z [36;1m  -D config_file_path="$config_file_path" \[0m
2026-02-22T18:13:59.9515485Z [36;1m  -D npu_per_node="$npu_per_node" \[0m
2026-02-22T18:13:59.9515683Z [36;1m  -D fail_tag="$fail_tag" \[0m
2026-02-22T18:13:59.9515871Z [36;1m  --outfile lws.yaml[0m
2026-02-22T18:13:59.9516134Z [36;1m[0m
2026-02-22T18:13:59.9516267Z [36;1mkubectl apply -f ./lws.yaml[0m
2026-02-22T18:13:59.9516584Z shell: bash -el {0}
2026-02-22T18:13:59.9516725Z ##[endgroup]
2026-02-22T18:13:59.9604944Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-22T18:13:59.9605688Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-22T18:13:59.9605899Z ##[endgroup]
2026-02-22T18:14:00.3135181Z (node:591) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-22T18:14:00.3136157Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-22T18:14:01.2324477Z leaderworkerset.leaderworkerset.x-k8s.io/vllm created
2026-02-22T18:14:01.2561823Z service/vllm-leader created
2026-02-22T18:14:03.0358969Z ##[group]Run POD_PREFIX="${POD_PREFIX:-vllm-0}"
2026-02-22T18:14:03.0359276Z [36;1mPOD_PREFIX="${POD_PREFIX:-vllm-0}"[0m
2026-02-22T18:14:03.0359477Z [36;1mSIZE="2"[0m
2026-02-22T18:14:03.0359646Z [36;1mTIMEOUT=1200  # default timeout 20 minutes[0m
2026-02-22T18:14:03.0359861Z [36;1m[0m
2026-02-22T18:14:03.0360171Z [36;1mecho "Waiting for Pods in namespace [$NAMESPACE] to become Running and Ready (timeout ${TIMEOUT}s)..."[0m
2026-02-22T18:14:03.0360516Z [36;1m[0m
2026-02-22T18:14:03.0360658Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-22T18:14:03.0360834Z [36;1m[0m
2026-02-22T18:14:03.0360960Z [36;1mwhile true; do[0m
2026-02-22T18:14:03.0361126Z [36;1m  NOW=$(date +%s)[0m
2026-02-22T18:14:03.0361310Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-22T18:14:03.0361522Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-22T18:14:03.0361773Z [36;1m    echo "Timeout reached after ${ELAPSED}s"[0m
2026-02-22T18:14:03.0362211Z [36;1m    echo "Dumping pod status for debugging:"[0m
2026-02-22T18:14:03.0362447Z [36;1m    kubectl get pods -n "$NAMESPACE"[0m
2026-02-22T18:14:03.0362703Z [36;1m    kubectl describe pod "$LEADER_POD" -n "$NAMESPACE"[0m
2026-02-22T18:14:03.0362936Z [36;1m    exit 1[0m
2026-02-22T18:14:03.0363075Z [36;1m  fi[0m
2026-02-22T18:14:03.0363211Z [36;1m[0m
2026-02-22T18:14:03.0363354Z [36;1m  # 1) check follower pods[0m
2026-02-22T18:14:03.0363603Z [36;1m  ALL_FOLLOWERS_READY=true[0m
2026-02-22T18:14:03.0363805Z [36;1m  for ((i=1; i<SIZE; i++)); do[0m
2026-02-22T18:14:03.0364002Z [36;1m    POD="${POD_PREFIX}-${i}"[0m
2026-02-22T18:14:03.0364381Z [36;1m    PHASE=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-22T18:14:03.0364944Z [36;1m    READY=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-22T18:14:03.0365445Z [36;1m[0m
2026-02-22T18:14:03.0365625Z [36;1m    echo "Follower [$POD] phase=$PHASE ready=$READY"[0m
2026-02-22T18:14:03.0365848Z [36;1m[0m
2026-02-22T18:14:03.0366027Z [36;1m    if [[ "$PHASE" != "Running" || "$READY" != "true" ]]; then[0m
2026-02-22T18:14:03.0366305Z [36;1m      echo "Follower [$POD] not Ready yet..."[0m
2026-02-22T18:14:03.0366532Z [36;1m      ALL_FOLLOWERS_READY=false[0m
2026-02-22T18:14:03.0366715Z [36;1m      break[0m
2026-02-22T18:14:03.0366861Z [36;1m    fi[0m
2026-02-22T18:14:03.0366998Z [36;1m  done[0m
2026-02-22T18:14:03.0367123Z [36;1m[0m
2026-02-22T18:14:03.0367257Z [36;1m  # 2) check leader pod[0m
2026-02-22T18:14:03.0367656Z [36;1m  LEADER_PHASE=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-22T18:14:03.0368266Z [36;1m  LEADER_READY=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-22T18:14:03.0368675Z [36;1m[0m
2026-02-22T18:14:03.0368896Z [36;1m  echo "Leader [$LEADER_POD] phase=$LEADER_PHASE ready=$LEADER_READY"[0m
2026-02-22T18:14:03.0369159Z [36;1m[0m
2026-02-22T18:14:03.0369368Z [36;1m  if [[ "$LEADER_PHASE" != "Running" || "$LEADER_READY" != "true" ]]; then[0m
2026-02-22T18:14:03.0369655Z [36;1m    echo "Leader not Ready yet..."[0m
2026-02-22T18:14:03.0369858Z [36;1m    ALL_FOLLOWERS_READY=false[0m
2026-02-22T18:14:03.0370041Z [36;1m  fi[0m
2026-02-22T18:14:03.0370169Z [36;1m[0m
2026-02-22T18:14:03.0370338Z [36;1m  if [[ "$ALL_FOLLOWERS_READY" == "true" ]]; then[0m
2026-02-22T18:14:03.0370669Z [36;1m    echo "All follower pods and leader pod are Running and Ready â€” continuing."[0m
2026-02-22T18:14:03.0370953Z [36;1m    break[0m
2026-02-22T18:14:03.0371097Z [36;1m  fi[0m
2026-02-22T18:14:03.0371230Z [36;1m[0m
2026-02-22T18:14:03.0371351Z [36;1m  sleep 2[0m
2026-02-22T18:14:03.0371493Z [36;1mdone[0m
2026-02-22T18:14:03.0371767Z shell: bash -el {0}
2026-02-22T18:14:03.0371904Z env:
2026-02-22T18:14:03.0372361Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-22T18:14:03.0372594Z ##[endgroup]
2026-02-22T18:14:03.0473775Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-22T18:14:03.0474478Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-22T18:14:03.0474718Z ##[endgroup]
2026-02-22T18:14:03.3990499Z (node:702) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-22T18:14:03.3991222Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-22T18:14:03.9311103Z Waiting for Pods in namespace [vllm-project] to become Running and Ready (timeout 1200s)...
2026-02-22T18:14:04.0538425Z Follower [vllm-0-1] phase=Pending ready=
2026-02-22T18:14:04.1719822Z Follower [vllm-0-1] not Ready yet...
2026-02-22T18:14:04.1720150Z Leader [vllm-0] phase=Pending ready=
2026-02-22T18:14:04.1720474Z Leader not Ready yet...
2026-02-22T18:14:06.2971903Z Follower [vllm-0-1] phase=Pending ready=
2026-02-22T18:14:06.2972442Z Follower [vllm-0-1] not Ready yet...
2026-02-22T18:14:06.4086695Z Leader [vllm-0] phase=Pending ready=
2026-02-22T18:14:06.4087071Z Leader not Ready yet...
2026-02-22T18:14:08.5322814Z Follower [vllm-0-1] phase=Pending ready=
2026-02-22T18:14:08.5323165Z Follower [vllm-0-1] not Ready yet...
2026-02-22T18:14:08.6472880Z Leader [vllm-0] phase=Pending ready=
2026-02-22T18:14:08.6473219Z Leader not Ready yet...
2026-02-22T18:14:10.7699555Z Follower [vllm-0-1] phase=Pending ready=
2026-02-22T18:14:10.7699911Z Follower [vllm-0-1] not Ready yet...
2026-02-22T18:14:10.8791190Z Leader [vllm-0] phase=Pending ready=
2026-02-22T18:14:10.8791547Z Leader not Ready yet...
2026-02-22T18:14:13.0085896Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-22T18:14:13.0086294Z Follower [vllm-0-1] not Ready yet...
2026-02-22T18:14:13.1394490Z Leader [vllm-0] phase=Pending ready=
2026-02-22T18:14:13.1395009Z Leader not Ready yet...
2026-02-22T18:14:15.2673873Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:14:15.3887757Z Leader [vllm-0] phase=Pending ready=false
2026-02-22T18:14:15.3888068Z Leader not Ready yet...
2026-02-22T18:14:17.5100280Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:14:17.6245501Z Leader [vllm-0] phase=Pending ready=false
2026-02-22T18:14:17.6245860Z Leader not Ready yet...
2026-02-22T18:14:19.7459749Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:14:19.8696583Z Leader [vllm-0] phase=Pending ready=false
2026-02-22T18:14:19.8696895Z Leader not Ready yet...
2026-02-22T18:14:21.9897961Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:14:22.1106828Z Leader [vllm-0] phase=Pending ready=false
2026-02-22T18:14:22.1107160Z Leader not Ready yet...
2026-02-22T18:14:24.2397351Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:14:24.3602677Z Leader [vllm-0] phase=Pending ready=false
2026-02-22T18:14:24.3603000Z Leader not Ready yet...
2026-02-22T18:14:26.4808742Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:14:26.5993124Z Leader [vllm-0] phase=Pending ready=false
2026-02-22T18:14:26.5993402Z Leader not Ready yet...
2026-02-22T18:14:28.7134987Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:14:28.8265555Z Leader [vllm-0] phase=Pending ready=false
2026-02-22T18:14:28.8265785Z Leader not Ready yet...
2026-02-22T18:14:30.9445496Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:14:31.0596743Z Leader [vllm-0] phase=Pending ready=false
2026-02-22T18:14:31.0597008Z Leader not Ready yet...
2026-02-22T18:14:33.1842527Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:14:33.2995458Z Leader [vllm-0] phase=Pending ready=false
2026-02-22T18:14:33.2995762Z Leader not Ready yet...
2026-02-22T18:14:35.4189916Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:14:35.5412984Z Leader [vllm-0] phase=Pending ready=false
2026-02-22T18:14:35.5413209Z Leader not Ready yet...
2026-02-22T18:14:37.6561464Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:14:37.7717746Z Leader [vllm-0] phase=Pending ready=false
2026-02-22T18:14:37.7718333Z Leader not Ready yet...
2026-02-22T18:14:39.8890980Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:14:40.0138292Z Leader [vllm-0] phase=Pending ready=false
2026-02-22T18:14:40.0138548Z Leader not Ready yet...
2026-02-22T18:14:42.1407552Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:14:42.2500389Z Leader [vllm-0] phase=Pending ready=false
2026-02-22T18:14:42.2500641Z Leader not Ready yet...
2026-02-22T18:14:44.3755128Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:14:44.4952242Z Leader [vllm-0] phase=Pending ready=false
2026-02-22T18:14:44.4952483Z Leader not Ready yet...
2026-02-22T18:14:46.6149476Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:14:46.7303944Z Leader [vllm-0] phase=Pending ready=false
2026-02-22T18:14:46.7304181Z Leader not Ready yet...
2026-02-22T18:14:48.8436258Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:14:48.9725433Z Leader [vllm-0] phase=Pending ready=false
2026-02-22T18:14:48.9725674Z Leader not Ready yet...
2026-02-22T18:14:51.0907134Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:14:51.2022639Z Leader [vllm-0] phase=Pending ready=false
2026-02-22T18:14:51.2022894Z Leader not Ready yet...
2026-02-22T18:14:53.3248110Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:14:53.4397450Z Leader [vllm-0] phase=Pending ready=false
2026-02-22T18:14:53.4397716Z Leader not Ready yet...
2026-02-22T18:14:55.5645057Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:14:55.6839866Z Leader [vllm-0] phase=Pending ready=false
2026-02-22T18:14:55.6840120Z Leader not Ready yet...
2026-02-22T18:14:57.8070087Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:14:57.9255903Z Leader [vllm-0] phase=Pending ready=false
2026-02-22T18:14:57.9256120Z Leader not Ready yet...
2026-02-22T18:15:00.0685494Z Follower [vllm-0-1] phase=Running ready=true
2026-02-22T18:15:00.1998710Z Leader [vllm-0] phase=Running ready=true
2026-02-22T18:15:00.1999361Z All follower pods and leader pod are Running and Ready â€” continuing.
2026-02-22T18:15:00.6183731Z ##[group]Run set -euo pipefail
2026-02-22T18:15:00.6183987Z [36;1mset -euo pipefail[0m
2026-02-22T18:15:00.6184148Z [36;1m[0m
2026-02-22T18:15:00.6184287Z [36;1msize="2"[0m
2026-02-22T18:15:00.6184432Z [36;1mpids=()[0m
2026-02-22T18:15:00.6184564Z [36;1m[0m
2026-02-22T18:15:00.6184696Z [36;1mcleanup() {[0m
2026-02-22T18:15:00.6184891Z [36;1m  echo "Cleaning up background log streams..."[0m
2026-02-22T18:15:00.6185136Z [36;1m  for pid in "${pids[@]}"; do[0m
2026-02-22T18:15:00.6185347Z [36;1m    kill "$pid" 2>/dev/null || true[0m
2026-02-22T18:15:00.6185537Z [36;1m  done[0m
2026-02-22T18:15:00.6185675Z [36;1m}[0m
2026-02-22T18:15:00.6185815Z [36;1mtrap cleanup EXIT[0m
2026-02-22T18:15:00.6185970Z [36;1m[0m
2026-02-22T18:15:00.6186115Z [36;1mfor i in $(seq 1 $((size - 1))); do[0m
2026-02-22T18:15:00.6186328Z [36;1m  POD="vllm-0-${i}"[0m
2026-02-22T18:15:00.6186482Z [36;1m[0m
2026-02-22T18:15:00.6186697Z [36;1m  echo "==== Collecting logs from worker pod: $POD ===="[0m
2026-02-22T18:15:00.6186971Z [36;1m  kubectl logs -f "$POD" -n "$NAMESPACE" \[0m
2026-02-22T18:15:00.6187206Z [36;1m    > "/tmp/${POD}_logs.txt" 2>&1 &[0m
2026-02-22T18:15:00.6187387Z [36;1m[0m
2026-02-22T18:15:00.6187518Z [36;1m  pids+=($!)[0m
2026-02-22T18:15:00.6187673Z [36;1mdone[0m
2026-02-22T18:15:00.6187801Z [36;1m[0m
2026-02-22T18:15:00.6187998Z [36;1mecho "==== Streaming logs from leader pod: $LEADER_POD ===="[0m
2026-02-22T18:15:00.6188287Z [36;1mecho "Looking for logs containing: $FAIL_TAG"[0m
2026-02-22T18:15:00.6188493Z [36;1m[0m
2026-02-22T18:15:00.6188728Z [36;1mkubectl logs -f "$LEADER_POD" -n "$NAMESPACE" | while IFS= read -r line; do[0m
2026-02-22T18:15:00.6189098Z [36;1m  echo "$line"[0m
2026-02-22T18:15:00.6189292Z [36;1m  if echo "$line" | grep -q "$FAIL_TAG"; then[0m
2026-02-22T18:15:00.6189513Z [36;1m    exit 1[0m
2026-02-22T18:15:00.6189652Z [36;1m  fi[0m
2026-02-22T18:15:00.6189786Z [36;1mdone[0m
2026-02-22T18:15:00.6190091Z shell: bash -el {0}
2026-02-22T18:15:00.6190232Z env:
2026-02-22T18:15:00.6190422Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-22T18:15:00.6190653Z ##[endgroup]
2026-02-22T18:15:00.6278531Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-22T18:15:00.6279269Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-22T18:15:00.6279485Z ##[endgroup]
2026-02-22T18:15:00.9799603Z (node:804) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-22T18:15:00.9800322Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-22T18:15:01.5205285Z ==== Collecting logs from worker pod: vllm-0-1 ====
2026-02-22T18:15:01.5205791Z ==== Streaming logs from leader pod: vllm-0 ====
2026-02-22T18:15:01.5206132Z Looking for logs containing: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-22T18:15:01.5977709Z /usr/local/Ascend/ascend-toolkit/set_env.sh: line 31: CMAKE_PREFIX_PATH: unbound variable
2026-02-22T18:15:01.5990981Z ====> Check NPU info
2026-02-22T18:15:01.6002381Z +------------------------------------------------------------------------------------------------+
2026-02-22T18:15:01.6013726Z | npu-smi 25.5.0                   Version: 25.5.0                                               |
2026-02-22T18:15:01.6023692Z +---------------------------+---------------+----------------------------------------------------+
2026-02-22T18:15:01.6034320Z | NPU   Name                | Health        | Power(W)    Temp(C)           Hugepages-Usage(page)|
2026-02-22T18:15:01.6045859Z | Chip  Phy-ID              | Bus-Id        | AICore(%)   Memory-Usage(MB)  HBM-Usage(MB)        |
2026-02-22T18:15:01.6056468Z +===========================+===============+====================================================+
2026-02-22T18:15:01.6066267Z | 0     Ascend910           | OK            | 167.4       36                0    / 0             |
2026-02-22T18:15:01.6076679Z | 0     0                   | 0000:9D:00.0  | 0           0    / 0          3169 / 65536         |
2026-02-22T18:15:01.6087662Z +------------------------------------------------------------------------------------------------+
2026-02-22T18:15:01.6097623Z | 0     Ascend910           | OK            | -           36                0    / 0             |
2026-02-22T18:15:01.6108629Z | 1     1                   | 0000:9F:00.0  | 0           0    / 0          2887 / 65536         |
2026-02-22T18:15:01.6119707Z +===========================+===============+====================================================+
2026-02-22T18:15:01.6129442Z | 1     Ascend910           | OK            | 166.9       35                0    / 0             |
2026-02-22T18:15:01.6140208Z | 0     2                   | 0000:99:00.0  | 0           0    / 0          3155 / 65536         |
2026-02-22T18:15:01.6150508Z +------------------------------------------------------------------------------------------------+
2026-02-22T18:15:01.6162254Z | 1     Ascend910           | OK            | -           36                0    / 0             |
2026-02-22T18:15:01.6173143Z | 1     3                   | 0000:9B:00.0  | 0           0    / 0          2892 / 65536         |
2026-02-22T18:15:01.6184101Z +===========================+===============+====================================================+
2026-02-22T18:15:01.6195693Z | 2     Ascend910           | OK            | 166.7       36                0    / 0             |
2026-02-22T18:15:01.6207476Z | 0     4                   | 0000:95:00.0  | 0           0    / 0          3164 / 65536         |
2026-02-22T18:15:01.6218573Z +------------------------------------------------------------------------------------------------+
2026-02-22T18:15:01.6229839Z | 2     Ascend910           | OK            | -           35                0    / 0             |
2026-02-22T18:15:01.6242305Z | 1     5                   | 0000:97:00.0  | 0           0    / 0          2885 / 65536         |
2026-02-22T18:15:01.6253601Z +===========================+===============+====================================================+
2026-02-22T18:15:01.6264921Z | 3     Ascend910           | OK            | 155.8       36                0    / 0             |
2026-02-22T18:15:01.6276490Z | 0     6                   | 0000:91:00.0  | 0           0    / 0          3160 / 65536         |
2026-02-22T18:15:01.6288671Z +------------------------------------------------------------------------------------------------+
2026-02-22T18:15:01.6299773Z | 3     Ascend910           | OK            | -           35                0    / 0             |
2026-02-22T18:15:01.6310650Z | 1     7                   | 0000:93:00.0  | 0           0    / 0          2888 / 65536         |
2026-02-22T18:15:01.6322467Z +===========================+===============+====================================================+
2026-02-22T18:15:01.6331935Z | 4     Ascend910           | OK            | 163.9       36                0    / 0             |
2026-02-22T18:15:01.6342209Z | 0     8                   | 0000:8D:00.0  | 0           0    / 0          3168 / 65536         |
2026-02-22T18:15:01.6352255Z +------------------------------------------------------------------------------------------------+
2026-02-22T18:15:01.6361953Z | 4     Ascend910           | OK            | -           35                0    / 0             |
2026-02-22T18:15:01.6371530Z | 1     9                   | 0000:8F:00.0  | 0           0    / 0          2888 / 65536         |
2026-02-22T18:15:01.6382316Z +===========================+===============+====================================================+
2026-02-22T18:15:01.6392552Z | 5     Ascend910           | OK            | 168.9       36                0    / 0             |
2026-02-22T18:15:01.6401869Z | 0     10                  | 0000:89:00.0  | 0           0    / 0          3150 / 65536         |
2026-02-22T18:15:01.6411720Z +------------------------------------------------------------------------------------------------+
2026-02-22T18:15:01.6421950Z | 5     Ascend910           | OK            | -           37                0    / 0             |
2026-02-22T18:15:01.6432198Z | 1     11                  | 0000:8B:00.0  | 0           0    / 0          2900 / 65536         |
2026-02-22T18:15:01.6442428Z +===========================+===============+====================================================+
2026-02-22T18:15:01.6451626Z | 6     Ascend910           | OK            | 164.6       36                0    / 0             |
2026-02-22T18:15:01.6462085Z | 0     12                  | 0000:85:00.0  | 0           0    / 0          3165 / 65536         |
2026-02-22T18:15:01.6471917Z +------------------------------------------------------------------------------------------------+
2026-02-22T18:15:01.6482370Z | 6     Ascend910           | OK            | -           36                0    / 0             |
2026-02-22T18:15:01.6549518Z | 1     13                  | 0000:87:00.0  | 0           0    / 0          2889 / 65536         |
2026-02-22T18:15:01.6549816Z +===========================+===============+====================================================+
2026-02-22T18:15:01.6550113Z | 7     Ascend910           | OK            | 164.4       34                0    / 0             |
2026-02-22T18:15:01.6550426Z | 0     14                  | 0000:81:00.0  | 0           0    / 0          3151 / 65536         |
2026-02-22T18:15:01.6550764Z +------------------------------------------------------------------------------------------------+
2026-02-22T18:15:01.6552285Z | 7     Ascend910           | OK            | -           36                0    / 0             |
2026-02-22T18:15:01.6553856Z | 1     15                  | 0000:83:00.0  | 0           0    / 0          2899 / 65536         |
2026-02-22T18:15:01.6564719Z +===========================+===============+====================================================+
2026-02-22T18:15:01.6574895Z +---------------------------+---------------+----------------------------------------------------+
2026-02-22T18:15:01.6585021Z | NPU     Chip              | Process id    | Process name             | Process memory(MB)      |
2026-02-22T18:15:01.6595381Z +===========================+===============+====================================================+
2026-02-22T18:15:01.6606503Z | No running processes found in NPU 0                                                            |
2026-02-22T18:15:01.6616604Z +===========================+===============+====================================================+
2026-02-22T18:15:01.6626745Z | No running processes found in NPU 1                                                            |
2026-02-22T18:15:01.6637212Z +===========================+===============+====================================================+
2026-02-22T18:15:01.6647710Z | No running processes found in NPU 2                                                            |
2026-02-22T18:15:01.6657989Z +===========================+===============+====================================================+
2026-02-22T18:15:01.6668515Z | No running processes found in NPU 3                                                            |
2026-02-22T18:15:01.6678832Z +===========================+===============+====================================================+
2026-02-22T18:15:01.6689179Z | No running processes found in NPU 4                                                            |
2026-02-22T18:15:01.6698863Z +===========================+===============+====================================================+
2026-02-22T18:15:01.6708770Z | No running processes found in NPU 5                                                            |
2026-02-22T18:15:01.6719099Z +===========================+===============+====================================================+
2026-02-22T18:15:01.6729392Z | No running processes found in NPU 6                                                            |
2026-02-22T18:15:01.6739203Z +===========================+===============+====================================================+
2026-02-22T18:15:01.6749731Z | No running processes found in NPU 7                                                            |
2026-02-22T18:15:01.6760544Z +===========================+===============+====================================================+
2026-02-22T18:15:01.6771842Z package_name=Ascend-cann-toolkit
2026-02-22T18:15:01.6783663Z version=8.5.0
2026-02-22T18:15:01.6796807Z innerversion=V100R001C25SPC001B232
2026-02-22T18:15:01.6808033Z compatible_version=[V100R001C15],[V100R001C18],[V100R001C19],[V100R001C20],[V100R001C21],[V100R001C23]
2026-02-22T18:15:01.6817550Z arch=aarch64
2026-02-22T18:15:01.6827859Z os=linux
2026-02-22T18:15:01.6837508Z path=/usr/local/Ascend/cann-8.5.0
2026-02-22T18:15:01.6848013Z ====> Configure mirrors and git proxy
2026-02-22T18:15:01.6858613Z Writing to /root/.config/pip/pip.conf
2026-02-22T18:15:01.6868726Z Installed vLLM-related Python packages:
2026-02-22T18:15:02.0039206Z ais_bench_benchmark               3.0.20250930                /vllm-workspace/vllm-ascend/benchmark
2026-02-22T18:15:02.0093397Z vllm                              0.15.0+empty                /vllm-workspace/vllm
2026-02-22T18:15:02.0103083Z vllm_ascend                       0.14.0rc2.dev171+gf0caeeadc /vllm-workspace/vllm-ascend
2026-02-22T18:15:02.0112966Z 
2026-02-22T18:15:02.0124143Z ============================
2026-02-22T18:15:02.0132950Z vLLM Git information
2026-02-22T18:15:02.0142152Z ============================
2026-02-22T18:15:02.0152054Z Branch:      HEAD
2026-02-22T18:15:02.0163369Z Commit hash: f176443446f659dbab5315e056e605d8984fd976
2026-02-22T18:15:02.0173795Z Author:      TJian <tunjian.tan@embeddedllm.com>
2026-02-22T18:15:02.0196215Z Date:        2026-01-29 14:45:42 +0800
2026-02-22T18:15:02.0221119Z Message:     [Release] [CI] Optim release pipeline (#33156)
2026-02-22T18:15:02.0248153Z Tags:        v0.15.0
2026-02-22T18:15:02.0309741Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm.git (fetch)
2026-02-22T18:15:02.0320017Z 
2026-02-22T18:15:02.0329451Z 
2026-02-22T18:15:02.0339100Z ============================
2026-02-22T18:15:02.0348455Z vLLM-Ascend Git information
2026-02-22T18:15:02.0357951Z ============================
2026-02-22T18:15:02.0367999Z Branch:      main
2026-02-22T18:15:02.0377065Z Commit hash: f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-22T18:15:02.0413962Z Author:      Nengjun Ma <nengjunma@outlook.com>
2026-02-22T18:15:02.0438709Z Date:        2026-02-14 18:54:04 +0800
2026-02-22T18:15:02.0463164Z Message:     [CI] unlock when load model (#6771)
2026-02-22T18:15:02.0539230Z Tags:        
2026-02-22T18:15:02.0539895Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm-ascend (fetch)
2026-02-22T18:15:02.0545366Z 
2026-02-22T18:15:02.0641554Z ====> Check triton ascend info
2026-02-22T18:15:02.1285881Z Ubuntu clang version 15.0.7
2026-02-22T18:15:02.1289300Z Target: aarch64-unknown-linux-gnu
2026-02-22T18:15:02.1297325Z Thread model: posix
2026-02-22T18:15:02.1306493Z InstalledDir: /usr/bin
2026-02-22T18:15:02.1317290Z Found candidate GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-22T18:15:02.1325917Z Selected GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-22T18:15:02.1335209Z Candidate multilib: .;@m64
2026-02-22T18:15:02.1346545Z Selected multilib: .;@m64
2026-02-22T18:15:02.1393909Z /usr/local/Ascend/cann-8.5.0/tools/bishengir/bin/bishengir-compile
2026-02-22T18:15:02.9809795Z Name: triton-ascend
2026-02-22T18:15:02.9888932Z Version: 3.2.0
2026-02-22T18:15:02.9898883Z Summary: A language and compiler for custom Deep Learning operations on Ascend hardwares
2026-02-22T18:15:02.9908847Z Home-page: https://gitcode.com/Ascend/triton-ascend/
2026-02-22T18:15:02.9917706Z Author: 
2026-02-22T18:15:02.9927720Z Author-email: 
2026-02-22T18:15:02.9938130Z License: 
2026-02-22T18:15:02.9948108Z Location: /usr/local/python3.11.14/lib/python3.11/site-packages
2026-02-22T18:15:02.9957459Z Requires: 
2026-02-22T18:15:02.9967883Z Required-by: vllm_ascend
2026-02-22T18:15:22.1397728Z INFO 02-22 18:15:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:15:22.1406994Z INFO 02-22 18:15:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:15:22.1417397Z INFO 02-22 18:15:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:15:22.1880683Z INFO 02-22 18:15:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:15:28.9326652Z ============================= test session starts ==============================
2026-02-22T18:15:28.9335563Z platform linux -- Python 3.11.14, pytest-8.4.2, pluggy-1.6.0 -- /usr/local/python3.11.14/bin/python3
2026-02-22T18:15:28.9345624Z cachedir: .pytest_cache
2026-02-22T18:15:28.9356910Z rootdir: /vllm-workspace/vllm-ascend
2026-02-22T18:15:28.9368256Z configfile: pyproject.toml
2026-02-22T18:15:28.9377549Z plugins: asyncio-1.3.0, cov-7.0.0, mock-3.15.1, anyio-4.12.1
2026-02-22T18:15:28.9388928Z asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
2026-02-22T18:15:29.7936134Z collecting ... collected 1 item
2026-02-22T18:15:29.7943884Z 
2026-02-22T18:15:29.7956273Z [2026-02-22 18:15:29] INFO multi_node_config.py:294: Loading config yaml: tests/e2e/nightly/multi_node/config/DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-22T18:15:29.7996179Z [2026-02-22 18:15:29] INFO multi_node_config.py:351: Resolving cluster IPs via DNS...
2026-02-22T18:15:29.8052307Z [2026-02-22 18:15:29] INFO multi_node_config.py:212: Node 0 envs: {'HCCL_OP_EXPANSION_MODE': 'AIV', 'VLLM_USE_MODELSCOPE': 'True', 'HCCL_BUFFSIZE': '1024', 'SERVER_PORT': '8080', 'OMP_PROC_BIND': 'False', 'OMP_NUM_THREADS': '1', 'PYTORCH_NPU_ALLOC_CONF': 'expandable_segments:True', 'VLLM_ASCEND_ENABLE_FLASHCOMM1': '1', 'ASCEND_A3_EBA_ENABLE': '1', 'HCCL_IF_IP': '10.0.0.52', 'HCCL_SOCKET_IFNAME': 'eth0', 'GLOO_SOCKET_IFNAME': 'eth0', 'TP_SOCKET_IFNAME': 'eth0', 'LOCAL_IP': '10.0.0.52', 'NIC_NAME': 'eth0', 'MASTER_IP': '10.0.0.52'}
2026-02-22T18:15:29.8075664Z [2026-02-22 18:15:29] INFO multi_node_config.py:137: Not launching proxy on non-master node
2026-02-22T18:15:29.8089756Z [2026-02-22 18:15:29] INFO conftest.py:241: Starting server with command: vllm serve vllm-ascend/DeepSeek-V3.2-W8A8 --host 0.0.0.0 --port 8080 --data-parallel-size 4 --data-parallel-size-local 2 --data-parallel-address 10.0.0.52 --data-parallel-rpc-port 13399 --tensor-parallel-size 8 --quantization ascend --seed 1024 --enable-expert-parallel --max-num-seqs 16 --max-model-len 8192 --max-num-batched-tokens 4096 --no-enable-prefix-caching --gpu-memory-utilization 0.85 --trust-remote-code --speculative-config {"num_speculative_tokens": 2, "method":"deepseek_mtp"} --compilation-config {"cudagraph_capture_sizes": [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], "cudagraph_mode": "FULL_DECODE_ONLY"} --additional-config {"layer_sharding": ["q_b_proj", "o_proj"]} --tokenizer-mode deepseek_v32 --reasoning-parser deepseek_v3
2026-02-22T18:15:34.4102911Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node INFO 02-22 18:15:34 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:15:34.4106768Z INFO 02-22 18:15:34 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:15:34.4116973Z INFO 02-22 18:15:34 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:15:34.4167127Z INFO 02-22 18:15:34 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:15:40.8563573Z 2026-02-22 18:15:40,850 - 139 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:15:40.8842808Z INFO 02-22 18:15:40 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:15:41.0266978Z INFO 02-22 18:15:41 [serve.py:100] Defaulting api_server_count to data_parallel_size (4).
2026-02-22T18:15:41.0297092Z INFO 02-22 18:15:41 [utils.py:325] 
2026-02-22T18:15:41.0307265Z INFO 02-22 18:15:41 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
2026-02-22T18:15:41.0317294Z INFO 02-22 18:15:41 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
2026-02-22T18:15:41.0327008Z INFO 02-22 18:15:41 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   vllm-ascend/DeepSeek-V3.2-W8A8
2026-02-22T18:15:41.0337213Z INFO 02-22 18:15:41 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
2026-02-22T18:15:41.0347048Z INFO 02-22 18:15:41 [utils.py:325] 
2026-02-22T18:15:41.0367554Z INFO 02-22 18:15:41 [utils.py:261] non-default args: {'model_tag': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'api_server_count': 4, 'host': '0.0.0.0', 'port': 8080, 'model': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'tokenizer_mode': 'deepseek_v32', 'trust_remote_code': True, 'seed': 1024, 'max_model_len': 8192, 'quantization': 'ascend', 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 8, 'data_parallel_size': 4, 'data_parallel_size_local': 2, 'data_parallel_address': '10.0.0.52', 'data_parallel_rpc_port': 13399, 'enable_expert_parallel': True, 'gpu_memory_utilization': 0.85, 'enable_prefix_caching': False, 'max_num_batched_tokens': 4096, 'max_num_seqs': 16, 'speculative_config': {'num_speculative_tokens': 2, 'method': 'deepseek_mtp'}, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}, 'additional_config': {'layer_sharding': ['q_b_proj', 'o_proj']}}
2026-02-22T18:15:41.0744709Z 2026-02-22 18:15:41,072 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-22T18:15:41.0810518Z INFO 02-22 18:15:41 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-22T18:15:41.0832438Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:15:41.0869753Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:15:41.0882262Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:15:41.0911156Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-22T18:15:41.1100043Z INFO 02-22 18:15:41 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-22T18:15:41.1108930Z INFO 02-22 18:15:41 [model.py:1561] Using max model len 8192
2026-02-22T18:15:41.3762183Z WARNING 02-22 18:15:41 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-22T18:15:41.3783161Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:15:41.3793714Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:15:41.3803463Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-22T18:15:41.3878390Z INFO 02-22 18:15:41 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-22T18:15:41.3898684Z INFO 02-22 18:15:41 [model.py:1561] Using max model len 163840
2026-02-22T18:15:41.3910107Z WARNING 02-22 18:15:41 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-22T18:15:41.3920681Z INFO 02-22 18:15:41 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-22T18:15:41.7156728Z INFO 02-22 18:15:41 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-22T18:15:41.7164960Z INFO 02-22 18:15:41 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-22T18:15:41.7176374Z WARNING 02-22 18:15:41 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-22T18:15:41.7186736Z WARNING 02-22 18:15:41 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-22T18:15:41.7194851Z INFO 02-22 18:15:41 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:15:41.7205096Z INFO 02-22 18:15:41 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:15:41.7215756Z INFO 02-22 18:15:41 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:15:41.7225012Z WARNING 02-22 18:15:41 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-22T18:15:41.7234365Z INFO 02-22 18:15:41 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-22T18:15:41.7244510Z WARNING 02-22 18:15:41 [platform.py:335] [91m
2026-02-22T18:15:41.7254377Z WARNING 02-22 18:15:41 [platform.py:335]             **********************************************************************************
2026-02-22T18:15:41.7265144Z WARNING 02-22 18:15:41 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-22T18:15:41.7275250Z WARNING 02-22 18:15:41 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-22T18:15:41.7285299Z WARNING 02-22 18:15:41 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-22T18:15:41.7296782Z WARNING 02-22 18:15:41 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-22T18:15:41.7305686Z WARNING 02-22 18:15:41 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-22T18:15:41.7316065Z WARNING 02-22 18:15:41 [platform.py:335]             * batch size for graph capture.
2026-02-22T18:15:41.7326622Z WARNING 02-22 18:15:41 [platform.py:335]             * For more details, please refer to:
2026-02-22T18:15:41.7337659Z WARNING 02-22 18:15:41 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-22T18:15:41.7346784Z WARNING 02-22 18:15:41 [platform.py:335]             **********************************************************************************[0m
2026-02-22T18:15:41.7356543Z WARNING 02-22 18:15:41 [platform.py:335]             
2026-02-22T18:15:41.7366248Z INFO 02-22 18:15:41 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-22T18:15:41.7376169Z INFO 02-22 18:15:41 [utils.py:851] Started DP Coordinator process (PID: 152)
2026-02-22T18:15:46.1363340Z INFO 02-22 18:15:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:15:46.1371489Z INFO 02-22 18:15:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:15:46.1381844Z INFO 02-22 18:15:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:15:46.1430657Z INFO 02-22 18:15:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:15:46.3670805Z INFO 02-22 18:15:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:15:46.3679175Z INFO 02-22 18:15:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:15:46.3689702Z INFO 02-22 18:15:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:15:46.3744253Z INFO 02-22 18:15:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:15:56.2493402Z INFO 02-22 18:15:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:15:56.2502957Z INFO 02-22 18:15:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:15:56.2514342Z INFO 02-22 18:15:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:15:56.2563369Z INFO 02-22 18:15:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:16:01.5155355Z INFO 02-22 18:16:01 [utils.py:218] Started 4 API server processes
2026-02-22T18:16:01.7122882Z [0;36m(EngineCore_DP0 pid=155)[0;0m 2026-02-22 18:16:01,710 - 155 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:16:01.7151367Z [0;36m(EngineCore_DP1 pid=174)[0;0m 2026-02-22 18:16:01,712 - 174 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:16:01.7180195Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-22 18:16:01 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:16:01.7207567Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-22 18:16:01 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', speculative_config=SpeculativeConfig(method='mtp', model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', num_spec_tokens=2), tokenizer='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', skip_tokenizer_init=False, tokenizer_mode=deepseek_v32, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=4, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=bfloat16, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=1024, served_model_name=/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [24, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 48, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
2026-02-22T18:16:01.7249959Z [0;36m(EngineCore_DP1 pid=174)[0;0m INFO 02-22 18:16:01 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:16:06.1898772Z INFO 02-22 18:16:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:16:06.1909231Z INFO 02-22 18:16:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:16:06.1919669Z INFO 02-22 18:16:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:16:06.1974573Z INFO 02-22 18:16:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:16:06.5636623Z INFO 02-22 18:16:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:16:06.5657364Z INFO 02-22 18:16:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:16:06.5671123Z INFO 02-22 18:16:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:16:06.5722483Z INFO 02-22 18:16:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:16:06.5842476Z INFO 02-22 18:16:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:16:06.5854773Z INFO 02-22 18:16:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:16:06.5866622Z INFO 02-22 18:16:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:16:06.5877450Z INFO 02-22 18:16:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:16:06.5889148Z INFO 02-22 18:16:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:16:06.5899451Z INFO 02-22 18:16:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:16:06.5919915Z INFO 02-22 18:16:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:16:06.5932577Z INFO 02-22 18:16:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:16:06.7871414Z INFO 02-22 18:16:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:16:06.7880248Z INFO 02-22 18:16:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:16:06.7891233Z INFO 02-22 18:16:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:16:06.7985211Z INFO 02-22 18:16:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:16:06.8754722Z INFO 02-22 18:16:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:16:06.8763977Z INFO 02-22 18:16:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:16:06.8774810Z INFO 02-22 18:16:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:16:06.8835508Z INFO 02-22 18:16:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:16:11.5122331Z 2026-02-22 18:16:11,510 - 203 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:16:11.5159719Z INFO 02-22 18:16:11 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:16:11.8950837Z [0;36m(ApiServer_0 pid=185)[0;0m 2026-02-22 18:16:11,893 - 185 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:16:11.9105279Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:16:11 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:16:11.9369437Z [0;36m(ApiServer_0 pid=185)[0;0m 2026-02-22 18:16:11,935 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-22T18:16:11.9428555Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:16:11 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-22T18:16:12.0387361Z [0;36m(ApiServer_0 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:16:12.0410515Z [0;36m(ApiServer_0 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:16:12.0431296Z [0;36m(ApiServer_0 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:16:12.0442785Z [0;36m(ApiServer_0 pid=185)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-22T18:16:12.0519308Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:16:12 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-22T18:16:12.0539957Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:16:12 [model.py:1561] Using max model len 8192
2026-02-22T18:16:12.1606496Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:16:12 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-22T18:16:12.1690113Z [0;36m(ApiServer_0 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:16:12.1690803Z [0;36m(ApiServer_0 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:16:12.1691631Z [0;36m(ApiServer_0 pid=185)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-22T18:16:12.1703892Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:16:12 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-22T18:16:12.1713415Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:16:12 [model.py:1561] Using max model len 163840
2026-02-22T18:16:12.1724486Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:16:12 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-22T18:16:12.1735399Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:16:12 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-22T18:16:12.2865084Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:16:12 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-22T18:16:12.2873310Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:16:12 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-22T18:16:12.2895164Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:16:12 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-22T18:16:12.2908762Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:16:12 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-22T18:16:12.2920799Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:16:12 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:16:12.2930173Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:16:12 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:16:12.2942113Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:16:12 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:16:12.2953277Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:16:12 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-22T18:16:12.2963841Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:16:12 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-22T18:16:12.2973164Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:16:12 [platform.py:335] [91m
2026-02-22T18:16:12.2983351Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:16:12 [platform.py:335]             **********************************************************************************
2026-02-22T18:16:12.2993069Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:16:12 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-22T18:16:12.3004273Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:16:12 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-22T18:16:12.3014475Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:16:12 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-22T18:16:12.3024263Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:16:12 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-22T18:16:12.3034445Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:16:12 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-22T18:16:12.3044184Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:16:12 [platform.py:335]             * batch size for graph capture.
2026-02-22T18:16:12.3053835Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:16:12 [platform.py:335]             * For more details, please refer to:
2026-02-22T18:16:12.3063413Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:16:12 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-22T18:16:12.3073286Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:16:12 [platform.py:335]             **********************************************************************************[0m
2026-02-22T18:16:12.3082920Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:16:12 [platform.py:335]             
2026-02-22T18:16:12.3093435Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:16:12 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-22T18:16:12.3103187Z 2026-02-22 18:16:12,294 - 202 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:16:12.3114060Z INFO 02-22 18:16:12 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:16:12.6591427Z [0;36m(ApiServer_2 pid=187)[0;0m 2026-02-22 18:16:12,657 - 187 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:16:12.6739197Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:16:12 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:16:12.6902731Z [0;36m(ApiServer_2 pid=187)[0;0m 2026-02-22 18:16:12,688 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-22T18:16:12.7014182Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:16:12 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-22T18:16:12.7989500Z [0;36m(ApiServer_2 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:16:12.8016915Z [0;36m(ApiServer_2 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:16:12.8028072Z [0;36m(ApiServer_2 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:16:12.8038292Z [0;36m(ApiServer_2 pid=187)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-22T18:16:12.8079865Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:16:12 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-22T18:16:12.8101539Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:16:12 [model.py:1561] Using max model len 8192
2026-02-22T18:16:12.8917126Z [0;36m(ApiServer_1 pid=186)[0;0m 2026-02-22 18:16:12,889 - 186 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:16:12.9083615Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:16:12 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:16:12.9160329Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:16:12 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-22T18:16:12.9190022Z [0;36m(ApiServer_2 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:16:12.9200689Z [0;36m(ApiServer_2 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:16:12.9211456Z [0;36m(ApiServer_2 pid=187)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-22T18:16:12.9243844Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:16:12 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-22T18:16:12.9266622Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:16:12 [model.py:1561] Using max model len 163840
2026-02-22T18:16:12.9277130Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:16:12 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-22T18:16:12.9287258Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:16:12 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-22T18:16:12.9297020Z [0;36m(ApiServer_1 pid=186)[0;0m 2026-02-22 18:16:12,924 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-22T18:16:12.9326704Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:16:12 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-22T18:16:12.9885189Z [0;36m(ApiServer_3 pid=188)[0;0m 2026-02-22 18:16:12,983 - 188 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:16:13.0022209Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:16:13 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:16:13.0200514Z [0;36m(ApiServer_3 pid=188)[0;0m 2026-02-22 18:16:13,018 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-22T18:16:13.0314934Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:16:13 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-22T18:16:13.0337695Z [0;36m(ApiServer_1 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:16:13.0349460Z [0;36m(ApiServer_1 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:16:13.0361387Z [0;36m(ApiServer_1 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:16:13.0372209Z [0;36m(ApiServer_1 pid=186)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-22T18:16:13.0428868Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:16:13 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-22T18:16:13.0450576Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:16:13 [model.py:1561] Using max model len 8192
2026-02-22T18:16:13.0473571Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:16:13 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-22T18:16:13.0483286Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:16:13 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-22T18:16:13.0493941Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:16:13 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-22T18:16:13.0503782Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:16:13 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-22T18:16:13.0514011Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:16:13 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:16:13.0524613Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:16:13 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:16:13.0535450Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:16:13 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:16:13.0545177Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:16:13 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-22T18:16:13.0555687Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:16:13 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-22T18:16:13.0566340Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:16:13 [platform.py:335] [91m
2026-02-22T18:16:13.0576153Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             **********************************************************************************
2026-02-22T18:16:13.0586270Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-22T18:16:13.0597963Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-22T18:16:13.0606764Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-22T18:16:13.0616871Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-22T18:16:13.0626667Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-22T18:16:13.0637187Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * batch size for graph capture.
2026-02-22T18:16:13.0647541Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * For more details, please refer to:
2026-02-22T18:16:13.0657868Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-22T18:16:13.0666945Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             **********************************************************************************[0m
2026-02-22T18:16:13.0676311Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             
2026-02-22T18:16:13.0686194Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:16:13 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-22T18:16:13.1365863Z [0;36m(ApiServer_3 pid=188)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:16:13.1389376Z [0;36m(ApiServer_3 pid=188)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:16:13.1399744Z [0;36m(ApiServer_3 pid=188)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:16:13.1414248Z [0;36m(ApiServer_3 pid=188)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-22T18:16:13.1460114Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:16:13 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-22T18:16:13.1479733Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:16:13 [model.py:1561] Using max model len 8192
2026-02-22T18:16:13.1564269Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:16:13 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-22T18:16:13.1584299Z [0;36m(ApiServer_1 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:16:13.1594195Z [0;36m(ApiServer_1 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:16:13.1604230Z [0;36m(ApiServer_1 pid=186)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-22T18:16:13.1648066Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:16:13 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-22T18:16:13.1672080Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:16:13 [model.py:1561] Using max model len 163840
2026-02-22T18:16:13.1683173Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:16:13 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-22T18:16:13.1693561Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:16:13 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-22T18:16:13.2610312Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:16:13 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-22T18:16:13.2639896Z [0;36m(ApiServer_3 pid=188)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:16:13.2648159Z [0;36m(ApiServer_3 pid=188)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-22T18:16:13.2659687Z [0;36m(ApiServer_3 pid=188)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-22T18:16:13.2751637Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:16:13 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-22T18:16:13.2768665Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:16:13 [model.py:1561] Using max model len 163840
2026-02-22T18:16:13.2788245Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:16:13 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-22T18:16:13.2798458Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:16:13 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-22T18:16:13.2855867Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:16:13 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-22T18:16:13.2866691Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:16:13 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-22T18:16:13.2879385Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:16:13 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-22T18:16:13.2890454Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:16:13 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-22T18:16:13.2900052Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:16:13 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:16:13.2909483Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:16:13 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:16:13.2921135Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:16:13 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:16:13.2932409Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:16:13 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-22T18:16:13.2941590Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:16:13 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-22T18:16:13.2951036Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:16:13 [platform.py:335] [91m
2026-02-22T18:16:13.2961318Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             **********************************************************************************
2026-02-22T18:16:13.2972374Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-22T18:16:13.2984711Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-22T18:16:13.2994395Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-22T18:16:13.3004681Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-22T18:16:13.3015490Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-22T18:16:13.3025639Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * batch size for graph capture.
2026-02-22T18:16:13.3084006Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * For more details, please refer to:
2026-02-22T18:16:13.3084799Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-22T18:16:13.3085551Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             **********************************************************************************[0m
2026-02-22T18:16:13.3086070Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             
2026-02-22T18:16:13.3086563Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:16:13 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-22T18:16:13.3894963Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:16:13 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-22T18:16:13.3904298Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:16:13 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-22T18:16:13.3924442Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:16:13 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-22T18:16:13.3935958Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:16:13 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-22T18:16:13.3945303Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:16:13 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:16:13.3957175Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:16:13 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:16:13.3966376Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:16:13 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:16:13.3975585Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:16:13 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-22T18:16:13.3985611Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:16:13 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-22T18:16:13.3995155Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:16:13 [platform.py:335] [91m
2026-02-22T18:16:13.4006240Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             **********************************************************************************
2026-02-22T18:16:13.4016406Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-22T18:16:13.4026086Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-22T18:16:13.4036111Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-22T18:16:13.4046231Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-22T18:16:13.4056130Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-22T18:16:13.4066100Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * batch size for graph capture.
2026-02-22T18:16:13.4076365Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * For more details, please refer to:
2026-02-22T18:16:13.4087032Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-22T18:16:13.4096525Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             **********************************************************************************[0m
2026-02-22T18:16:13.4106679Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:16:13 [platform.py:335]             
2026-02-22T18:16:13.4116553Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:16:13 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-22T18:16:13.5329768Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-22T18:16:13.5336907Z   warnings.warn(
2026-02-22T18:16:13.7101463Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-22T18:16:13.7110305Z   warnings.warn(
2026-02-22T18:16:15.8524903Z INFO 02-22 18:16:15 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:16:15.8533199Z INFO 02-22 18:16:15 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:16:15.8544656Z INFO 02-22 18:16:15 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:16:15.8599490Z INFO 02-22 18:16:15 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:16:16.8042357Z INFO 02-22 18:16:16 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:16:16.8064635Z INFO 02-22 18:16:16 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:16:16.8076185Z INFO 02-22 18:16:16 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:16:16.8150536Z INFO 02-22 18:16:16 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:16:17.2642622Z INFO 02-22 18:16:17 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:16:17.2650165Z INFO 02-22 18:16:17 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:16:17.2660027Z INFO 02-22 18:16:17 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:16:17.2668972Z INFO 02-22 18:16:17 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:16:17.2679933Z INFO 02-22 18:16:17 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:16:17.2690861Z INFO 02-22 18:16:17 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:16:17.3522406Z INFO 02-22 18:16:17 [parallel_state.py:1212] world_size=32 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.52:49069 backend=hccl
2026-02-22T18:16:17.3530558Z INFO 02-22 18:16:17 [parallel_state.py:1212] world_size=32 rank=8 local_rank=0 distributed_init_method=tcp://10.0.0.52:49069 backend=hccl
2026-02-22T18:16:20.8614077Z 2026-02-22 18:16:20,857 - 252 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:16:20.8663664Z INFO 02-22 18:16:20 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:16:21.8300017Z 2026-02-22 18:16:21,827 - 258 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:16:21.8356404Z INFO 02-22 18:16:21 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:16:22.1953656Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-22T18:16:22.1959504Z   warnings.warn(
2026-02-22T18:16:23.1730934Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-22T18:16:23.1737765Z   warnings.warn(
2026-02-22T18:16:24.3298807Z INFO 02-22 18:16:24 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:16:24.3307652Z INFO 02-22 18:16:24 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:16:24.3317456Z INFO 02-22 18:16:24 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:16:24.7750420Z INFO 02-22 18:16:24 [parallel_state.py:1212] world_size=32 rank=1 local_rank=1 distributed_init_method=tcp://10.0.0.52:49069 backend=hccl
2026-02-22T18:16:25.2994580Z INFO 02-22 18:16:25 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:16:25.3004289Z INFO 02-22 18:16:25 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:16:25.3015352Z INFO 02-22 18:16:25 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:16:25.3724236Z INFO 02-22 18:16:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:16:25.3733891Z INFO 02-22 18:16:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:16:25.3743009Z INFO 02-22 18:16:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:16:25.3823937Z INFO 02-22 18:16:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:16:25.7215536Z INFO 02-22 18:16:25 [parallel_state.py:1212] world_size=32 rank=9 local_rank=1 distributed_init_method=tcp://10.0.0.52:49069 backend=hccl
2026-02-22T18:16:26.3028482Z INFO 02-22 18:16:26 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:16:26.3037658Z INFO 02-22 18:16:26 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:16:26.3049001Z INFO 02-22 18:16:26 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:16:26.3112969Z INFO 02-22 18:16:26 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:16:30.5523428Z 2026-02-22 18:16:30,549 - 370 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:16:30.5555479Z INFO 02-22 18:16:30 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:16:31.2736284Z 2026-02-22 18:16:31,271 - 373 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:16:31.2770236Z INFO 02-22 18:16:31 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:16:32.9601153Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-22T18:16:32.9609049Z   warnings.warn(
2026-02-22T18:16:33.4927836Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-22T18:16:33.4935081Z   warnings.warn(
2026-02-22T18:16:35.0190402Z INFO 02-22 18:16:35 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:16:35.0199436Z INFO 02-22 18:16:35 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:16:35.0211140Z INFO 02-22 18:16:35 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:16:35.4701815Z INFO 02-22 18:16:35 [parallel_state.py:1212] world_size=32 rank=2 local_rank=2 distributed_init_method=tcp://10.0.0.52:49069 backend=hccl
2026-02-22T18:16:35.4975750Z INFO 02-22 18:16:35 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:16:35.4984397Z INFO 02-22 18:16:35 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:16:35.4994103Z INFO 02-22 18:16:35 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:16:35.5054802Z INFO 02-22 18:16:35 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:16:35.6593745Z INFO 02-22 18:16:35 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:16:35.6602171Z INFO 02-22 18:16:35 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:16:35.6613265Z INFO 02-22 18:16:35 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:16:36.0880901Z INFO 02-22 18:16:36 [parallel_state.py:1212] world_size=32 rank=10 local_rank=2 distributed_init_method=tcp://10.0.0.52:49069 backend=hccl
2026-02-22T18:16:36.7995047Z INFO 02-22 18:16:36 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:16:36.8051427Z INFO 02-22 18:16:36 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:16:36.8061884Z INFO 02-22 18:16:36 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:16:36.8133544Z INFO 02-22 18:16:36 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:16:40.5618275Z 2026-02-22 18:16:40,559 - 474 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:16:40.5674709Z INFO 02-22 18:16:40 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:16:41.8692785Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-22T18:16:41.8700598Z   warnings.warn(
2026-02-22T18:16:42.0237098Z 2026-02-22 18:16:42,021 - 477 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:16:42.0276440Z INFO 02-22 18:16:42 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:16:43.3693608Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-22T18:16:43.3700859Z   warnings.warn(
2026-02-22T18:16:43.9724130Z INFO 02-22 18:16:43 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:16:43.9732398Z INFO 02-22 18:16:43 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:16:43.9743871Z INFO 02-22 18:16:43 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:16:44.4208787Z INFO 02-22 18:16:44 [parallel_state.py:1212] world_size=32 rank=3 local_rank=3 distributed_init_method=tcp://10.0.0.52:49069 backend=hccl
2026-02-22T18:16:45.1469817Z INFO 02-22 18:16:45 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:16:45.1477132Z INFO 02-22 18:16:45 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:16:45.1486328Z INFO 02-22 18:16:45 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:16:45.1590380Z INFO 02-22 18:16:45 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:16:45.6291699Z INFO 02-22 18:16:45 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:16:45.6299368Z INFO 02-22 18:16:45 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:16:45.6310768Z INFO 02-22 18:16:45 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:16:46.1084393Z INFO 02-22 18:16:46 [parallel_state.py:1212] world_size=32 rank=11 local_rank=3 distributed_init_method=tcp://10.0.0.52:49069 backend=hccl
2026-02-22T18:16:46.6226161Z INFO 02-22 18:16:46 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:16:46.6233217Z INFO 02-22 18:16:46 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:16:46.6243539Z INFO 02-22 18:16:46 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:16:46.6299637Z INFO 02-22 18:16:46 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:16:50.2347425Z 2026-02-22 18:16:50,232 - 578 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:16:50.2404610Z INFO 02-22 18:16:50 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:16:51.5541169Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-22T18:16:51.5546782Z   warnings.warn(
2026-02-22T18:16:51.6725002Z 2026-02-22 18:16:51,670 - 582 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:16:51.6759744Z INFO 02-22 18:16:51 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:16:52.9945734Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-22T18:16:52.9951211Z   warnings.warn(
2026-02-22T18:16:53.7299572Z INFO 02-22 18:16:53 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:16:53.7307095Z INFO 02-22 18:16:53 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:16:53.7318702Z INFO 02-22 18:16:53 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:16:54.1811633Z INFO 02-22 18:16:54 [parallel_state.py:1212] world_size=32 rank=4 local_rank=4 distributed_init_method=tcp://10.0.0.52:49069 backend=hccl
2026-02-22T18:16:54.7416406Z INFO 02-22 18:16:54 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:16:54.7423306Z INFO 02-22 18:16:54 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:16:54.7433720Z INFO 02-22 18:16:54 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:16:54.7489051Z INFO 02-22 18:16:54 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:16:55.0984450Z INFO 02-22 18:16:55 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:16:55.0992346Z INFO 02-22 18:16:55 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:16:55.1002784Z INFO 02-22 18:16:55 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:16:55.5685193Z INFO 02-22 18:16:55 [parallel_state.py:1212] world_size=32 rank=12 local_rank=4 distributed_init_method=tcp://10.0.0.52:49069 backend=hccl
2026-02-22T18:16:56.3834630Z INFO 02-22 18:16:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:16:56.3843440Z INFO 02-22 18:16:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:16:56.3854081Z INFO 02-22 18:16:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:16:56.3905610Z INFO 02-22 18:16:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:16:59.9465172Z 2026-02-22 18:16:59,944 - 682 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:16:59.9498392Z INFO 02-22 18:16:59 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:17:01.2680878Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-22T18:17:01.2686395Z   warnings.warn(
2026-02-22T18:17:01.3214333Z 2026-02-22 18:17:01,319 - 689 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:17:01.3274574Z INFO 02-22 18:17:01 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:17:02.7835000Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-22T18:17:02.7850340Z   warnings.warn(
2026-02-22T18:17:03.3337700Z INFO 02-22 18:17:03 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:17:03.3345121Z INFO 02-22 18:17:03 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:17:03.3355630Z INFO 02-22 18:17:03 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:17:03.7633154Z INFO 02-22 18:17:03 [parallel_state.py:1212] world_size=32 rank=5 local_rank=5 distributed_init_method=tcp://10.0.0.52:49069 backend=hccl
2026-02-22T18:17:04.4724001Z INFO 02-22 18:17:04 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:17:04.4731715Z INFO 02-22 18:17:04 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:17:04.4740801Z INFO 02-22 18:17:04 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:17:04.4811237Z INFO 02-22 18:17:04 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:17:04.8895605Z INFO 02-22 18:17:04 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:17:04.8905400Z INFO 02-22 18:17:04 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:17:04.8916961Z INFO 02-22 18:17:04 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:17:05.3276323Z INFO 02-22 18:17:05 [parallel_state.py:1212] world_size=32 rank=13 local_rank=5 distributed_init_method=tcp://10.0.0.52:49069 backend=hccl
2026-02-22T18:17:05.8527166Z INFO 02-22 18:17:05 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:17:05.8536524Z INFO 02-22 18:17:05 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:17:05.8545719Z INFO 02-22 18:17:05 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:17:05.8599767Z INFO 02-22 18:17:05 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:17:09.6614010Z 2026-02-22 18:17:09,654 - 786 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:17:09.6616127Z INFO 02-22 18:17:09 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:17:10.8840055Z 2026-02-22 18:17:10,881 - 790 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:17:10.8876512Z INFO 02-22 18:17:10 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:17:11.0246495Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-22T18:17:11.0253849Z   warnings.warn(
2026-02-22T18:17:12.2132779Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-22T18:17:12.2138064Z   warnings.warn(
2026-02-22T18:17:13.1606828Z INFO 02-22 18:17:13 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:17:13.1614680Z INFO 02-22 18:17:13 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:17:13.1624897Z INFO 02-22 18:17:13 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:17:13.6411527Z INFO 02-22 18:17:13 [parallel_state.py:1212] world_size=32 rank=6 local_rank=6 distributed_init_method=tcp://10.0.0.52:49069 backend=hccl
2026-02-22T18:17:14.2498783Z INFO 02-22 18:17:14 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:17:14.2505974Z INFO 02-22 18:17:14 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:17:14.2514973Z INFO 02-22 18:17:14 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:17:14.2626435Z INFO 02-22 18:17:14 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:17:14.3365185Z INFO 02-22 18:17:14 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:17:14.3373041Z INFO 02-22 18:17:14 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:17:14.3383750Z INFO 02-22 18:17:14 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:17:14.8178646Z INFO 02-22 18:17:14 [parallel_state.py:1212] world_size=32 rank=14 local_rank=6 distributed_init_method=tcp://10.0.0.52:49069 backend=hccl
2026-02-22T18:17:15.4630242Z INFO 02-22 18:17:15 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:17:15.4637393Z INFO 02-22 18:17:15 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:17:15.4646250Z INFO 02-22 18:17:15 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:17:15.4702818Z INFO 02-22 18:17:15 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:17:19.2201104Z 2026-02-22 18:17:19,217 - 890 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:17:19.2257168Z INFO 02-22 18:17:19 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:17:20.4877370Z 2026-02-22 18:17:20,485 - 894 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-22T18:17:20.4910923Z INFO 02-22 18:17:20 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-22T18:17:20.5063882Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-22T18:17:20.5072270Z   warnings.warn(
2026-02-22T18:17:21.7921186Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-22T18:17:21.8021848Z   warnings.warn(
2026-02-22T18:17:22.5337820Z INFO 02-22 18:17:22 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:17:22.5346018Z INFO 02-22 18:17:22 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:17:22.5358987Z INFO 02-22 18:17:22 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:17:22.9629516Z INFO 02-22 18:17:22 [parallel_state.py:1212] world_size=32 rank=7 local_rank=7 distributed_init_method=tcp://10.0.0.52:49069 backend=hccl
2026-02-22T18:17:23.8235403Z INFO 02-22 18:17:23 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:17:23.8243495Z INFO 02-22 18:17:23 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:17:23.8255810Z INFO 02-22 18:17:23 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:17:24.2469932Z INFO 02-22 18:17:24 [parallel_state.py:1212] world_size=32 rank=15 local_rank=7 distributed_init_method=tcp://10.0.0.52:49069 backend=hccl
2026-02-22T18:17:24.2917414Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.2941586Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.2952152Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.2967155Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.3190994Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.3611444Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.3611900Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.3612382Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.3612782Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.3613202Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.3613598Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.3620341Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.3629534Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.3643899Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.3650030Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.3829319Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.4193236Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-22T18:17:24.4202830Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-22T18:17:24.4212724Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-22T18:17:24.4223007Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-22T18:17:24.4231795Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-22T18:17:24.4240924Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-22T18:17:24.4250912Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-22T18:17:24.4259664Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-22T18:17:24.4269332Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-22T18:17:24.4279110Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-22T18:17:24.4288434Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-22T18:17:24.4297898Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-22T18:17:24.4307340Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-22T18:17:24.4317878Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-22T18:17:24.4327892Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-22T18:17:24.4337556Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-22T18:17:24.4346690Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4355798Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4366722Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4376202Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4385701Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4395615Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4405810Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4415519Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4425079Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4435291Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4445600Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4455738Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4464391Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4474332Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4484376Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4494006Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4503937Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4513476Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4523239Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4533394Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4541927Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4551814Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4562131Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4571724Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4747826Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4769207Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4780359Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4790375Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4800119Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4810610Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4820285Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4830073Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4840551Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4850459Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4860497Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4869737Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4880434Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4890034Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4899495Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4909417Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4919617Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4929379Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4938386Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4947324Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4958036Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4968171Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4978502Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4987705Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-22T18:17:24.4996652Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.5007961Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.5016555Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.5024823Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.5034109Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.5044133Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.5053824Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.5063147Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.5074262Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.5086328Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.5095249Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.5105037Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.5116211Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.5127399Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.5149395Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.5162337Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.5224730Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.5249280Z INFO 02-22 18:17:24 [parallel_state.py:1423] rank 0 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
2026-02-22T18:17:24.5945982Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.5966654Z INFO 02-22 18:17:24 [parallel_state.py:1423] rank 1 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
2026-02-22T18:17:24.6063152Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6085272Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6096538Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6105255Z INFO 02-22 18:17:24 [parallel_state.py:1423] rank 3 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
2026-02-22T18:17:24.6115116Z INFO 02-22 18:17:24 [parallel_state.py:1423] rank 4 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 4, EP rank 4
2026-02-22T18:17:24.6125064Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6134289Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6143286Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6153151Z INFO 02-22 18:17:24 [parallel_state.py:1423] rank 6 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 6, EP rank 6
2026-02-22T18:17:24.6162160Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6171530Z INFO 02-22 18:17:24 [parallel_state.py:1423] rank 5 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 5, EP rank 5
2026-02-22T18:17:24.6181349Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6190804Z INFO 02-22 18:17:24 [parallel_state.py:1423] rank 9 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 1, EP rank 9
2026-02-22T18:17:24.6200268Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6210331Z INFO 02-22 18:17:24 [parallel_state.py:1423] rank 7 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 7, EP rank 7
2026-02-22T18:17:24.6219678Z INFO 02-22 18:17:24 [parallel_state.py:1423] rank 8 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 0, EP rank 8
2026-02-22T18:17:24.6229561Z INFO 02-22 18:17:24 [parallel_state.py:1423] rank 11 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 3, EP rank 11
2026-02-22T18:17:24.6239366Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6248644Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6258501Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6267811Z INFO 02-22 18:17:24 [parallel_state.py:1423] rank 10 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 2, EP rank 10
2026-02-22T18:17:24.6277584Z INFO 02-22 18:17:24 [parallel_state.py:1423] rank 13 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 5, EP rank 13
2026-02-22T18:17:24.6287319Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6297263Z INFO 02-22 18:17:24 [parallel_state.py:1423] rank 14 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 6, EP rank 14
2026-02-22T18:17:24.6307105Z INFO 02-22 18:17:24 [parallel_state.py:1423] rank 12 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 4, EP rank 12
2026-02-22T18:17:24.6317515Z INFO 02-22 18:17:24 [parallel_state.py:1423] rank 15 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 7, EP rank 15
2026-02-22T18:17:24.6434444Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6453797Z INFO 02-22 18:17:24 [parallel_state.py:1423] rank 2 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
2026-02-22T18:17:24.6738478Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6760142Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6771632Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6781112Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6791104Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6801552Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6810274Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6820003Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6829378Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6839312Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6849262Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6859352Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6869045Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6878984Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6888219Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6897779Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-22T18:17:24.6907240Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.6915953Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.6926383Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.6935833Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.6945550Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.6955661Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.6965339Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.6975165Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.6984958Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.6993778Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.7003630Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.7013655Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.7023800Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.7032172Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.7042312Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.7051156Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-22T18:17:24.8275721Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8301289Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8329299Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8349134Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8359664Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8369875Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8379435Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8389089Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8408473Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8418734Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8428327Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8486752Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8495985Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8537273Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8547061Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8577263Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8839105Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8849396Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8863601Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8873481Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8882847Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8892313Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8902238Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8911488Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8922094Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8931267Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8941546Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m INFO 02-22 18:17:24 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-22T18:17:24.8950455Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-22 18:17:24 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-22T18:17:24.8960526Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-22 18:17:24 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-22T18:17:24.8970715Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8979821Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.8989517Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m INFO 02-22 18:17:24 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-22T18:17:24.8999909Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-22 18:17:24 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-22T18:17:24.9009931Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-22 18:17:24 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-22T18:17:24.9019704Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m INFO 02-22 18:17:24 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-22T18:17:24.9029259Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.9039793Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-22 18:17:24 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-22T18:17:24.9049624Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.9058725Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-22 18:17:24 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-22T18:17:24.9068121Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-22 18:17:24 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-22T18:17:24.9078209Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-22 18:17:24 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-22T18:17:24.9088149Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m INFO 02-22 18:17:24 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-22T18:17:24.9098124Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.9107580Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m INFO 02-22 18:17:24 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-22T18:17:24.9155687Z WARNING 02-22 18:17:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-22T18:17:24.9189824Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-22 18:17:24 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-22T18:17:24.9210505Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-22 18:17:24 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-22T18:17:25.0397692Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m INFO 02-22 18:17:25 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-22T18:17:25.2902483Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m [2026-02-22 18:17:25] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-22T18:17:25.3071636Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m [2026-02-22 18:17:25] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-22T18:17:25.3092240Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m [2026-02-22 18:17:25] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-22T18:17:25.3463711Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m [2026-02-22 18:17:25] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-22T18:17:25.3496702Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m [2026-02-22 18:17:25] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-22T18:17:25.4867798Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-22 18:17:25 [layer.py:475] [EP Rank 5/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-22T18:17:25.4876336Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m INFO 02-22 18:17:25 [layer.py:475] [EP Rank 14/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-22T18:17:25.4896974Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-22 18:17:25 [layer.py:475] [EP Rank 11/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-22T18:17:25.4905002Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m [2026-02-22 18:17:25] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-22T18:17:25.5048642Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-22 18:17:25 [layer.py:475] [EP Rank 9/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-22T18:17:25.5182844Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-22 18:17:25 [layer.py:475] [EP Rank 8/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-22T18:17:25.5394204Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m [2026-02-22 18:17:25] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-22T18:17:25.6452045Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-22 18:17:25 [layer.py:475] [EP Rank 7/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-22T18:17:25.6513300Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m INFO 02-22 18:17:25 [layer.py:475] [EP Rank 0/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-22T18:17:25.6596174Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m [2026-02-22 18:17:25] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-22T18:17:25.6781926Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m [2026-02-22 18:17:25] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-22T18:17:25.7309406Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m [2026-02-22 18:17:25] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-22T18:17:25.7496154Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m [2026-02-22 18:17:25] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-22T18:17:25.7574851Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m [2026-02-22 18:17:25] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-22T18:17:25.7692992Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m INFO 02-22 18:17:25 [layer.py:475] [EP Rank 12/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-22T18:17:25.7904977Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-22 18:17:25 [layer.py:475] [EP Rank 10/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-22T18:17:25.8470436Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-22 18:17:25 [layer.py:475] [EP Rank 4/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-22T18:17:25.8622891Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-22 18:17:25 [layer.py:475] [EP Rank 6/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-22T18:17:25.8710340Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m INFO 02-22 18:17:25 [layer.py:475] [EP Rank 13/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-22T18:17:25.9074133Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m [2026-02-22 18:17:25] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-22T18:17:25.9526414Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m [2026-02-22 18:17:25] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-22T18:17:26.0034159Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m [2026-02-22 18:17:26] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-22T18:17:26.0190323Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-22 18:17:26 [layer.py:475] [EP Rank 2/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-22T18:17:26.0751356Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m INFO 02-22 18:17:26 [layer.py:475] [EP Rank 1/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-22T18:17:26.1122373Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-22 18:17:26 [layer.py:475] [EP Rank 3/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-22T18:17:26.3159833Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m [2026-02-22 18:17:26] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-22T18:17:26.4304334Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m INFO 02-22 18:17:26 [layer.py:475] [EP Rank 15/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-22T18:17:27.4049317Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-22T18:17:27.4058897Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m   return func(*args, **kwargs)
2026-02-22T18:17:27.4280594Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-22T18:17:27.4286293Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m   return func(*args, **kwargs)
2026-02-22T18:17:27.4502753Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-22T18:17:27.4511026Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m   return func(*args, **kwargs)
2026-02-22T18:17:27.4543192Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-22T18:17:27.4552856Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m   return func(*args, **kwargs)
2026-02-22T18:17:27.4568748Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-22T18:17:27.4577821Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m   return func(*args, **kwargs)
2026-02-22T18:17:27.4603121Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-22T18:17:27.4613444Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m   return func(*args, **kwargs)
2026-02-22T18:17:27.4631270Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-22T18:17:27.4641645Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m   return func(*args, **kwargs)
2026-02-22T18:17:27.4749483Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-22T18:17:27.4758121Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m   return func(*args, **kwargs)
2026-02-22T18:17:27.4773401Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-22 18:17:27 [fused_moe.py:207] [EP Rank 8/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-22T18:17:27.4837248Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m INFO 02-22 18:17:27 [fused_moe.py:207] [EP Rank 0/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-22T18:17:27.5181547Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-22T18:17:27.5189326Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m   return func(*args, **kwargs)
2026-02-22T18:17:27.5206861Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-22 18:17:27 [fused_moe.py:207] [EP Rank 11/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-22T18:17:27.5214742Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-22 18:17:27 [fused_moe.py:207] [EP Rank 9/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-22T18:17:27.5226124Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-22 18:17:27 [fused_moe.py:207] [EP Rank 7/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-22T18:17:27.5313953Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-22T18:17:27.5322407Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m   return func(*args, **kwargs)
2026-02-22T18:17:27.5532471Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-22T18:17:27.5533605Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m   return func(*args, **kwargs)
2026-02-22T18:17:27.5607139Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m INFO 02-22 18:17:27 [fused_moe.py:207] [EP Rank 14/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-22T18:17:27.5618088Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-22T18:17:27.5626208Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m   return func(*args, **kwargs)
2026-02-22T18:17:27.5636987Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-22 18:17:27 [fused_moe.py:207] [EP Rank 3/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-22T18:17:27.5785016Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-22 18:17:27 [fused_moe.py:207] [EP Rank 10/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-22T18:17:27.5871061Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-22T18:17:27.5879866Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m   return func(*args, **kwargs)
2026-02-22T18:17:27.5894235Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-22 18:17:27 [fused_moe.py:207] [EP Rank 5/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-22T18:17:27.6058643Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-22T18:17:27.6059849Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m   return func(*args, **kwargs)
2026-02-22T18:17:27.6140312Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m INFO 02-22 18:17:27 [fused_moe.py:207] [EP Rank 15/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-22T18:17:27.6212138Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-22 18:17:27 [fused_moe.py:207] [EP Rank 2/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-22T18:17:27.6352169Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-22T18:17:27.6358768Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m   return func(*args, **kwargs)
2026-02-22T18:17:27.6372933Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m INFO 02-22 18:17:27 [fused_moe.py:207] [EP Rank 12/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-22T18:17:27.6425212Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-22T18:17:27.6433058Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m   return func(*args, **kwargs)
2026-02-22T18:17:27.6481216Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-22 18:17:27 [fused_moe.py:207] [EP Rank 4/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-22T18:17:27.6643643Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m INFO 02-22 18:17:27 [fused_moe.py:207] [EP Rank 1/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-22T18:17:27.6961236Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m INFO 02-22 18:17:27 [fused_moe.py:207] [EP Rank 13/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-22T18:17:27.7049156Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-22 18:17:27 [fused_moe.py:207] [EP Rank 6/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-22T18:17:27.8580870Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m INFO 02-22 18:17:27 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-22T18:17:27.8609908Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-22 18:17:27 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-22T18:17:27.8747186Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-22 18:17:27 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-22T18:17:27.8973291Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-22 18:17:27 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-22T18:17:27.9169905Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m INFO 02-22 18:17:27 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-22T18:17:27.9339868Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-22 18:17:27 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-22T18:17:27.9364692Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m INFO 02-22 18:17:27 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-22T18:17:27.9395065Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m INFO 02-22 18:17:27 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-22T18:17:27.9699548Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-22 18:17:27 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-22T18:17:27.9910438Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m INFO 02-22 18:17:27 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-22T18:17:28.0000404Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-22 18:17:27 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-22T18:17:28.0029884Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-22 18:17:28 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-22T18:17:28.0211941Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m INFO 02-22 18:17:28 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-22T18:17:28.0393519Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-22 18:17:28 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-22T18:17:28.0722266Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-22 18:17:28 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-22T18:17:28.0781910Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-22 18:17:28 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-22T18:17:31.2652497Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m INFO 02-22 18:17:31 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-22T18:17:31.3537799Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-22 18:17:31 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-22T18:17:31.3557823Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m INFO 02-22 18:17:31 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-22T18:17:31.3840923Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m INFO 02-22 18:17:31 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-22T18:17:31.3869267Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-22 18:17:31 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-22T18:17:31.4073837Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-22 18:17:31 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-22T18:17:31.4110970Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-22 18:17:31 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-22T18:17:31.4168017Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-22 18:17:31 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-22T18:17:31.4219127Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m INFO 02-22 18:17:31 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-22T18:17:31.4239931Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-22 18:17:31 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-22T18:17:31.4249435Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m INFO 02-22 18:17:31 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-22T18:17:31.4518330Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-22 18:17:31 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-22T18:17:31.4934222Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:17:31.4934582Z Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
2026-02-22T18:17:31.4987077Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-22 18:17:31 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-22T18:17:31.5497401Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-22 18:17:31 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-22T18:17:31.5726149Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m INFO 02-22 18:17:31 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-22T18:17:31.6418587Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-22 18:17:31 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-22T18:17:32.8204438Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:17:32.8204815Z Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:01<03:34,  1.33s/it]
2026-02-22T18:17:35.5209284Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:17:35.5209676Z Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:04<05:43,  2.13s/it]
2026-02-22T18:17:36.6482898Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:17:36.6483322Z Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:05<04:27,  1.67s/it]
2026-02-22T18:17:38.0442586Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:17:38.0442962Z Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:06<04:08,  1.56s/it]
2026-02-22T18:17:40.1410736Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:17:40.1411159Z Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:08<04:37,  1.76s/it]
2026-02-22T18:17:42.7529438Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:17:42.7530091Z Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:11<05:21,  2.05s/it]
2026-02-22T18:17:44.9905135Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:17:44.9905626Z Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:13<05:29,  2.11s/it]
2026-02-22T18:17:47.0016486Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:17:47.0016949Z Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:15<05:22,  2.08s/it]
2026-02-22T18:17:48.5523027Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:17:48.5523827Z Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:17<04:54,  1.91s/it]
2026-02-22T18:17:50.0343046Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:17:50.0343493Z Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:18<04:32,  1.78s/it]
2026-02-22T18:17:51.4346844Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:17:51.4347306Z Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:19<04:11,  1.66s/it]
2026-02-22T18:17:52.8484947Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:17:52.8485459Z Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:21<04:00,  1.59s/it]
2026-02-22T18:17:54.1040345Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:17:54.1040789Z Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:22<03:43,  1.49s/it]
2026-02-22T18:17:56.3954302Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:17:56.3954765Z Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:24<04:17,  1.73s/it]
2026-02-22T18:17:58.8408236Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:17:58.8408718Z Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:27<04:48,  1.95s/it]
2026-02-22T18:18:00.8626229Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:00.8626735Z Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:29<04:49,  1.97s/it]
2026-02-22T18:18:02.7890724Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:02.7891264Z Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:31<04:45,  1.96s/it]
2026-02-22T18:18:04.8952920Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:04.8953398Z Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:33<04:50,  2.00s/it]
2026-02-22T18:18:06.2116860Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:06.2117333Z Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:34<04:18,  1.80s/it]
2026-02-22T18:18:07.5898023Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:07.5898518Z Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:36<03:58,  1.67s/it]
2026-02-22T18:18:10.0129352Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:10.0129917Z Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:38<04:29,  1.90s/it]
2026-02-22T18:18:11.2578922Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:11.2579367Z Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:39<03:59,  1.70s/it]
2026-02-22T18:18:12.4614486Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:12.4614909Z Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:40<03:37,  1.55s/it]
2026-02-22T18:18:13.8580203Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:13.8580665Z Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:42<03:29,  1.50s/it]
2026-02-22T18:18:16.6348546Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:16.6349067Z Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:45<04:20,  1.89s/it]
2026-02-22T18:18:17.4728019Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:17.4728532Z Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:45<03:35,  1.57s/it]
2026-02-22T18:18:18.7904434Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:18.7904998Z Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:47<03:23,  1.50s/it]
2026-02-22T18:18:21.2142801Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:21.2143275Z Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:49<03:59,  1.77s/it]
2026-02-22T18:18:22.7473152Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:22.7473679Z Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:51<03:48,  1.70s/it]
2026-02-22T18:18:24.1394805Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:24.1395665Z Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:52<03:33,  1.61s/it]
2026-02-22T18:18:25.4833650Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:25.4834165Z Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:53<03:21,  1.53s/it]
2026-02-22T18:18:26.9599281Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:26.9599692Z Loading safetensors checkpoint shards:  20% Completed | 32/163 [00:55<03:18,  1.51s/it]
2026-02-22T18:18:28.2742434Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:28.2742890Z Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:56<03:09,  1.45s/it]
2026-02-22T18:18:29.6147548Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:29.6147981Z Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:58<03:03,  1.42s/it]
2026-02-22T18:18:30.9457714Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:30.9458140Z Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:59<02:58,  1.39s/it]
2026-02-22T18:18:34.9193596Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:34.9194182Z Loading safetensors checkpoint shards:  22% Completed | 36/163 [01:03<04:35,  2.17s/it]
2026-02-22T18:18:36.1307982Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:36.1308511Z Loading safetensors checkpoint shards:  23% Completed | 37/163 [01:04<03:56,  1.88s/it]
2026-02-22T18:18:37.3691010Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:37.3691417Z Loading safetensors checkpoint shards:  23% Completed | 38/163 [01:05<03:30,  1.69s/it]
2026-02-22T18:18:38.7573554Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:38.7573988Z Loading safetensors checkpoint shards:  24% Completed | 39/163 [01:07<03:18,  1.60s/it]
2026-02-22T18:18:42.1874634Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:42.1875057Z Loading safetensors checkpoint shards:  25% Completed | 40/163 [01:10<04:24,  2.15s/it]
2026-02-22T18:18:43.2661515Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:43.2662129Z Loading safetensors checkpoint shards:  25% Completed | 41/163 [01:11<03:42,  1.83s/it]
2026-02-22T18:18:44.3470902Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:44.3471404Z Loading safetensors checkpoint shards:  26% Completed | 42/163 [01:12<03:14,  1.60s/it]
2026-02-22T18:18:47.6034376Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:47.6034854Z Loading safetensors checkpoint shards:  26% Completed | 43/163 [01:16<04:11,  2.10s/it]
2026-02-22T18:18:48.7032302Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:48.7032798Z Loading safetensors checkpoint shards:  27% Completed | 44/163 [01:17<03:34,  1.80s/it]
2026-02-22T18:18:50.0418768Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:50.0419219Z Loading safetensors checkpoint shards:  28% Completed | 45/163 [01:18<03:16,  1.66s/it]
2026-02-22T18:18:51.1787270Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:51.1787743Z Loading safetensors checkpoint shards:  28% Completed | 46/163 [01:19<02:55,  1.50s/it]
2026-02-22T18:18:57.9968898Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:18:57.9969428Z Loading safetensors checkpoint shards:  29% Completed | 47/163 [01:26<05:59,  3.10s/it]
2026-02-22T18:19:00.4509892Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:00.4510551Z Loading safetensors checkpoint shards:  29% Completed | 48/163 [01:28<05:34,  2.90s/it]
2026-02-22T18:19:00.8897351Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:00.8897768Z Loading safetensors checkpoint shards:  30% Completed | 49/163 [01:29<04:06,  2.16s/it]
2026-02-22T18:19:02.6469985Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:02.6470464Z Loading safetensors checkpoint shards:  31% Completed | 50/163 [01:31<03:50,  2.04s/it]
2026-02-22T18:19:04.9007824Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:04.9008314Z Loading safetensors checkpoint shards:  31% Completed | 51/163 [01:33<03:55,  2.11s/it]
2026-02-22T18:19:05.0926787Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:05.0927576Z Loading safetensors checkpoint shards:  32% Completed | 52/163 [01:33<02:50,  1.53s/it]
2026-02-22T18:19:08.0603110Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:08.0603695Z Loading safetensors checkpoint shards:  33% Completed | 53/163 [01:36<03:35,  1.96s/it]
2026-02-22T18:19:08.9197449Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:08.9198096Z Loading safetensors checkpoint shards:  33% Completed | 54/163 [01:37<02:57,  1.63s/it]
2026-02-22T18:19:09.9143799Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:09.9144344Z Loading safetensors checkpoint shards:  34% Completed | 55/163 [01:38<02:35,  1.44s/it]
2026-02-22T18:19:11.0871902Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:11.0872568Z Loading safetensors checkpoint shards:  34% Completed | 56/163 [01:39<02:25,  1.36s/it]
2026-02-22T18:19:12.3739692Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:12.3740287Z Loading safetensors checkpoint shards:  35% Completed | 57/163 [01:40<02:21,  1.34s/it]
2026-02-22T18:19:13.6134164Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:13.6134632Z Loading safetensors checkpoint shards:  36% Completed | 58/163 [01:42<02:17,  1.31s/it]
2026-02-22T18:19:17.0140808Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:17.0141272Z Loading safetensors checkpoint shards:  36% Completed | 59/163 [01:45<03:21,  1.94s/it]
2026-02-22T18:19:17.9856746Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:17.9857185Z Loading safetensors checkpoint shards:  37% Completed | 60/163 [01:46<02:49,  1.65s/it]
2026-02-22T18:19:18.9113314Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:18.9113787Z Loading safetensors checkpoint shards:  37% Completed | 61/163 [01:47<02:25,  1.43s/it]
2026-02-22T18:19:21.4639554Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:21.4640008Z Loading safetensors checkpoint shards:  38% Completed | 62/163 [01:49<02:58,  1.77s/it]
2026-02-22T18:19:22.4742914Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:22.4743361Z Loading safetensors checkpoint shards:  39% Completed | 63/163 [01:50<02:34,  1.54s/it]
2026-02-22T18:19:23.5106987Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:23.5107415Z Loading safetensors checkpoint shards:  39% Completed | 64/163 [01:52<02:17,  1.39s/it]
2026-02-22T18:19:27.1607738Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:27.1608299Z Loading safetensors checkpoint shards:  40% Completed | 65/163 [01:55<03:22,  2.07s/it]
2026-02-22T18:19:29.2201705Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:29.2202355Z Loading safetensors checkpoint shards:  40% Completed | 66/163 [01:57<03:20,  2.07s/it]
2026-02-22T18:19:30.2808670Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:30.2809180Z Loading safetensors checkpoint shards:  41% Completed | 67/163 [01:58<02:49,  1.76s/it]
2026-02-22T18:19:32.8594225Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:32.8594814Z Loading safetensors checkpoint shards:  42% Completed | 68/163 [02:01<03:10,  2.01s/it]
2026-02-22T18:19:33.8699856Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:33.8700361Z Loading safetensors checkpoint shards:  42% Completed | 69/163 [02:02<02:40,  1.71s/it]
2026-02-22T18:19:34.9130326Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:34.9130820Z Loading safetensors checkpoint shards:  43% Completed | 70/163 [02:03<02:20,  1.51s/it]
2026-02-22T18:19:36.7351103Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:36.7351557Z Loading safetensors checkpoint shards:  44% Completed | 71/163 [02:05<02:27,  1.60s/it]
2026-02-22T18:19:39.0059228Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:39.0059704Z Loading safetensors checkpoint shards:  44% Completed | 72/163 [02:07<02:44,  1.80s/it]
2026-02-22T18:19:39.8050902Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:40.8763947Z Loading safetensors checkpoint shards:  45% Completed | 73/163 [02:08<02:15,  1.50s/it]
2026-02-22T18:19:40.8765139Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:40.8765662Z Loading safetensors checkpoint shards:  45% Completed | 74/163 [02:09<02:02,  1.37s/it]
2026-02-22T18:19:41.9215732Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:41.9216158Z Loading safetensors checkpoint shards:  46% Completed | 75/163 [02:10<01:52,  1.27s/it]
2026-02-22T18:19:43.0516174Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:43.0516592Z Loading safetensors checkpoint shards:  47% Completed | 76/163 [02:11<01:47,  1.23s/it]
2026-02-22T18:19:46.0897185Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:46.0897673Z Loading safetensors checkpoint shards:  47% Completed | 77/163 [02:14<02:32,  1.77s/it]
2026-02-22T18:19:47.0583119Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:47.0583648Z Loading safetensors checkpoint shards:  48% Completed | 78/163 [02:15<02:10,  1.53s/it]
2026-02-22T18:19:48.1151875Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:48.1152381Z Loading safetensors checkpoint shards:  48% Completed | 79/163 [02:16<01:56,  1.39s/it]
2026-02-22T18:19:50.1798787Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:50.1799234Z Loading safetensors checkpoint shards:  49% Completed | 80/163 [02:18<02:12,  1.59s/it]
2026-02-22T18:19:52.7935236Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:52.7935750Z Loading safetensors checkpoint shards:  50% Completed | 81/163 [02:21<02:35,  1.90s/it]
2026-02-22T18:19:53.6691570Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:53.6692114Z Loading safetensors checkpoint shards:  50% Completed | 82/163 [02:22<02:08,  1.59s/it]
2026-02-22T18:19:55.5601818Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:55.5602330Z Loading safetensors checkpoint shards:  51% Completed | 83/163 [02:24<02:14,  1.68s/it]
2026-02-22T18:19:56.8811094Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:56.8811600Z Loading safetensors checkpoint shards:  52% Completed | 84/163 [02:25<02:04,  1.57s/it]
2026-02-22T18:19:59.0598597Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:19:59.0599072Z Loading safetensors checkpoint shards:  52% Completed | 85/163 [02:27<02:16,  1.75s/it]
2026-02-22T18:20:00.2834981Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:00.2835437Z Loading safetensors checkpoint shards:  53% Completed | 86/163 [02:28<02:02,  1.60s/it]
2026-02-22T18:20:01.9675611Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:01.9676076Z Loading safetensors checkpoint shards:  53% Completed | 87/163 [02:30<02:03,  1.62s/it]
2026-02-22T18:20:04.7548317Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:04.7548758Z Loading safetensors checkpoint shards:  54% Completed | 88/163 [02:33<02:27,  1.97s/it]
2026-02-22T18:20:06.2268672Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:06.2269161Z Loading safetensors checkpoint shards:  55% Completed | 89/163 [02:34<02:14,  1.82s/it]
2026-02-22T18:20:09.0242238Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:09.0242814Z Loading safetensors checkpoint shards:  55% Completed | 90/163 [02:37<02:34,  2.11s/it]
2026-02-22T18:20:10.1382399Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:10.1382836Z Loading safetensors checkpoint shards:  56% Completed | 91/163 [02:38<02:10,  1.81s/it]
2026-02-22T18:20:13.3586964Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:13.3587468Z Loading safetensors checkpoint shards:  56% Completed | 92/163 [02:41<02:38,  2.24s/it]
2026-02-22T18:20:14.3594048Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:14.3594484Z Loading safetensors checkpoint shards:  57% Completed | 93/163 [02:42<02:10,  1.87s/it]
2026-02-22T18:20:15.3682286Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:15.3682720Z Loading safetensors checkpoint shards:  58% Completed | 94/163 [02:43<01:50,  1.61s/it]
2026-02-22T18:20:16.4321719Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:16.4322583Z Loading safetensors checkpoint shards:  58% Completed | 95/163 [02:44<01:38,  1.45s/it]
2026-02-22T18:20:18.9244324Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:18.9244886Z Loading safetensors checkpoint shards:  59% Completed | 96/163 [02:47<01:57,  1.76s/it]
2026-02-22T18:20:20.1934481Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:20.1935001Z Loading safetensors checkpoint shards:  60% Completed | 97/163 [02:48<01:46,  1.61s/it]
2026-02-22T18:20:21.5910761Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:21.5911295Z Loading safetensors checkpoint shards:  60% Completed | 98/163 [02:50<01:40,  1.55s/it]
2026-02-22T18:20:22.9706347Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:22.9706785Z Loading safetensors checkpoint shards:  61% Completed | 99/163 [02:51<01:35,  1.50s/it]
2026-02-22T18:20:24.2483371Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:24.2483923Z Loading safetensors checkpoint shards:  61% Completed | 100/163 [02:52<01:30,  1.43s/it]
2026-02-22T18:20:25.4319497Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:25.4319947Z Loading safetensors checkpoint shards:  62% Completed | 101/163 [02:53<01:24,  1.36s/it]
2026-02-22T18:20:27.8562561Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:27.8563077Z Loading safetensors checkpoint shards:  63% Completed | 102/163 [02:56<01:42,  1.68s/it]
2026-02-22T18:20:29.9476334Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:29.9476853Z Loading safetensors checkpoint shards:  63% Completed | 103/163 [02:58<01:48,  1.80s/it]
2026-02-22T18:20:32.4005918Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:32.4006564Z Loading safetensors checkpoint shards:  64% Completed | 104/163 [03:00<01:57,  2.00s/it]
2026-02-22T18:20:33.4074372Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:33.4074865Z Loading safetensors checkpoint shards:  64% Completed | 105/163 [03:01<01:38,  1.70s/it]
2026-02-22T18:20:35.6265200Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:35.6265676Z Loading safetensors checkpoint shards:  65% Completed | 106/163 [03:04<01:45,  1.86s/it]
2026-02-22T18:20:36.8567105Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:36.8567554Z Loading safetensors checkpoint shards:  66% Completed | 107/163 [03:05<01:33,  1.67s/it]
2026-02-22T18:20:39.3792866Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:39.3793363Z Loading safetensors checkpoint shards:  66% Completed | 108/163 [03:07<01:45,  1.92s/it]
2026-02-22T18:20:40.4996585Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:40.4997021Z Loading safetensors checkpoint shards:  67% Completed | 109/163 [03:09<01:30,  1.68s/it]
2026-02-22T18:20:42.6851582Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:42.6852129Z Loading safetensors checkpoint shards:  67% Completed | 110/163 [03:11<01:37,  1.83s/it]
2026-02-22T18:20:45.3408981Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:45.3409457Z Loading safetensors checkpoint shards:  68% Completed | 111/163 [03:13<01:48,  2.08s/it]
2026-02-22T18:20:47.0352361Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:47.0352926Z Loading safetensors checkpoint shards:  69% Completed | 112/163 [03:15<01:40,  1.96s/it]
2026-02-22T18:20:49.1184406Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:49.1184930Z Loading safetensors checkpoint shards:  69% Completed | 113/163 [03:17<01:40,  2.00s/it]
2026-02-22T18:20:50.4394747Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:50.4395220Z Loading safetensors checkpoint shards:  70% Completed | 114/163 [03:18<01:28,  1.80s/it]
2026-02-22T18:20:51.7249584Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:51.7249995Z Loading safetensors checkpoint shards:  71% Completed | 115/163 [03:20<01:18,  1.64s/it]
2026-02-22T18:20:54.4883647Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:54.4884080Z Loading safetensors checkpoint shards:  71% Completed | 116/163 [03:22<01:33,  1.98s/it]
2026-02-22T18:20:55.6494321Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:55.6495078Z Loading safetensors checkpoint shards:  72% Completed | 117/163 [03:24<01:19,  1.73s/it]
2026-02-22T18:20:58.2071403Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:58.2071880Z Loading safetensors checkpoint shards:  72% Completed | 118/163 [03:26<01:29,  1.98s/it]
2026-02-22T18:20:59.2286809Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:20:59.2287323Z Loading safetensors checkpoint shards:  73% Completed | 119/163 [03:27<01:14,  1.69s/it]
2026-02-22T18:21:00.5197833Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:00.5198405Z Loading safetensors checkpoint shards:  74% Completed | 120/163 [03:29<01:07,  1.57s/it]
2026-02-22T18:21:01.7405340Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:01.7405812Z Loading safetensors checkpoint shards:  74% Completed | 121/163 [03:30<01:01,  1.47s/it]
2026-02-22T18:21:03.0477043Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:03.0477557Z Loading safetensors checkpoint shards:  75% Completed | 122/163 [03:31<00:58,  1.42s/it]
2026-02-22T18:21:05.4395459Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:05.4395936Z Loading safetensors checkpoint shards:  75% Completed | 123/163 [03:33<01:08,  1.71s/it]
2026-02-22T18:21:06.7539962Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:06.7540525Z Loading safetensors checkpoint shards:  76% Completed | 124/163 [03:35<01:02,  1.59s/it]
2026-02-22T18:21:09.2051876Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:09.2052398Z Loading safetensors checkpoint shards:  77% Completed | 125/163 [03:37<01:10,  1.85s/it]
2026-02-22T18:21:10.5031131Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:10.5031615Z Loading safetensors checkpoint shards:  77% Completed | 126/163 [03:39<01:02,  1.68s/it]
2026-02-22T18:21:11.8216637Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:11.8217067Z Loading safetensors checkpoint shards:  78% Completed | 127/163 [03:40<00:56,  1.57s/it]
2026-02-22T18:21:14.1893680Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:14.1894193Z Loading safetensors checkpoint shards:  79% Completed | 128/163 [03:42<01:03,  1.81s/it]
2026-02-22T18:21:16.3015601Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:16.3016011Z Loading safetensors checkpoint shards:  79% Completed | 129/163 [03:44<01:04,  1.90s/it]
2026-02-22T18:21:17.3128435Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:17.3128900Z Loading safetensors checkpoint shards:  80% Completed | 130/163 [03:45<00:53,  1.64s/it]
2026-02-22T18:21:18.6171967Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:18.6172600Z Loading safetensors checkpoint shards:  80% Completed | 131/163 [03:47<00:49,  1.54s/it]
2026-02-22T18:21:19.9597711Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:19.9598293Z Loading safetensors checkpoint shards:  81% Completed | 132/163 [03:48<00:45,  1.48s/it]
2026-02-22T18:21:22.2007881Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:22.2008425Z Loading safetensors checkpoint shards:  82% Completed | 133/163 [03:50<00:51,  1.71s/it]
2026-02-22T18:21:23.5093481Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:23.5094021Z Loading safetensors checkpoint shards:  82% Completed | 134/163 [03:52<00:46,  1.59s/it]
2026-02-22T18:21:26.4768951Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:26.4769461Z Loading safetensors checkpoint shards:  83% Completed | 135/163 [03:54<00:56,  2.00s/it]
2026-02-22T18:21:27.5791280Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:27.5791731Z Loading safetensors checkpoint shards:  83% Completed | 136/163 [03:56<00:46,  1.73s/it]
2026-02-22T18:21:28.5684265Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:28.5684730Z Loading safetensors checkpoint shards:  84% Completed | 137/163 [03:57<00:39,  1.51s/it]
2026-02-22T18:21:31.8802466Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:31.8803240Z Loading safetensors checkpoint shards:  85% Completed | 138/163 [04:00<00:51,  2.05s/it]
2026-02-22T18:21:33.9797994Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:33.9798783Z Loading safetensors checkpoint shards:  85% Completed | 139/163 [04:02<00:49,  2.06s/it]
2026-02-22T18:21:34.7819367Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:34.7819819Z Loading safetensors checkpoint shards:  86% Completed | 140/163 [04:03<00:38,  1.69s/it]
2026-02-22T18:21:35.9036692Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:35.9037134Z Loading safetensors checkpoint shards:  87% Completed | 141/163 [04:04<00:33,  1.52s/it]
2026-02-22T18:21:36.9446968Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:36.9447533Z Loading safetensors checkpoint shards:  87% Completed | 142/163 [04:05<00:28,  1.37s/it]
2026-02-22T18:21:39.4440145Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:39.4440594Z Loading safetensors checkpoint shards:  88% Completed | 143/163 [04:07<00:34,  1.71s/it]
2026-02-22T18:21:41.5010785Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:41.5011350Z Loading safetensors checkpoint shards:  88% Completed | 144/163 [04:10<00:34,  1.82s/it]
2026-02-22T18:21:43.3188361Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:43.3188893Z Loading safetensors checkpoint shards:  89% Completed | 145/163 [04:11<00:32,  1.82s/it]
2026-02-22T18:21:44.9718692Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:44.9719235Z Loading safetensors checkpoint shards:  90% Completed | 146/163 [04:13<00:30,  1.77s/it]
2026-02-22T18:21:46.2129084Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:46.2129523Z Loading safetensors checkpoint shards:  90% Completed | 147/163 [04:14<00:25,  1.61s/it]
2026-02-22T18:21:47.5098002Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:47.5098534Z Loading safetensors checkpoint shards:  91% Completed | 148/163 [04:16<00:22,  1.51s/it]
2026-02-22T18:21:48.7426129Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:48.7426634Z Loading safetensors checkpoint shards:  91% Completed | 149/163 [04:17<00:20,  1.43s/it]
2026-02-22T18:21:51.2469668Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:51.2470130Z Loading safetensors checkpoint shards:  93% Completed | 151/163 [04:19<00:16,  1.35s/it]
2026-02-22T18:21:52.3094760Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:52.3095199Z Loading safetensors checkpoint shards:  93% Completed | 152/163 [04:20<00:14,  1.28s/it]
2026-02-22T18:21:53.6126819Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:53.6127333Z Loading safetensors checkpoint shards:  94% Completed | 153/163 [04:22<00:12,  1.28s/it]
2026-02-22T18:21:55.2220286Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:55.2220843Z Loading safetensors checkpoint shards:  94% Completed | 154/163 [04:23<00:12,  1.37s/it]
2026-02-22T18:21:56.2467623Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:56.2468180Z Loading safetensors checkpoint shards:  95% Completed | 155/163 [04:24<00:10,  1.28s/it]
2026-02-22T18:21:57.3424037Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:57.3424544Z Loading safetensors checkpoint shards:  96% Completed | 156/163 [04:25<00:08,  1.22s/it]
2026-02-22T18:21:58.6468534Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:21:58.6469069Z Loading safetensors checkpoint shards:  96% Completed | 157/163 [04:27<00:07,  1.25s/it]
2026-02-22T18:22:01.7781738Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:01.7782391Z Loading safetensors checkpoint shards:  97% Completed | 158/163 [04:30<00:08,  1.80s/it]
2026-02-22T18:22:02.9110268Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:02.9110725Z Loading safetensors checkpoint shards:  98% Completed | 159/163 [04:31<00:06,  1.60s/it]
2026-02-22T18:22:04.1820096Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:04.1820503Z Loading safetensors checkpoint shards:  98% Completed | 160/163 [04:32<00:04,  1.50s/it]
2026-02-22T18:22:05.2195671Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:05.2196420Z Loading safetensors checkpoint shards:  99% Completed | 161/163 [04:33<00:02,  1.37s/it]
2026-02-22T18:22:07.2949195Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:07.2949643Z Loading safetensors checkpoint shards:  99% Completed | 162/163 [04:35<00:01,  1.58s/it]
2026-02-22T18:22:08.6643897Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:08.6644439Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [04:37<00:00,  1.52s/it]
2026-02-22T18:22:08.6671900Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:08.6672470Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [04:37<00:00,  1.70s/it]
2026-02-22T18:22:08.6683172Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:08.6801711Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m INFO 02-22 18:22:08 [default_loader.py:291] Loading weights took 277.19 seconds
2026-02-22T18:22:09.0803995Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-22 18:22:09 [default_loader.py:291] Loading weights took 277.47 seconds
2026-02-22T18:22:22.1197862Z INFO 02-22 18:22:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:22:22.1206890Z INFO 02-22 18:22:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:22:22.1219729Z INFO 02-22 18:22:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:22:22.1228438Z INFO 02-22 18:22:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:22:22.1239205Z INFO 02-22 18:22:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:22:22.1249599Z INFO 02-22 18:22:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:22:22.1300592Z INFO 02-22 18:22:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:22:22.1310401Z INFO 02-22 18:22:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:22:22.1320771Z INFO 02-22 18:22:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:22:22.1344569Z INFO 02-22 18:22:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:22:22.1354465Z INFO 02-22 18:22:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:22:22.1364918Z INFO 02-22 18:22:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:22:22.1374951Z INFO 02-22 18:22:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:22:22.1384423Z INFO 02-22 18:22:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:22:22.1394704Z INFO 02-22 18:22:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:22:22.1405199Z INFO 02-22 18:22:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:22:22.1414932Z INFO 02-22 18:22:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:22:22.1424945Z INFO 02-22 18:22:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:22:22.1435309Z INFO 02-22 18:22:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:22:22.1445784Z INFO 02-22 18:22:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:22:22.1456506Z INFO 02-22 18:22:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:22:22.1464397Z INFO 02-22 18:22:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:22:22.1474763Z INFO 02-22 18:22:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:22:22.1484922Z INFO 02-22 18:22:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:22:22.1495439Z INFO 02-22 18:22:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:22:22.1504998Z INFO 02-22 18:22:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:22:22.1515652Z INFO 02-22 18:22:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:22:22.1525079Z INFO 02-22 18:22:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:22:22.1536716Z INFO 02-22 18:22:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:22:22.1544500Z INFO 02-22 18:22:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:22:22.1554069Z INFO 02-22 18:22:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:22:22.1563903Z INFO 02-22 18:22:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:22:22.1573094Z INFO 02-22 18:22:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:22:22.1583146Z INFO 02-22 18:22:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:22:22.1615314Z INFO 02-22 18:22:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:22:22.1615752Z INFO 02-22 18:22:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:22:22.1616215Z INFO 02-22 18:22:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:22:22.1621931Z INFO 02-22 18:22:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:22:22.1631687Z INFO 02-22 18:22:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:22:22.1644335Z INFO 02-22 18:22:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:22:22.1650901Z INFO 02-22 18:22:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:22:22.1660521Z INFO 02-22 18:22:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:22:22.1670290Z INFO 02-22 18:22:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:22:22.1681011Z INFO 02-22 18:22:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:22:22.1689863Z INFO 02-22 18:22:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:22:22.1701560Z INFO 02-22 18:22:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:22:22.1709468Z INFO 02-22 18:22:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:22:22.1719282Z INFO 02-22 18:22:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:22:22.1728808Z INFO 02-22 18:22:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:22:22.1738389Z INFO 02-22 18:22:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:22:22.1747936Z INFO 02-22 18:22:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:22:22.1758047Z INFO 02-22 18:22:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:22:22.1896214Z INFO 02-22 18:22:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:22:22.1905509Z INFO 02-22 18:22:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:22:22.1915208Z INFO 02-22 18:22:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:22:22.1926088Z INFO 02-22 18:22:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:22:22.1935642Z INFO 02-22 18:22:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:22:22.1945668Z INFO 02-22 18:22:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:22:22.2031199Z INFO 02-22 18:22:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:22:22.2031793Z INFO 02-22 18:22:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:22:22.2032361Z INFO 02-22 18:22:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-22T18:22:22.2032838Z INFO 02-22 18:22:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-22T18:22:22.2033329Z INFO 02-22 18:22:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-22T18:22:22.2079125Z INFO 02-22 18:22:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-22T18:22:47.8991075Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m WARNING 02-22 18:22:47 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-22T18:22:47.9015809Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m WARNING 02-22 18:22:47 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-22T18:22:47.9029910Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m WARNING 02-22 18:22:47 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-22T18:22:47.9036625Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m WARNING 02-22 18:22:47 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-22T18:22:47.9047766Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m WARNING 02-22 18:22:47 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-22T18:22:47.9060098Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m WARNING 02-22 18:22:47 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-22T18:22:47.9070708Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m WARNING 02-22 18:22:47 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-22T18:22:47.9131408Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m WARNING 02-22 18:22:47 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-22T18:22:47.9168711Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m WARNING 02-22 18:22:47 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-22T18:22:47.9178655Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m WARNING 02-22 18:22:47 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-22T18:22:47.9232892Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m WARNING 02-22 18:22:47 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-22T18:22:47.9353435Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m WARNING 02-22 18:22:47 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-22T18:22:47.9485156Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m WARNING 02-22 18:22:47 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-22T18:22:47.9639910Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m INFO 02-22 18:22:47 [model_runner_v1.py:2315] Loading drafter model...
2026-02-22T18:22:47.9658058Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m INFO 02-22 18:22:47 [model_runner_v1.py:2315] Loading drafter model...
2026-02-22T18:22:47.9668548Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-22 18:22:47 [model_runner_v1.py:2315] Loading drafter model...
2026-02-22T18:22:47.9722170Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-22 18:22:47 [model_runner_v1.py:2315] Loading drafter model...
2026-02-22T18:22:47.9724688Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m WARNING 02-22 18:22:47 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-22T18:22:47.9725756Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m WARNING 02-22 18:22:47 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-22T18:22:47.9726886Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-22 18:22:47 [model_runner_v1.py:2315] Loading drafter model...
2026-02-22T18:22:47.9727576Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-22 18:22:47 [model_runner_v1.py:2315] Loading drafter model...
2026-02-22T18:22:47.9748306Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m INFO 02-22 18:22:47 [model_runner_v1.py:2315] Loading drafter model...
2026-02-22T18:22:47.9782748Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-22 18:22:47 [model_runner_v1.py:2315] Loading drafter model...
2026-02-22T18:22:47.9866119Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m INFO 02-22 18:22:47 [model_runner_v1.py:2315] Loading drafter model...
2026-02-22T18:22:47.9875991Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-22 18:22:47 [model_runner_v1.py:2315] Loading drafter model...
2026-02-22T18:22:47.9886606Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m WARNING 02-22 18:22:47 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-22T18:22:47.9895962Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m INFO 02-22 18:22:47 [model_runner_v1.py:2315] Loading drafter model...
2026-02-22T18:22:48.0064818Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-22 18:22:48 [model_runner_v1.py:2315] Loading drafter model...
2026-02-22T18:22:48.0116773Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-22 18:22:48 [model_runner_v1.py:2315] Loading drafter model...
2026-02-22T18:22:48.0203441Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m INFO 02-22 18:22:48 [model_runner_v1.py:2315] Loading drafter model...
2026-02-22T18:22:48.0225700Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-22 18:22:48 [model_runner_v1.py:2315] Loading drafter model...
2026-02-22T18:22:48.0497333Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-22 18:22:48 [model_runner_v1.py:2315] Loading drafter model...
2026-02-22T18:22:48.0699316Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:48.0699722Z Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
2026-02-22T18:22:48.7969639Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:48.7970150Z Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<01:57,  1.38it/s]
2026-02-22T18:22:49.5025861Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:49.5026355Z Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:01<01:54,  1.41it/s]
2026-02-22T18:22:50.3392108Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:50.3392536Z Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:02<02:03,  1.30it/s]
2026-02-22T18:22:51.1510275Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:51.1510700Z Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:03<02:05,  1.27it/s]
2026-02-22T18:22:51.8450509Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:51.8450954Z Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:03<01:59,  1.33it/s]
2026-02-22T18:22:52.6504699Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:52.6505131Z Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:04<02:01,  1.30it/s]
2026-02-22T18:22:53.4841967Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:53.4842538Z Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:05<02:03,  1.26it/s]
2026-02-22T18:22:54.4721821Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:54.4722345Z Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:06<02:12,  1.17it/s]
2026-02-22T18:22:55.5559089Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:55.5559550Z Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:07<02:22,  1.08it/s]
2026-02-22T18:22:56.5944733Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:56.5945214Z Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:08<02:26,  1.04it/s]
2026-02-22T18:22:57.6698173Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:58.7730929Z Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:09<02:31,  1.00it/s]
2026-02-22T18:22:58.7731884Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:58.7732597Z Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:10<02:35,  1.03s/it]
2026-02-22T18:22:59.8690205Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:22:59.8690740Z Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:11<02:37,  1.05s/it]
2026-02-22T18:23:00.9088070Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:00.9088651Z Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:12<02:35,  1.04s/it]
2026-02-22T18:23:01.9368655Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:01.9369181Z Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:13<02:34,  1.04s/it]
2026-02-22T18:23:02.9691628Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:02.9692291Z Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:14<02:32,  1.04s/it]
2026-02-22T18:23:03.9840495Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:03.9840960Z Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:15<02:30,  1.03s/it]
2026-02-22T18:23:05.0583185Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:05.0583623Z Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:16<02:31,  1.04s/it]
2026-02-22T18:23:06.1277996Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:06.1278473Z Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:18<02:31,  1.05s/it]
2026-02-22T18:23:07.3058564Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:07.3058983Z Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:19<02:35,  1.09s/it]
2026-02-22T18:23:08.3123965Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:08.3124449Z Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:20<02:31,  1.06s/it]
2026-02-22T18:23:09.3905218Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:09.3905709Z Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:21<02:30,  1.07s/it]
2026-02-22T18:23:10.3425783Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:10.3426348Z Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:22<02:24,  1.03s/it]
2026-02-22T18:23:11.3731521Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:11.3731978Z Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:23<02:23,  1.03s/it]
2026-02-22T18:23:12.3907113Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:12.3907619Z Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:24<02:21,  1.03s/it]
2026-02-22T18:23:13.6548209Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:14.7511817Z Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:25<02:30,  1.10s/it]
2026-02-22T18:23:14.7512631Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:14.7513159Z Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:26<02:29,  1.10s/it]
2026-02-22T18:23:15.7602199Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:15.7602697Z Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:27<02:24,  1.07s/it]
2026-02-22T18:23:16.8519050Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:16.8519528Z Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:28<02:24,  1.08s/it]
2026-02-22T18:23:17.9114205Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:17.9114683Z Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:29<02:22,  1.07s/it]
2026-02-22T18:23:19.0682728Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:19.0683284Z Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:30<02:24,  1.10s/it]
2026-02-22T18:23:20.1833474Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:20.1833921Z Loading safetensors checkpoint shards:  20% Completed | 32/163 [00:32<02:24,  1.10s/it]
2026-02-22T18:23:21.3017532Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:21.3018263Z Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:33<02:23,  1.11s/it]
2026-02-22T18:23:22.3000329Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:22.3000799Z Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:34<02:18,  1.07s/it]
2026-02-22T18:23:23.3766526Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:23.3766980Z Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:35<02:17,  1.08s/it]
2026-02-22T18:23:24.3686454Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:24.3686896Z Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:36<02:13,  1.05s/it]
2026-02-22T18:23:25.3287143Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:25.3287592Z Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:37<02:08,  1.02s/it]
2026-02-22T18:23:26.3071212Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:26.3071625Z Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:38<02:06,  1.01s/it]
2026-02-22T18:23:27.3727946Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:27.3728391Z Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:39<02:07,  1.03s/it]
2026-02-22T18:23:28.3664728Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:28.3665197Z Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:40<02:05,  1.02s/it]
2026-02-22T18:23:29.4367582Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:29.4368006Z Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:41<02:05,  1.03s/it]
2026-02-22T18:23:30.5357207Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:30.5357687Z Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:42<02:07,  1.05s/it]
2026-02-22T18:23:31.5739468Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:31.5739923Z Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:43<02:05,  1.05s/it]
2026-02-22T18:23:32.6759893Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:32.6760372Z Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:44<02:06,  1.06s/it]
2026-02-22T18:23:33.7754553Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:33.7755022Z Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:45<02:06,  1.07s/it]
2026-02-22T18:23:34.9764462Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:34.9764928Z Loading safetensors checkpoint shards:  28% Completed | 46/163 [00:46<02:10,  1.11s/it]
2026-02-22T18:23:35.6201974Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:35.6202597Z Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:47<01:52,  1.03it/s]
2026-02-22T18:23:36.7642506Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:36.7643017Z Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:48<01:29,  1.27it/s]
2026-02-22T18:23:37.8642294Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:37.8642855Z Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:49<01:37,  1.16it/s]
2026-02-22T18:23:38.9315886Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:38.9316413Z Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:50<01:42,  1.09it/s]
2026-02-22T18:23:40.0507500Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:40.0508007Z Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:51<01:47,  1.03it/s]
2026-02-22T18:23:41.1234420Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:41.1234991Z Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:53<01:50,  1.00s/it]
2026-02-22T18:23:42.2071413Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:42.2071857Z Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:54<01:51,  1.02s/it]
2026-02-22T18:23:43.3272604Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:43.3273083Z Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:55<01:53,  1.05s/it]
2026-02-22T18:23:44.4076650Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:44.4077382Z Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:56<01:53,  1.06s/it]
2026-02-22T18:23:45.5790978Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:45.5791407Z Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:57<01:55,  1.09s/it]
2026-02-22T18:23:46.6646158Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:46.6646674Z Loading safetensors checkpoint shards:  36% Completed | 58/163 [00:58<01:54,  1.09s/it]
2026-02-22T18:23:47.7802352Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:47.7802798Z Loading safetensors checkpoint shards:  36% Completed | 59/163 [00:59<01:54,  1.10s/it]
2026-02-22T18:23:48.8749737Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:48.8750158Z Loading safetensors checkpoint shards:  37% Completed | 60/163 [01:00<01:53,  1.10s/it]
2026-02-22T18:23:49.9553836Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:49.9554392Z Loading safetensors checkpoint shards:  37% Completed | 61/163 [01:01<01:51,  1.09s/it]
2026-02-22T18:23:50.9501929Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:50.9502546Z Loading safetensors checkpoint shards:  38% Completed | 62/163 [01:02<01:47,  1.06s/it]
2026-02-22T18:23:52.0778648Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:52.0779164Z Loading safetensors checkpoint shards:  39% Completed | 63/163 [01:04<01:48,  1.08s/it]
2026-02-22T18:23:53.2744045Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:53.2744504Z Loading safetensors checkpoint shards:  39% Completed | 64/163 [01:05<01:50,  1.12s/it]
2026-02-22T18:23:54.2713632Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:54.2714170Z Loading safetensors checkpoint shards:  40% Completed | 65/163 [01:06<01:45,  1.08s/it]
2026-02-22T18:23:55.3276550Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:55.3277007Z Loading safetensors checkpoint shards:  40% Completed | 66/163 [01:07<01:44,  1.07s/it]
2026-02-22T18:23:56.3895704Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:56.3896167Z Loading safetensors checkpoint shards:  41% Completed | 67/163 [01:08<01:42,  1.07s/it]
2026-02-22T18:23:57.4936918Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:57.4937334Z Loading safetensors checkpoint shards:  42% Completed | 68/163 [01:09<01:42,  1.08s/it]
2026-02-22T18:23:58.7576200Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:58.7576659Z Loading safetensors checkpoint shards:  42% Completed | 69/163 [01:10<01:46,  1.14s/it]
2026-02-22T18:23:59.6512554Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:23:59.6513011Z Loading safetensors checkpoint shards:  43% Completed | 70/163 [01:11<01:38,  1.06s/it]
2026-02-22T18:24:00.6832693Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:00.6833228Z Loading safetensors checkpoint shards:  44% Completed | 71/163 [01:12<01:36,  1.05s/it]
2026-02-22T18:24:01.6486183Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:01.6486678Z Loading safetensors checkpoint shards:  44% Completed | 72/163 [01:13<01:33,  1.03s/it]
2026-02-22T18:24:02.7351721Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:02.7352284Z Loading safetensors checkpoint shards:  45% Completed | 73/163 [01:14<01:34,  1.04s/it]
2026-02-22T18:24:03.7743344Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:03.7743868Z Loading safetensors checkpoint shards:  45% Completed | 74/163 [01:15<01:32,  1.04s/it]
2026-02-22T18:24:04.8441469Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:04.8442111Z Loading safetensors checkpoint shards:  46% Completed | 75/163 [01:16<01:32,  1.05s/it]
2026-02-22T18:24:05.9719651Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:05.9720100Z Loading safetensors checkpoint shards:  47% Completed | 76/163 [01:17<01:33,  1.07s/it]
2026-02-22T18:24:06.9284265Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:06.9284724Z Loading safetensors checkpoint shards:  47% Completed | 77/163 [01:18<01:29,  1.04s/it]
2026-02-22T18:24:08.0246198Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:08.0247018Z Loading safetensors checkpoint shards:  48% Completed | 78/163 [01:19<01:29,  1.06s/it]
2026-02-22T18:24:09.1006854Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:09.1007282Z Loading safetensors checkpoint shards:  48% Completed | 79/163 [01:21<01:29,  1.06s/it]
2026-02-22T18:24:10.0497221Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:10.0497681Z Loading safetensors checkpoint shards:  49% Completed | 80/163 [01:21<01:25,  1.03s/it]
2026-02-22T18:24:11.0598765Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:11.0599219Z Loading safetensors checkpoint shards:  50% Completed | 81/163 [01:22<01:23,  1.02s/it]
2026-02-22T18:24:12.0888434Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:12.0888909Z Loading safetensors checkpoint shards:  50% Completed | 82/163 [01:24<01:23,  1.03s/it]
2026-02-22T18:24:13.1142713Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:13.1143179Z Loading safetensors checkpoint shards:  51% Completed | 83/163 [01:25<01:22,  1.03s/it]
2026-02-22T18:24:14.2221442Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:14.2221911Z Loading safetensors checkpoint shards:  52% Completed | 84/163 [01:26<01:22,  1.05s/it]
2026-02-22T18:24:15.2965953Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:15.2966434Z Loading safetensors checkpoint shards:  52% Completed | 85/163 [01:27<01:22,  1.06s/it]
2026-02-22T18:24:16.3405170Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:16.3405679Z Loading safetensors checkpoint shards:  53% Completed | 86/163 [01:28<01:21,  1.05s/it]
2026-02-22T18:24:17.3982258Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:17.3982680Z Loading safetensors checkpoint shards:  53% Completed | 87/163 [01:29<01:20,  1.05s/it]
2026-02-22T18:24:17.5280630Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:17.5281073Z Loading safetensors checkpoint shards:  55% Completed | 90/163 [01:29<00:35,  2.06it/s]
2026-02-22T18:24:17.6426630Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:17.6427052Z Loading safetensors checkpoint shards:  57% Completed | 93/163 [01:29<00:20,  3.49it/s]
2026-02-22T18:24:17.7594264Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:17.7594725Z Loading safetensors checkpoint shards:  59% Completed | 96/163 [01:29<00:12,  5.26it/s]
2026-02-22T18:24:17.8769326Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:17.8769739Z Loading safetensors checkpoint shards:  61% Completed | 99/163 [01:29<00:08,  7.34it/s]
2026-02-22T18:24:18.0745322Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:18.0745733Z Loading safetensors checkpoint shards:  62% Completed | 101/163 [01:30<00:07,  7.89it/s]
2026-02-22T18:24:18.1791968Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:18.1792537Z Loading safetensors checkpoint shards:  64% Completed | 104/163 [01:30<00:05, 10.60it/s]
2026-02-22T18:24:19.8381322Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:19.8381785Z Loading safetensors checkpoint shards:  66% Completed | 107/163 [01:31<00:13,  4.03it/s]
2026-02-22T18:24:19.9405705Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:19.9406131Z Loading safetensors checkpoint shards:  67% Completed | 110/163 [01:31<00:09,  5.59it/s]
2026-02-22T18:24:20.0419042Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:20.0419541Z Loading safetensors checkpoint shards:  69% Completed | 113/163 [01:31<00:06,  7.52it/s]
2026-02-22T18:24:20.1477721Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:20.1478137Z Loading safetensors checkpoint shards:  71% Completed | 116/163 [01:32<00:04,  9.75it/s]
2026-02-22T18:24:20.2530492Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:20.2530927Z Loading safetensors checkpoint shards:  73% Completed | 119/163 [01:32<00:03, 12.22it/s]
2026-02-22T18:24:20.3587321Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:20.3588154Z Loading safetensors checkpoint shards:  75% Completed | 122/163 [01:32<00:02, 14.80it/s]
2026-02-22T18:24:20.4632317Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:20.4632782Z Loading safetensors checkpoint shards:  77% Completed | 125/163 [01:32<00:02, 17.36it/s]
2026-02-22T18:24:20.5682065Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:20.5683516Z Loading safetensors checkpoint shards:  79% Completed | 128/163 [01:32<00:01, 19.71it/s]
2026-02-22T18:24:20.6758091Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:20.6758510Z Loading safetensors checkpoint shards:  81% Completed | 132/163 [01:32<00:01, 23.76it/s]
2026-02-22T18:24:20.8175859Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:20.8176240Z Loading safetensors checkpoint shards:  83% Completed | 136/163 [01:32<00:01, 25.13it/s]
2026-02-22T18:24:20.9215610Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:20.9216063Z Loading safetensors checkpoint shards:  85% Completed | 139/163 [01:32<00:00, 26.04it/s]
2026-02-22T18:24:21.0281443Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:21.0281850Z Loading safetensors checkpoint shards:  87% Completed | 142/163 [01:32<00:00, 26.58it/s]
2026-02-22T18:24:21.1341775Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:21.1342330Z Loading safetensors checkpoint shards:  89% Completed | 145/163 [01:33<00:00, 27.05it/s]
2026-02-22T18:24:21.2400471Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:21.2400878Z Loading safetensors checkpoint shards:  91% Completed | 148/163 [01:33<00:00, 27.43it/s]
2026-02-22T18:24:24.1429940Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:24.1430420Z Loading safetensors checkpoint shards:  93% Completed | 151/163 [01:36<00:03,  3.24it/s]
2026-02-22T18:24:24.2464063Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:24.2464612Z Loading safetensors checkpoint shards:  94% Completed | 154/163 [01:36<00:02,  4.39it/s]
2026-02-22T18:24:24.3492673Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:24.3493237Z Loading safetensors checkpoint shards:  96% Completed | 157/163 [01:36<00:01,  5.87it/s]
2026-02-22T18:24:24.4522508Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:24.4523053Z Loading safetensors checkpoint shards:  98% Completed | 160/163 [01:36<00:00,  7.69it/s]
2026-02-22T18:24:24.5321704Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-22 18:24:24 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-22T18:24:24.5554303Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:24.5554737Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [01:36<00:00,  9.85it/s]
2026-02-22T18:24:24.5576343Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:24.5576741Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [01:36<00:00,  1.69it/s]
2026-02-22T18:24:24.5585875Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:24:24.5867222Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-22 18:24:24 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-22T18:24:24.6012189Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m INFO 02-22 18:24:24 [default_loader.py:291] Loading weights took 96.53 seconds
2026-02-22T18:24:24.6378180Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m INFO 02-22 18:24:24 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-22T18:24:24.7014974Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m INFO 02-22 18:24:24 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-22T18:24:24.7240963Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-22 18:24:24 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-22T18:24:24.7294716Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m INFO 02-22 18:24:24 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-22T18:24:24.7404344Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m INFO 02-22 18:24:24 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-22T18:24:24.7483216Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-22 18:24:24 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-22T18:24:24.7559726Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m INFO 02-22 18:24:24 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-22T18:24:24.7985499Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-22 18:24:24 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-22T18:24:25.0073193Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-22 18:24:25 [default_loader.py:291] Loading weights took 96.94 seconds
2026-02-22T18:24:25.0446894Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-22 18:24:25 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-22T18:24:25.1808461Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-22 18:24:25 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-22T18:24:25.2449591Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-22 18:24:25 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-22T18:24:25.2530100Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-22 18:24:25 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-22T18:24:25.2555698Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-22 18:24:25 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-22T18:24:25.2623268Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m INFO 02-22 18:24:25 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-22T18:24:25.9911512Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-22 18:24:25 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-22T18:24:26.2671973Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-22 18:24:26 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-22T18:24:26.4137259Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m INFO 02-22 18:24:26 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-22T18:24:26.4799786Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-22 18:24:26 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-22T18:24:26.5385327Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-22 18:24:26 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-22T18:24:26.5698982Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m INFO 02-22 18:24:26 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-22T18:24:26.5790551Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-22 18:24:26 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-22T18:24:26.8408170Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m INFO 02-22 18:24:26 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-22T18:24:26.9338369Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m INFO 02-22 18:24:26 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-22T18:24:27.1635296Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-22 18:24:27 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-22T18:24:27.2288757Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-22 18:24:27 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-22T18:24:27.4711818Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-22 18:24:27 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-22T18:24:27.4778528Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m INFO 02-22 18:24:27 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-22T18:24:27.8228671Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m INFO 02-22 18:24:27 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-22T18:24:27.8416556Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-22 18:24:27 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-22T18:24:27.9927391Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-22 18:24:27 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-22T18:24:48.2217447Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-22 18:24:48 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/5f4e5deba5/rank_0_1/backbone for vLLM's torch.compile
2026-02-22T18:24:48.2246115Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-22 18:24:48 [backends.py:865] Dynamo bytecode transform time: 4.64 s
2026-02-22T18:24:48.4550826Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m INFO 02-22 18:24:48 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/5f4e5deba5/rank_0_0/backbone for vLLM's torch.compile
2026-02-22T18:24:48.4580185Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m INFO 02-22 18:24:48 [backends.py:865] Dynamo bytecode transform time: 4.87 s
2026-02-22T18:24:54.4968529Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-22T18:24:54.4975220Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m   warnings.warn(
2026-02-22T18:24:54.5001419Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-22T18:24:54.5011045Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m   warnings.warn(
2026-02-22T18:24:54.5326307Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-22T18:24:54.5335039Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m   warnings.warn(
2026-02-22T18:24:54.5436880Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-22T18:24:54.5445290Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m   warnings.warn(
2026-02-22T18:24:54.5639570Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-22T18:24:54.5648077Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m   warnings.warn(
2026-02-22T18:24:54.5659007Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-22T18:24:54.5667500Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m   warnings.warn(
2026-02-22T18:24:54.5694442Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-22T18:24:54.5702842Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m   warnings.warn(
2026-02-22T18:24:54.5835633Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-22T18:24:54.5845815Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m   warnings.warn(
2026-02-22T18:24:54.6322980Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-22T18:24:54.6331602Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m   warnings.warn(
2026-02-22T18:24:54.6343098Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-22T18:24:54.6352245Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m   warnings.warn(
2026-02-22T18:24:54.6363242Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-22T18:24:54.6371720Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m   warnings.warn(
2026-02-22T18:24:54.6384225Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-22T18:24:54.6392869Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m   warnings.warn(
2026-02-22T18:24:54.6479007Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-22T18:24:54.6487183Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m   warnings.warn(
2026-02-22T18:24:54.6601732Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-22T18:24:54.6609639Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m   warnings.warn(
2026-02-22T18:24:54.6688467Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-22T18:24:54.6698806Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m   warnings.warn(
2026-02-22T18:24:54.7182999Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-22T18:24:54.7191336Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m   warnings.warn(
2026-02-22T18:25:07.4460860Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-22 18:25:07 [backends.py:319] Compiling a graph for compile range (1, 4096) takes 12.88 s
2026-02-22T18:25:07.4619117Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-22 18:25:07 [monitor.py:34] torch.compile takes 17.52 s in total
2026-02-22T18:25:08.3201602Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m INFO 02-22 18:25:08 [backends.py:319] Compiling a graph for compile range (1, 4096) takes 13.60 s
2026-02-22T18:25:08.3232645Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m INFO 02-22 18:25:08 [monitor.py:34] torch.compile takes 18.47 s in total
2026-02-22T18:25:14.4067075Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-22 18:25:14 [worker.py:338] Available memory: 18547923660, total memory: 65796046848
2026-02-22T18:25:14.4211187Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-22 18:25:14 [worker.py:338] Available memory: 18554516172, total memory: 65796046848
2026-02-22T18:25:14.4525373Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-22 18:25:14 [worker.py:338] Available memory: 18269484544, total memory: 65787658240
2026-02-22T18:25:14.4700693Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-22 18:25:14 [worker.py:338] Available memory: 18244125184, total memory: 65787658240
2026-02-22T18:25:14.4887556Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-22 18:25:14 [worker.py:338] Available memory: 18257846784, total memory: 65787658240
2026-02-22T18:25:14.5206670Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-22 18:25:14 [worker.py:338] Available memory: 17856297472, total memory: 65787658240
2026-02-22T18:25:14.5302239Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m INFO 02-22 18:25:14 [worker.py:338] Available memory: 18545101516, total memory: 65796046848
2026-02-22T18:25:14.7008100Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-22 18:25:14 [worker.py:338] Available memory: 18546428620, total memory: 65796046848
2026-02-22T18:25:14.7935042Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m INFO 02-22 18:25:14 [worker.py:338] Available memory: 18551983820, total memory: 65796046848
2026-02-22T18:25:15.0924228Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-22 18:25:15 [worker.py:338] Available memory: 18554237644, total memory: 65796046848
2026-02-22T18:25:15.1505191Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m INFO 02-22 18:25:15 [worker.py:338] Available memory: 18263664128, total memory: 65787658240
2026-02-22T18:25:15.2846152Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-22 18:25:15 [worker.py:338] Available memory: 18535264972, total memory: 65796046848
2026-02-22T18:25:15.2970939Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m INFO 02-22 18:25:15 [worker.py:338] Available memory: 17859045888, total memory: 65787658240
2026-02-22T18:25:15.3308856Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-22 18:25:15 [worker.py:338] Available memory: 18255014400, total memory: 65787658240
2026-02-22T18:25:15.4527525Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m INFO 02-22 18:25:15 [worker.py:338] Available memory: 18561620684, total memory: 65796046848
2026-02-22T18:25:15.4553522Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-22 18:25:15 [kv_cache_utils.py:1307] GPU KV cache size: 204,544 tokens
2026-02-22T18:25:15.4565554Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-22 18:25:15 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 24.97x
2026-02-22T18:25:15.9304774Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m INFO 02-22 18:25:15 [worker.py:338] Available memory: 18258657792, total memory: 65787658240
2026-02-22T18:25:15.9335825Z [0;36m(EngineCore_DP1 pid=174)[0;0m INFO 02-22 18:25:15 [kv_cache_utils.py:1307] GPU KV cache size: 204,544 tokens
2026-02-22T18:25:15.9376685Z [0;36m(EngineCore_DP1 pid=174)[0;0m INFO 02-22 18:25:15 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 24.97x
2026-02-22T18:25:33.8569000Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m 
2026-02-22T18:25:33.8577394Z Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s][rank9]:[W222 18:25:33.781592958 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-22T18:25:33.8578499Z [rank14]:[W222 18:25:33.781608778 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-22T18:25:33.8587584Z [rank8]:[W222 18:25:33.781621278 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-22T18:25:33.8598222Z [rank13]:[W222 18:25:33.781631928 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-22T18:25:33.8608955Z [rank15]:[W222 18:25:33.781635438 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-22T18:25:33.8617633Z [rank4]:[W222 18:25:33.782596955 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-22T18:25:33.8627722Z [rank5]:[W222 18:25:33.782872527 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-22T18:25:33.8637717Z [rank3]:[W222 18:25:33.783113879 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-22T18:25:33.8647272Z [rank2]:[W222 18:25:33.783160639 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-22T18:25:33.8657702Z [rank1]:[W222 18:25:33.783169599 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-22T18:25:33.8667287Z [rank10]:[W222 18:25:33.783258010 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-22T18:25:33.8677947Z [rank12]:[W222 18:25:33.783276920 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-22T18:25:33.8687862Z [rank6]:[W222 18:25:33.783938795 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-22T18:25:33.8697323Z [rank0]:[W222 18:25:33.784874632 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-22T18:25:33.8707015Z [rank11]:[W222 18:25:33.791972233 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-22T18:25:33.8717249Z [rank7]:[W222 18:25:33.795886642 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-22T18:25:36.2267091Z [rank4]:[W222 18:25:36.151681079 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-22T18:25:36.2274953Z [rank3]:[W222 18:25:36.151763590 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-22T18:25:36.2296259Z [rank7]:[W222 18:25:36.152279684 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-22T18:25:36.2305610Z [rank1]:[W222 18:25:36.152335404 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-22T18:25:36.2315679Z [rank6]:[W222 18:25:36.152349074 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-22T18:25:36.2325349Z [rank0]:[W222 18:25:36.152794487 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-22T18:25:36.2335352Z [rank5]:[W222 18:25:36.152990409 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-22T18:25:36.2346202Z [rank2]:[W222 18:25:36.153798805 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-22T18:25:36.3228306Z [rank12]:[W222 18:25:36.248304114 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-22T18:25:36.3255403Z [rank9]:[W222 18:25:36.248918389 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-22T18:25:36.3264703Z [rank13]:[W222 18:25:36.250251028 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-22T18:25:36.3274628Z [rank10]:[W222 18:25:36.250300539 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-22T18:25:36.3284590Z [rank15]:[W222 18:25:36.250617541 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-22T18:25:36.3294440Z [rank8]:[W222 18:25:36.250632531 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-22T18:25:36.3303674Z [rank14]:[W222 18:25:36.250672841 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-22T18:25:36.3313210Z [rank11]:[W222 18:25:36.250920703 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-22T18:25:39.2896793Z 
2026-02-22T18:25:39.2897798Z Capturing CUDA graphs (decode, FULL):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:18<00:18, 18.43s/it]
2026-02-22T18:25:39.2898443Z Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:21<00:00,  9.32s/it]
2026-02-22T18:25:39.2899000Z Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:21<00:00, 10.69s/it]
2026-02-22T18:25:39.9296449Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-22 18:25:39 [gpu_model_runner.py:5051] Graph capturing finished in 22 secs, took 0.27 GiB
2026-02-22T18:25:41.0380749Z [0;36m(EngineCore_DP1 pid=174)[0;0m INFO 02-22 18:25:41 [core.py:272] init engine (profile, create kv cache, warmup model) took 73.04 seconds
2026-02-22T18:25:41.0780634Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m INFO 02-22 18:25:41 [gpu_model_runner.py:5051] Graph capturing finished in 24 secs, took 0.27 GiB
2026-02-22T18:25:41.0887187Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-22 18:25:41 [core.py:272] init engine (profile, create kv cache, warmup model) took 73.24 seconds
2026-02-22T18:25:41.9864431Z INFO 02-22 18:25:41 [coordinator.py:200] All engine subscriptions received by DP coordinator
2026-02-22T18:25:41.9873281Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:25:41 [loggers.py:1219] AsyncLLM created with api_server_count more than 1; disabling stats logging to avoid incomplete stats.
2026-02-22T18:25:41.9883446Z [0;36m(EngineCore_DP1 pid=174)[0;0m INFO 02-22 18:25:41 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-22T18:25:41.9893211Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:25:41 [loggers.py:1219] AsyncLLM created with api_server_count more than 1; disabling stats logging to avoid incomplete stats.
2026-02-22T18:25:41.9902714Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-22 18:25:41 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-22T18:25:41.9913404Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:25:41 [loggers.py:1219] AsyncLLM created with api_server_count more than 1; disabling stats logging to avoid incomplete stats.
2026-02-22T18:25:41.9923573Z [0;36m(EngineCore_DP1 pid=174)[0;0m INFO 02-22 18:25:41 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:25:41.9933931Z [0;36m(EngineCore_DP1 pid=174)[0;0m INFO 02-22 18:25:41 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:25:41.9944951Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-22 18:25:41 [ascend_config.py:412] Dynamic EPLB is False
2026-02-22T18:25:41.9957450Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:25:41 [loggers.py:1219] AsyncLLM created with api_server_count more than 1; disabling stats logging to avoid incomplete stats.
2026-02-22T18:25:41.9968319Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-22 18:25:41 [ascend_config.py:413] The number of redundant experts is 0
2026-02-22T18:25:41.9979026Z [0;36m(EngineCore_DP1 pid=174)[0;0m INFO 02-22 18:25:41 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:25:41.9989417Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-22 18:25:41 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-22T18:25:41.9999940Z [0;36m(EngineCore_DP1 pid=174)[0;0m INFO 02-22 18:25:41 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-22T18:25:42.0010514Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-22 18:25:41 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-22T18:25:42.0019493Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-22 18:25:41 [platform.py:335] [91m
2026-02-22T18:25:42.0029986Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             **********************************************************************************
2026-02-22T18:25:42.0040601Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-22T18:25:42.0051372Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-22T18:25:42.0061943Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-22T18:25:42.0071864Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-22T18:25:42.0083049Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-22T18:25:42.0092728Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             * batch size for graph capture.
2026-02-22T18:25:42.0120028Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             * For more details, please refer to:
2026-02-22T18:25:42.0120872Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-22T18:25:42.0134209Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             **********************************************************************************[0m
2026-02-22T18:25:42.0154606Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             
2026-02-22T18:25:42.0172470Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-22 18:25:41 [platform.py:335] [91m
2026-02-22T18:25:42.0179856Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             **********************************************************************************
2026-02-22T18:25:42.0196355Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-22T18:25:42.0206214Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-22T18:25:42.0217432Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-22T18:25:42.0226873Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-22T18:25:42.0238783Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-22T18:25:42.0248908Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             * batch size for graph capture.
2026-02-22T18:25:42.0259028Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             * For more details, please refer to:
2026-02-22T18:25:42.0269571Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-22T18:25:42.0280483Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             **********************************************************************************[0m
2026-02-22T18:25:42.0289756Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-22 18:25:41 [platform.py:335]             
2026-02-22T18:25:42.0299861Z INFO 02-22 18:25:41 [utils.py:249] Waiting for API servers to complete ...
2026-02-22T18:25:42.0310419Z [0;36m(EngineCore_DP1 pid=174)[0;0m INFO 02-22 18:25:41 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-22T18:25:42.0320803Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-22 18:25:41 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-22T18:25:42.5039068Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [api_server.py:665] Supported tasks: ['generate']
2026-02-22T18:25:42.5045635Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [api_server.py:665] Supported tasks: ['generate']
2026-02-22T18:25:42.5065302Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [api_server.py:665] Supported tasks: ['generate']
2026-02-22T18:25:42.5100324Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:25:42 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'top_p': 0.95}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
2026-02-22T18:25:42.5108511Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:25:42 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'top_p': 0.95}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
2026-02-22T18:25:42.5118072Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:25:42 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'top_p': 0.95}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
2026-02-22T18:25:42.5130234Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [serving.py:177] Warming up chat template processing...
2026-02-22T18:25:42.5139641Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [serving.py:177] Warming up chat template processing...
2026-02-22T18:25:42.5147418Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [serving.py:177] Warming up chat template processing...
2026-02-22T18:25:42.5157629Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [api_server.py:665] Supported tasks: ['generate']
2026-02-22T18:25:42.5208363Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:25:42 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'top_p': 0.95}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
2026-02-22T18:25:42.5216071Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [serving.py:177] Warming up chat template processing...
2026-02-22T18:25:42.5409618Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [serving.py:212] Chat template warmup completed in 31.2ms
2026-02-22T18:25:42.5427558Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [serving.py:212] Chat template warmup completed in 30.6ms
2026-02-22T18:25:42.5438889Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [serving.py:212] Chat template warmup completed in 25.4ms
2026-02-22T18:25:42.5448432Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [serving.py:212] Chat template warmup completed in 31.3ms
2026-02-22T18:25:42.5463628Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [api_server.py:946] Starting vLLM API server 1 on http://0.0.0.0:8080
2026-02-22T18:25:42.5474014Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:38] Available routes are:
2026-02-22T18:25:42.5484669Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
2026-02-22T18:25:42.5496338Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /docs, Methods: HEAD, GET
2026-02-22T18:25:42.5504930Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
2026-02-22T18:25:42.5513476Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
2026-02-22T18:25:42.5525564Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
2026-02-22T18:25:42.5536890Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
2026-02-22T18:25:42.5544580Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /tokenize, Methods: POST
2026-02-22T18:25:42.5555917Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /detokenize, Methods: POST
2026-02-22T18:25:42.5564690Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
2026-02-22T18:25:42.5575560Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /pause, Methods: POST
2026-02-22T18:25:42.5584957Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /resume, Methods: POST
2026-02-22T18:25:42.5594428Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /is_paused, Methods: GET
2026-02-22T18:25:42.5603744Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /metrics, Methods: GET
2026-02-22T18:25:42.5613973Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /health, Methods: GET
2026-02-22T18:25:42.5623485Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
2026-02-22T18:25:42.5633434Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
2026-02-22T18:25:42.5644318Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/responses, Methods: POST
2026-02-22T18:25:42.5650467Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
2026-02-22T18:25:42.5661478Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
2026-02-22T18:25:42.5670038Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
2026-02-22T18:25:42.5679383Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
2026-02-22T18:25:42.5690325Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/completions, Methods: POST
2026-02-22T18:25:42.5700245Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/completions/render, Methods: POST
2026-02-22T18:25:42.5707777Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/messages, Methods: POST
2026-02-22T18:25:42.5717439Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/models, Methods: GET
2026-02-22T18:25:42.5727165Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /load, Methods: GET
2026-02-22T18:25:42.5739495Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /version, Methods: GET
2026-02-22T18:25:42.5748056Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [api_server.py:946] Starting vLLM API server 3 on http://0.0.0.0:8080
2026-02-22T18:25:42.5757651Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /ping, Methods: GET
2026-02-22T18:25:42.5770540Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /ping, Methods: POST
2026-02-22T18:25:42.5808619Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /invocations, Methods: POST
2026-02-22T18:25:42.5814268Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /classify, Methods: POST
2026-02-22T18:25:42.5822895Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/embeddings, Methods: POST
2026-02-22T18:25:42.5831336Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:38] Available routes are:
2026-02-22T18:25:42.5841410Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /score, Methods: POST
2026-02-22T18:25:42.5853019Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/score, Methods: POST
2026-02-22T18:25:42.5861191Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
2026-02-22T18:25:42.5872404Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /rerank, Methods: POST
2026-02-22T18:25:42.5881366Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/rerank, Methods: POST
2026-02-22T18:25:42.5891881Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /docs, Methods: HEAD, GET
2026-02-22T18:25:42.5901033Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v2/rerank, Methods: POST
2026-02-22T18:25:42.5911736Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
2026-02-22T18:25:42.5923218Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /pooling, Methods: POST
2026-02-22T18:25:42.5932986Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
2026-02-22T18:25:42.5942666Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
2026-02-22T18:25:42.5952464Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
2026-02-22T18:25:42.5962440Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /tokenize, Methods: POST
2026-02-22T18:25:42.5973094Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /detokenize, Methods: POST
2026-02-22T18:25:42.5982803Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
2026-02-22T18:25:42.5993784Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /pause, Methods: POST
2026-02-22T18:25:42.6002852Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /resume, Methods: POST
2026-02-22T18:25:42.6015610Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /is_paused, Methods: GET
2026-02-22T18:25:42.6020597Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /metrics, Methods: GET
2026-02-22T18:25:42.6030259Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /health, Methods: GET
2026-02-22T18:25:42.6039962Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
2026-02-22T18:25:42.6052296Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
2026-02-22T18:25:42.6059986Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/responses, Methods: POST
2026-02-22T18:25:42.6072434Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
2026-02-22T18:25:42.6082358Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
2026-02-22T18:25:42.6090521Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [api_server.py:946] Starting vLLM API server 2 on http://0.0.0.0:8080
2026-02-22T18:25:42.6098945Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
2026-02-22T18:25:42.6109414Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
2026-02-22T18:25:42.6119677Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/completions, Methods: POST
2026-02-22T18:25:42.6129459Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:38] Available routes are:
2026-02-22T18:25:42.6140134Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/completions/render, Methods: POST
2026-02-22T18:25:42.6150824Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/messages, Methods: POST
2026-02-22T18:25:42.6161382Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/models, Methods: GET
2026-02-22T18:25:42.6170212Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
2026-02-22T18:25:42.6180468Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /load, Methods: GET
2026-02-22T18:25:42.6189312Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /docs, Methods: GET, HEAD
2026-02-22T18:25:42.6198427Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /version, Methods: GET
2026-02-22T18:25:42.6208661Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
2026-02-22T18:25:42.6217551Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /ping, Methods: GET
2026-02-22T18:25:42.6229602Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
2026-02-22T18:25:42.6238366Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /ping, Methods: POST
2026-02-22T18:25:42.6249515Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /invocations, Methods: POST
2026-02-22T18:25:42.6259453Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
2026-02-22T18:25:42.6267156Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /classify, Methods: POST
2026-02-22T18:25:42.6274731Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/embeddings, Methods: POST
2026-02-22T18:25:42.6287603Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
2026-02-22T18:25:42.6297052Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /score, Methods: POST
2026-02-22T18:25:42.6306153Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /tokenize, Methods: POST
2026-02-22T18:25:42.6314467Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/score, Methods: POST
2026-02-22T18:25:42.6324228Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /detokenize, Methods: POST
2026-02-22T18:25:42.6334812Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /rerank, Methods: POST
2026-02-22T18:25:42.6343409Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
2026-02-22T18:25:42.6353738Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/rerank, Methods: POST
2026-02-22T18:25:42.6364999Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v2/rerank, Methods: POST
2026-02-22T18:25:42.6372380Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /pause, Methods: POST
2026-02-22T18:25:42.6383023Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /pooling, Methods: POST
2026-02-22T18:25:42.6393072Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /resume, Methods: POST
2026-02-22T18:25:42.6402867Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /is_paused, Methods: GET
2026-02-22T18:25:42.6413360Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /metrics, Methods: GET
2026-02-22T18:25:42.6421336Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /health, Methods: GET
2026-02-22T18:25:42.6431095Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
2026-02-22T18:25:42.6440009Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
2026-02-22T18:25:42.6451218Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/responses, Methods: POST
2026-02-22T18:25:42.6462075Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
2026-02-22T18:25:42.6470837Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
2026-02-22T18:25:42.6477945Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
2026-02-22T18:25:42.6490288Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
2026-02-22T18:25:42.6497900Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/completions, Methods: POST
2026-02-22T18:25:42.6508653Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/completions/render, Methods: POST
2026-02-22T18:25:42.6517627Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/messages, Methods: POST
2026-02-22T18:25:42.6528560Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/models, Methods: GET
2026-02-22T18:25:42.6535827Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /load, Methods: GET
2026-02-22T18:25:42.6546496Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /version, Methods: GET
2026-02-22T18:25:42.6555471Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /ping, Methods: GET
2026-02-22T18:25:42.6565704Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /ping, Methods: POST
2026-02-22T18:25:42.6575729Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /invocations, Methods: POST
2026-02-22T18:25:42.6585198Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /classify, Methods: POST
2026-02-22T18:25:42.6593844Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/embeddings, Methods: POST
2026-02-22T18:25:42.6604043Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /score, Methods: POST
2026-02-22T18:25:42.6613546Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/score, Methods: POST
2026-02-22T18:25:42.6623279Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /rerank, Methods: POST
2026-02-22T18:25:42.6631551Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/rerank, Methods: POST
2026-02-22T18:25:42.6641644Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v2/rerank, Methods: POST
2026-02-22T18:25:42.6653707Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /pooling, Methods: POST
2026-02-22T18:25:42.6665485Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8080
2026-02-22T18:25:42.6672107Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:38] Available routes are:
2026-02-22T18:25:42.6683083Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
2026-02-22T18:25:42.6691894Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /docs, Methods: HEAD, GET
2026-02-22T18:25:42.6702468Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
2026-02-22T18:25:42.6711113Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
2026-02-22T18:25:42.6721552Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
2026-02-22T18:25:42.6730541Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
2026-02-22T18:25:42.6740122Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /tokenize, Methods: POST
2026-02-22T18:25:42.6748809Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /detokenize, Methods: POST
2026-02-22T18:25:42.6760151Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
2026-02-22T18:25:42.6773857Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /pause, Methods: POST
2026-02-22T18:25:42.6787860Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /resume, Methods: POST
2026-02-22T18:25:42.6798704Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /is_paused, Methods: GET
2026-02-22T18:25:42.6810781Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /metrics, Methods: GET
2026-02-22T18:25:42.6819367Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /health, Methods: GET
2026-02-22T18:25:42.6828197Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
2026-02-22T18:25:42.6837825Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
2026-02-22T18:25:42.6847904Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/responses, Methods: POST
2026-02-22T18:25:42.6857230Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
2026-02-22T18:25:42.6870743Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
2026-02-22T18:25:42.6881268Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
2026-02-22T18:25:42.6892808Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
2026-02-22T18:25:42.6903671Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/completions, Methods: POST
2026-02-22T18:25:42.6915243Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/completions/render, Methods: POST
2026-02-22T18:25:42.6924941Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/messages, Methods: POST
2026-02-22T18:25:42.6935322Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/models, Methods: GET
2026-02-22T18:25:42.6945532Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /load, Methods: GET
2026-02-22T18:25:42.6952825Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /version, Methods: GET
2026-02-22T18:25:42.6964975Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /ping, Methods: GET
2026-02-22T18:25:42.6974193Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /ping, Methods: POST
2026-02-22T18:25:42.6983932Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /invocations, Methods: POST
2026-02-22T18:25:42.6994666Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /classify, Methods: POST
2026-02-22T18:25:42.7002869Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/embeddings, Methods: POST
2026-02-22T18:25:42.7011718Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /score, Methods: POST
2026-02-22T18:25:42.7021365Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/score, Methods: POST
2026-02-22T18:25:42.7029918Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /rerank, Methods: POST
2026-02-22T18:25:42.7040437Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v1/rerank, Methods: POST
2026-02-22T18:25:42.7050444Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /v2/rerank, Methods: POST
2026-02-22T18:25:42.7059936Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:25:42 [launcher.py:46] Route: /pooling, Methods: POST
2026-02-22T18:25:42.7072053Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:25:42 [warnings.py:110] /usr/local/python3.11.14/lib/python3.11/site-packages/websockets/legacy/__init__.py:6: DeprecationWarning: websockets.legacy is deprecated; see https://websockets.readthedocs.io/en/stable/howto/upgrade.html for upgrade instructions
2026-02-22T18:25:42.7081238Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:25:42 [warnings.py:110] /usr/local/python3.11.14/lib/python3.11/site-packages/websockets/legacy/__init__.py:6: DeprecationWarning: websockets.legacy is deprecated; see https://websockets.readthedocs.io/en/stable/howto/upgrade.html for upgrade instructions
2026-02-22T18:25:42.7091228Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:25:42 [warnings.py:110] /usr/local/python3.11.14/lib/python3.11/site-packages/websockets/legacy/__init__.py:6: DeprecationWarning: websockets.legacy is deprecated; see https://websockets.readthedocs.io/en/stable/howto/upgrade.html for upgrade instructions
2026-02-22T18:25:42.7100968Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:25:42 [warnings.py:110] /usr/local/python3.11.14/lib/python3.11/site-packages/websockets/legacy/__init__.py:6: DeprecationWarning: websockets.legacy is deprecated; see https://websockets.readthedocs.io/en/stable/howto/upgrade.html for upgrade instructions
2026-02-22T18:25:42.7111126Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:25:42 [warnings.py:110] /usr/local/python3.11.14/lib/python3.11/site-packages/uvicorn/protocols/websockets/websockets_impl.py:17: DeprecationWarning: websockets.server.WebSocketServerProtocol is deprecated
2026-02-22T18:25:42.7120793Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:25:42 [warnings.py:110] /usr/local/python3.11.14/lib/python3.11/site-packages/uvicorn/protocols/websockets/websockets_impl.py:17: DeprecationWarning: websockets.server.WebSocketServerProtocol is deprecated
2026-02-22T18:25:42.7132113Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:25:42 [warnings.py:110] /usr/local/python3.11.14/lib/python3.11/site-packages/uvicorn/protocols/websockets/websockets_impl.py:17: DeprecationWarning: websockets.server.WebSocketServerProtocol is deprecated
2026-02-22T18:25:42.7140605Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:25:42 [warnings.py:110] /usr/local/python3.11.14/lib/python3.11/site-packages/uvicorn/protocols/websockets/websockets_impl.py:17: DeprecationWarning: websockets.server.WebSocketServerProtocol is deprecated
2026-02-22T18:25:42.7150737Z [0;36m(ApiServer_1 pid=186)[0;0m INFO:     Started server process [186]
2026-02-22T18:25:42.7159032Z [0;36m(ApiServer_1 pid=186)[0;0m INFO:     Waiting for application startup.
2026-02-22T18:25:42.7170615Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     Started server process [188]
2026-02-22T18:25:42.7178892Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     Waiting for application startup.
2026-02-22T18:25:42.7188376Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     Started server process [187]
2026-02-22T18:25:42.7200556Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     Waiting for application startup.
2026-02-22T18:25:42.7207286Z [0;36m(ApiServer_0 pid=185)[0;0m INFO:     Started server process [185]
2026-02-22T18:25:42.7215925Z [0;36m(ApiServer_0 pid=185)[0;0m INFO:     Waiting for application startup.
2026-02-22T18:25:42.9440778Z [0;36m(ApiServer_1 pid=186)[0;0m INFO:     Application startup complete.
2026-02-22T18:25:42.9468325Z [0;36m(ApiServer_0 pid=185)[0;0m INFO:     Application startup complete.
2026-02-22T18:25:43.0756328Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     Application startup complete.
2026-02-22T18:25:43.4566025Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     Application startup complete.
2026-02-22T18:25:44.9909277Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.52:42666 - "GET /health HTTP/1.1" 200 OK
2026-02-22T18:25:44.9932274Z [2026-02-22 18:25:44] INFO conftest.py:390: [READY] Node 10.0.0.52 is ready.
2026-02-22T18:25:45.0371088Z 2026-02-22 18:25:45,035 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-22T18:25:45.0744605Z 2026-02-22 18:25:45,069 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-22T18:25:45.1316933Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.102:36788 - "GET /health HTTP/1.1" 200 OK
2026-02-22T18:25:45.1356911Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     10.0.0.102:36794 - "GET /health HTTP/1.1" 200 OK
2026-02-22T18:25:50.1395749Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.102:33752 - "GET /health HTTP/1.1" 200 OK
2026-02-22T18:25:55.1463018Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.102:33766 - "GET /health HTTP/1.1" 200 OK
2026-02-22T18:26:00.1454194Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.102:45170 - "GET /health HTTP/1.1" 200 OK
2026-02-22T18:26:01.8443364Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:26:01 [warnings.py:110] /vllm-workspace/vllm/vllm/entrypoints/openai/engine/serving.py:1067: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
2026-02-22T18:26:01.8451781Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-22 18:26:01 [warnings.py:110] /vllm-workspace/vllm/vllm/entrypoints/utils.py:218: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
2026-02-22T18:26:01.8464396Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:26:01 [warnings.py:110] /vllm-workspace/vllm/vllm/entrypoints/openai/engine/serving.py:1067: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
2026-02-22T18:26:01.8475644Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-22 18:26:01 [warnings.py:110] /vllm-workspace/vllm/vllm/entrypoints/utils.py:218: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
2026-02-22T18:26:01.8490349Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:26:01 [warnings.py:110] /vllm-workspace/vllm/vllm/entrypoints/openai/engine/serving.py:1067: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
2026-02-22T18:26:01.8500415Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-22 18:26:01 [warnings.py:110] /vllm-workspace/vllm/vllm/entrypoints/utils.py:218: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
2026-02-22T18:26:01.8511929Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:26:01 [warnings.py:110] /vllm-workspace/vllm/vllm/entrypoints/openai/engine/serving.py:1067: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
2026-02-22T18:26:01.8522999Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-22 18:26:01 [warnings.py:110] /vllm-workspace/vllm/vllm/entrypoints/utils.py:218: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
2026-02-22T18:26:05.1487814Z ................[0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.102:45182 - "GET /health HTTP/1.1" 200 OK
2026-02-22T18:26:10.1531925Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.102:55594 - "GET /health HTTP/1.1" 200 OK
2026-02-22T18:26:15.1612928Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.102:55600 - "GET /health HTTP/1.1" 200 OK
2026-02-22T18:26:20.1632561Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.102:59688 - "GET /health HTTP/1.1" 200 OK
2026-02-22T18:26:25.1670488Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.102:59704 - "GET /health HTTP/1.1" 200 OK
2026-02-22T18:26:29.3469875Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m INFO 02-22 18:26:29 [acl_graph.py:185] Replaying aclgraph
2026-02-22T18:26:29.3479462Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m INFO 02-22 18:26:29 [acl_graph.py:185] Replaying aclgraph
2026-02-22T18:26:29.3493213Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-22 18:26:29 [acl_graph.py:185] Replaying aclgraph
2026-02-22T18:26:29.3498120Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-22 18:26:29 [acl_graph.py:185] Replaying aclgraph
2026-02-22T18:26:29.3508198Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-22 18:26:29 [acl_graph.py:185] Replaying aclgraph
2026-02-22T18:26:29.3517953Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-22 18:26:29 [acl_graph.py:185] Replaying aclgraph
2026-02-22T18:26:29.3529426Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-22 18:26:29 [acl_graph.py:185] Replaying aclgraph
2026-02-22T18:26:29.3538199Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m INFO 02-22 18:26:29 [acl_graph.py:185] Replaying aclgraph
2026-02-22T18:26:29.3581046Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m INFO 02-22 18:26:29 [acl_graph.py:185] Replaying aclgraph
2026-02-22T18:26:29.3583487Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-22 18:26:29 [acl_graph.py:185] Replaying aclgraph
2026-02-22T18:26:29.3584581Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m INFO 02-22 18:26:29 [acl_graph.py:185] Replaying aclgraph
2026-02-22T18:26:29.3585122Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-22 18:26:29 [acl_graph.py:185] Replaying aclgraph
2026-02-22T18:26:29.3585716Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-22 18:26:29 [acl_graph.py:185] Replaying aclgraph
2026-02-22T18:26:29.3595198Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m INFO 02-22 18:26:29 [acl_graph.py:185] Replaying aclgraph
2026-02-22T18:26:29.3605170Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-22 18:26:29 [acl_graph.py:185] Replaying aclgraph
2026-02-22T18:26:29.3615465Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-22 18:26:29 [acl_graph.py:185] Replaying aclgraph
2026-02-22T18:26:30.1720367Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.102:37222 - "GET /health HTTP/1.1" 200 OK
2026-02-22T18:26:35.1743360Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     10.0.0.102:37236 - "GET /health HTTP/1.1" 200 OK
2026-02-22T18:26:40.1784791Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.102:42934 - "GET /health HTTP/1.1" 200 OK
2026-02-22T18:26:45.1837496Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     10.0.0.102:42938 - "GET /health HTTP/1.1" 200 OK
2026-02-22T18:26:50.1863607Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     10.0.0.102:59772 - "GET /health HTTP/1.1" 200 OK
2026-02-22T18:26:55.1905012Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.102:59780 - "GET /health HTTP/1.1" 200 OK
2026-02-22T18:27:00.1935629Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.102:55842 - "GET /health HTTP/1.1" 200 OK
2026-02-22T18:27:02.6615057Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693] AsyncLLM output_handler failed.
2026-02-22T18:27:02.6623861Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693] Traceback (most recent call last):
2026-02-22T18:27:02.6634153Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 664, in output_handler
2026-02-22T18:27:02.6644969Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     processed_outputs = output_processor.process_outputs(
2026-02-22T18:27:02.6658215Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:02.6666280Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/vllm-workspace/vllm/vllm/v1/engine/output_processor.py", line 632, in process_outputs
2026-02-22T18:27:02.6677648Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     stop_string = req_state.detokenizer.update(
2026-02-22T18:27:02.6686955Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:02.6698312Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/vllm-workspace/vllm/vllm/v1/engine/detokenizer.py", line 120, in update
2026-02-22T18:27:02.6709188Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     self.output_text += self.decode_next(new_token_id)
2026-02-22T18:27:02.6717528Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:02.6727754Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/vllm-workspace/vllm/vllm/v1/engine/detokenizer.py", line 299, in decode_next
2026-02-22T18:27:02.6737450Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     new_tokens, decoded_text, prefix_offset, read_offset = detokenize_incrementally(
2026-02-22T18:27:02.6748840Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:02.6759072Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/vllm-workspace/vllm/vllm/tokenizers/detokenizer_utils.py", line 175, in detokenize_incrementally
2026-02-22T18:27:02.6770525Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     new_text = tokenizer.convert_tokens_to_string(output_tokens[prefix_offset:])
2026-02-22T18:27:02.6781567Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:02.6792479Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/vllm-workspace/vllm/vllm/tokenizers/deepseek_v32.py", line 175, in convert_tokens_to_string
2026-02-22T18:27:02.6804429Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     return self.tokenizer.convert_tokens_to_string(tokens)
2026-02-22T18:27:02.6815285Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:02.6825733Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py", line 666, in convert_tokens_to_string
2026-02-22T18:27:02.6835845Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     self.backend_tokenizer.decoder.decode(tokens)
2026-02-22T18:27:02.6846647Z [0;36m(ApiServer_3 pid=188)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693] TypeError: argument 'tokens': 'NoneType' object cannot be converted to 'PyString'
2026-02-22T18:27:02.6857122Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     10.0.0.52:45858 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.6867726Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     10.0.0.52:45884 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.6879660Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     10.0.0.52:45968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.6893322Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     10.0.0.52:45974 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.6904192Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     10.0.0.52:45982 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.6914753Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     10.0.0.52:45996 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.6925798Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     10.0.0.52:46024 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.6935219Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     10.0.0.52:46048 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.6945498Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     10.0.0.52:46066 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.6955553Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     10.0.0.52:46082 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.6966732Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     10.0.0.52:46098 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.7630693Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693] AsyncLLM output_handler failed.
2026-02-22T18:27:02.7640234Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693] Traceback (most recent call last):
2026-02-22T18:27:02.7649573Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 664, in output_handler
2026-02-22T18:27:02.7658754Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     processed_outputs = output_processor.process_outputs(
2026-02-22T18:27:02.7667846Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:02.7678782Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/vllm-workspace/vllm/vllm/v1/engine/output_processor.py", line 632, in process_outputs
2026-02-22T18:27:02.7688435Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     stop_string = req_state.detokenizer.update(
2026-02-22T18:27:02.7698501Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:02.7707972Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/vllm-workspace/vllm/vllm/v1/engine/detokenizer.py", line 120, in update
2026-02-22T18:27:02.7717386Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     self.output_text += self.decode_next(new_token_id)
2026-02-22T18:27:02.7732389Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:02.7769849Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/vllm-workspace/vllm/vllm/v1/engine/detokenizer.py", line 299, in decode_next
2026-02-22T18:27:02.7780733Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     new_tokens, decoded_text, prefix_offset, read_offset = detokenize_incrementally(
2026-02-22T18:27:02.7790917Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:02.7805555Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/vllm-workspace/vllm/vllm/tokenizers/detokenizer_utils.py", line 175, in detokenize_incrementally
2026-02-22T18:27:02.7819547Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     new_text = tokenizer.convert_tokens_to_string(output_tokens[prefix_offset:])
2026-02-22T18:27:02.7828995Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:02.7839188Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/vllm-workspace/vllm/vllm/tokenizers/deepseek_v32.py", line 175, in convert_tokens_to_string
2026-02-22T18:27:02.7850028Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     return self.tokenizer.convert_tokens_to_string(tokens)
2026-02-22T18:27:02.7860017Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:02.7870974Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py", line 666, in convert_tokens_to_string
2026-02-22T18:27:02.7880505Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     self.backend_tokenizer.decoder.decode(tokens)
2026-02-22T18:27:02.7894750Z [0;36m(ApiServer_1 pid=186)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693] TypeError: argument 'tokens': 'NoneType' object cannot be converted to 'PyString'
2026-02-22T18:27:02.7905187Z [0;36m(ApiServer_1 pid=186)[0;0m INFO:     10.0.0.52:45896 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.7915342Z [0;36m(ApiServer_1 pid=186)[0;0m INFO:     10.0.0.52:45908 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.7926370Z [0;36m(ApiServer_1 pid=186)[0;0m INFO:     10.0.0.52:45914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.7937158Z [0;36m(ApiServer_1 pid=186)[0;0m INFO:     10.0.0.52:45928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.7949135Z [0;36m(ApiServer_1 pid=186)[0;0m INFO:     10.0.0.52:45938 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.7956627Z [0;36m(ApiServer_1 pid=186)[0;0m INFO:     10.0.0.52:45948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.7968193Z [0;36m(ApiServer_1 pid=186)[0;0m INFO:     10.0.0.52:45954 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.7976963Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693] AsyncLLM output_handler failed.
2026-02-22T18:27:02.7986979Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693] Traceback (most recent call last):
2026-02-22T18:27:02.7997568Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 664, in output_handler
2026-02-22T18:27:02.8007774Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     processed_outputs = output_processor.process_outputs(
2026-02-22T18:27:02.8018391Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:02.8028760Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/vllm-workspace/vllm/vllm/v1/engine/output_processor.py", line 632, in process_outputs
2026-02-22T18:27:02.8037831Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     stop_string = req_state.detokenizer.update(
2026-02-22T18:27:02.8047818Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:02.8058018Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/vllm-workspace/vllm/vllm/v1/engine/detokenizer.py", line 120, in update
2026-02-22T18:27:02.8068488Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     self.output_text += self.decode_next(new_token_id)
2026-02-22T18:27:02.8078465Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:02.8088248Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/vllm-workspace/vllm/vllm/v1/engine/detokenizer.py", line 299, in decode_next
2026-02-22T18:27:02.8098158Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     new_tokens, decoded_text, prefix_offset, read_offset = detokenize_incrementally(
2026-02-22T18:27:02.8107416Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:02.8117674Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/vllm-workspace/vllm/vllm/tokenizers/detokenizer_utils.py", line 175, in detokenize_incrementally
2026-02-22T18:27:02.8128718Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     new_text = tokenizer.convert_tokens_to_string(output_tokens[prefix_offset:])
2026-02-22T18:27:02.8138438Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:02.8149050Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/vllm-workspace/vllm/vllm/tokenizers/deepseek_v32.py", line 175, in convert_tokens_to_string
2026-02-22T18:27:02.8159202Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     return self.tokenizer.convert_tokens_to_string(tokens)
2026-02-22T18:27:02.8170518Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:02.8179123Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py", line 666, in convert_tokens_to_string
2026-02-22T18:27:02.8188210Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693]     self.backend_tokenizer.decoder.decode(tokens)
2026-02-22T18:27:02.8198459Z [0;36m(ApiServer_2 pid=187)[0;0m ERROR 02-22 18:27:02 [async_llm.py:693] TypeError: argument 'tokens': 'NoneType' object cannot be converted to 'PyString'
2026-02-22T18:27:02.8207958Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.52:45850 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.8217075Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.52:45878 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.8226771Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.52:45894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.8237025Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.52:45898 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.8246903Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.52:45976 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.8256755Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.52:45994 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.8266419Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.52:46012 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:02.8276614Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.52:46050 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-22T18:27:05.1971792Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     10.0.0.102:55848 - "GET /health HTTP/1.1" 503 Service Unavailable
2026-02-22T18:27:07.6495762Z [0;36m(ApiServer_1 pid=186)[0;0m INFO:     Shutting down
2026-02-22T18:27:07.6623615Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     Shutting down
2026-02-22T18:27:07.6747870Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     Shutting down
2026-02-22T18:27:07.7490206Z [0;36m(ApiServer_1 pid=186)[0;0m INFO:     Waiting for application shutdown.
2026-02-22T18:27:07.7537201Z [0;36m(ApiServer_1 pid=186)[0;0m INFO:     Application shutdown complete.
2026-02-22T18:27:07.7547039Z [0;36m(ApiServer_1 pid=186)[0;0m INFO:     Finished server process [186]
2026-02-22T18:27:07.7687201Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     Waiting for application shutdown.
2026-02-22T18:27:07.7700289Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     Application shutdown complete.
2026-02-22T18:27:07.7706927Z [0;36m(ApiServer_2 pid=187)[0;0m INFO:     Finished server process [187]
2026-02-22T18:27:07.7750122Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     Waiting for application shutdown.
2026-02-22T18:27:07.7778710Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     Application shutdown complete.
2026-02-22T18:27:07.7789032Z [0;36m(ApiServer_3 pid=188)[0;0m INFO:     Finished server process [188]
2026-02-22T18:27:07.8193553Z [0;36m(ApiServer_3 pid=188)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-22T18:27:07.8374772Z [0;36m(ApiServer_1 pid=186)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-22T18:27:07.9294119Z [0;36m(ApiServer_2 pid=187)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-22T18:27:08.6274909Z ERROR 02-22 18:27:08 [utils.py:290] Exception occurred while running API servers: Process ApiServer_1 (PID: 186) died with exit code None
2026-02-22T18:27:08.6284158Z ERROR 02-22 18:27:08 [utils.py:290] Traceback (most recent call last):
2026-02-22T18:27:08.6294321Z ERROR 02-22 18:27:08 [utils.py:290]   File "/vllm-workspace/vllm/vllm/v1/utils.py", line 277, in wait_for_completion_or_failure
2026-02-22T18:27:08.6304425Z ERROR 02-22 18:27:08 [utils.py:290]     raise RuntimeError(
2026-02-22T18:27:08.6314751Z ERROR 02-22 18:27:08 [utils.py:290] RuntimeError: Process ApiServer_1 (PID: 186) died with exit code None
2026-02-22T18:27:08.6325796Z INFO 02-22 18:27:08 [utils.py:293] Terminating remaining processes ...
2026-02-22T18:27:08.7133373Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-22 18:27:08 [launcher.py:110] Shutting down FastAPI HTTP server.
2026-02-22T18:27:08.7149643Z [0;36m(ApiServer_0 pid=185)[0;0m INFO:     Shutting down
2026-02-22T18:27:08.8133331Z [0;36m(ApiServer_0 pid=185)[0;0m INFO:     Waiting for connections to close. (CTRL+C to force quit)
2026-02-22T18:27:10.2331844Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:10.2340472Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.2349230Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.2359391Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.2368724Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.2378854Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.2389704Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.2399052Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.2409073Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.2418846Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.2429516Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.2440489Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.2449309Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.2462457Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.2472985Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.2484345Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.2494857Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.2505792Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.2533363Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.2534274Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.2538098Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.2548364Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.2559594Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.2568864Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.2578641Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.2590442Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.2600457Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.2611418Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:40676
2026-02-22T18:27:10.2621503Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.2632616Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.2644153Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.2652363Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.2663103Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.2682591Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.2684146Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.2695060Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.2705352Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.2716074Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.2728045Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.2737937Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.2747160Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.2758504Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.2770072Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.2779845Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.2791379Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.2802525Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.2812781Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.2824224Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.2833913Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.2847890Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.2859690Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.2870586Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.2882222Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.2892878Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.2904673Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:40676
2026-02-22T18:27:10.2914857Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:10.2925984Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:10.2936793Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.2947326Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.2957367Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.2968334Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.2980174Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.2990630Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.3001660Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3013737Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.3023509Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.3033997Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3045542Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.3054765Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.3064921Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3075605Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.3086258Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.3096716Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3107277Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.3118338Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.3128298Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3139444Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.3149511Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.3161382Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.3170121Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.3180102Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3191218Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.3201348Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.3212890Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:58569
2026-02-22T18:27:10.3222681Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.3233248Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.3242811Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.3253217Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3263831Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.3274204Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.3285278Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3298004Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.3306159Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.3318094Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3328376Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.3338881Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.3350015Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3361688Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.3370383Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.3380717Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3392187Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.3402361Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.3413203Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3423792Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.3433868Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.3445879Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.3459732Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.3474544Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3486630Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.3496870Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.3512789Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:58569
2026-02-22T18:27:10.3522697Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:10.3533203Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:10.3543923Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.3554984Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.3565548Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.3575130Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3586188Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.3595917Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.3607286Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3617990Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.3628104Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.3639197Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3650721Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.3659946Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.3670089Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3681134Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.3691575Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.3701300Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3712497Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.3723495Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.3734366Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3745265Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.3756166Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.3769386Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.3780296Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.3791184Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3803609Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.3814373Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.3825792Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:20702
2026-02-22T18:27:10.3836841Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.3849586Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.3859458Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.3870566Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3882954Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.3893644Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.3903481Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3914265Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.3926385Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.3936349Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3948064Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.3958643Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.3969207Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.3980502Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.3991171Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.4002477Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4013730Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.4024354Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.4035562Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4047193Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.4057350Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.4068816Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.4078651Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:10.4091434Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.4100889Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.4111464Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.4121119Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4132925Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.4142196Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.4152613Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4163220Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.4173762Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.4184186Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4195467Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.4206224Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.4216016Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4226883Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.4236874Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.4247378Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4258031Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.4268139Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.4279228Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4289478Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.4300304Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.4311144Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.4321759Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.4331792Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4342869Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.4352544Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.4364248Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:9946
2026-02-22T18:27:10.4374011Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.4385586Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.4394759Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.4405894Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4416844Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.4427012Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.4437742Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4448185Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.4460039Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.4472496Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4482759Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.4492944Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.4502918Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4512707Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.4523050Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.4533006Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4544150Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.4554259Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.4599227Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4600182Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.4601098Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.4602113Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.4608237Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.4618631Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4629663Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.4641230Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.4651440Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:20702
2026-02-22T18:27:10.4660769Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:10.4671168Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.4682063Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4693443Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.4703104Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.4714027Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:9946
2026-02-22T18:27:10.4724323Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:10.4734352Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:10.4786135Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.4787385Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.4788255Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.4788951Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4790763Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.4801731Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.4814687Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4827131Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.4838737Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.4857679Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4864739Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.4873248Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.4883905Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4894900Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.4905176Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.4916532Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4927935Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.4938399Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.4948489Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.4960516Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.4971759Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.4982963Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.4992554Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.5003981Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5015306Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.5024723Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.5035964Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:7364
2026-02-22T18:27:10.5046555Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.5057155Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.5066711Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.5076844Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5088804Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.5098889Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.5109280Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5120376Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.5131352Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.5142516Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5169672Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.5177865Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.5193932Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5218098Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.5236897Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.5255825Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5256818Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.5265481Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.5276595Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5287920Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.5301487Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.5312569Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.5322764Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.5338543Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5344189Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.5353913Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.5366523Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:7364
2026-02-22T18:27:10.5377264Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:10.5387754Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:10.5398567Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.5409958Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.5420169Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.5430882Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5443337Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.5454486Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.5464877Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5476087Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.5487460Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.5500363Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5508819Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.5519094Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.5529714Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5540617Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.5550806Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.5560233Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5571161Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.5621274Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.5622175Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5623314Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.5624237Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.5625212Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.5633809Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.5644425Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5654842Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.5664278Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.5674939Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:39574
2026-02-22T18:27:10.5685773Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.5696281Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.5705925Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.5716301Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5727202Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.5736772Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.5747363Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5758234Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.5770506Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.5782326Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5794753Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.5805596Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.5816177Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5872719Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.5873933Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.5874632Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5875714Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.5876761Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.5881048Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5891947Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.5901482Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.5912862Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.5921956Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:10.5931550Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.5942637Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.5952966Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.5962778Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.5974183Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.5984408Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.5994184Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6006275Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.6016216Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.6027069Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6038417Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.6047776Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.6057419Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6067504Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.6077931Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.6088529Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6099159Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.6109303Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.6120179Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6130926Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.6140423Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.6151402Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.6161689Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.6172301Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6183470Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.6193341Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.6206063Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:37504
2026-02-22T18:27:10.6217239Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.6228096Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.6238305Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.6249802Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6260187Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.6270190Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.6280613Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6291410Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.6301771Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.6312622Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6323019Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.6333041Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.6343156Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6353853Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.6364749Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.6374525Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6385470Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.6396085Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.6407236Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6418006Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.6427473Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.6438394Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.6450162Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.6458688Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6470803Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.6479310Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.6492418Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:39574
2026-02-22T18:27:10.6500556Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:10.6510023Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.6521524Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6533026Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.6548707Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.6580645Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:37504
2026-02-22T18:27:10.6581417Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:10.6582076Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:10.6594330Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.6606545Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.6616691Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.6626955Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6670679Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.6672263Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.6672983Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6673839Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.6684021Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.6694284Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6706011Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.6715617Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.6727023Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6737779Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.6749955Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.6759297Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6771627Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.6782886Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.6794365Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6806532Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.6817161Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.6828847Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.6839430Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.6850363Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6862865Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.6872721Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.6884358Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:42206
2026-02-22T18:27:10.6894882Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.6905833Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.6917115Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.6927806Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6939047Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.6950236Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.6961452Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.6972930Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.6983787Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.6994775Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7007059Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.7017741Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.7028590Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7040587Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.7051063Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.7062096Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7073657Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.7084320Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.7095665Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7107071Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.7310635Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.7311643Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.7312660Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.7313285Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7314206Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.7315011Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.7316036Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:42206
2026-02-22T18:27:10.7316781Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:10.7317279Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:10.7317847Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.7318868Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.7319610Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.7320193Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7320923Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.7321685Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.7322459Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7323252Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.7324162Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.7324919Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7325853Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.7331496Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.7341628Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7352793Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.7363657Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.7374040Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7385167Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.7395866Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.7408919Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7419075Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.7429363Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.7440550Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.7453961Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.7460547Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7471787Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.7480591Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.7491711Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:41189
2026-02-22T18:27:10.7501804Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.7512569Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.7522520Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.7532981Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7543102Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.7554030Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.7562998Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7573695Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.7584152Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.7594768Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7605721Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.7615144Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.7625260Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7636075Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.7646774Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.7656562Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7668035Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.7680267Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.7690981Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7702610Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.7712562Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.7723379Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.7733432Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.7743295Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7754117Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.7765403Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.7777724Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:41189
2026-02-22T18:27:10.7787945Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:10.7798716Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:10.7809107Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.7820041Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.7829738Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.7839771Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7850485Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.7860861Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.7870761Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7881441Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.7892499Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.7902431Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7913422Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.7923348Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.7933365Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7943825Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.7953929Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.7965193Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.7976480Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.7986421Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.7996937Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8007800Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.8017881Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.8028975Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.8038817Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.8048993Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8059870Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.8069246Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.8080499Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:40676
2026-02-22T18:27:10.8090840Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.8101630Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.8111036Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.8121425Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8132161Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.8142465Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.8152254Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8162997Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.8173692Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.8183946Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8194716Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.8205475Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.8215467Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8225805Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.8235030Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.8245895Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8256351Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.8267283Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.8278286Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8289436Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.8300635Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.8311388Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.8321742Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.8330284Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8341146Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.8350403Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.8361585Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:40676
2026-02-22T18:27:10.8371497Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:10.8381471Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:10.8390993Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.8402348Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.8413850Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.8424894Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8438313Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.8448628Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.8458609Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8469167Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.8480161Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.8490336Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8503117Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.8513350Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.8524579Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8560854Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.8561668Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.8562406Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8569989Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.8582194Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.8591132Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8601873Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.8612601Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.8624244Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.8633875Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.8644295Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8655480Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.8665460Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.8676838Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:48768
2026-02-22T18:27:10.8686816Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.8697725Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.8707696Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.8717894Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8728873Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.8738691Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.8748887Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8760488Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.8771533Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.8782248Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8792746Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.8802926Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.8812960Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8823608Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.8833343Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.8844388Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8855301Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.8865187Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.8875845Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8886516Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.8896670Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.8907137Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.8917020Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.8927149Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.8938570Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.8947109Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.8958730Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:48768
2026-02-22T18:27:10.8967921Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:10.9003146Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [dump_input.py:72] Dumping input data for V1 LLM engine (v0.15.0) with config: model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', speculative_config=SpeculativeConfig(method='mtp', model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', num_spec_tokens=2), tokenizer='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', skip_tokenizer_init=False, tokenizer_mode=deepseek_v32, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=4, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=bfloat16, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=1024, served_model_name=/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [24, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 48, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}, 
2026-02-22T18:27:10.9010841Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:10.9015452Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.9026475Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.9036370Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.9047219Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9058237Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.9068788Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.9078840Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9090393Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.9100962Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.9109979Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9121693Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.9166346Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.9176150Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9187123Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.9197238Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.9207550Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9218362Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.9228095Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.9240198Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9251342Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.9261023Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.9273233Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.9283395Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.9293733Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9304278Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.9314164Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.9325807Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:4317
2026-02-22T18:27:10.9335289Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.9346340Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.9355529Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.9366531Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9376787Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.9387158Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.9398115Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9408922Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.9418802Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.9429036Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9438367Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.9448327Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.9457942Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9468645Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.9479134Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.9490094Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9500104Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.9509712Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.9520434Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9530471Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.9540364Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.9550931Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.9561552Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.9571553Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9584221Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.9591366Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.9601606Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:4317
2026-02-22T18:27:10.9613341Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:10.9626321Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:10.9636329Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.9647572Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.9657190Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.9667623Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9681102Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.9692137Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.9702488Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9712774Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:10.9725813Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:10.9736883Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9748204Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:10.9760492Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.9771768Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9782910Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:10.9793619Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:10.9804757Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9816039Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:10.9826724Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:10.9836766Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9850990Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:10.9859059Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:10.9871122Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:10.9881843Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:10.9891392Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9902704Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:10.9911926Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:10.9923537Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:58569
2026-02-22T18:27:10.9933727Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:10.9944655Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:10.9955467Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:10.9966077Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:10.9976728Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:10.9987101Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:10.9996710Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0008556Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.0018983Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.0029475Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0040407Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.0051049Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.0060977Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0072225Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.0082663Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.0093031Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0104485Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.0114965Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.0125820Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0136292Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.0149178Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.0158139Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.0167907Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.0177488Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0188560Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.0197993Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.0209088Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:58569
2026-02-22T18:27:11.0217721Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:11.0235563Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [dump_input.py:79] Dumping scheduler output for model execution: SchedulerOutput(scheduled_new_reqs=[], scheduled_cached_reqs=CachedRequestData(req_ids=['chatcmpl-99eaf58acfce4616-a13f9fb3', 'chatcmpl-be1c3e2609a33bc3-9d4718c7'],resumed_req_ids=set(),new_token_ids_lens=[],all_token_ids_lens={},new_block_ids=[None, None],num_computed_tokens=[491, 456],num_output_tokens=[348, 348]), num_scheduled_tokens={chatcmpl-be1c3e2609a33bc3-9d4718c7: 3, chatcmpl-99eaf58acfce4616-a13f9fb3: 3}, total_num_scheduled_tokens=6, scheduled_spec_decode_tokens={chatcmpl-99eaf58acfce4616-a13f9fb3: [-1, -1], chatcmpl-be1c3e2609a33bc3-9d4718c7: [-1, -1]}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[0], finished_req_ids=[], free_encoder_mm_hashes=[], preempted_req_ids=[], has_structured_output_requests=false, pending_structured_output_tokens=false, num_invalid_spec_tokens=null, kv_connector_metadata=null, ec_connector_metadata=null)
2026-02-22T18:27:11.0243341Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:11.0253469Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.0264363Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.0274662Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.0285149Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0295846Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.0305053Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.0314996Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0326271Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.0336097Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.0346240Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0359154Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.0367696Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.0377266Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0388216Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.0398382Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.0410865Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0419072Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.0432249Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.0445418Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0451789Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.0460605Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.0471105Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.0481288Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.0491375Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0501657Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.0511427Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.0544614Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:20702
2026-02-22T18:27:11.0545474Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.0546513Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.0553183Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.0580846Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0581571Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.0584565Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.0595397Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0613373Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.0617050Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.0627680Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0639931Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.0650048Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.0661200Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0670515Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.0681010Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.0691529Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0702318Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.0711866Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.0722597Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0733533Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.0743405Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.0754703Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.0765474Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.0776980Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0789494Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.0802463Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.0815458Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:20702
2026-02-22T18:27:11.0823894Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:11.0834312Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:11.0845572Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.0855860Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.0865827Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.0876099Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0887829Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.0898344Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.0908651Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0919265Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.0931182Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.0940906Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0952427Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.0962630Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.0973049Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.0983332Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.0993707Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.1004606Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1015993Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.1025820Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.1036255Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1048495Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.1058522Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.1068876Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.1078279Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.1088592Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1099926Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.1109669Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.1121458Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:7364
2026-02-22T18:27:11.1131087Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.1143556Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.1151414Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.1162336Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1173445Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.1183402Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.1193144Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1204497Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.1215269Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.1226042Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1236933Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.1247404Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.1257321Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1268363Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.1279210Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.1289114Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1300273Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.1310607Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.1321440Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1331915Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.1342389Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.1353690Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.1363609Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.1373911Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1384863Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.1394108Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.1405983Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:7364
2026-02-22T18:27:11.1415803Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:11.1425905Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:11.1435904Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.1447619Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.1457164Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.1467891Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1482326Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.1490378Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.1501810Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1510532Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.1523542Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.1537260Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1546449Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.1556828Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.1568261Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1581504Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.1593536Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.1608428Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1620152Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.1637269Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.1642734Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1654132Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.1664246Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.1675004Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.1685584Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.1695413Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1706400Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.1716146Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.1729144Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:39574
2026-02-22T18:27:11.1739134Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.1750099Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.1760537Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.1772090Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1783560Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.1794321Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.1805430Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1816425Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.1827287Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.1837020Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1848966Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.1858619Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.1869198Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1880817Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.1891365Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.1901922Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1912736Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.1923454Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.1934461Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1945253Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.1955801Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.1967774Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.1977652Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.1987668Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.1998827Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.2009077Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.2020289Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:39574
2026-02-22T18:27:11.2029936Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:11.2040059Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:11.2050132Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.2061032Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.2070893Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.2081833Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2092980Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.2103170Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.2113232Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2124381Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.2134907Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.2145211Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2156355Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.2166889Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.2177955Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2188610Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.2198532Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.2208151Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2218802Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.2229241Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.2239900Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2250830Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.2260761Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.2271625Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.2282418Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.2292672Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2303587Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.2312526Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.2323813Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:37504
2026-02-22T18:27:11.2333915Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.2344641Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.2354064Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.2364231Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2374378Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.2384090Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.2393965Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2405357Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.2415612Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.2426978Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2437058Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.2447298Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.2456205Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2466875Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.2477097Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.2488708Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2498861Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.2508621Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.2519604Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2530397Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.2539989Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.2550363Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.2560446Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.2570457Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2581691Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.2593196Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.2603802Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:37504
2026-02-22T18:27:11.2612895Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:11.2627197Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [dump_input.py:81] Dumping scheduler stats: SchedulerStats(num_running_reqs=2, num_waiting_reqs=0, step_counter=0, current_wave=0, kv_cache_usage=0.0050093926111458575, prefix_cache_stats=PrefixCacheStats(reset=False, requests=0, queries=0, hits=0, preempted_requests=0, preempted_queries=0, preempted_hits=0), connector_prefix_cache_stats=None, kv_cache_eviction_events=[], spec_decoding_stats=None, kv_connector_stats=None, waiting_lora_adapters={}, running_lora_adapters={}, cudagraph_stats=None, perf_stats=None)
2026-02-22T18:27:11.2660239Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [dump_input.py:72] Dumping input data for V1 LLM engine (v0.15.0) with config: model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', speculative_config=SpeculativeConfig(method='mtp', model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', num_spec_tokens=2), tokenizer='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', skip_tokenizer_init=False, tokenizer_mode=deepseek_v32, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=4, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=bfloat16, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=1024, served_model_name=/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [24, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 48, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}, 
2026-02-22T18:27:11.2667683Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:11.2672914Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.2683451Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.2693462Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.2703178Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2713780Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.2724342Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.2734847Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2745440Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.2755705Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.2767898Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2780129Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.2790590Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.2802505Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2812659Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.2822749Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.2832793Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2847053Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.2870837Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.2871940Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2881309Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.2892171Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.2902594Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.2912966Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.2921398Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.2933061Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.2942108Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.2952823Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:9946
2026-02-22T18:27:11.2962925Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.2974062Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.2983879Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.2994184Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3005228Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.3015059Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.3025442Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3035384Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.3046431Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.3057247Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3067600Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.3077937Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.3088135Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3098651Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.3108579Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.3118977Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3130293Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.3140604Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.3150206Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3161160Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.3171637Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.3182657Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.3192118Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.3202703Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3214406Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.3223776Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.3234977Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:9946
2026-02-22T18:27:11.3244609Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:11.3262931Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [dump_input.py:79] Dumping scheduler output for model execution: SchedulerOutput(scheduled_new_reqs=[], scheduled_cached_reqs=CachedRequestData(req_ids=['chatcmpl-a8e604047ea07fca-a793ac4a', 'chatcmpl-92560d90c8aa0a4e-902dca5e'],resumed_req_ids=set(),new_token_ids_lens=[],all_token_ids_lens={},new_block_ids=[None, None],num_computed_tokens=[439, 445],num_output_tokens=[348, 348]), num_scheduled_tokens={chatcmpl-92560d90c8aa0a4e-902dca5e: 3, chatcmpl-a8e604047ea07fca-a793ac4a: 3}, total_num_scheduled_tokens=6, scheduled_spec_decode_tokens={chatcmpl-a8e604047ea07fca-a793ac4a: [-1, -1], chatcmpl-92560d90c8aa0a4e-902dca5e: [-1, -1]}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[0], finished_req_ids=[], free_encoder_mm_hashes=[], preempted_req_ids=[], has_structured_output_requests=false, pending_structured_output_tokens=false, num_invalid_spec_tokens=null, kv_connector_metadata=null, ec_connector_metadata=null)
2026-02-22T18:27:11.3269870Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:11.3280514Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.3291879Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.3301196Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.3311464Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3322863Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.3332838Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.3342950Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3353457Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.3363864Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.3374332Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3385160Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.3394677Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.3405048Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3415946Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.3426160Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.3436095Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3447822Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.3457421Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.3467411Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3478557Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.3488900Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.3499475Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.3508909Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.3519918Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3530838Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.3540012Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.3550234Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:42206
2026-02-22T18:27:11.3560119Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.3621268Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.3622113Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.3622715Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3623448Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.3624202Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.3624863Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3636874Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.3648009Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.3658135Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3668875Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.3681513Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.3716770Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3717775Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.3718577Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.3722738Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3733774Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.3743939Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.3754322Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3766671Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.3778638Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.3789885Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.3799950Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.3810673Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3821910Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.3831307Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.3842555Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:42206
2026-02-22T18:27:11.3852518Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:11.3867420Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [dump_input.py:81] Dumping scheduler stats: SchedulerStats(num_running_reqs=2, num_waiting_reqs=0, step_counter=0, current_wave=0, kv_cache_usage=0.0050093926111458575, prefix_cache_stats=PrefixCacheStats(reset=False, requests=0, queries=0, hits=0, preempted_requests=0, preempted_queries=0, preempted_hits=0), connector_prefix_cache_stats=None, kv_cache_eviction_events=[], spec_decoding_stats=None, kv_connector_stats=None, waiting_lora_adapters={}, running_lora_adapters={}, cudagraph_stats=None, perf_stats=None)
2026-02-22T18:27:11.3876207Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:11.3887706Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.3898493Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.3908156Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.3918502Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3929468Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.3939414Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.3949845Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3961676Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.3971330Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.3982153Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.3994153Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.4010653Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.4014400Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4024977Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.4037814Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.4047385Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4057193Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.4067274Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.4077284Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4088690Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.4099058Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.4110337Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.4120240Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.4130640Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4141475Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.4150797Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.4163063Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:41189
2026-02-22T18:27:11.4173122Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.4184223Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.4194655Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.4205529Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4215961Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.4226297Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.4236794Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4248228Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.4265036Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.4270433Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4280820Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.4292154Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.4301897Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4312913Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.4323965Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.4334901Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4347213Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.4358541Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.4369237Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4380142Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.4390148Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.4403210Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.4411278Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.4421187Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4432659Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.4442527Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.4453893Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:41189
2026-02-22T18:27:11.4463341Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:11.4473942Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:11.4486384Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.4496680Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.4506259Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.4516305Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4527920Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.4537725Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.4548515Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4570615Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.4571508Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.4580461Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4591670Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.4601941Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.4612762Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4625106Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.4634966Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.4646247Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4657725Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.4680972Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.4692397Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4704951Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.4715024Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.4728683Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.4739317Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.4749640Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4761698Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.4772142Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.4783693Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:48768
2026-02-22T18:27:11.4794780Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.4805963Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.4816550Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.4826008Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4837022Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.4848295Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.4858538Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4868547Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.4879063Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.4890298Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4901041Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.4910642Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.4922129Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4931600Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.4941617Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.4954096Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4963439Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.4974041Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.4984161Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.4995139Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.5006604Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.5021002Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.5030768Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.5040684Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5054543Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.5065190Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.5098057Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:48768
2026-02-22T18:27:11.5098782Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:11.5099376Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:11.5107817Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.5118766Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.5128881Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.5139537Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5151046Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.5162594Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.5173383Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5184415Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.5195390Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.5206800Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5218251Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.5228028Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.5238154Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5249296Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.5259853Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.5270003Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5281455Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.5293003Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.5302559Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5313745Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.5324423Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.5336007Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.5345364Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.5355186Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5366590Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.5376709Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.5387670Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:4317
2026-02-22T18:27:11.5397327Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.5408389Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.5418131Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.5428515Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5439154Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.5449887Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.5459761Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5470253Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.5481039Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.5493671Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5502693Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.5512852Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.5523098Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5533895Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.5543795Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.5553345Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5575251Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.5577344Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.5585574Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5596620Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.5606982Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.5618601Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.5627827Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.5684606Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5685470Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.5686311Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.5687102Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:4317
2026-02-22T18:27:11.5687817Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:11.5689587Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:11.5700066Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.5709928Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.5720020Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.5730740Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5741277Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.5751425Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.5788427Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5789206Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.5790060Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.5798054Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5809094Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.5818814Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.5829274Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5839880Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.5850656Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.5861129Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5872754Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.5884013Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.5894911Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5906555Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.5917204Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.5928699Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.5938497Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.5979141Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.5979985Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.5980746Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.5981547Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:43192
2026-02-22T18:27:11.5991262Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.6002272Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.6012518Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.6023117Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6032904Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.6044112Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.6055176Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6065766Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.6076473Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.6089551Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6099935Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.6109920Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.6120341Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6132676Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.6141442Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.6151640Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6163431Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.6173618Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.6184055Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6195567Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.6206314Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.6217566Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.6227174Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.6237177Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6248486Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.6259339Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.6268465Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:43192
2026-02-22T18:27:11.6277907Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:11.6288601Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:11.6298646Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.6310044Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.6319955Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.6330471Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6341336Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.6351950Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.6362400Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6441411Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.6442457Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.6443254Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6444138Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.6444939Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.6445496Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6446323Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.6447378Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.6456117Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6466820Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.6477125Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.6491572Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6498304Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.6523118Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.6523992Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.6531949Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.6544292Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6550912Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.6628214Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.6628989Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:30943
2026-02-22T18:27:11.6629792Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.6634891Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.6635644Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.6636286Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6636997Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.6637795Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.6674911Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6675714Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.6676852Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.6677613Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6689152Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.6698488Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.6708679Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6719942Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.6730405Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.6740240Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6751093Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.6762161Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.6774128Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6785684Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.6796869Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.6808503Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.6817982Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.6827841Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6839737Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.6849661Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.6860894Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:30943
2026-02-22T18:27:11.6870668Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:11.6881165Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:11.6891457Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.6902666Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.6912482Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.6922443Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6933455Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.6943895Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.6953466Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6964935Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.6976929Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.6986097Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.6997304Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.7007329Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.7017483Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7028338Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.7038402Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.7048894Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7060039Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.7069800Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.7080838Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7091786Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.7103055Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.7113033Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.7123275Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.7133682Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7144915Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.7154384Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.7166820Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:43192
2026-02-22T18:27:11.7176999Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.7188145Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.7198706Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.7209933Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7220218Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.7229957Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.7240099Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7251053Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.7261468Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.7272061Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7283065Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.7293135Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.7303235Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7313757Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.7324234Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.7334463Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7345474Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.7355371Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.7366939Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7378034Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.7388706Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.7400046Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.7410154Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.7420513Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7431104Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.7441882Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.7452371Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:43192
2026-02-22T18:27:11.7461746Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:11.7473431Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:11.7481723Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.7492171Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.7502078Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.7512389Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7523665Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.7533978Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.7544159Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7555180Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.7565073Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.7575433Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7585902Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.7596635Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.7608115Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7633238Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.7634037Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.7654711Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7655613Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.7680335Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.7681092Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7690975Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.7702614Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.7715056Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.7727051Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.7740073Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7760816Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.7772992Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.7786897Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:38590
2026-02-22T18:27:11.7800702Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.7812587Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.7824763Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.7834310Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7845071Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.7856151Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.7866224Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7877593Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.7888773Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.7899958Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7910336Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.7920700Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.7931336Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7942418Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.7952309Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.7962207Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.7972085Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.7982106Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.7992464Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8002664Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.8013885Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.8134340Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.8175939Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.8186229Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8195527Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.8205112Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.8218758Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:38590
2026-02-22T18:27:11.8227300Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:11.8240384Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:11.8250032Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.8259792Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.8270188Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.8279665Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8290188Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.8300026Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.8310113Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8320162Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.8330184Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.8339678Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8350229Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.8360176Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.8370355Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8380230Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.8390079Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.8400406Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8410571Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.8419887Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.8429707Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8440205Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.8450758Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.8460335Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.8469677Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.8480013Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8490335Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.8500058Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.8508739Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:17567
2026-02-22T18:27:11.8518798Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.8528466Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.8538391Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.8547974Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8559873Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.8569787Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.8578822Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8589885Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.8599914Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.8609140Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8618799Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.8628155Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.8638883Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8648838Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.8658339Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.8668065Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8678702Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.8688164Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.8697700Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8707801Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.8717852Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.8728121Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.8737271Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.8746882Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8757193Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.8787779Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.8801710Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:17567
2026-02-22T18:27:11.8810785Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:11.8820461Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:11.8831576Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.8840888Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.8850413Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.8859782Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8870083Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.8880698Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.8890434Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8901052Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.8910548Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.8920551Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8930884Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.8939899Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.8949204Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8960381Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.8970083Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.8980023Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.8990531Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.9000752Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.9011345Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9020613Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.9029987Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.9040457Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.9050182Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.9059704Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9070686Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.9080321Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.9091109Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:30943
2026-02-22T18:27:11.9100813Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.9110104Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.9119590Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.9128951Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9138928Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.9148679Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.9158757Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9168956Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.9179318Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.9189352Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9199909Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.9208835Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.9218428Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9228669Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.9239342Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.9249617Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9260062Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.9269562Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.9279621Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9290249Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.9299413Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.9309842Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.9322509Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.9332480Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9342984Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.9352388Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.9362907Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:30943
2026-02-22T18:27:11.9372661Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:11.9382106Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:11.9390982Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.9401247Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.9411747Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.9421789Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9431620Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.9441402Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.9451183Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9460772Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.9470433Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.9480273Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9491186Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.9500361Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.9509785Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9520975Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.9530210Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.9539791Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9549545Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.9559188Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.9569392Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9579466Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.9590302Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.9599655Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.9609255Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.9618511Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9630268Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.9638936Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.9649515Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:38590
2026-02-22T18:27:11.9659492Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.9669388Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.9680158Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.9689643Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9699144Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:11.9709045Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:11.9718804Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9730105Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:11.9739661Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:11.9749243Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9761287Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:11.9772810Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.9782406Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9793126Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:11.9803254Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:11.9813330Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9824009Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:11.9833930Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:11.9844412Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9854590Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:11.9863939Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:11.9874185Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:11.9884202Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:11.9893957Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9904373Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:11.9913693Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:11.9924328Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:38590
2026-02-22T18:27:11.9933970Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:11.9943425Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:11.9953727Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:11.9963177Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:11.9974252Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:11.9985581Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:11.9992662Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:12.0002548Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:12.0012065Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.0022777Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:12.0031829Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:12.0041476Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.0051940Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:12.0061423Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:12.0070378Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.0080788Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:12.0090694Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:12.0100376Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.0110046Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:12.0119901Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:12.0129938Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.0139630Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:12.0149331Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:12.0160249Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:12.0170073Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:12.0179698Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.0190139Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:12.0199501Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:12.0209742Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:17567
2026-02-22T18:27:12.0219148Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:12.0229090Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:12.0238388Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:12.0248494Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.0258452Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:12.0267796Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:12.0278191Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.0287758Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:12.0297454Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:12.0307841Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.0318119Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:12.0328207Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:12.0337309Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.0347256Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:12.0356584Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:12.0366741Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.0377150Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:12.0386526Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:12.0396082Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.0408187Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:12.0417463Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:12.0427975Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:12.0437978Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:12.0447619Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.0458045Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:12.0467480Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:12.0478497Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:17567
2026-02-22T18:27:12.0488208Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:12.0497799Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948] EngineCore encountered a fatal error.
2026-02-22T18:27:12.0506746Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948] Traceback (most recent call last):
2026-02-22T18:27:12.0516994Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 939, in run_engine_core
2026-02-22T18:27:12.0526597Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]     engine_core.run_busy_loop()
2026-02-22T18:27:12.0536706Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 1357, in run_busy_loop
2026-02-22T18:27:12.0546105Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]     executed = self._process_engine_step()
2026-02-22T18:27:12.0556131Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.0566548Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 999, in _process_engine_step
2026-02-22T18:27:12.0620391Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]     outputs, model_executed = self.step_fn()
2026-02-22T18:27:12.0620924Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]                               ^^^^^^^^^^^^^^
2026-02-22T18:27:12.0621566Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 490, in step_with_batch_queue
2026-02-22T18:27:12.0622425Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]     exec_model_fut.result()
2026-02-22T18:27:12.0623071Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 80, in result
2026-02-22T18:27:12.0625329Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]     return super().result()
2026-02-22T18:27:12.0633842Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]            ^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.0644480Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]   File "/usr/local/python3.11.14/lib/python3.11/concurrent/futures/_base.py", line 449, in result
2026-02-22T18:27:12.0653644Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]     return self.__get_result()
2026-02-22T18:27:12.0663741Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]            ^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.0673675Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]   File "/usr/local/python3.11.14/lib/python3.11/concurrent/futures/_base.py", line 401, in __get_result
2026-02-22T18:27:12.0683594Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]     raise self._exception
2026-02-22T18:27:12.0693161Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 84, in wait_for_response
2026-02-22T18:27:12.0704365Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]     response = self.aggregate(get_response())
2026-02-22T18:27:12.0712677Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]                               ^^^^^^^^^^^^^^
2026-02-22T18:27:12.0722846Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
2026-02-22T18:27:12.0733876Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948]     raise RuntimeError(
2026-02-22T18:27:12.0743111Z [0;36m(EngineCore_DP0 pid=155)[0;0m ERROR 02-22 18:27:10 [core.py:948] RuntimeError: Worker failed with error '[/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:4317', please check the stack trace above for the root cause
2026-02-22T18:27:12.0752744Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948] EngineCore encountered a fatal error.
2026-02-22T18:27:12.0763444Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948] Traceback (most recent call last):
2026-02-22T18:27:12.0773509Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 939, in run_engine_core
2026-02-22T18:27:12.0783392Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]     engine_core.run_busy_loop()
2026-02-22T18:27:12.0793458Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 1357, in run_busy_loop
2026-02-22T18:27:12.0803334Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]     executed = self._process_engine_step()
2026-02-22T18:27:12.0813595Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.0828322Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 999, in _process_engine_step
2026-02-22T18:27:12.0838335Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]     outputs, model_executed = self.step_fn()
2026-02-22T18:27:12.0848623Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]                               ^^^^^^^^^^^^^^
2026-02-22T18:27:12.0859334Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 490, in step_with_batch_queue
2026-02-22T18:27:12.0870063Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]     exec_model_fut.result()
2026-02-22T18:27:12.0881231Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 80, in result
2026-02-22T18:27:12.0891749Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]     return super().result()
2026-02-22T18:27:12.0901630Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]            ^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.0911794Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]   File "/usr/local/python3.11.14/lib/python3.11/concurrent/futures/_base.py", line 449, in result
2026-02-22T18:27:12.0921437Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]     return self.__get_result()
2026-02-22T18:27:12.0931782Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]            ^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.0941587Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]   File "/usr/local/python3.11.14/lib/python3.11/concurrent/futures/_base.py", line 401, in __get_result
2026-02-22T18:27:12.0950326Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]     raise self._exception
2026-02-22T18:27:12.0960929Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 84, in wait_for_response
2026-02-22T18:27:12.0970525Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]     response = self.aggregate(get_response())
2026-02-22T18:27:12.0980650Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]                               ^^^^^^^^^^^^^^
2026-02-22T18:27:12.0990617Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
2026-02-22T18:27:12.1000294Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948]     raise RuntimeError(
2026-02-22T18:27:12.1011138Z [0;36m(EngineCore_DP1 pid=174)[0;0m ERROR 02-22 18:27:10 [core.py:948] RuntimeError: Worker failed with error '[/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:37504', please check the stack trace above for the root cause
2026-02-22T18:27:12.1020734Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:12.1030210Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:12.1040243Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:12.1049923Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:12.1059847Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1070099Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:12.1079683Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:12.1089851Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1100226Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:12.1109368Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:12.1119103Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1129438Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:12.1138837Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:12.1148430Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1158868Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:12.1168394Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:12.1178846Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1188531Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:12.1199329Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:12.1210012Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1220585Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:12.1230182Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:12.1240609Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:12.1252645Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:12.1263862Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1273736Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:12.1283786Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:12.1294149Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:59027
2026-02-22T18:27:12.1303794Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:12.1314307Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:12.1360701Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:12.1361275Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1362173Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:12.1362958Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:12.1363619Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1373611Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:12.1383134Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:12.1392779Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1402990Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:12.1412280Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:12.1421591Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1431902Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:12.1441505Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:12.1451548Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1461824Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:12.1470752Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:12.1480827Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1491278Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:12.1500851Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:12.1510996Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:12.1520834Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:12.1530857Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1540772Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:12.1549430Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:12.1560042Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:59027
2026-02-22T18:27:12.1568807Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:12.1579394Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] WorkerProc hit an exception.
2026-02-22T18:27:12.1588693Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:12.1598708Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:12.1608362Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:12.1618313Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1627971Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:12.1638013Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:12.1647913Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1657739Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:12.1667023Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:12.1678937Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1687467Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:12.1699899Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:12.1707043Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1717928Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:12.1726936Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:12.1736499Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1746452Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:12.1755368Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:12.1766790Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1777953Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:12.1788667Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:12.1799351Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:12.1808798Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:12.1818775Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1840754Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:12.1854350Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:12.1864705Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:59027
2026-02-22T18:27:12.1873915Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] Traceback (most recent call last):
2026-02-22T18:27:12.1884659Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 847, in worker_busy_loop
2026-02-22T18:27:12.1893578Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = func(*args, **kwargs)
2026-02-22T18:27:12.1903712Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1915072Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm/vllm/v1/worker/worker_base.py", line 365, in execute_model
2026-02-22T18:27:12.1923703Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return self.worker.execute_model(scheduler_output)
2026-02-22T18:27:12.1933807Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1943656Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 362, in execute_model
2026-02-22T18:27:12.1953554Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     output = self.model_runner.execute_model(scheduler_output, intermediate_tensors)
2026-02-22T18:27:12.1963656Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.1973770Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
2026-02-22T18:27:12.1983646Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:12.1993575Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.2005491Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1135, in execute_model
2026-02-22T18:27:12.2015647Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     ) = self._determine_batch_execution_and_padding(
2026-02-22T18:27:12.2024880Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.2034245Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1774, in _determine_batch_execution_and_padding
2026-02-22T18:27:12.2044252Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     _, num_tokens_across_dp, synced_cudagraph_mode = self._sync_batch_across_dp(
2026-02-22T18:27:12.2053494Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.2063948Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 1711, in _sync_batch_across_dp
2026-02-22T18:27:12.2073432Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     dist.all_reduce(tensor, group=get_dp_group().cpu_group)
2026-02-22T18:27:12.2084194Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
2026-02-22T18:27:12.2093952Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     return func(*args, **kwargs)
2026-02-22T18:27:12.2103433Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]            ^^^^^^^^^^^^^^^^^^^^^
2026-02-22T18:27:12.2113584Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]   File "/usr/local/python3.11.14/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
2026-02-22T18:27:12.2122679Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852]     work.wait()
2026-02-22T18:27:12.2132930Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [10.0.0.102]:59027
2026-02-22T18:27:12.2142869Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m ERROR 02-22 18:27:10 [multiproc_executor.py:852] 
2026-02-22T18:27:13.9298807Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-22T18:27:13.9306532Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-22T18:27:13.9316298Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-22T18:27:13.9326757Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-22T18:27:13.9336380Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-22T18:27:13.9346343Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-22T18:27:13.9355669Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-22T18:27:13.9365797Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-22T18:27:13.9375323Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-22T18:27:13.9384602Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-22T18:27:13.9394436Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-22T18:27:13.9404389Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-22T18:27:13.9414097Z [0;36m(Worker_DP0_TP0_EP0 pid=203)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-22T18:27:13.9423346Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-22T18:27:13.9432594Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-22T18:27:13.9442755Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-22T18:27:13.9452484Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-22T18:27:13.9461870Z [0;36m(Worker_DP0_TP1_EP1 pid=252)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-22T18:27:13.9471492Z [0;36m(Worker_DP1_TP5_EP13 pid=689)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-22T18:27:13.9481875Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-22T18:27:13.9492393Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-22T18:27:13.9502522Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-22T18:27:13.9511638Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-22T18:27:13.9521209Z [0;36m(Worker_DP1_TP7_EP15 pid=894)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-22T18:27:13.9531499Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-22T18:27:13.9543594Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-22T18:27:13.9553539Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-22T18:27:13.9560864Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-22T18:27:13.9572245Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-22T18:27:13.9581499Z [0;36m(Worker_DP1_TP1_EP9 pid=258)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-22T18:27:13.9591114Z [0;36m(Worker_DP1_TP4_EP12 pid=582)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-22T18:27:13.9600972Z [0;36m(Worker_DP1_TP6_EP14 pid=790)[0;0m INFO 02-22 18:27:13 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-22T18:27:18.9603735Z Traceback (most recent call last):
2026-02-22T18:27:18.9604096Z   File "/usr/local/python3.11.14/bin/vllm", line 7, in <module>
2026-02-22T18:27:18.9604359Z     sys.exit(main())
2026-02-22T18:27:18.9613564Z              ^^^^^^
2026-02-22T18:27:18.9669290Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/main.py", line 73, in main
2026-02-22T18:27:18.9669653Z     args.dispatch_function(args)
2026-02-22T18:27:18.9669944Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 108, in cmd
2026-02-22T18:27:18.9670248Z     run_multi_api_server(args)
2026-02-22T18:27:18.9670570Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 282, in run_multi_api_server
2026-02-22T18:27:18.9670939Z     wait_for_completion_or_failure(
2026-02-22T18:27:18.9680465Z   File "/vllm-workspace/vllm/vllm/v1/utils.py", line 277, in wait_for_completion_or_failure
2026-02-22T18:27:18.9690039Z     raise RuntimeError(
2026-02-22T18:27:18.9700011Z RuntimeError: Process ApiServer_1 (PID: 186) died with exit code None
2026-02-22T18:27:19.0450874Z [ERROR] 2026-02-22-18:27:18 (PID:139, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
2026-02-22T18:27:20.3003897Z sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-22T18:27:22.0499850Z /usr/local/python3.11.14/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked shared_memory objects to clean up at shutdown
2026-02-22T18:27:22.0506339Z   warnings.warn('resource_tracker: There appear to be %d '
2026-02-22T18:27:33.3875565Z 2026-02-22 18:27:33,385 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-22T18:27:33.4238956Z 2026-02-22 18:27:33,422 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-22T18:28:30.0100762Z [2026-02-22 18:28:30] ERROR aisbench.py:243: The following aisbench case failed: {'case_type': 'accuracy', 'dataset_path': 'vllm-ascend/gsm8k-lite', 'request_conf': 'vllm_api_general_chat', 'dataset_conf': 'gsm8k/gsm8k_gen_0_shot_cot_chat_prompt', 'max_out_len': 4096, 'batch_size': 64, 'baseline': 95, 'threshold': 5}, reason is Accuracy verification failed. The accuracy of /root/.cache/modelscope/hub/datasets/vllm-ascend/gsm8k-lite is 0.0, which is not within 5 relative to baseline 95.
2026-02-22T18:28:30.0113866Z [2026-02-22 18:28:30] ERROR aisbench.py:243: The following aisbench case failed: {'case_type': 'performance', 'dataset_path': 'vllm-ascend/GSM8K-in3500-bs2800', 'request_conf': 'vllm_api_stream_chat', 'dataset_conf': 'gsm8k/gsm8k_gen_0_shot_cot_str_perf', 'num_prompts': 512, 'max_out_len': 3000, 'batch_size': 512, 'request_rate': 11.2, 'baseline': 1253.8466, 'threshold': 0.97}, reason is Some errors happened to Aisbench runtime, the first error is 02/22 18:28:30 - AISBench - ERROR - /vllm-workspace/vllm-ascend/benchmark/ais_bench/benchmark/calculators/default_perf_metric_calculator.py - _init_datas - 21 - All requests failed, can't calculate performance results. Please check the ERROR log from every responses!
2026-02-22T18:28:30.3277330Z The request config is
2026-02-22T18:28:30.3286821Z  from ais_bench.benchmark.models import VLLMCustomAPIChat
2026-02-22T18:28:30.3296326Z from ais_bench.benchmark.utils.model_postprocessors import extract_non_reasoning_content
2026-02-22T18:28:30.3307040Z 
2026-02-22T18:28:30.3317283Z models = [
2026-02-22T18:28:30.3326734Z     dict(
2026-02-22T18:28:30.3336506Z         attr="service",
2026-02-22T18:28:30.3346465Z         type=VLLMCustomAPIChat,
2026-02-22T18:28:30.3357141Z         abbr='vllm-api-general-chat',
2026-02-22T18:28:30.3366673Z         path="",
2026-02-22T18:28:30.3376059Z         model="vllm-ascend/DeepSeek-V3.2-W8A8",
2026-02-22T18:28:30.3386046Z         request_rate = 0,
2026-02-22T18:28:30.3396163Z         retry = 2,
2026-02-22T18:28:30.3406452Z         host_ip = "10.0.0.52",
2026-02-22T18:28:30.3416718Z         host_port = 8080,
2026-02-22T18:28:30.3426937Z         max_out_len = 4096,
2026-02-22T18:28:30.3436195Z         batch_size = 64,
2026-02-22T18:28:30.3445972Z         trust_remote_code=False,
2026-02-22T18:28:30.3456133Z         generation_kwargs = dict(
2026-02-22T18:28:30.3466822Z             temperature = 0.6,
2026-02-22T18:28:30.3477237Z             ignore_eos = False,
2026-02-22T18:28:30.3487731Z             #top_k = 10,
2026-02-22T18:28:30.3496950Z             top_p = 0.95,
2026-02-22T18:28:30.3506632Z             #seed = None,
2026-02-22T18:28:30.3516339Z             #repetition_penalty = 1.03,
2026-02-22T18:28:30.3528786Z         ),
2026-02-22T18:28:30.3537980Z         pred_postprocessor=dict(type=extract_non_reasoning_content)
2026-02-22T18:28:30.3547314Z     )
2026-02-22T18:28:30.3557433Z ]
2026-02-22T18:28:30.3604621Z 
2026-02-22T18:28:30.3605397Z running aisbench cmd: ais_bench --models vllm_api_general_chat_custom --datasets gsm8k_gen_0_shot_cot_chat_prompt
2026-02-22T18:28:30.3606358Z 02/22 18:25:59 - AISBench - INFO - Loading gsm8k_gen_0_shot_cot_chat_prompt: /vllm-workspace/vllm-ascend/benchmark/ais_bench/benchmark/configs/./datasets/gsm8k/gsm8k_gen_0_shot_cot_chat_prompt.py
2026-02-22T18:28:30.3607529Z 02/22 18:25:59 - AISBench - INFO - Loading vllm_api_general_chat_custom: /vllm-workspace/vllm-ascend/benchmark/ais_bench/benchmark/configs/./models/vllm_api/vllm_api_general_chat_custom.py
2026-02-22T18:28:30.3611338Z 02/22 18:25:59 - AISBench - INFO - Loading example: /vllm-workspace/vllm-ascend/benchmark/ais_bench/benchmark/configs/./summarizers/example.py
2026-02-22T18:28:30.3621011Z 02/22 18:25:59 - AISBench - INFO - Current exp folder: outputs/default/20260222_182559
2026-02-22T18:28:30.3631876Z 02/22 18:26:01 - AISBench - INFO - Starting inference tasks...
2026-02-22T18:28:30.3642364Z 02/22 18:26:01 - AISBench - INFO - Partitioned into 1 tasks.
2026-02-22T18:28:30.3653686Z 02/22 18:26:01 - AISBench - INFO - Continuous batch enable! All the logs and processes for each task should be checked in each infer/.out file.
2026-02-22T18:28:30.3663850Z Launch OpenICLInfer[vllm-api-general-chat/gsm8k] on CPU
2026-02-22T18:28:30.3674045Z Launch OpenICLInfer[vllm-api-general-chat/gsm8k] on CPU
2026-02-22T18:28:30.3684557Z 02/22 18:27:20 - AISBench - INFO - Inference tasks completed.
2026-02-22T18:28:30.3695271Z 02/22 18:27:22 - AISBench - INFO - Starting evaluation tasks...
2026-02-22T18:28:30.3705712Z 02/22 18:27:22 - AISBench - INFO - Partitioned into 1 tasks.
2026-02-22T18:28:30.3715761Z launch OpenICLEval[vllm-api-general-chat/gsm8k] on CPU
2026-02-22T18:28:30.3726684Z 02/22 18:27:33 - AISBench - INFO - Evaluation tasks completed.
2026-02-22T18:28:30.3737202Z 02/22 18:27:33 - AISBench - INFO - Summarizing evaluation results...
2026-02-22T18:28:30.3748517Z dataset    version    metric    mode      vllm-api-general-chat
2026-02-22T18:28:30.3760116Z ---------  ---------  --------  ------  -----------------------
2026-02-22T18:28:30.3771398Z gsm8k      7cd45e     accuracy  gen                        0.00
2026-02-22T18:28:30.3782902Z 02/22 18:27:33 - AISBench - INFO - write summary to /vllm-workspace/vllm-ascend/outputs/default/20260222_182559/summary/summary_20260222_182559.txt
2026-02-22T18:28:30.3792677Z 02/22 18:27:33 - AISBench - INFO - write csv to /vllm-workspace/vllm-ascend/outputs/default/20260222_182559/summary/summary_20260222_182559.csv
2026-02-22T18:28:30.3803495Z Accuracy verification failed. The accuracy of /root/.cache/modelscope/hub/datasets/vllm-ascend/gsm8k-lite is 0.0, which is not within 5 relative to baseline 95.
2026-02-22T18:28:30.3813983Z The request config is
2026-02-22T18:28:30.3824352Z  from ais_bench.benchmark.models import VLLMCustomAPIChatStream
2026-02-22T18:28:30.3834999Z from ais_bench.benchmark.utils.model_postprocessors import extract_non_reasoning_content
2026-02-22T18:28:30.3845538Z 
2026-02-22T18:28:30.3856274Z models = [
2026-02-22T18:28:30.3866913Z     dict(
2026-02-22T18:28:30.3877466Z         attr="service",
2026-02-22T18:28:30.3888434Z         type=VLLMCustomAPIChatStream,
2026-02-22T18:28:30.3898352Z         abbr='vllm-api-stream-chat',
2026-02-22T18:28:30.3909278Z         path="/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8",
2026-02-22T18:28:30.3921605Z         model="vllm-ascend/DeepSeek-V3.2-W8A8",
2026-02-22T18:28:30.3930199Z         request_rate = 11.2,
2026-02-22T18:28:30.3939813Z         retry = 2,
2026-02-22T18:28:30.3950653Z         host_ip = "10.0.0.52",
2026-02-22T18:28:30.3960950Z         host_port = 8080,
2026-02-22T18:28:30.3970928Z         max_out_len = 3000,
2026-02-22T18:28:30.3980660Z         batch_size = 512,
2026-02-22T18:28:30.3991074Z         trust_remote_code=False,
2026-02-22T18:28:30.4001604Z         generation_kwargs = dict(
2026-02-22T18:28:30.4012093Z             temperature = 0,
2026-02-22T18:28:30.4021936Z             ignore_eos = True,
2026-02-22T18:28:30.4030944Z             #top_k = 10,
2026-02-22T18:28:30.4040449Z             #top_p = 0.95,
2026-02-22T18:28:30.4050790Z             #seed = None,
2026-02-22T18:28:30.4061006Z             #repetition_penalty = 1.03,
2026-02-22T18:28:30.4070844Z         ),
2026-02-22T18:28:30.4081417Z         pred_postprocessor=dict(type=extract_non_reasoning_content)
2026-02-22T18:28:30.4091544Z     )
2026-02-22T18:28:30.4100692Z ]
2026-02-22T18:28:30.4110899Z 
2026-02-22T18:28:30.4153738Z running aisbench cmd: ais_bench --models vllm_api_stream_chat_custom --datasets gsm8k_gen_0_shot_cot_str_perf_custom --mode perf --num-prompts 512
2026-02-22T18:28:30.4167022Z 02/22 18:27:42 - AISBench - INFO - Loading gsm8k_gen_0_shot_cot_str_perf_custom: /vllm-workspace/vllm-ascend/benchmark/ais_bench/benchmark/configs/./datasets/gsm8k/gsm8k_gen_0_shot_cot_str_perf_custom.py
2026-02-22T18:28:30.4177165Z 02/22 18:27:42 - AISBench - INFO - Loading vllm_api_stream_chat_custom: /vllm-workspace/vllm-ascend/benchmark/ais_bench/benchmark/configs/./models/vllm_api/vllm_api_stream_chat_custom.py
2026-02-22T18:28:30.4187268Z 02/22 18:27:42 - AISBench - INFO - Loading example: /vllm-workspace/vllm-ascend/benchmark/ais_bench/benchmark/configs/./summarizers/example.py
2026-02-22T18:28:30.4197194Z 02/22 18:27:42 - AISBench - INFO - Current exp folder: outputs/default/20260222_182742
2026-02-22T18:28:30.4208001Z 02/22 18:27:42 - AISBench - INFO - Starting performance evaluation tasks...
2026-02-22T18:28:30.4216994Z 02/22 18:27:42 - AISBench - INFO - Partitioned into 1 tasks.
2026-02-22T18:28:30.4227293Z 02/22 18:27:42 - AISBench - INFO - Continuous batch enable! All the logs and processes for each task should be checked in each infer/.out file.
2026-02-22T18:28:30.4237214Z Launch OpenICLPerf[vllm-api-stream-chat/gsm8k] on CPU
2026-02-22T18:28:30.4247109Z Launch OpenICLPerf[vllm-api-stream-chat/gsm8k] on CPU
2026-02-22T18:28:30.4257028Z 02/22 18:28:29 - AISBench - INFO - Performance evaluation tasks completed.
2026-02-22T18:28:30.4267095Z 02/22 18:28:29 - AISBench - INFO - Loading detail perf data of model='vllm-api-stream-chat' dataset='gsm8kdataset' ...
2026-02-22T18:28:30.4276978Z 02/22 18:28:29 - AISBench - INFO - Starting request timeline processing...
2026-02-22T18:28:30.4287278Z 02/22 18:28:29 - AISBench - WARNING - [Non-streaming scenario] The plot will only show the request concurrency chart!
2026-02-22T18:28:30.4297008Z 02/22 18:28:29 - AISBench - INFO - Data preprocessing completed in 0.0002s
2026-02-22T18:28:30.4307447Z 02/22 18:28:29 - AISBench - INFO - Generating concurrency traces...
2026-02-22T18:28:30.4316265Z 02/22 18:28:29 - AISBench - WARNING - No valid requests for concurrency plot!
2026-02-22T18:28:30.4326707Z 02/22 18:28:29 - AISBench - INFO - Generated concurrency trace chunks in 0.0001s
2026-02-22T18:28:30.4335887Z 02/22 18:28:29 - AISBench - INFO - Creating figure layout...
2026-02-22T18:28:30.4346555Z 02/22 18:28:29 - AISBench - INFO - Figure layout created in 0.0155s
2026-02-22T18:28:30.4356215Z 02/22 18:28:29 - AISBench - INFO - Writing to outputs/default/20260222_182742/performances/vllm-api-stream-chat/gsm8kdataset_plot.html...
2026-02-22T18:28:30.4366197Z 02/22 18:28:30 - AISBench - INFO - HTML written in 0.0112s
2026-02-22T18:28:30.4376514Z 02/22 18:28:30 - AISBench - INFO - Completed! Total execution time: 0.0273s
2026-02-22T18:28:30.4386531Z 02/22 18:28:30 - AISBench - INFO - The gsm8kdataset_plot has been saved in outputs/default/20260222_182742/performances/vllm-api-stream-chat/gsm8kdataset_plot.html
2026-02-22T18:28:30.4397106Z 02/22 18:28:30 - AISBench - ERROR - /vllm-workspace/vllm-ascend/benchmark/ais_bench/benchmark/calculators/default_perf_metric_calculator.py - _init_datas - 21 - All requests failed, can't calculate performance results. Please check the ERROR log from every responses!
2026-02-22T18:28:30.4408529Z Some errors happened to Aisbench runtime, the first error is 02/22 18:28:30 - AISBench - ERROR - /vllm-workspace/vllm-ascend/benchmark/ais_bench/benchmark/calculators/default_perf_metric_calculator.py - _init_datas - 21 - All requests failed, can't calculate performance results. Please check the ERROR log from every responses!
2026-02-22T18:28:31.7212554Z FAILED
2026-02-22T18:28:31.7221618Z 
2026-02-22T18:28:31.7232835Z =================================== FAILURES ===================================
2026-02-22T18:28:31.7243811Z _______________________________ test_multi_node ________________________________
2026-02-22T18:28:31.7253993Z 
2026-02-22T18:28:31.7262992Z     @pytest.mark.asyncio
2026-02-22T18:28:31.7272340Z     async def test_multi_node() -> None:
2026-02-22T18:28:31.7282230Z         config = MultiNodeConfigLoader.from_yaml()
2026-02-22T18:28:31.7291743Z     
2026-02-22T18:28:31.7300789Z         with ProxyLauncher(
2026-02-22T18:28:31.7310536Z                 nodes=config.nodes,
2026-02-22T18:28:31.7319768Z                 disagg_cfg=config.disagg_cfg,
2026-02-22T18:28:31.7329563Z                 envs=config.envs,
2026-02-22T18:28:31.7338972Z                 proxy_port=config.proxy_port,
2026-02-22T18:28:31.7348393Z                 cur_index=config.cur_index,
2026-02-22T18:28:31.7357511Z         ) as proxy:
2026-02-22T18:28:31.7367134Z     
2026-02-22T18:28:31.7377104Z             with RemoteOpenAIServer(
2026-02-22T18:28:31.7386523Z                     model=config.model,
2026-02-22T18:28:31.7396976Z                     vllm_serve_args=config.server_cmd,
2026-02-22T18:28:31.7406591Z                     server_port=config.server_port,
2026-02-22T18:28:31.7415952Z                     server_host=config.master_ip,
2026-02-22T18:28:31.7424788Z                     env_dict=config.envs,
2026-02-22T18:28:31.7434434Z                     auto_port=False,
2026-02-22T18:28:31.7445273Z                     proxy_port=proxy.proxy_port,
2026-02-22T18:28:31.7454679Z                     disaggregated_prefill=config.disagg_cfg,
2026-02-22T18:28:31.7464299Z                     nodes_info=config.nodes,
2026-02-22T18:28:31.7473511Z                     max_wait_seconds=2800,
2026-02-22T18:28:31.7483572Z             ) as server:
2026-02-22T18:28:31.7494216Z     
2026-02-22T18:28:31.7502570Z                 host, port = config.benchmark_endpoint
2026-02-22T18:28:31.7511194Z     
2026-02-22T18:28:31.7521530Z                 if config.is_master:
2026-02-22T18:28:31.7530755Z >                   run_aisbench_cases(
2026-02-22T18:28:31.7540635Z                         model=config.model,
2026-02-22T18:28:31.7550043Z                         port=port,
2026-02-22T18:28:31.7560714Z                         aisbench_cases=[config.acc_cmd, config.perf_cmd],
2026-02-22T18:28:31.7569608Z                         host_ip=host,
2026-02-22T18:28:31.7579320Z                     )
2026-02-22T18:28:31.7588812Z 
2026-02-22T18:28:31.7619762Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py:37: 
2026-02-22T18:28:31.7620055Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-22T18:28:31.7620570Z 
2026-02-22T18:28:31.7628521Z model = 'vllm-ascend/DeepSeek-V3.2-W8A8', port = '8080'
2026-02-22T18:28:31.7640996Z aisbench_cases = [{'baseline': 95, 'batch_size': 64, 'case_type': 'accuracy', 'dataset_conf': 'gsm8k/gsm8k_gen_0_shot_cot_chat_prompt',... 1253.8466, 'batch_size': 512, 'case_type': 'performance', 'dataset_conf': 'gsm8k/gsm8k_gen_0_shot_cot_str_perf', ...}]
2026-02-22T18:28:31.7649356Z server_args = '', host_ip = '10.0.0.52'
2026-02-22T18:28:31.7658889Z 
2026-02-22T18:28:31.7668656Z     def run_aisbench_cases(model, port, aisbench_cases, server_args="", host_ip="localhost"):
2026-02-22T18:28:31.7677709Z         aisbench_results = []
2026-02-22T18:28:31.7687328Z         aisbench_errors = []
2026-02-22T18:28:31.7697581Z         for aisbench_case in aisbench_cases:
2026-02-22T18:28:31.7707054Z             if not aisbench_case:
2026-02-22T18:28:31.7716527Z                 continue
2026-02-22T18:28:31.7726665Z             try:
2026-02-22T18:28:31.7736478Z                 with AisbenchRunner(model=model, port=port, host_ip=host_ip, aisbench_config=aisbench_case) as aisbench:
2026-02-22T18:28:31.7746013Z                     aisbench_results.append(aisbench.result)
2026-02-22T18:28:31.7755701Z             except Exception as e:
2026-02-22T18:28:31.7766199Z                 aisbench_results.append("")
2026-02-22T18:28:31.7776757Z                 aisbench_errors.append([aisbench_case, e])
2026-02-22T18:28:31.7787036Z                 print(e)
2026-02-22T18:28:31.7797430Z         for failed_case, error_info in aisbench_errors:
2026-02-22T18:28:31.7808316Z             error_msg = f"The following aisbench case failed: {failed_case}, reason is {error_info}"
2026-02-22T18:28:31.7818300Z             if server_args:
2026-02-22T18:28:31.7829148Z                 error_msg += f"\nserver_args are {server_args}"
2026-02-22T18:28:31.7839309Z             logging.error(error_msg)
2026-02-22T18:28:31.7850073Z >       assert not aisbench_errors, "some aisbench cases failed, info were shown above."
2026-02-22T18:28:31.7860137Z                ^^^^^^^^^^^^^^^^^^^
2026-02-22T18:28:31.7870791Z E       AssertionError: some aisbench cases failed, info were shown above.
2026-02-22T18:28:31.7880956Z 
2026-02-22T18:28:31.7891339Z tools/aisbench.py:244: AssertionError
2026-02-22T18:28:31.7901891Z =============================== warnings summary ===============================
2026-02-22T18:28:31.7911258Z <frozen importlib._bootstrap>:241
2026-02-22T18:28:31.7922366Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute
2026-02-22T18:28:31.7932154Z 
2026-02-22T18:28:31.7942127Z <frozen importlib._bootstrap>:241
2026-02-22T18:28:31.7951899Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute
2026-02-22T18:28:31.7961471Z 
2026-02-22T18:28:31.7971796Z ../../usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647
2026-02-22T18:28:31.7982408Z   /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-22T18:28:31.7991378Z     warnings.warn(
2026-02-22T18:28:31.8001452Z 
2026-02-22T18:28:31.8011769Z ../../usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8
2026-02-22T18:28:31.8022359Z   /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
2026-02-22T18:28:31.8031429Z     import pkg_resources
2026-02-22T18:28:31.8040923Z 
2026-02-22T18:28:31.8051321Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node
2026-02-22T18:28:31.8061340Z   /usr/local/python3.11.14/lib/python3.11/site-packages/pandas/core/dtypes/cast.py:1641: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.
2026-02-22T18:28:31.8073349Z   See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)
2026-02-22T18:28:31.8090708Z     return np.find_common_type(types, [])
2026-02-22T18:28:31.8102087Z 
2026-02-22T18:28:31.8112819Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2026-02-22T18:28:31.8123170Z =========================== short test summary info ============================
2026-02-22T18:28:31.8134619Z FAILED tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node
2026-02-22T18:28:31.8144532Z ================== 1 failed, 5 warnings in 781.40s (0:13:01) ===================
2026-02-22T18:28:31.9955699Z sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-22T18:28:33.4106362Z [0;31mFAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml âœ— ERROR: Some tests failed, please check the error stack above for details. If this is insufficient to pinpoint the error, please download and review the logs of all other nodes from the job's summary.[0m
2026-02-22T18:28:33.6401672Z Cleaning up background log streams...
2026-02-22T18:28:33.7139520Z ##[error]Error: failed to run script step: Error: command terminated with non-zero exit code: command terminated with exit code 1
2026-02-22T18:28:33.7209840Z ##[error]Process completed with exit code 1.
2026-02-22T18:28:33.7303269Z ##[error]Executing the custom container implementation failed. Please contact your self hosted runner administrator.
2026-02-22T18:28:33.7733258Z ##[group]Run actions/upload-artifact@v6
2026-02-22T18:28:33.7733466Z with:
2026-02-22T18:28:33.7733645Z   name: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs
2026-02-22T18:28:33.7733882Z   path: /tmp/vllm*_logs.txt
2026-02-22T18:28:33.7734056Z   retention-days: 7
2026-02-22T18:28:33.7734210Z   if-no-files-found: warn
2026-02-22T18:28:33.7734385Z   compression-level: 6
2026-02-22T18:28:33.7734534Z   overwrite: false
2026-02-22T18:28:33.7734706Z   include-hidden-files: false
2026-02-22T18:28:33.7734876Z env:
2026-02-22T18:28:33.7735053Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-22T18:28:33.7735286Z ##[endgroup]
2026-02-22T18:28:33.7759341Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-22T18:28:33.7759966Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-22T18:28:33.7760177Z ##[endgroup]
2026-02-22T18:28:34.1306463Z (node:961) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-22T18:28:34.1307229Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-22T18:28:35.1983558Z With the provided path, there will be 1 file uploaded
2026-02-22T18:28:35.1986914Z Artifact name is valid!
2026-02-22T18:28:35.1987821Z Root directory input is valid!
2026-02-22T18:28:36.1838485Z Beginning upload of artifact content to blob storage
2026-02-22T18:28:37.5083430Z Uploaded bytes 13566
2026-02-22T18:28:37.7353284Z Finished uploading artifact content to blob storage!
2026-02-22T18:28:37.7355401Z SHA256 digest of uploaded artifact zip is f7f347902a19f787bb2e0e4c3cfeb6583e7b4483a5a176a7a7f5df2eed648a7d
2026-02-22T18:28:37.7355983Z Finalizing artifact upload
2026-02-22T18:28:38.6024254Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs.zip successfully finalized. Artifact ID 5608527846
2026-02-22T18:28:38.6024955Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs has been successfully uploaded! Final size is 13566 bytes. Artifact ID is 5608527846
2026-02-22T18:28:38.6028983Z Artifact download URL: https://github.com/vllm-project/vllm-ascend/actions/runs/22280686910/artifacts/5608527846
2026-02-22T18:28:39.0668431Z ##[group]Run kubectl get pods -n "$NAMESPACE" --ignore-not-found=true
2026-02-22T18:28:39.0668836Z [36;1mkubectl get pods -n "$NAMESPACE" --ignore-not-found=true[0m
2026-02-22T18:28:39.0669235Z [36;1mkubectl delete -f ./lws.yaml --ignore-not-found=true || true[0m
2026-02-22T18:28:39.0669875Z shell: bash -el {0}
2026-02-22T18:28:39.0670048Z env:
2026-02-22T18:28:39.0670241Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-22T18:28:39.0670473Z ##[endgroup]
2026-02-22T18:28:39.0743311Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-22T18:28:39.0743945Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-22T18:28:39.0744162Z ##[endgroup]
2026-02-22T18:28:39.4278985Z (node:1075) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-22T18:28:39.4279767Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-22T18:28:40.0895529Z NAME                                             READY   STATUS    RESTARTS      AGE
2026-02-22T18:28:40.0895925Z linux-aarch64-a3-0-n4cwm-runner-tgpp7            1/1     Running   0             15m
2026-02-22T18:28:40.0896370Z linux-aarch64-a3-0-n4cwm-runner-tgpp7-workflow   1/1     Running   0             15m
2026-02-22T18:28:40.0896732Z vllm-0                                           1/1     Running   1 (7s ago)    14m
2026-02-22T18:28:40.0897019Z vllm-0-1                                         1/1     Running   1 (85s ago)   14m
2026-02-22T18:28:40.1651128Z leaderworkerset.leaderworkerset.x-k8s.io "vllm" deleted from vllm-project namespace
2026-02-22T18:28:40.1824651Z service "vllm-leader" deleted from vllm-project namespace
2026-02-22T18:28:40.6459990Z Post job cleanup.
2026-02-22T18:28:40.6480914Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-22T18:28:40.6481573Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-22T18:28:40.6481797Z ##[endgroup]
2026-02-22T18:28:41.0016096Z (node:1199) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-22T18:28:41.0016821Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-22T18:28:41.6463104Z [command]/usr/bin/git version
2026-02-22T18:28:41.6634717Z git version 2.34.1
2026-02-22T18:28:41.6670911Z Copying '/root/.gitconfig' to '/__w/_temp/76ba827c-99bc-4c21-bc46-5fdb04f00a17/.gitconfig'
2026-02-22T18:28:41.6677886Z Temporarily overriding HOME='/__w/_temp/76ba827c-99bc-4c21-bc46-5fdb04f00a17' before making global git config changes
2026-02-22T18:28:41.6678470Z Adding repository directory to the temporary git global config as a safe directory
2026-02-22T18:28:41.6682718Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-22T18:28:41.6717783Z Removing SSH command configuration
2026-02-22T18:28:41.6722906Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-22T18:28:41.6776097Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-22T18:28:41.7233480Z Removing HTTP extra header
2026-02-22T18:28:41.7235822Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-22T18:28:41.7262091Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-22T18:28:41.7439917Z Removing includeIf entries pointing to credentials config files
2026-02-22T18:28:41.7445062Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-22T18:28:41.7464294Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-22T18:28:41.7464617Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-22T18:28:41.7464916Z includeif.gitdir:/github/workspace/.git.path
2026-02-22T18:28:41.7465190Z includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-22T18:28:41.7470490Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-22T18:28:41.7487721Z /__w/_temp/git-credentials-9a4e7775-8d2d-4d1d-92ce-2621dc1395c4.config
2026-02-22T18:28:41.7496278Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-9a4e7775-8d2d-4d1d-92ce-2621dc1395c4.config
2026-02-22T18:28:41.7540929Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-22T18:28:41.7548748Z /__w/_temp/git-credentials-9a4e7775-8d2d-4d1d-92ce-2621dc1395c4.config
2026-02-22T18:28:41.7556483Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-9a4e7775-8d2d-4d1d-92ce-2621dc1395c4.config
2026-02-22T18:28:41.7582631Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git.path
2026-02-22T18:28:41.7602313Z /github/runner_temp/git-credentials-9a4e7775-8d2d-4d1d-92ce-2621dc1395c4.config
2026-02-22T18:28:41.7607642Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-9a4e7775-8d2d-4d1d-92ce-2621dc1395c4.config
2026-02-22T18:28:41.7634046Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-22T18:28:41.7654751Z /github/runner_temp/git-credentials-9a4e7775-8d2d-4d1d-92ce-2621dc1395c4.config
2026-02-22T18:28:41.7662086Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-9a4e7775-8d2d-4d1d-92ce-2621dc1395c4.config
2026-02-22T18:28:41.7690406Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-22T18:28:41.7899666Z Removing credentials config '/__w/_temp/git-credentials-9a4e7775-8d2d-4d1d-92ce-2621dc1395c4.config'
2026-02-22T18:29:00.2754044Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-22T18:29:00.2754748Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-22T18:29:00.2755059Z ##[endgroup]
2026-02-22T18:29:00.6851589Z Cleaning up orphan processes
