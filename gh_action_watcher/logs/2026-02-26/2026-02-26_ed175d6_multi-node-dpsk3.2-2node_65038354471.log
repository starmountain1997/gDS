# Run ID: 22451755861
# Commit: ed175d6d92ada7fbb344da7f99d000f644a8cd23
# Job: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
# Date: 2026-02-26
============================================================

ï»¿2026-02-26T18:47:42.3714641Z Current runner version: '2.330.0'
2026-02-26T18:47:42.3718891Z Runner name: 'linux-aarch64-a3-0-n4cwm-runner-2b4j6'
2026-02-26T18:47:42.3719646Z Runner group name: 'Default'
2026-02-26T18:47:42.3720394Z Machine name: 'linux-aarch64-a3-0-n4cwm-runner-2b4j6'
2026-02-26T18:47:42.3723997Z ##[group]GITHUB_TOKEN Permissions
2026-02-26T18:47:42.3725986Z Actions: write
2026-02-26T18:47:42.3726415Z ArtifactMetadata: write
2026-02-26T18:47:42.3726821Z Attestations: write
2026-02-26T18:47:42.3727230Z Checks: write
2026-02-26T18:47:42.3727646Z Contents: write
2026-02-26T18:47:42.3728014Z Deployments: write
2026-02-26T18:47:42.3728478Z Discussions: write
2026-02-26T18:47:42.3728821Z Issues: write
2026-02-26T18:47:42.3729221Z Metadata: read
2026-02-26T18:47:42.3729592Z Models: read
2026-02-26T18:47:42.3729924Z Packages: write
2026-02-26T18:47:42.3730318Z Pages: write
2026-02-26T18:47:42.3730696Z PullRequests: write
2026-02-26T18:47:42.3731056Z RepositoryProjects: write
2026-02-26T18:47:42.3731740Z SecurityEvents: write
2026-02-26T18:47:42.3732355Z Statuses: write
2026-02-26T18:47:42.3732760Z ##[endgroup]
2026-02-26T18:47:42.3734549Z Secret source: Actions
2026-02-26T18:47:42.3735049Z Prepare workflow directory
2026-02-26T18:47:42.4314031Z Prepare all required actions
2026-02-26T18:47:42.4345918Z Getting action download info
2026-02-26T18:47:43.4929125Z Download action repository 'actions/checkout@v6' (SHA:de0fac2e4500dabe0009e67214ff5f5447ce83dd)
2026-02-26T18:47:49.2813061Z Download action repository 'actions/upload-artifact@v6' (SHA:b7c566a772e6b6bfb58ed0dc250532a479d7789f)
2026-02-26T18:47:57.2542709Z Uses: vllm-project/vllm-ascend/.github/workflows/_e2e_nightly_multi_node.yaml@refs/heads/main (ed175d6d92ada7fbb344da7f99d000f644a8cd23)
2026-02-26T18:47:57.2546066Z ##[group] Inputs
2026-02-26T18:47:57.2546415Z   soc_version: a3
2026-02-26T18:47:57.2546702Z   runner: linux-aarch64-a3-0
2026-02-26T18:47:57.2547109Z   image: swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3
2026-02-26T18:47:57.2547647Z   config_file_path: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-26T18:47:57.2548072Z   replicas: 1
2026-02-26T18:47:57.2548263Z   size: 2
2026-02-26T18:47:57.2548498Z   vllm_version: v0.15.0
2026-02-26T18:47:57.2548869Z   vllm_ascend_remote_url: https://github.com/vllm-project/vllm-ascend.git
2026-02-26T18:47:57.2549213Z   vllm_ascend_ref: main
2026-02-26T18:47:57.2549464Z ##[endgroup]
2026-02-26T18:47:57.2549980Z Complete job name: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-26T18:47:57.3144298Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-26T18:47:57.3146856Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-26T18:47:57.3147308Z ##[endgroup]
2026-02-26T18:48:12.8395424Z (node:74) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-26T18:48:12.8396218Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-26T18:48:14.6667929Z ##[group]Run # Decode and save kubeconfig
2026-02-26T18:48:14.6668364Z [36;1m# Decode and save kubeconfig[0m
2026-02-26T18:48:14.6700523Z [36;1mecho "***" | base64 -d > "$KUBECONFIG"[0m
2026-02-26T18:48:14.6701093Z shell: bash -el {0}
2026-02-26T18:48:14.6701358Z ##[endgroup]
2026-02-26T18:48:14.6826923Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-26T18:48:14.6827894Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-26T18:48:14.6828217Z ##[endgroup]
2026-02-26T18:48:15.0388988Z (node:405) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-26T18:48:15.0389846Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-26T18:48:15.9774118Z ##[group]Run actions/checkout@v6
2026-02-26T18:48:15.9774483Z with:
2026-02-26T18:48:15.9774787Z   repository: vllm-project/vllm-ascend
2026-02-26T18:48:15.9775515Z   token: ***
2026-02-26T18:48:15.9775744Z   ssh-strict: true
2026-02-26T18:48:15.9775960Z   ssh-user: git
2026-02-26T18:48:15.9776192Z   persist-credentials: true
2026-02-26T18:48:15.9776433Z   clean: true
2026-02-26T18:48:15.9776654Z   sparse-checkout-cone-mode: true
2026-02-26T18:48:15.9776927Z   fetch-depth: 1
2026-02-26T18:48:15.9777159Z   fetch-tags: false
2026-02-26T18:48:15.9777376Z   show-progress: true
2026-02-26T18:48:15.9777606Z   lfs: false
2026-02-26T18:48:15.9777779Z   submodules: false
2026-02-26T18:48:15.9778069Z   set-safe-directory: true
2026-02-26T18:48:15.9778313Z ##[endgroup]
2026-02-26T18:48:15.9819193Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-26T18:48:15.9819994Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-26T18:48:15.9820303Z ##[endgroup]
2026-02-26T18:48:16.3603938Z (node:435) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-26T18:48:16.3604805Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-26T18:48:16.9206545Z Syncing repository: vllm-project/vllm-ascend
2026-02-26T18:48:16.9207756Z ##[group]Getting Git version info
2026-02-26T18:48:16.9208109Z Working directory is '/__w/vllm-ascend/vllm-ascend'
2026-02-26T18:48:16.9208615Z [command]/usr/bin/git version
2026-02-26T18:48:16.9208885Z git version 2.34.1
2026-02-26T18:48:16.9210327Z ##[endgroup]
2026-02-26T18:48:16.9213733Z Copying '/root/.gitconfig' to '/__w/_temp/56e4cb2f-dcfd-4f86-b461-0e6499b0cfa3/.gitconfig'
2026-02-26T18:48:16.9220258Z Temporarily overriding HOME='/__w/_temp/56e4cb2f-dcfd-4f86-b461-0e6499b0cfa3' before making global git config changes
2026-02-26T18:48:16.9221066Z Adding repository directory to the temporary git global config as a safe directory
2026-02-26T18:48:16.9223843Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-26T18:48:16.9252867Z Deleting the contents of '/__w/vllm-ascend/vllm-ascend'
2026-02-26T18:48:16.9255863Z ##[group]Initializing the repository
2026-02-26T18:48:16.9259381Z [command]/usr/bin/git init /__w/vllm-ascend/vllm-ascend
2026-02-26T18:48:16.9383628Z hint: Using 'master' as the name for the initial branch. This default branch name
2026-02-26T18:48:16.9384150Z hint: is subject to change. To configure the initial branch name to use in all
2026-02-26T18:48:16.9384610Z hint: of your new repositories, which will suppress this warning, call:
2026-02-26T18:48:16.9385021Z hint: 
2026-02-26T18:48:16.9385343Z hint: 	git config --global init.defaultBranch <name>
2026-02-26T18:48:16.9385642Z hint: 
2026-02-26T18:48:16.9385929Z hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and
2026-02-26T18:48:16.9386345Z hint: 'development'. The just-created branch can be renamed via this command:
2026-02-26T18:48:16.9386703Z hint: 
2026-02-26T18:48:16.9386928Z hint: 	git branch -m <name>
2026-02-26T18:48:16.9390782Z Initialized empty Git repository in /__w/vllm-ascend/vllm-ascend/.git/
2026-02-26T18:48:16.9399070Z [command]/usr/bin/git remote add origin https://github.com/vllm-project/vllm-ascend
2026-02-26T18:48:16.9445151Z ##[endgroup]
2026-02-26T18:48:16.9445568Z ##[group]Disabling automatic garbage collection
2026-02-26T18:48:16.9448580Z [command]/usr/bin/git config --local gc.auto 0
2026-02-26T18:48:16.9474634Z ##[endgroup]
2026-02-26T18:48:16.9475100Z ##[group]Setting up auth
2026-02-26T18:48:16.9475403Z Removing SSH command configuration
2026-02-26T18:48:16.9478544Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-26T18:48:16.9504490Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-26T18:48:16.9700773Z Removing HTTP extra header
2026-02-26T18:48:16.9701452Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-26T18:48:16.9717497Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-26T18:48:16.9898791Z Removing includeIf entries pointing to credentials config files
2026-02-26T18:48:16.9902744Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-26T18:48:16.9928228Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-26T18:48:17.0113131Z [command]/usr/bin/git config --file /__w/_temp/git-credentials-ec556a8e-7e51-48a1-8ee7-65884d12efa2.config http.https://github.com/.extraheader AUTHORIZATION: basic ***
2026-02-26T18:48:17.0150685Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-ec556a8e-7e51-48a1-8ee7-65884d12efa2.config
2026-02-26T18:48:17.0180436Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-ec556a8e-7e51-48a1-8ee7-65884d12efa2.config
2026-02-26T18:48:17.0207326Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-ec556a8e-7e51-48a1-8ee7-65884d12efa2.config
2026-02-26T18:48:17.0232163Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-ec556a8e-7e51-48a1-8ee7-65884d12efa2.config
2026-02-26T18:48:17.0258501Z ##[endgroup]
2026-02-26T18:48:17.0258956Z ##[group]Fetching the repository
2026-02-26T18:48:17.0266519Z [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --no-recurse-submodules --depth=1 origin +ed175d6d92ada7fbb344da7f99d000f644a8cd23:refs/remotes/origin/main
2026-02-26T18:48:18.7820386Z From https://gh-proxy.test.osinfra.cn/https://github.com/vllm-project/vllm-ascend
2026-02-26T18:48:18.7821083Z  * [new ref]         ed175d6d92ada7fbb344da7f99d000f644a8cd23 -> origin/main
2026-02-26T18:48:18.7838914Z [command]/usr/bin/git branch --list --remote origin/main
2026-02-26T18:48:18.7863073Z   origin/main
2026-02-26T18:48:18.7870258Z [command]/usr/bin/git rev-parse refs/remotes/origin/main
2026-02-26T18:48:18.7888682Z ed175d6d92ada7fbb344da7f99d000f644a8cd23
2026-02-26T18:48:18.7892748Z ##[endgroup]
2026-02-26T18:48:18.7893144Z ##[group]Determining the checkout info
2026-02-26T18:48:18.7894504Z ##[endgroup]
2026-02-26T18:48:18.7897831Z [command]/usr/bin/git sparse-checkout disable
2026-02-26T18:48:18.7938157Z [command]/usr/bin/git config --local --unset-all extensions.worktreeConfig
2026-02-26T18:48:18.7960906Z ##[group]Checking out the ref
2026-02-26T18:48:18.7964641Z [command]/usr/bin/git checkout --progress --force -B main refs/remotes/origin/main
2026-02-26T18:48:18.8846331Z Switched to a new branch 'main'
2026-02-26T18:48:18.8846899Z Branch 'main' set up to track remote branch 'main' from 'origin'.
2026-02-26T18:48:18.8855615Z ##[endgroup]
2026-02-26T18:48:18.8906993Z [command]/usr/bin/git log -1 --format=%H
2026-02-26T18:48:18.8927861Z ed175d6d92ada7fbb344da7f99d000f644a8cd23
2026-02-26T18:48:19.3083970Z ##[group]Run # prepare for lws entrypoint scripts
2026-02-26T18:48:19.3084384Z [36;1m# prepare for lws entrypoint scripts[0m
2026-02-26T18:48:19.3084772Z [36;1minstall -D tests/e2e/nightly/multi_node/scripts/run.sh /root/.cache/tests/run.sh[0m
2026-02-26T18:48:19.3085284Z shell: bash -el {0}
2026-02-26T18:48:19.3085527Z ##[endgroup]
2026-02-26T18:48:19.3196099Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-26T18:48:19.3197026Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-26T18:48:19.3197334Z ##[endgroup]
2026-02-26T18:48:19.6777152Z (node:477) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-26T18:48:19.6778011Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-26T18:48:20.6457237Z ##[group]Run set -euo pipefail
2026-02-26T18:48:20.6457506Z [36;1mset -euo pipefail[0m
2026-02-26T18:48:20.6457686Z [36;1m[0m
2026-02-26T18:48:20.6457839Z [36;1mCRD_NAME="${CRD_NAME:-vllm}"[0m
2026-02-26T18:48:20.6458041Z [36;1mTIMEOUT=${TIMEOUT:-120}[0m
2026-02-26T18:48:20.6458236Z [36;1mSLEEP_INTERVAL=2[0m
2026-02-26T18:48:20.6458407Z [36;1m[0m
2026-02-26T18:48:20.6458643Z [36;1mecho "Deleting leaderworkerset [$CRD_NAME] in namespace [$NAMESPACE]..."[0m
2026-02-26T18:48:20.6459057Z [36;1mkubectl delete leaderworkerset "$CRD_NAME" -n "$NAMESPACE" --ignore-not-found[0m
2026-02-26T18:48:20.6459356Z [36;1m[0m
2026-02-26T18:48:20.6459569Z [36;1mecho "Waiting for all pods starting with 'vllm' to be deleted..."[0m
2026-02-26T18:48:20.6459846Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-26T18:48:20.6460020Z [36;1m[0m
2026-02-26T18:48:20.6460164Z [36;1mwhile true; do[0m
2026-02-26T18:48:20.6460340Z [36;1m  NOW=$(date +%s)[0m
2026-02-26T18:48:20.6460583Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-26T18:48:20.6460771Z [36;1m[0m
2026-02-26T18:48:20.6460940Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-26T18:48:20.6461235Z [36;1m    echo "Timeout reached ($TIMEOUT seconds), some pods still exist:"[0m
2026-02-26T18:48:20.6461616Z [36;1m    kubectl get pods -n "$NAMESPACE" | grep '^vllm' || true[0m
2026-02-26T18:48:20.6461864Z [36;1m    exit 1[0m
2026-02-26T18:48:20.6462282Z [36;1m  fi[0m
2026-02-26T18:48:20.6462446Z [36;1m[0m
2026-02-26T18:48:20.6462816Z [36;1m  PODS_EXIST=$(kubectl get pods -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null | tr ' ' '\n' | grep '^vllm' || true)[0m
2026-02-26T18:48:20.6463267Z [36;1m[0m
2026-02-26T18:48:20.6463430Z [36;1m  if [[ -z "$PODS_EXIST" ]]; then[0m
2026-02-26T18:48:20.6463650Z [36;1m    echo "All vllm pods deleted."[0m
2026-02-26T18:48:20.6463837Z [36;1m    break[0m
2026-02-26T18:48:20.6463988Z [36;1m  else[0m
2026-02-26T18:48:20.6464203Z [36;1m    echo "Waiting for pods to be deleted: $PODS_EXIST"[0m
2026-02-26T18:48:20.6464446Z [36;1m    sleep $SLEEP_INTERVAL[0m
2026-02-26T18:48:20.6464731Z [36;1m  fi[0m
2026-02-26T18:48:20.6464873Z [36;1mdone[0m
2026-02-26T18:48:20.6465196Z shell: bash -el {0}
2026-02-26T18:48:20.6465360Z ##[endgroup]
2026-02-26T18:48:20.6628757Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-26T18:48:20.6629385Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-26T18:48:20.6629606Z ##[endgroup]
2026-02-26T18:48:21.0434332Z (node:531) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-26T18:48:21.3725770Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-26T18:48:21.5593897Z Deleting leaderworkerset [vllm] in namespace [vllm-project]...
2026-02-26T18:48:21.6386203Z Waiting for all pods starting with 'vllm' to be deleted...
2026-02-26T18:48:21.7025036Z All vllm pods deleted.
2026-02-26T18:48:22.1309885Z ##[group]Run set -e
2026-02-26T18:48:22.1310113Z [36;1mset -e[0m
2026-02-26T18:48:22.1310266Z [36;1m[0m
2026-02-26T18:48:22.1310397Z [36;1msize="2"[0m
2026-02-26T18:48:22.1310544Z [36;1mreplicas="1"[0m
2026-02-26T18:48:22.1310868Z [36;1mimage="swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3"[0m
2026-02-26T18:48:22.1311281Z [36;1mconfig_file_path="DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-26T18:48:22.1311591Z [36;1mfail_tag=FAIL_TAG_"DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-26T18:48:22.1311869Z [36;1mecho "FAIL_TAG=${fail_tag}" >> "$GITHUB_ENV"[0m
2026-02-26T18:48:22.1312214Z [36;1m[0m
2026-02-26T18:48:22.1312511Z [36;1mrequired_params=("size" "replicas" "image" "config_file_path")[0m
2026-02-26T18:48:22.1312806Z [36;1mfor param in "${required_params[@]}"; do[0m
2026-02-26T18:48:22.1313037Z [36;1m  if [ -z "${!param}" ]; then[0m
2026-02-26T18:48:22.1313477Z [36;1m    echo "Error: Parameter '$param' is required but empty"[0m
2026-02-26T18:48:22.1313719Z [36;1m    exit 1[0m
2026-02-26T18:48:22.1313864Z [36;1m  fi[0m
2026-02-26T18:48:22.1313994Z [36;1mdone[0m
2026-02-26T18:48:22.1314132Z [36;1m[0m
2026-02-26T18:48:22.1314277Z [36;1mif [ "a3" = "a3" ]; then[0m
2026-02-26T18:48:22.1314454Z [36;1m  npu_per_node=16[0m
2026-02-26T18:48:22.1314719Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws.yaml.jinja2"[0m
2026-02-26T18:48:22.1314995Z [36;1melse[0m
2026-02-26T18:48:22.1315135Z [36;1m  npu_per_node=8[0m
2026-02-26T18:48:22.1315397Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws-a2.yaml.jinja2"[0m
2026-02-26T18:48:22.1315672Z [36;1mfi[0m
2026-02-26T18:48:22.1315808Z [36;1m[0m
2026-02-26T18:48:22.1315955Z [36;1mjinja2 $TEMPLATE_FILE \[0m
2026-02-26T18:48:22.1316138Z [36;1m  -D size="$size" \[0m
2026-02-26T18:48:22.1316323Z [36;1m  -D replicas="$replicas" \[0m
2026-02-26T18:48:22.1316526Z [36;1m  -D image="$image" \[0m
2026-02-26T18:48:22.1316776Z [36;1m  -D config_file_path="$config_file_path" \[0m
2026-02-26T18:48:22.1317015Z [36;1m  -D npu_per_node="$npu_per_node" \[0m
2026-02-26T18:48:22.1317226Z [36;1m  -D fail_tag="$fail_tag" \[0m
2026-02-26T18:48:22.1317411Z [36;1m  --outfile lws.yaml[0m
2026-02-26T18:48:22.1317582Z [36;1m[0m
2026-02-26T18:48:22.1317724Z [36;1mkubectl apply -f ./lws.yaml[0m
2026-02-26T18:48:22.1318040Z shell: bash -el {0}
2026-02-26T18:48:22.1318203Z ##[endgroup]
2026-02-26T18:48:22.1433119Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-26T18:48:22.1433920Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-26T18:48:22.1434235Z ##[endgroup]
2026-02-26T18:48:22.4947805Z (node:597) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-26T18:48:22.4948481Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-26T18:48:23.4330384Z leaderworkerset.leaderworkerset.x-k8s.io/vllm created
2026-02-26T18:48:23.5282251Z service/vllm-leader created
2026-02-26T18:48:24.1141938Z ##[group]Run POD_PREFIX="${POD_PREFIX:-vllm-0}"
2026-02-26T18:48:24.1142457Z [36;1mPOD_PREFIX="${POD_PREFIX:-vllm-0}"[0m
2026-02-26T18:48:24.1142730Z [36;1mSIZE="2"[0m
2026-02-26T18:48:24.1143147Z [36;1mTIMEOUT=1200  # default timeout 20 minutes[0m
2026-02-26T18:48:24.1143414Z [36;1m[0m
2026-02-26T18:48:24.1143778Z [36;1mecho "Waiting for Pods in namespace [$NAMESPACE] to become Running and Ready (timeout ${TIMEOUT}s)..."[0m
2026-02-26T18:48:24.1144226Z [36;1m[0m
2026-02-26T18:48:24.1144414Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-26T18:48:24.1144647Z [36;1m[0m
2026-02-26T18:48:24.1144853Z [36;1mwhile true; do[0m
2026-02-26T18:48:24.1145095Z [36;1m  NOW=$(date +%s)[0m
2026-02-26T18:48:24.1145347Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-26T18:48:24.1145640Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-26T18:48:24.1145960Z [36;1m    echo "Timeout reached after ${ELAPSED}s"[0m
2026-02-26T18:48:24.1146329Z [36;1m    echo "Dumping pod status for debugging:"[0m
2026-02-26T18:48:24.1146731Z [36;1m    kubectl get pods -n "$NAMESPACE"[0m
2026-02-26T18:48:24.1147082Z [36;1m    kubectl describe pod "$LEADER_POD" -n "$NAMESPACE"[0m
2026-02-26T18:48:24.1147355Z [36;1m    exit 1[0m
2026-02-26T18:48:24.1147594Z [36;1m  fi[0m
2026-02-26T18:48:24.1147816Z [36;1m[0m
2026-02-26T18:48:24.1147998Z [36;1m  # 1) check follower pods[0m
2026-02-26T18:48:24.1148284Z [36;1m  ALL_FOLLOWERS_READY=true[0m
2026-02-26T18:48:24.1148561Z [36;1m  for ((i=1; i<SIZE; i++)); do[0m
2026-02-26T18:48:24.1148797Z [36;1m    POD="${POD_PREFIX}-${i}"[0m
2026-02-26T18:48:24.1149249Z [36;1m    PHASE=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-26T18:48:24.1149857Z [36;1m    READY=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-26T18:48:24.1150448Z [36;1m[0m
2026-02-26T18:48:24.1150742Z [36;1m    echo "Follower [$POD] phase=$PHASE ready=$READY"[0m
2026-02-26T18:48:24.1151049Z [36;1m[0m
2026-02-26T18:48:24.1151270Z [36;1m    if [[ "$PHASE" != "Running" || "$READY" != "true" ]]; then[0m
2026-02-26T18:48:24.1151684Z [36;1m      echo "Follower [$POD] not Ready yet..."[0m
2026-02-26T18:48:24.1152063Z [36;1m      ALL_FOLLOWERS_READY=false[0m
2026-02-26T18:48:24.1152311Z [36;1m      break[0m
2026-02-26T18:48:24.1152546Z [36;1m    fi[0m
2026-02-26T18:48:24.1152762Z [36;1m  done[0m
2026-02-26T18:48:24.1152920Z [36;1m[0m
2026-02-26T18:48:24.1153124Z [36;1m  # 2) check leader pod[0m
2026-02-26T18:48:24.1153507Z [36;1m  LEADER_PHASE=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-26T18:48:24.1154087Z [36;1m  LEADER_READY=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-26T18:48:24.1154465Z [36;1m[0m
2026-02-26T18:48:24.1154686Z [36;1m  echo "Leader [$LEADER_POD] phase=$LEADER_PHASE ready=$LEADER_READY"[0m
2026-02-26T18:48:24.1154949Z [36;1m[0m
2026-02-26T18:48:24.1155151Z [36;1m  if [[ "$LEADER_PHASE" != "Running" || "$LEADER_READY" != "true" ]]; then[0m
2026-02-26T18:48:24.1155428Z [36;1m    echo "Leader not Ready yet..."[0m
2026-02-26T18:48:24.1155635Z [36;1m    ALL_FOLLOWERS_READY=false[0m
2026-02-26T18:48:24.1155808Z [36;1m  fi[0m
2026-02-26T18:48:24.1155944Z [36;1m[0m
2026-02-26T18:48:24.1156105Z [36;1m  if [[ "$ALL_FOLLOWERS_READY" == "true" ]]; then[0m
2026-02-26T18:48:24.1156422Z [36;1m    echo "All follower pods and leader pod are Running and Ready â€” continuing."[0m
2026-02-26T18:48:24.1156707Z [36;1m    break[0m
2026-02-26T18:48:24.1156845Z [36;1m  fi[0m
2026-02-26T18:48:24.1156980Z [36;1m[0m
2026-02-26T18:48:24.1157108Z [36;1m  sleep 2[0m
2026-02-26T18:48:24.1157246Z [36;1mdone[0m
2026-02-26T18:48:24.1157540Z shell: bash -el {0}
2026-02-26T18:48:24.1157688Z env:
2026-02-26T18:48:24.1158014Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-26T18:48:24.1158252Z ##[endgroup]
2026-02-26T18:48:24.1249781Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-26T18:48:24.1250449Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-26T18:48:24.1250657Z ##[endgroup]
2026-02-26T18:48:24.4806563Z (node:675) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-26T18:48:24.4807287Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-26T18:48:25.0393627Z Waiting for Pods in namespace [vllm-project] to become Running and Ready (timeout 1200s)...
2026-02-26T18:48:25.1581592Z Follower [vllm-0-1] phase=Pending ready=
2026-02-26T18:48:25.1581820Z Follower [vllm-0-1] not Ready yet...
2026-02-26T18:48:25.2788286Z Leader [vllm-0] phase=Pending ready=
2026-02-26T18:48:25.2788593Z Leader not Ready yet...
2026-02-26T18:48:27.4258345Z Follower [vllm-0-1] phase=Pending ready=
2026-02-26T18:48:27.4258692Z Follower [vllm-0-1] not Ready yet...
2026-02-26T18:48:27.5925023Z Leader [vllm-0] phase=Pending ready=
2026-02-26T18:48:27.5925350Z Leader not Ready yet...
2026-02-26T18:48:29.7021930Z Follower [vllm-0-1] phase=Pending ready=
2026-02-26T18:48:29.7022329Z Follower [vllm-0-1] not Ready yet...
2026-02-26T18:48:29.8108884Z Leader [vllm-0] phase=Pending ready=
2026-02-26T18:48:29.8109099Z Leader not Ready yet...
2026-02-26T18:48:31.9223571Z Follower [vllm-0-1] phase=Pending ready=
2026-02-26T18:48:31.9223868Z Follower [vllm-0-1] not Ready yet...
2026-02-26T18:48:32.0387271Z Leader [vllm-0] phase=Pending ready=
2026-02-26T18:48:32.0387579Z Leader not Ready yet...
2026-02-26T18:48:34.1503852Z Follower [vllm-0-1] phase=Pending ready=
2026-02-26T18:48:34.1504162Z Follower [vllm-0-1] not Ready yet...
2026-02-26T18:48:34.2645115Z Leader [vllm-0] phase=Pending ready=
2026-02-26T18:48:34.2645852Z Leader not Ready yet...
2026-02-26T18:48:36.4148316Z Follower [vllm-0-1] phase=Pending ready=
2026-02-26T18:48:36.4148633Z Follower [vllm-0-1] not Ready yet...
2026-02-26T18:48:36.5294098Z Leader [vllm-0] phase=Pending ready=false
2026-02-26T18:48:36.5294374Z Leader not Ready yet...
2026-02-26T18:48:38.6848125Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-26T18:48:38.6848414Z Follower [vllm-0-1] not Ready yet...
2026-02-26T18:48:38.7962564Z Leader [vllm-0] phase=Running ready=true
2026-02-26T18:48:40.9119302Z Follower [vllm-0-1] phase=Running ready=true
2026-02-26T18:48:41.0283702Z Leader [vllm-0] phase=Running ready=true
2026-02-26T18:48:41.0284335Z All follower pods and leader pod are Running and Ready â€” continuing.
2026-02-26T18:48:41.4755049Z ##[group]Run set -euo pipefail
2026-02-26T18:48:41.4755416Z [36;1mset -euo pipefail[0m
2026-02-26T18:48:41.4755627Z [36;1m[0m
2026-02-26T18:48:41.4755814Z [36;1msize="2"[0m
2026-02-26T18:48:41.4755963Z [36;1mpids=()[0m
2026-02-26T18:48:41.4756128Z [36;1m[0m
2026-02-26T18:48:41.4756269Z [36;1mcleanup() {[0m
2026-02-26T18:48:41.4756466Z [36;1m  echo "Cleaning up background log streams..."[0m
2026-02-26T18:48:41.4756713Z [36;1m  for pid in "${pids[@]}"; do[0m
2026-02-26T18:48:41.4756922Z [36;1m    kill "$pid" 2>/dev/null || true[0m
2026-02-26T18:48:41.4757115Z [36;1m  done[0m
2026-02-26T18:48:41.4757264Z [36;1m}[0m
2026-02-26T18:48:41.4757404Z [36;1mtrap cleanup EXIT[0m
2026-02-26T18:48:41.4757572Z [36;1m[0m
2026-02-26T18:48:41.4757813Z [36;1mfor i in $(seq 1 $((size - 1))); do[0m
2026-02-26T18:48:41.4758024Z [36;1m  POD="vllm-0-${i}"[0m
2026-02-26T18:48:41.4758199Z [36;1m[0m
2026-02-26T18:48:41.4758433Z [36;1m  echo "==== Collecting logs from worker pod: $POD ===="[0m
2026-02-26T18:48:41.4758737Z [36;1m  kubectl logs -f "$POD" -n "$NAMESPACE" \[0m
2026-02-26T18:48:41.4758974Z [36;1m    > "/tmp/${POD}_logs.txt" 2>&1 &[0m
2026-02-26T18:48:41.4759158Z [36;1m[0m
2026-02-26T18:48:41.4759295Z [36;1m  pids+=($!)[0m
2026-02-26T18:48:41.4759455Z [36;1mdone[0m
2026-02-26T18:48:41.4759590Z [36;1m[0m
2026-02-26T18:48:41.4759787Z [36;1mecho "==== Streaming logs from leader pod: $LEADER_POD ===="[0m
2026-02-26T18:48:41.4760067Z [36;1mecho "Looking for logs containing: $FAIL_TAG"[0m
2026-02-26T18:48:41.4760279Z [36;1m[0m
2026-02-26T18:48:41.4760517Z [36;1mkubectl logs -f "$LEADER_POD" -n "$NAMESPACE" | while IFS= read -r line; do[0m
2026-02-26T18:48:41.4760802Z [36;1m  echo "$line"[0m
2026-02-26T18:48:41.4761000Z [36;1m  if echo "$line" | grep -q "$FAIL_TAG"; then[0m
2026-02-26T18:48:41.4761213Z [36;1m    exit 1[0m
2026-02-26T18:48:41.4761354Z [36;1m  fi[0m
2026-02-26T18:48:41.4761490Z [36;1mdone[0m
2026-02-26T18:48:41.4761840Z shell: bash -el {0}
2026-02-26T18:48:41.4762090Z env:
2026-02-26T18:48:41.4762313Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-26T18:48:41.4762539Z ##[endgroup]
2026-02-26T18:48:41.4870926Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-26T18:48:41.4871649Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-26T18:48:41.4871896Z ##[endgroup]
2026-02-26T18:48:41.8481250Z (node:768) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-26T18:48:41.8481905Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-26T18:48:42.4363373Z ==== Collecting logs from worker pod: vllm-0-1 ====
2026-02-26T18:48:42.4363901Z ==== Streaming logs from leader pod: vllm-0 ====
2026-02-26T18:48:42.4364299Z Looking for logs containing: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-26T18:48:42.5090523Z /usr/local/Ascend/ascend-toolkit/set_env.sh: line 31: CMAKE_PREFIX_PATH: unbound variable
2026-02-26T18:48:42.5101214Z ====> Check NPU info
2026-02-26T18:48:42.5112839Z +------------------------------------------------------------------------------------------------+
2026-02-26T18:48:42.5122817Z | npu-smi 25.5.0                   Version: 25.5.0                                               |
2026-02-26T18:48:42.5132857Z +---------------------------+---------------+----------------------------------------------------+
2026-02-26T18:48:42.5142645Z | NPU   Name                | Health        | Power(W)    Temp(C)           Hugepages-Usage(page)|
2026-02-26T18:48:42.5153499Z | Chip  Phy-ID              | Bus-Id        | AICore(%)   Memory-Usage(MB)  HBM-Usage(MB)        |
2026-02-26T18:48:42.5164002Z +===========================+===============+====================================================+
2026-02-26T18:48:42.5174199Z | 0     Ascend910           | OK            | 165.0       36                0    / 0             |
2026-02-26T18:48:42.5184192Z | 0     0                   | 0000:9D:00.0  | 0           0    / 0          3156 / 65536         |
2026-02-26T18:48:42.5194151Z +------------------------------------------------------------------------------------------------+
2026-02-26T18:48:42.5204794Z | 0     Ascend910           | OK            | -           35                0    / 0             |
2026-02-26T18:48:42.5214110Z | 1     1                   | 0000:9F:00.0  | 0           0    / 0          2892 / 65536         |
2026-02-26T18:48:42.5223094Z +===========================+===============+====================================================+
2026-02-26T18:48:42.5232883Z | 1     Ascend910           | OK            | 163.1       34                0    / 0             |
2026-02-26T18:48:42.5243564Z | 0     2                   | 0000:99:00.0  | 0           0    / 0          3165 / 65536         |
2026-02-26T18:48:42.5255493Z +------------------------------------------------------------------------------------------------+
2026-02-26T18:48:42.5265489Z | 1     Ascend910           | OK            | -           34                0    / 0             |
2026-02-26T18:48:42.5275923Z | 1     3                   | 0000:9B:00.0  | 0           0    / 0          2881 / 65536         |
2026-02-26T18:48:42.5285892Z +===========================+===============+====================================================+
2026-02-26T18:48:42.5297236Z | 2     Ascend910           | OK            | 161.0       34                0    / 0             |
2026-02-26T18:48:42.5306890Z | 0     4                   | 0000:95:00.0  | 0           0    / 0          3150 / 65536         |
2026-02-26T18:48:42.5316347Z +------------------------------------------------------------------------------------------------+
2026-02-26T18:48:42.5326394Z | 2     Ascend910           | OK            | -           34                0    / 0             |
2026-02-26T18:48:42.5336258Z | 1     5                   | 0000:97:00.0  | 0           0    / 0          2893 / 65536         |
2026-02-26T18:48:42.5346232Z +===========================+===============+====================================================+
2026-02-26T18:48:42.5356749Z | 3     Ascend910           | OK            | 164.5       34                0    / 0             |
2026-02-26T18:48:42.5366669Z | 0     6                   | 0000:91:00.0  | 0           0    / 0          3159 / 65536         |
2026-02-26T18:48:42.5376433Z +------------------------------------------------------------------------------------------------+
2026-02-26T18:48:42.5385665Z | 3     Ascend910           | OK            | -           33                0    / 0             |
2026-02-26T18:48:42.5395334Z | 1     7                   | 0000:93:00.0  | 0           0    / 0          2881 / 65536         |
2026-02-26T18:48:42.5405849Z +===========================+===============+====================================================+
2026-02-26T18:48:42.5415600Z | 4     Ascend910           | OK            | 167.4       34                0    / 0             |
2026-02-26T18:48:42.5429718Z | 0     8                   | 0000:8D:00.0  | 0           0    / 0          3151 / 65536         |
2026-02-26T18:48:42.5435587Z +------------------------------------------------------------------------------------------------+
2026-02-26T18:48:42.5445563Z | 4     Ascend910           | OK            | -           34                0    / 0             |
2026-02-26T18:48:42.5455257Z | 1     9                   | 0000:8F:00.0  | 0           0    / 0          2892 / 65536         |
2026-02-26T18:48:42.5464663Z +===========================+===============+====================================================+
2026-02-26T18:48:42.5474308Z | 5     Ascend910           | OK            | 166.3       34                0    / 0             |
2026-02-26T18:48:42.5484270Z | 0     10                  | 0000:89:00.0  | 0           0    / 0          3150 / 65536         |
2026-02-26T18:48:42.5493947Z +------------------------------------------------------------------------------------------------+
2026-02-26T18:48:42.5503529Z | 5     Ascend910           | OK            | -           37                0    / 0             |
2026-02-26T18:48:42.5513024Z | 1     11                  | 0000:8B:00.0  | 0           0    / 0          2892 / 65536         |
2026-02-26T18:48:42.5524276Z +===========================+===============+====================================================+
2026-02-26T18:48:42.5532878Z | 6     Ascend910           | OK            | 162.1       36                0    / 0             |
2026-02-26T18:48:42.5542627Z | 0     12                  | 0000:85:00.0  | 0           0    / 0          3148 / 65536         |
2026-02-26T18:48:42.5552234Z +------------------------------------------------------------------------------------------------+
2026-02-26T18:48:42.5562170Z | 6     Ascend910           | OK            | -           36                0    / 0             |
2026-02-26T18:48:42.5572271Z | 1     13                  | 0000:87:00.0  | 0           0    / 0          2892 / 65536         |
2026-02-26T18:48:42.5581681Z +===========================+===============+====================================================+
2026-02-26T18:48:42.5590788Z | 7     Ascend910           | OK            | 158.3       34                0    / 0             |
2026-02-26T18:48:42.5600967Z | 0     14                  | 0000:81:00.0  | 0           0    / 0          3157 / 65536         |
2026-02-26T18:48:42.5610504Z +------------------------------------------------------------------------------------------------+
2026-02-26T18:48:42.5620508Z | 7     Ascend910           | OK            | -           35                0    / 0             |
2026-02-26T18:48:42.5630182Z | 1     15                  | 0000:83:00.0  | 0           0    / 0          2880 / 65536         |
2026-02-26T18:48:42.5641151Z +===========================+===============+====================================================+
2026-02-26T18:48:42.5650382Z +---------------------------+---------------+----------------------------------------------------+
2026-02-26T18:48:42.5659817Z | NPU     Chip              | Process id    | Process name             | Process memory(MB)      |
2026-02-26T18:48:42.5668934Z +===========================+===============+====================================================+
2026-02-26T18:48:42.5678780Z | No running processes found in NPU 0                                                            |
2026-02-26T18:48:42.5688610Z +===========================+===============+====================================================+
2026-02-26T18:48:42.5698251Z | No running processes found in NPU 1                                                            |
2026-02-26T18:48:42.5708048Z +===========================+===============+====================================================+
2026-02-26T18:48:42.5720871Z | No running processes found in NPU 2                                                            |
2026-02-26T18:48:42.5731061Z +===========================+===============+====================================================+
2026-02-26T18:48:42.5740931Z | No running processes found in NPU 3                                                            |
2026-02-26T18:48:42.5750478Z +===========================+===============+====================================================+
2026-02-26T18:48:42.5761796Z | No running processes found in NPU 4                                                            |
2026-02-26T18:48:42.5771588Z +===========================+===============+====================================================+
2026-02-26T18:48:42.5780832Z | No running processes found in NPU 5                                                            |
2026-02-26T18:48:42.5790570Z +===========================+===============+====================================================+
2026-02-26T18:48:42.5801673Z | No running processes found in NPU 6                                                            |
2026-02-26T18:48:42.5811435Z +===========================+===============+====================================================+
2026-02-26T18:48:42.5821713Z | No running processes found in NPU 7                                                            |
2026-02-26T18:48:42.5831689Z +===========================+===============+====================================================+
2026-02-26T18:48:42.5842850Z package_name=Ascend-cann-toolkit
2026-02-26T18:48:42.5851969Z version=8.5.0
2026-02-26T18:48:42.5862156Z innerversion=V100R001C25SPC001B232
2026-02-26T18:48:42.5872526Z compatible_version=[V100R001C15],[V100R001C18],[V100R001C19],[V100R001C20],[V100R001C21],[V100R001C23]
2026-02-26T18:48:42.5882147Z arch=aarch64
2026-02-26T18:48:42.5895506Z os=linux
2026-02-26T18:48:42.5906131Z path=/usr/local/Ascend/cann-8.5.0
2026-02-26T18:48:42.5916762Z ====> Configure mirrors and git proxy
2026-02-26T18:48:42.5927434Z Writing to /root/.config/pip/pip.conf
2026-02-26T18:48:42.5937637Z Installed vLLM-related Python packages:
2026-02-26T18:48:42.5947709Z ais_bench_benchmark               3.0.20250930                /vllm-workspace/vllm-ascend/benchmark
2026-02-26T18:48:42.5958433Z vllm                              0.15.0+empty                /vllm-workspace/vllm
2026-02-26T18:48:42.5967584Z vllm_ascend                       0.14.0rc2.dev198+g3953dcf78 /vllm-workspace/vllm-ascend
2026-02-26T18:48:42.5977252Z 
2026-02-26T18:48:42.5987508Z ============================
2026-02-26T18:48:42.5996851Z vLLM Git information
2026-02-26T18:48:42.6006854Z ============================
2026-02-26T18:48:42.6017000Z Branch:      HEAD
2026-02-26T18:48:42.6026097Z Commit hash: f176443446f659dbab5315e056e605d8984fd976
2026-02-26T18:48:42.6035789Z Author:      TJian <tunjian.tan@embeddedllm.com>
2026-02-26T18:48:42.6046490Z Date:        2026-01-29 14:45:42 +0800
2026-02-26T18:48:42.6056320Z Message:     [Release] [CI] Optim release pipeline (#33156)
2026-02-26T18:48:42.6065638Z Tags:        v0.15.0
2026-02-26T18:48:42.6076542Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm.git (fetch)
2026-02-26T18:48:42.6086044Z 
2026-02-26T18:48:42.6094803Z 
2026-02-26T18:48:42.6104511Z ============================
2026-02-26T18:48:42.6114000Z vLLM-Ascend Git information
2026-02-26T18:48:42.6123887Z ============================
2026-02-26T18:48:42.6132755Z Branch:      main
2026-02-26T18:48:42.6145785Z Commit hash: 3953dcf784da3471eb99795da67803fb06f40cc9
2026-02-26T18:48:42.6151757Z Author:      Cao Yi <slightwindsec@gmail.com>
2026-02-26T18:48:42.6161559Z Date:        2026-02-26 10:59:25 +0800
2026-02-26T18:48:42.6171949Z Message:     [Feature][Quant] Auto-detect quantization format from model files (#6645)
2026-02-26T18:48:42.6180937Z Tags:        
2026-02-26T18:48:42.6190412Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm-ascend (fetch)
2026-02-26T18:48:42.6200153Z 
2026-02-26T18:48:42.6209389Z ====> Check triton ascend info
2026-02-26T18:48:42.6218786Z Ubuntu clang version 15.0.7
2026-02-26T18:48:42.6228058Z Target: aarch64-unknown-linux-gnu
2026-02-26T18:48:42.6238171Z Thread model: posix
2026-02-26T18:48:42.6247221Z InstalledDir: /usr/bin
2026-02-26T18:48:42.6257172Z Found candidate GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-26T18:48:42.6266248Z Selected GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-26T18:48:42.6276443Z Candidate multilib: .;@m64
2026-02-26T18:48:42.6286025Z Selected multilib: .;@m64
2026-02-26T18:48:42.6295712Z /usr/local/Ascend/cann-8.5.0/tools/bishengir/bin/bishengir-compile
2026-02-26T18:48:42.6304766Z Name: triton-ascend
2026-02-26T18:48:42.6314597Z Version: 3.2.0
2026-02-26T18:48:42.6325440Z Summary: A language and compiler for custom Deep Learning operations on Ascend hardwares
2026-02-26T18:48:42.6335007Z Home-page: https://gitcode.com/Ascend/triton-ascend/
2026-02-26T18:48:42.6344100Z Author: 
2026-02-26T18:48:42.6353885Z Author-email: 
2026-02-26T18:48:42.6363965Z License: 
2026-02-26T18:48:42.6373565Z Location: /usr/local/python3.11.14/lib/python3.11/site-packages
2026-02-26T18:48:42.6382778Z Requires: 
2026-02-26T18:48:42.6392254Z Required-by: vllm_ascend
2026-02-26T18:49:00.5736810Z INFO 02-26 18:49:00 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:49:00.5744962Z INFO 02-26 18:49:00 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:49:00.5754735Z INFO 02-26 18:49:00 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:49:00.6241783Z INFO 02-26 18:49:00 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:49:07.1786250Z ============================= test session starts ==============================
2026-02-26T18:49:07.1796277Z platform linux -- Python 3.11.14, pytest-8.4.2, pluggy-1.6.0 -- /usr/local/python3.11.14/bin/python3
2026-02-26T18:49:07.1805104Z cachedir: .pytest_cache
2026-02-26T18:49:07.1814982Z rootdir: /vllm-workspace/vllm-ascend
2026-02-26T18:49:07.1824005Z configfile: pyproject.toml
2026-02-26T18:49:07.1833640Z plugins: cov-7.0.0, mock-3.15.1, asyncio-1.3.0, anyio-4.12.1
2026-02-26T18:49:07.1845239Z asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
2026-02-26T18:49:08.0286954Z collecting ... collected 1 item
2026-02-26T18:49:08.0293427Z 
2026-02-26T18:49:08.0310741Z [2026-02-26 18:49:08] INFO multi_node_config.py:294: Loading config yaml: tests/e2e/nightly/multi_node/config/DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-26T18:49:08.0364224Z [2026-02-26 18:49:08] INFO multi_node_config.py:351: Resolving cluster IPs via DNS...
2026-02-26T18:49:08.0419660Z [2026-02-26 18:49:08] INFO multi_node_config.py:212: Node 0 envs: {'HCCL_OP_EXPANSION_MODE': 'AIV', 'VLLM_USE_MODELSCOPE': 'True', 'HCCL_BUFFSIZE': '1024', 'SERVER_PORT': '8080', 'OMP_PROC_BIND': 'False', 'OMP_NUM_THREADS': '1', 'PYTORCH_NPU_ALLOC_CONF': 'expandable_segments:True', 'VLLM_ASCEND_ENABLE_MLAPO': '1', 'VLLM_ASCEND_ENABLE_FLASHCOMM1': '1', 'ASCEND_A3_EBA_ENABLE': '1', 'HCCL_IF_IP': '10.0.0.222', 'HCCL_SOCKET_IFNAME': 'eth0', 'GLOO_SOCKET_IFNAME': 'eth0', 'TP_SOCKET_IFNAME': 'eth0', 'LOCAL_IP': '10.0.0.222', 'NIC_NAME': 'eth0', 'MASTER_IP': '10.0.0.222'}
2026-02-26T18:49:08.0543287Z [2026-02-26 18:49:08] INFO multi_node_config.py:137: Not launching proxy on non-master node
2026-02-26T18:49:08.0554795Z [2026-02-26 18:49:08] INFO conftest.py:241: Starting server with command: vllm serve vllm-ascend/DeepSeek-V3.2-W8A8 --host 0.0.0.0 --port 8080 --data-parallel-size 4 --data-parallel-size-local 2 --data-parallel-address 10.0.0.222 --data-parallel-rpc-port 13399 --tensor-parallel-size 8 --quantization ascend --seed 1024 --enable-expert-parallel --max-num-seqs 16 --max-model-len 68000 --max-num-batched-tokens 4096 --no-enable-prefix-caching --gpu-memory-utilization 0.85 --trust-remote-code --speculative-config {"num_speculative_tokens": 3, "method":"deepseek_mtp"} --compilation-config {"cudagraph_capture_sizes": [8, 16, 24, 32, 40, 48], "cudagraph_mode": "FULL_DECODE_ONLY"} --additional-config {"layer_sharding": ["q_b_proj", "o_proj"]} --tokenizer-mode deepseek_v32 --reasoning-parser deepseek_v3
2026-02-26T18:49:12.4869244Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node INFO 02-26 18:49:12 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:49:12.4877164Z INFO 02-26 18:49:12 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:49:12.4888131Z INFO 02-26 18:49:12 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:49:12.4935092Z INFO 02-26 18:49:12 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:49:18.9357838Z 2026-02-26 18:49:18,933 - 138 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:49:18.9678792Z INFO 02-26 18:49:18 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:49:19.1105320Z INFO 02-26 18:49:19 [serve.py:100] Defaulting api_server_count to data_parallel_size (4).
2026-02-26T18:49:19.1128452Z INFO 02-26 18:49:19 [utils.py:325] 
2026-02-26T18:49:19.1140454Z INFO 02-26 18:49:19 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
2026-02-26T18:49:19.1151711Z INFO 02-26 18:49:19 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
2026-02-26T18:49:19.1162505Z INFO 02-26 18:49:19 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   vllm-ascend/DeepSeek-V3.2-W8A8
2026-02-26T18:49:19.1173168Z INFO 02-26 18:49:19 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
2026-02-26T18:49:19.1183160Z INFO 02-26 18:49:19 [utils.py:325] 
2026-02-26T18:49:19.1202534Z INFO 02-26 18:49:19 [utils.py:261] non-default args: {'model_tag': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'api_server_count': 4, 'host': '0.0.0.0', 'port': 8080, 'model': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'tokenizer_mode': 'deepseek_v32', 'trust_remote_code': True, 'seed': 1024, 'max_model_len': 68000, 'quantization': 'ascend', 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 8, 'data_parallel_size': 4, 'data_parallel_size_local': 2, 'data_parallel_address': '10.0.0.222', 'data_parallel_rpc_port': 13399, 'enable_expert_parallel': True, 'gpu_memory_utilization': 0.85, 'enable_prefix_caching': False, 'max_num_batched_tokens': 4096, 'max_num_seqs': 16, 'speculative_config': {'num_speculative_tokens': 3, 'method': 'deepseek_mtp'}, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [8, 16, 24, 32, 40, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}, 'additional_config': {'layer_sharding': ['q_b_proj', 'o_proj']}}
2026-02-26T18:49:19.1691230Z 2026-02-26 18:49:19,167 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-26T18:49:19.1750877Z INFO 02-26 18:49:19 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-26T18:49:19.1769794Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:49:19.1813894Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:49:19.1835409Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:49:19.1856881Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T18:49:19.2135490Z INFO 02-26 18:49:19 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-26T18:49:19.2137904Z INFO 02-26 18:49:19 [model.py:1561] Using max model len 68000
2026-02-26T18:49:19.5157087Z WARNING 02-26 18:49:19 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-26T18:49:19.5176028Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:49:19.5185698Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:49:19.5195567Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T18:49:28.5691702Z INFO 02-26 18:49:28 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-26T18:49:28.5711436Z INFO 02-26 18:49:28 [model.py:1561] Using max model len 163840
2026-02-26T18:49:28.5721098Z WARNING 02-26 18:49:28 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-26T18:49:28.5730588Z INFO 02-26 18:49:28 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-26T18:49:28.9126343Z INFO 02-26 18:49:28 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-26T18:49:28.9139277Z INFO 02-26 18:49:28 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-26T18:49:28.9150204Z WARNING 02-26 18:49:28 [platform.py:735] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-26T18:49:28.9160497Z WARNING 02-26 18:49:28 [platform.py:776] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-26T18:49:28.9170396Z INFO 02-26 18:49:28 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:49:28.9180573Z INFO 02-26 18:49:28 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:49:28.9191911Z INFO 02-26 18:49:28 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:49:28.9205043Z INFO 02-26 18:49:28 [platform.py:323] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-26T18:49:28.9216007Z WARNING 02-26 18:49:28 [platform.py:340] [91m
2026-02-26T18:49:28.9226302Z WARNING 02-26 18:49:28 [platform.py:340]             **********************************************************************************
2026-02-26T18:49:28.9237352Z WARNING 02-26 18:49:28 [platform.py:340]             * WARNING: You have enabled the *full graph* feature.
2026-02-26T18:49:28.9247252Z WARNING 02-26 18:49:28 [platform.py:340]             * This is an early experimental stage and may involve various unknown issues.
2026-02-26T18:49:28.9257332Z WARNING 02-26 18:49:28 [platform.py:340]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-26T18:49:28.9269437Z WARNING 02-26 18:49:28 [platform.py:340]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-26T18:49:28.9278710Z WARNING 02-26 18:49:28 [platform.py:340]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-26T18:49:28.9288519Z WARNING 02-26 18:49:28 [platform.py:340]             * batch size for graph capture.
2026-02-26T18:49:28.9298172Z WARNING 02-26 18:49:28 [platform.py:340]             * For more details, please refer to:
2026-02-26T18:49:28.9308997Z WARNING 02-26 18:49:28 [platform.py:340]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-26T18:49:28.9322803Z WARNING 02-26 18:49:28 [platform.py:340]             **********************************************************************************[0m
2026-02-26T18:49:28.9332185Z WARNING 02-26 18:49:28 [platform.py:340]             
2026-02-26T18:49:28.9342328Z INFO 02-26 18:49:28 [platform.py:448] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-26T18:49:28.9352813Z INFO 02-26 18:49:28 [utils.py:851] Started DP Coordinator process (PID: 158)
2026-02-26T18:49:33.7042895Z INFO 02-26 18:49:33 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:49:33.7052942Z INFO 02-26 18:49:33 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:49:33.7063669Z INFO 02-26 18:49:33 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:49:33.7117422Z INFO 02-26 18:49:33 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:49:33.7920722Z INFO 02-26 18:49:33 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:49:33.7929066Z INFO 02-26 18:49:33 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:49:33.7938630Z INFO 02-26 18:49:33 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:49:33.7997846Z INFO 02-26 18:49:33 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:49:43.5176374Z INFO 02-26 18:49:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:49:43.5185413Z INFO 02-26 18:49:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:49:43.5196007Z INFO 02-26 18:49:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:49:43.5244138Z INFO 02-26 18:49:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:49:48.7361352Z INFO 02-26 18:49:48 [utils.py:218] Started 4 API server processes
2026-02-26T18:49:49.1877298Z [0;36m(EngineCore_DP1 pid=180)[0;0m 2026-02-26 18:49:49,185 - 180 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:49:49.1885287Z [0;36m(EngineCore_DP0 pid=161)[0;0m 2026-02-26 18:49:49,185 - 161 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:49:49.1953647Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-26 18:49:49 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:49:49.1969560Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-26 18:49:49 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:49:49.1995206Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-26 18:49:49 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', speculative_config=SpeculativeConfig(method='mtp', model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', num_spec_tokens=3), tokenizer='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', skip_tokenizer_init=False, tokenizer_mode=deepseek_v32, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=68000, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=4, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=bfloat16, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=1024, served_model_name=/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [8, 16, 24, 32, 40, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 48, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
2026-02-26T18:49:54.0845142Z INFO 02-26 18:49:54 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:49:54.0854427Z INFO 02-26 18:49:54 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:49:54.0864952Z INFO 02-26 18:49:54 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:49:54.0878429Z INFO 02-26 18:49:54 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:49:54.0889601Z INFO 02-26 18:49:54 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:49:54.0899572Z INFO 02-26 18:49:54 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:49:54.0922859Z INFO 02-26 18:49:54 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:49:54.0944975Z INFO 02-26 18:49:54 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:49:54.1404915Z INFO 02-26 18:49:54 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:49:54.1413061Z INFO 02-26 18:49:54 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:49:54.1425414Z INFO 02-26 18:49:54 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:49:54.1492888Z INFO 02-26 18:49:54 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:49:54.1638255Z INFO 02-26 18:49:54 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:49:54.1645595Z INFO 02-26 18:49:54 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:49:54.1655730Z INFO 02-26 18:49:54 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:49:54.1717161Z INFO 02-26 18:49:54 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:49:54.2783817Z INFO 02-26 18:49:54 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:49:54.2789881Z INFO 02-26 18:49:54 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:49:54.2799908Z INFO 02-26 18:49:54 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:49:54.2870449Z INFO 02-26 18:49:54 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:49:54.4219154Z INFO 02-26 18:49:54 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:49:54.4227106Z INFO 02-26 18:49:54 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:49:54.4237892Z INFO 02-26 18:49:54 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:49:54.4295657Z INFO 02-26 18:49:54 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:49:59.3979398Z 2026-02-26 18:49:59,395 - 209 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:49:59.4014827Z INFO 02-26 18:49:59 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:49:59.7938095Z 2026-02-26 18:49:59,791 - 208 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:49:59.7971149Z INFO 02-26 18:49:59 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:50:00.5809161Z [0;36m(ApiServer_2 pid=193)[0;0m 2026-02-26 18:50:00,578 - 193 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:50:00.5984102Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 18:50:00 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:50:00.6264332Z [0;36m(ApiServer_2 pid=193)[0;0m 2026-02-26 18:50:00,624 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-26T18:50:00.6323657Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 18:50:00 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-26T18:50:00.7367605Z [0;36m(ApiServer_2 pid=193)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:50:00.7388955Z [0;36m(ApiServer_2 pid=193)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:50:00.7408249Z [0;36m(ApiServer_2 pid=193)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:50:00.7419124Z [0;36m(ApiServer_2 pid=193)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T18:50:00.7533119Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 18:50:00 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-26T18:50:00.7542811Z [0;36m(ApiServer_3 pid=194)[0;0m 2026-02-26 18:50:00,750 - 194 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:50:00.7551933Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 18:50:00 [model.py:1561] Using max model len 68000
2026-02-26T18:50:00.7702778Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 18:50:00 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:50:00.7856441Z [0;36m(ApiServer_3 pid=194)[0;0m 2026-02-26 18:50:00,783 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-26T18:50:00.7915002Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 18:50:00 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-26T18:50:00.8359294Z [0;36m(ApiServer_0 pid=191)[0;0m 2026-02-26 18:50:00,834 - 191 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:50:00.8524084Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 18:50:00 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:50:00.8688930Z [0;36m(ApiServer_0 pid=191)[0;0m 2026-02-26 18:50:00,867 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-26T18:50:00.8747975Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 18:50:00 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-26T18:50:00.9030992Z [0;36m(ApiServer_3 pid=194)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:50:00.9055586Z [0;36m(ApiServer_3 pid=194)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:50:00.9065208Z [0;36m(ApiServer_3 pid=194)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:50:00.9077668Z [0;36m(ApiServer_3 pid=194)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T18:50:00.9150129Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 18:50:00 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-26T18:50:00.9191923Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 18:50:00 [model.py:1561] Using max model len 68000
2026-02-26T18:50:00.9755540Z [0;36m(ApiServer_0 pid=191)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:50:00.9777621Z [0;36m(ApiServer_0 pid=191)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:50:00.9789951Z [0;36m(ApiServer_0 pid=191)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:50:00.9799499Z [0;36m(ApiServer_0 pid=191)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T18:50:00.9856818Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 18:50:00 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-26T18:50:00.9878363Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 18:50:00 [model.py:1561] Using max model len 68000
2026-02-26T18:50:01.0879598Z [0;36m(ApiServer_1 pid=192)[0;0m 2026-02-26 18:50:01,081 - 192 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:50:01.1054108Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 18:50:01 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:50:01.1199974Z [0;36m(ApiServer_1 pid=192)[0;0m 2026-02-26 18:50:01,118 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-26T18:50:01.1264725Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 18:50:01 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-26T18:50:01.2096146Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 18:50:01 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-26T18:50:01.2123448Z [0;36m(ApiServer_2 pid=193)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:50:01.2130815Z [0;36m(ApiServer_2 pid=193)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:50:01.2140973Z [0;36m(ApiServer_2 pid=193)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T18:50:01.2222917Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 18:50:01 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-26T18:50:01.2246322Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 18:50:01 [model.py:1561] Using max model len 163840
2026-02-26T18:50:01.2257178Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 18:50:01 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-26T18:50:01.2266873Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 18:50:01 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-26T18:50:01.2328348Z [0;36m(ApiServer_1 pid=192)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:50:01.2350572Z [0;36m(ApiServer_1 pid=192)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:50:01.2361718Z [0;36m(ApiServer_1 pid=192)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:50:01.2375819Z [0;36m(ApiServer_1 pid=192)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T18:50:01.2435366Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 18:50:01 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-26T18:50:01.2457659Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 18:50:01 [model.py:1561] Using max model len 68000
2026-02-26T18:50:01.3456851Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 18:50:01 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-26T18:50:01.3465778Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 18:50:01 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-26T18:50:01.3482710Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 18:50:01 [platform.py:735] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-26T18:50:01.3496498Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 18:50:01 [platform.py:776] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-26T18:50:01.3505507Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 18:50:01 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:50:01.3517281Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 18:50:01 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:50:01.3529590Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 18:50:01 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:50:01.3539698Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 18:50:01 [platform.py:323] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-26T18:50:01.3549169Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 18:50:01 [platform.py:340] [91m
2026-02-26T18:50:01.3559646Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             **********************************************************************************
2026-02-26T18:50:01.3570619Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * WARNING: You have enabled the *full graph* feature.
2026-02-26T18:50:01.3580345Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * This is an early experimental stage and may involve various unknown issues.
2026-02-26T18:50:01.3590029Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-26T18:50:01.3602096Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-26T18:50:01.3611920Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-26T18:50:01.3621639Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * batch size for graph capture.
2026-02-26T18:50:01.3632217Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * For more details, please refer to:
2026-02-26T18:50:01.3643594Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-26T18:50:01.3653866Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             **********************************************************************************[0m
2026-02-26T18:50:01.3664706Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             
2026-02-26T18:50:01.3673548Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 18:50:01 [platform.py:448] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-26T18:50:01.3951036Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 18:50:01 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-26T18:50:01.3973499Z [0;36m(ApiServer_3 pid=194)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:50:01.3983291Z [0;36m(ApiServer_3 pid=194)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:50:01.3994268Z [0;36m(ApiServer_3 pid=194)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T18:50:01.4044730Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 18:50:01 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-26T18:50:01.4067280Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 18:50:01 [model.py:1561] Using max model len 163840
2026-02-26T18:50:01.4077767Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 18:50:01 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-26T18:50:01.4087302Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 18:50:01 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-26T18:50:01.4110728Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 18:50:01 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-26T18:50:01.4120928Z [0;36m(ApiServer_0 pid=191)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:50:01.4135725Z [0;36m(ApiServer_0 pid=191)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:50:01.4145125Z [0;36m(ApiServer_0 pid=191)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T18:50:01.4202802Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 18:50:01 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-26T18:50:01.4224472Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 18:50:01 [model.py:1561] Using max model len 163840
2026-02-26T18:50:01.4234739Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 18:50:01 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-26T18:50:01.4244660Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 18:50:01 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-26T18:50:01.5246513Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 18:50:01 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-26T18:50:01.5292880Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 18:50:01 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-26T18:50:01.5303116Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 18:50:01 [platform.py:735] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-26T18:50:01.5313036Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 18:50:01 [platform.py:776] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-26T18:50:01.5328001Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 18:50:01 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:50:01.5338060Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 18:50:01 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:50:01.5348637Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 18:50:01 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:50:01.5358742Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 18:50:01 [platform.py:323] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-26T18:50:01.5367388Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 18:50:01 [platform.py:340] [91m
2026-02-26T18:50:01.5391963Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             **********************************************************************************
2026-02-26T18:50:01.5405640Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * WARNING: You have enabled the *full graph* feature.
2026-02-26T18:50:01.5415747Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * This is an early experimental stage and may involve various unknown issues.
2026-02-26T18:50:01.5427806Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-26T18:50:01.5435887Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-26T18:50:01.5446059Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-26T18:50:01.5455686Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * batch size for graph capture.
2026-02-26T18:50:01.5465645Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * For more details, please refer to:
2026-02-26T18:50:01.5476812Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-26T18:50:01.5486972Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             **********************************************************************************[0m
2026-02-26T18:50:01.5496225Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             
2026-02-26T18:50:01.5505776Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 18:50:01 [platform.py:448] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-26T18:50:01.5515898Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 18:50:01 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-26T18:50:01.5526780Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 18:50:01 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-26T18:50:01.5536633Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 18:50:01 [platform.py:735] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-26T18:50:01.5547376Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 18:50:01 [platform.py:776] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-26T18:50:01.5556440Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 18:50:01 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:50:01.5567685Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 18:50:01 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:50:01.5578405Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 18:50:01 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:50:01.5588051Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 18:50:01 [platform.py:323] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-26T18:50:01.5597845Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 18:50:01 [platform.py:340] [91m
2026-02-26T18:50:01.5608045Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             **********************************************************************************
2026-02-26T18:50:01.5618225Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * WARNING: You have enabled the *full graph* feature.
2026-02-26T18:50:01.5629206Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * This is an early experimental stage and may involve various unknown issues.
2026-02-26T18:50:01.5641431Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-26T18:50:01.5653815Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-26T18:50:01.5665414Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-26T18:50:01.5677014Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * batch size for graph capture.
2026-02-26T18:50:01.5688397Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * For more details, please refer to:
2026-02-26T18:50:01.5698734Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-26T18:50:01.5708902Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             **********************************************************************************[0m
2026-02-26T18:50:01.5719301Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             
2026-02-26T18:50:01.5730328Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 18:50:01 [platform.py:448] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-26T18:50:01.6794632Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 18:50:01 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-26T18:50:01.6849676Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T18:50:01.6859800Z   warnings.warn(
2026-02-26T18:50:01.6870957Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T18:50:01.6881068Z   warnings.warn(
2026-02-26T18:50:01.6892449Z [0;36m(ApiServer_1 pid=192)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:50:01.6903256Z [0;36m(ApiServer_1 pid=192)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T18:50:01.6913940Z [0;36m(ApiServer_1 pid=192)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T18:50:01.6924761Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 18:50:01 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-26T18:50:01.6979155Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 18:50:01 [model.py:1561] Using max model len 163840
2026-02-26T18:50:01.6979962Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 18:50:01 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-26T18:50:01.6980776Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 18:50:01 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-26T18:50:01.8145499Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 18:50:01 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-26T18:50:01.8199081Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 18:50:01 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-26T18:50:01.8208684Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 18:50:01 [platform.py:735] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-26T18:50:01.8219410Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 18:50:01 [platform.py:776] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-26T18:50:01.8229136Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 18:50:01 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:50:01.8239006Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 18:50:01 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:50:01.8250047Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 18:50:01 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:50:01.8259279Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 18:50:01 [platform.py:323] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-26T18:50:01.8268454Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 18:50:01 [platform.py:340] [91m
2026-02-26T18:50:01.8279810Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             **********************************************************************************
2026-02-26T18:50:01.8289405Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * WARNING: You have enabled the *full graph* feature.
2026-02-26T18:50:01.8299131Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * This is an early experimental stage and may involve various unknown issues.
2026-02-26T18:50:01.8309000Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-26T18:50:01.8335811Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-26T18:50:01.8345401Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-26T18:50:01.8354753Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * batch size for graph capture.
2026-02-26T18:50:01.8365297Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * For more details, please refer to:
2026-02-26T18:50:01.8374923Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-26T18:50:01.8385025Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             **********************************************************************************[0m
2026-02-26T18:50:01.8394478Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 18:50:01 [platform.py:340]             
2026-02-26T18:50:01.8404963Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 18:50:01 [platform.py:448] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-26T18:50:04.8048729Z INFO 02-26 18:50:04 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:50:04.8084904Z INFO 02-26 18:50:04 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:50:04.8094392Z INFO 02-26 18:50:04 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:50:04.8157898Z INFO 02-26 18:50:04 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:50:05.1360664Z INFO 02-26 18:50:05 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:50:05.1373541Z INFO 02-26 18:50:05 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:50:05.1382245Z INFO 02-26 18:50:05 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:50:05.1392390Z INFO 02-26 18:50:05 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:50:05.1403757Z INFO 02-26 18:50:05 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:50:05.1413803Z INFO 02-26 18:50:05 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:50:05.5378472Z INFO 02-26 18:50:05 [parallel_state.py:1212] world_size=32 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.222:37005 backend=hccl
2026-02-26T18:50:05.9396355Z INFO 02-26 18:50:05 [parallel_state.py:1212] world_size=32 rank=8 local_rank=0 distributed_init_method=tcp://10.0.0.222:37005 backend=hccl
2026-02-26T18:50:06.2509066Z INFO 02-26 18:50:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:50:06.2518556Z INFO 02-26 18:50:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:50:06.2528759Z INFO 02-26 18:50:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:50:06.2587233Z INFO 02-26 18:50:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:50:12.6233045Z 2026-02-26 18:50:12,620 - 258 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:50:12.6242456Z 2026-02-26 18:50:12,620 - 261 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:50:12.6423896Z INFO 02-26 18:50:12 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:50:12.6432986Z INFO 02-26 18:50:12 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:50:14.9508232Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T18:50:14.9514911Z   warnings.warn(
2026-02-26T18:50:14.9527548Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T18:50:14.9536830Z   warnings.warn(
2026-02-26T18:50:17.0700714Z INFO 02-26 18:50:17 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:50:17.0712764Z INFO 02-26 18:50:17 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:50:17.0723399Z INFO 02-26 18:50:17 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:50:17.0758923Z INFO 02-26 18:50:17 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:50:17.0772922Z INFO 02-26 18:50:17 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:50:17.0783606Z INFO 02-26 18:50:17 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:50:17.8892404Z INFO 02-26 18:50:17 [parallel_state.py:1212] world_size=32 rank=9 local_rank=1 distributed_init_method=tcp://10.0.0.222:37005 backend=hccl
2026-02-26T18:50:17.8921215Z INFO 02-26 18:50:17 [parallel_state.py:1212] world_size=32 rank=1 local_rank=1 distributed_init_method=tcp://10.0.0.222:37005 backend=hccl
2026-02-26T18:50:18.7986854Z INFO 02-26 18:50:18 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:50:18.7993227Z INFO 02-26 18:50:18 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:50:18.8003098Z INFO 02-26 18:50:18 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:50:18.8012415Z INFO 02-26 18:50:18 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:50:18.8021814Z INFO 02-26 18:50:18 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:50:18.8031101Z INFO 02-26 18:50:18 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:50:18.8087267Z INFO 02-26 18:50:18 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:50:18.8130351Z INFO 02-26 18:50:18 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:50:23.7564020Z 2026-02-26 18:50:23,754 - 377 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:50:23.7598390Z INFO 02-26 18:50:23 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:50:23.7631812Z 2026-02-26 18:50:23,760 - 376 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:50:23.7656528Z INFO 02-26 18:50:23 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:50:25.4051254Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T18:50:25.4057527Z   warnings.warn(
2026-02-26T18:50:25.5187908Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T18:50:25.5193331Z   warnings.warn(
2026-02-26T18:50:27.1877124Z INFO 02-26 18:50:27 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:50:27.1886944Z INFO 02-26 18:50:27 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:50:27.1898678Z INFO 02-26 18:50:27 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:50:27.3109944Z INFO 02-26 18:50:27 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:50:27.3118568Z INFO 02-26 18:50:27 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:50:27.3134057Z INFO 02-26 18:50:27 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:50:27.9706288Z INFO 02-26 18:50:27 [parallel_state.py:1212] world_size=32 rank=2 local_rank=2 distributed_init_method=tcp://10.0.0.222:37005 backend=hccl
2026-02-26T18:50:28.1913034Z INFO 02-26 18:50:28 [parallel_state.py:1212] world_size=32 rank=10 local_rank=2 distributed_init_method=tcp://10.0.0.222:37005 backend=hccl
2026-02-26T18:50:28.5472326Z INFO 02-26 18:50:28 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:50:28.5478832Z INFO 02-26 18:50:28 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:50:28.5489174Z INFO 02-26 18:50:28 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:50:28.5586018Z INFO 02-26 18:50:28 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:50:28.9127732Z INFO 02-26 18:50:28 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:50:28.9133545Z INFO 02-26 18:50:28 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:50:28.9143089Z INFO 02-26 18:50:28 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:50:28.9209159Z INFO 02-26 18:50:28 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:50:33.5738891Z 2026-02-26 18:50:33,571 - 480 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:50:33.5768218Z INFO 02-26 18:50:33 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:50:33.9997588Z 2026-02-26 18:50:33,997 - 481 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:50:34.0091731Z INFO 02-26 18:50:34 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:50:35.3041282Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T18:50:35.3047391Z   warnings.warn(
2026-02-26T18:50:35.8543513Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T18:50:35.8548656Z   warnings.warn(
2026-02-26T18:50:37.0740802Z INFO 02-26 18:50:37 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:50:37.0747879Z INFO 02-26 18:50:37 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:50:37.0758643Z INFO 02-26 18:50:37 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:50:37.7038264Z INFO 02-26 18:50:37 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:50:37.7045144Z INFO 02-26 18:50:37 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:50:37.7056745Z INFO 02-26 18:50:37 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:50:38.1606713Z INFO 02-26 18:50:38 [parallel_state.py:1212] world_size=32 rank=3 local_rank=3 distributed_init_method=tcp://10.0.0.222:37005 backend=hccl
2026-02-26T18:50:38.3035417Z INFO 02-26 18:50:38 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:50:38.3041918Z INFO 02-26 18:50:38 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:50:38.3053308Z INFO 02-26 18:50:38 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:50:38.3107571Z INFO 02-26 18:50:38 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:50:38.4749771Z INFO 02-26 18:50:38 [parallel_state.py:1212] world_size=32 rank=11 local_rank=3 distributed_init_method=tcp://10.0.0.222:37005 backend=hccl
2026-02-26T18:50:39.0592353Z INFO 02-26 18:50:39 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:50:39.0599211Z INFO 02-26 18:50:39 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:50:39.0608735Z INFO 02-26 18:50:39 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:50:39.0667061Z INFO 02-26 18:50:39 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:50:43.2044278Z 2026-02-26 18:50:43,202 - 584 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:50:43.2079532Z INFO 02-26 18:50:43 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:50:43.9745756Z 2026-02-26 18:50:43,972 - 587 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:50:43.9798171Z INFO 02-26 18:50:43 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:50:44.8010520Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T18:50:44.8016654Z   warnings.warn(
2026-02-26T18:50:45.6235447Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T18:50:45.6240786Z   warnings.warn(
2026-02-26T18:50:46.5869563Z INFO 02-26 18:50:46 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:50:46.5877988Z INFO 02-26 18:50:46 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:50:46.5888644Z INFO 02-26 18:50:46 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:50:47.3761133Z INFO 02-26 18:50:47 [parallel_state.py:1212] world_size=32 rank=4 local_rank=4 distributed_init_method=tcp://10.0.0.222:37005 backend=hccl
2026-02-26T18:50:47.4722647Z INFO 02-26 18:50:47 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:50:47.4728753Z INFO 02-26 18:50:47 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:50:47.4739506Z INFO 02-26 18:50:47 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:50:48.0187739Z INFO 02-26 18:50:48 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:50:48.0194236Z INFO 02-26 18:50:48 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:50:48.0203378Z INFO 02-26 18:50:48 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:50:48.0265908Z INFO 02-26 18:50:48 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:50:48.3238217Z INFO 02-26 18:50:48 [parallel_state.py:1212] world_size=32 rank=12 local_rank=4 distributed_init_method=tcp://10.0.0.222:37005 backend=hccl
2026-02-26T18:50:48.7421441Z INFO 02-26 18:50:48 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:50:48.7429915Z INFO 02-26 18:50:48 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:50:48.7442358Z INFO 02-26 18:50:48 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:50:48.7494827Z INFO 02-26 18:50:48 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:50:52.9759008Z 2026-02-26 18:50:52,973 - 688 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:50:52.9793458Z INFO 02-26 18:50:52 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:50:53.7010748Z 2026-02-26 18:50:53,698 - 691 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:50:53.7045203Z INFO 02-26 18:50:53 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:50:54.5963101Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T18:50:54.5968423Z   warnings.warn(
2026-02-26T18:50:55.3373515Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T18:50:55.3378538Z   warnings.warn(
2026-02-26T18:50:56.3853008Z INFO 02-26 18:50:56 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:50:56.3860094Z INFO 02-26 18:50:56 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:50:56.3870654Z INFO 02-26 18:50:56 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:50:57.1468114Z INFO 02-26 18:50:57 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:50:57.1474011Z INFO 02-26 18:50:57 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:50:57.1485215Z INFO 02-26 18:50:57 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:50:57.5961689Z INFO 02-26 18:50:57 [parallel_state.py:1212] world_size=32 rank=5 local_rank=5 distributed_init_method=tcp://10.0.0.222:37005 backend=hccl
2026-02-26T18:50:57.7030367Z INFO 02-26 18:50:57 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:50:57.7065987Z INFO 02-26 18:50:57 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:50:57.7068759Z INFO 02-26 18:50:57 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:50:57.7130568Z INFO 02-26 18:50:57 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:50:57.9122368Z INFO 02-26 18:50:57 [parallel_state.py:1212] world_size=32 rank=13 local_rank=5 distributed_init_method=tcp://10.0.0.222:37005 backend=hccl
2026-02-26T18:50:58.5082285Z INFO 02-26 18:50:58 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:50:58.5087976Z INFO 02-26 18:50:58 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:50:58.5099099Z INFO 02-26 18:50:58 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:50:58.5159023Z INFO 02-26 18:50:58 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:51:02.6830859Z 2026-02-26 18:51:02,678 - 792 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:51:02.6858825Z INFO 02-26 18:51:02 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:51:03.5117716Z 2026-02-26 18:51:03,509 - 795 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:51:03.5155632Z INFO 02-26 18:51:03 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:51:04.5524890Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T18:51:04.5531622Z   warnings.warn(
2026-02-26T18:51:05.1758287Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T18:51:05.1767837Z   warnings.warn(
2026-02-26T18:51:06.3303789Z INFO 02-26 18:51:06 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:51:06.3309575Z INFO 02-26 18:51:06 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:51:06.3320418Z INFO 02-26 18:51:06 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:51:06.9953193Z INFO 02-26 18:51:06 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:51:06.9959794Z INFO 02-26 18:51:06 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:51:06.9974092Z INFO 02-26 18:51:06 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:51:07.5497448Z INFO 02-26 18:51:07 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:51:07.5503782Z INFO 02-26 18:51:07 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:51:07.5526078Z INFO 02-26 18:51:07 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:51:07.5585076Z INFO 02-26 18:51:07 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:51:07.5854493Z INFO 02-26 18:51:07 [parallel_state.py:1212] world_size=32 rank=6 local_rank=6 distributed_init_method=tcp://10.0.0.222:37005 backend=hccl
2026-02-26T18:51:08.1501122Z INFO 02-26 18:51:08 [parallel_state.py:1212] world_size=32 rank=14 local_rank=6 distributed_init_method=tcp://10.0.0.222:37005 backend=hccl
2026-02-26T18:51:08.2692737Z INFO 02-26 18:51:08 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:51:08.2700185Z INFO 02-26 18:51:08 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:51:08.2751524Z INFO 02-26 18:51:08 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:51:08.2767493Z INFO 02-26 18:51:08 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:51:12.6049022Z 2026-02-26 18:51:12,602 - 896 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:51:12.6080693Z INFO 02-26 18:51:12 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:51:13.1381881Z 2026-02-26 18:51:13,136 - 899 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T18:51:13.1417463Z INFO 02-26 18:51:13 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T18:51:14.1619916Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T18:51:14.1627550Z   warnings.warn(
2026-02-26T18:51:14.7663163Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T18:51:14.7670518Z   warnings.warn(
2026-02-26T18:51:15.8973992Z INFO 02-26 18:51:15 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:51:15.8980985Z INFO 02-26 18:51:15 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:51:15.8991549Z INFO 02-26 18:51:15 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:51:16.5273928Z INFO 02-26 18:51:16 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T18:51:16.5274374Z INFO 02-26 18:51:16 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T18:51:16.5284126Z INFO 02-26 18:51:16 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T18:51:17.3023484Z INFO 02-26 18:51:17 [parallel_state.py:1212] world_size=32 rank=15 local_rank=7 distributed_init_method=tcp://10.0.0.222:37005 backend=hccl
2026-02-26T18:51:17.3222835Z INFO 02-26 18:51:17 [parallel_state.py:1212] world_size=32 rank=7 local_rank=7 distributed_init_method=tcp://10.0.0.222:37005 backend=hccl
2026-02-26T18:51:17.8439660Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:17.8453339Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:17.8462378Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:17.9053505Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:17.9073260Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:17.9102390Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:17.9122839Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:17.9123252Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:17.9151541Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:17.9160644Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:17.9170947Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:17.9180162Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:17.9190043Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:17.9199791Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:17.9699448Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:17.9720794Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.0994325Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T18:51:18.1002624Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T18:51:18.1012319Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T18:51:18.1021525Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T18:51:18.1031609Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T18:51:18.1039528Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T18:51:18.1050164Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T18:51:18.1059482Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T18:51:18.1069029Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T18:51:18.1079343Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T18:51:18.1089228Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T18:51:18.1098714Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T18:51:18.1108275Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T18:51:18.1118063Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T18:51:18.1128823Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T18:51:18.1138460Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T18:51:18.1147842Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1157591Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1167515Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1177143Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1186419Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1195996Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1206155Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1215777Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1225067Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1235043Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1245740Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1256629Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1265242Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1275042Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1286338Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1295784Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1305108Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1314860Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1324915Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1334897Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1343720Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1353832Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1364009Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1373904Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1823253Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1832611Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1846302Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1855300Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1865178Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1874583Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1884165Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1893362Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1905530Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1912654Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1922211Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1932070Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1942257Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1951318Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1961249Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1970673Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1979988Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1989562Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.1999379Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.2009026Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.2018732Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.2028192Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.2037694Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.2046811Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T18:51:18.2056012Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.2065787Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.2075822Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.2085724Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.2095159Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.2104447Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.2114336Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.2124137Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.2133748Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.2143014Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.2152547Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.2162396Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.2171886Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.2181550Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.2190644Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.2200254Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.2272598Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.2291284Z INFO 02-26 18:51:18 [parallel_state.py:1423] rank 0 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
2026-02-26T18:51:18.2909157Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.2958538Z INFO 02-26 18:51:18 [parallel_state.py:1423] rank 1 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
2026-02-26T18:51:18.3028181Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.3051805Z INFO 02-26 18:51:18 [parallel_state.py:1423] rank 7 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 7, EP rank 7
2026-02-26T18:51:18.3071800Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.3081347Z INFO 02-26 18:51:18 [parallel_state.py:1423] rank 3 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
2026-02-26T18:51:18.3090183Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.3101242Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.3108905Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.3118126Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.3129070Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.3136939Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.3145967Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.3155828Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.3166239Z INFO 02-26 18:51:18 [parallel_state.py:1423] rank 15 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 7, EP rank 15
2026-02-26T18:51:18.3175418Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.3184855Z INFO 02-26 18:51:18 [parallel_state.py:1423] rank 12 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 4, EP rank 12
2026-02-26T18:51:18.3194050Z INFO 02-26 18:51:18 [parallel_state.py:1423] rank 8 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 0, EP rank 8
2026-02-26T18:51:18.3203774Z INFO 02-26 18:51:18 [parallel_state.py:1423] rank 4 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 4, EP rank 4
2026-02-26T18:51:18.3212989Z INFO 02-26 18:51:18 [parallel_state.py:1423] rank 9 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 1, EP rank 9
2026-02-26T18:51:18.3222480Z INFO 02-26 18:51:18 [parallel_state.py:1423] rank 6 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 6, EP rank 6
2026-02-26T18:51:18.3231647Z INFO 02-26 18:51:18 [parallel_state.py:1423] rank 11 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 3, EP rank 11
2026-02-26T18:51:18.3241339Z INFO 02-26 18:51:18 [parallel_state.py:1423] rank 14 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 6, EP rank 14
2026-02-26T18:51:18.3251106Z INFO 02-26 18:51:18 [parallel_state.py:1423] rank 10 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 2, EP rank 10
2026-02-26T18:51:18.3384002Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.3404297Z INFO 02-26 18:51:18 [parallel_state.py:1423] rank 2 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
2026-02-26T18:51:18.3537496Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.3561464Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.3631126Z INFO 02-26 18:51:18 [parallel_state.py:1423] rank 5 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 5, EP rank 5
2026-02-26T18:51:18.3631689Z INFO 02-26 18:51:18 [parallel_state.py:1423] rank 13 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 5, EP rank 13
2026-02-26T18:51:18.4063899Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.4072850Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.4091104Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.4100571Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.4109682Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.4119305Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.4128764Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.4138505Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.4147508Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.4156977Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.4167095Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.4176214Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.4186032Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.4194946Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.4204370Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.4213727Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-26T18:51:18.4263571Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.4271460Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.4280934Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.4290035Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.4299735Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.4309261Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.4319216Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.4329147Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.4338156Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.4347068Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.4356214Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.4366652Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.4375739Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.4385523Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.4394306Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:18.4404072Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-26T18:51:21.7435184Z INFO 02-26 18:51:21 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T18:51:21.7443103Z INFO 02-26 18:51:21 [cpu_binding.py:302] NPU13: main=[202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237]  acl=[238]  release=[[239]]
2026-02-26T18:51:21.8154134Z INFO 02-26 18:51:21 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:21.8171396Z INFO 02-26 18:51:21 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:21.8724081Z WARNING 02-26 18:51:21 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:21.9284295Z WARNING 02-26 18:51:21 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:21.9337289Z [0;36m(Worker_DP1_TP5_EP13 pid=691)[0;0m INFO 02-26 18:51:21 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T18:51:22.0227808Z INFO 02-26 18:51:22 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T18:51:22.0236025Z INFO 02-26 18:51:22 [cpu_binding.py:302] NPU0: main=[2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37]  acl=[38]  release=[[39]]
2026-02-26T18:51:22.0874744Z INFO 02-26 18:51:22 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:22.1231257Z WARNING 02-26 18:51:22 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:22.1453034Z WARNING 02-26 18:51:22 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:22.1485492Z INFO 02-26 18:51:22 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T18:51:22.1495879Z INFO 02-26 18:51:22 [cpu_binding.py:302] NPU8: main=[2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37]  acl=[38]  release=[[39]]
2026-02-26T18:51:22.2037703Z INFO 02-26 18:51:22 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:22.2400359Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-26 18:51:22 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T18:51:22.2453272Z WARNING 02-26 18:51:22 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:22.2718103Z WARNING 02-26 18:51:22 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:22.2763886Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-26 18:51:22 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T18:51:22.5483176Z INFO 02-26 18:51:22 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T18:51:22.5489051Z INFO 02-26 18:51:22 [cpu_binding.py:302] NPU15: main=[282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317]  acl=[318]  release=[[319]]
2026-02-26T18:51:22.6111803Z [0;36m(Worker_DP1_TP5_EP13 pid=691)[0;0m [2026-02-26 18:51:22] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T18:51:22.6139873Z INFO 02-26 18:51:22 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:22.6164427Z INFO 02-26 18:51:22 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:22.6644755Z WARNING 02-26 18:51:22 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:22.6875011Z WARNING 02-26 18:51:22 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:22.6924933Z [0;36m(Worker_DP1_TP7_EP15 pid=899)[0;0m INFO 02-26 18:51:22 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T18:51:22.7554305Z INFO 02-26 18:51:22 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T18:51:22.7563001Z INFO 02-26 18:51:22 [cpu_binding.py:302] NPU4: main=[162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197]  acl=[198]  release=[[199]]
2026-02-26T18:51:22.8068286Z INFO 02-26 18:51:22 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:22.8092884Z INFO 02-26 18:51:22 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:22.8121963Z [0;36m(Worker_DP1_TP5_EP13 pid=691)[0;0m INFO 02-26 18:51:22 [layer.py:475] [EP Rank 13/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-26T18:51:22.8486967Z WARNING 02-26 18:51:22 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:22.8805146Z WARNING 02-26 18:51:22 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:22.8851039Z [0;36m(Worker_DP0_TP4_EP4 pid=584)[0;0m INFO 02-26 18:51:22 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T18:51:22.9596402Z INFO 02-26 18:51:22 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T18:51:22.9605526Z INFO 02-26 18:51:22 [cpu_binding.py:302] NPU12: main=[162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197]  acl=[198]  release=[[199]]
2026-02-26T18:51:22.9765530Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m [2026-02-26 18:51:22] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T18:51:23.0082947Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m [2026-02-26 18:51:23] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T18:51:23.0207315Z INFO 02-26 18:51:23 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:23.0240065Z INFO 02-26 18:51:23 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:23.0689017Z WARNING 02-26 18:51:23 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:23.0754691Z INFO 02-26 18:51:23 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T18:51:23.0765383Z INFO 02-26 18:51:23 [cpu_binding.py:302] NPU1: main=[42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77]  acl=[78]  release=[[79]]
2026-02-26T18:51:23.0964725Z WARNING 02-26 18:51:23 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:23.1020438Z [0;36m(Worker_DP1_TP4_EP12 pid=587)[0;0m INFO 02-26 18:51:23 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T18:51:23.1332713Z INFO 02-26 18:51:23 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:23.1352980Z INFO 02-26 18:51:23 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:23.1400563Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-26 18:51:23 [layer.py:475] [EP Rank 0/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-26T18:51:23.1810173Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-26 18:51:23 [layer.py:475] [EP Rank 8/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-26T18:51:23.1838366Z WARNING 02-26 18:51:23 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:23.2072667Z WARNING 02-26 18:51:23 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:23.2117720Z [0;36m(Worker_DP0_TP1_EP1 pid=258)[0;0m INFO 02-26 18:51:23 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T18:51:23.2709997Z INFO 02-26 18:51:23 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T18:51:23.2718711Z INFO 02-26 18:51:23 [cpu_binding.py:302] NPU5: main=[202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237]  acl=[238]  release=[[239]]
2026-02-26T18:51:23.3278141Z INFO 02-26 18:51:23 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:23.3303606Z INFO 02-26 18:51:23 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:23.3893231Z WARNING 02-26 18:51:23 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:23.4145242Z WARNING 02-26 18:51:23 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:23.4193620Z [0;36m(Worker_DP0_TP5_EP5 pid=688)[0;0m INFO 02-26 18:51:23 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T18:51:23.4225294Z [0;36m(Worker_DP1_TP7_EP15 pid=899)[0;0m [2026-02-26 18:51:23] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T18:51:23.4767020Z INFO 02-26 18:51:23 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T18:51:23.4775706Z INFO 02-26 18:51:23 [cpu_binding.py:302] NPU3: main=[122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157]  acl=[158]  release=[[159]]
2026-02-26T18:51:23.5372415Z INFO 02-26 18:51:23 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:23.5404118Z INFO 02-26 18:51:23 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:23.5429398Z [0;36m(Worker_DP1_TP7_EP15 pid=899)[0;0m INFO 02-26 18:51:23 [layer.py:475] [EP Rank 15/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-26T18:51:23.5746388Z WARNING 02-26 18:51:23 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:23.5938922Z [0;36m(Worker_DP0_TP4_EP4 pid=584)[0;0m [2026-02-26 18:51:23] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T18:51:23.5963449Z WARNING 02-26 18:51:23 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:23.6038731Z [0;36m(Worker_DP0_TP3_EP3 pid=480)[0;0m INFO 02-26 18:51:23 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T18:51:23.6231831Z INFO 02-26 18:51:23 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T18:51:23.6242271Z INFO 02-26 18:51:23 [cpu_binding.py:302] NPU6: main=[242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277]  acl=[278]  release=[[279]]
2026-02-26T18:51:23.6808286Z INFO 02-26 18:51:23 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:23.6839068Z INFO 02-26 18:51:23 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:23.7091189Z [0;36m(Worker_DP0_TP4_EP4 pid=584)[0;0m INFO 02-26 18:51:23 [layer.py:475] [EP Rank 4/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-26T18:51:23.7263517Z WARNING 02-26 18:51:23 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:23.7567738Z WARNING 02-26 18:51:23 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:23.7568781Z [0;36m(Worker_DP0_TP6_EP6 pid=792)[0;0m INFO 02-26 18:51:23 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T18:51:23.8632560Z [0;36m(Worker_DP1_TP4_EP12 pid=587)[0;0m [2026-02-26 18:51:23] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T18:51:23.8895969Z INFO 02-26 18:51:23 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T18:51:23.8905435Z INFO 02-26 18:51:23 [cpu_binding.py:302] NPU11: main=[122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157]  acl=[158]  release=[[159]]
2026-02-26T18:51:23.8954057Z [0;36m(Worker_DP0_TP1_EP1 pid=258)[0;0m [2026-02-26 18:51:23] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T18:51:23.9182701Z [0;36m(Worker_DP0_TP3_EP3 pid=480)[0;0m [2026-02-26 18:51:23] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T18:51:23.9471239Z INFO 02-26 18:51:23 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:23.9508618Z INFO 02-26 18:51:23 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:23.9761795Z [0;36m(Worker_DP1_TP4_EP12 pid=587)[0;0m INFO 02-26 18:51:23 [layer.py:475] [EP Rank 12/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-26T18:51:24.0025616Z WARNING 02-26 18:51:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:24.0128320Z [0;36m(Worker_DP0_TP1_EP1 pid=258)[0;0m INFO 02-26 18:51:24 [layer.py:475] [EP Rank 1/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-26T18:51:24.0295479Z WARNING 02-26 18:51:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:24.0322921Z [0;36m(Worker_DP0_TP3_EP3 pid=480)[0;0m INFO 02-26 18:51:24 [layer.py:475] [EP Rank 3/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-26T18:51:24.0346754Z [0;36m(Worker_DP1_TP3_EP11 pid=481)[0;0m INFO 02-26 18:51:24 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T18:51:24.1016124Z INFO 02-26 18:51:24 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T18:51:24.1025606Z INFO 02-26 18:51:24 [cpu_binding.py:302] NPU2: main=[82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117]  acl=[118]  release=[[119]]
2026-02-26T18:51:24.1632727Z INFO 02-26 18:51:24 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:24.1654568Z INFO 02-26 18:51:24 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:24.1998762Z WARNING 02-26 18:51:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:24.2213480Z WARNING 02-26 18:51:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:24.2255178Z [0;36m(Worker_DP0_TP2_EP2 pid=377)[0;0m INFO 02-26 18:51:24 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T18:51:24.2657224Z INFO 02-26 18:51:24 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T18:51:24.2667197Z INFO 02-26 18:51:24 [cpu_binding.py:302] NPU14: main=[242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277]  acl=[278]  release=[[279]]
2026-02-26T18:51:24.3178629Z INFO 02-26 18:51:24 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:24.3199507Z INFO 02-26 18:51:24 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:24.3588815Z WARNING 02-26 18:51:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:24.3867012Z WARNING 02-26 18:51:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:24.3958390Z [0;36m(Worker_DP0_TP6_EP6 pid=792)[0;0m [2026-02-26 18:51:24] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T18:51:24.3967389Z [0;36m(Worker_DP1_TP6_EP14 pid=795)[0;0m INFO 02-26 18:51:24 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T18:51:24.4201366Z [0;36m(Worker_DP0_TP5_EP5 pid=688)[0;0m [2026-02-26 18:51:24] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T18:51:24.4693111Z INFO 02-26 18:51:24 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T18:51:24.4702550Z INFO 02-26 18:51:24 [cpu_binding.py:302] NPU7: main=[282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317]  acl=[318]  release=[[319]]
2026-02-26T18:51:24.5079966Z [0;36m(Worker_DP0_TP6_EP6 pid=792)[0;0m INFO 02-26 18:51:24 [layer.py:475] [EP Rank 6/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-26T18:51:24.5163772Z INFO 02-26 18:51:24 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:24.5190053Z INFO 02-26 18:51:24 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:24.5355689Z [0;36m(Worker_DP0_TP5_EP5 pid=688)[0;0m INFO 02-26 18:51:24 [layer.py:475] [EP Rank 5/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-26T18:51:24.5738250Z INFO 02-26 18:51:24 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T18:51:24.5746777Z INFO 02-26 18:51:24 [cpu_binding.py:302] NPU9: main=[42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77]  acl=[78]  release=[[79]]
2026-02-26T18:51:24.5807708Z WARNING 02-26 18:51:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:24.5944363Z [0;36m(Worker_DP0_TP2_EP2 pid=377)[0;0m [2026-02-26 18:51:24] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T18:51:24.6054729Z WARNING 02-26 18:51:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:24.6103324Z [0;36m(Worker_DP0_TP7_EP7 pid=896)[0;0m INFO 02-26 18:51:24 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T18:51:24.6252738Z INFO 02-26 18:51:24 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:24.6277284Z INFO 02-26 18:51:24 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:24.6756361Z WARNING 02-26 18:51:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:24.6976876Z [0;36m(Worker_DP0_TP2_EP2 pid=377)[0;0m INFO 02-26 18:51:24 [layer.py:475] [EP Rank 2/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-26T18:51:24.7019631Z WARNING 02-26 18:51:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:24.7070026Z [0;36m(Worker_DP1_TP1_EP9 pid=261)[0;0m INFO 02-26 18:51:24 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T18:51:24.7249990Z INFO 02-26 18:51:24 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T18:51:24.7259474Z INFO 02-26 18:51:24 [cpu_binding.py:302] NPU10: main=[82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117]  acl=[118]  release=[[119]]
2026-02-26T18:51:24.7412484Z [0;36m(Worker_DP1_TP3_EP11 pid=481)[0;0m [2026-02-26 18:51:24] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T18:51:24.7761388Z INFO 02-26 18:51:24 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:24.7785105Z INFO 02-26 18:51:24 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T18:51:24.8253867Z WARNING 02-26 18:51:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:24.8503102Z WARNING 02-26 18:51:24 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T18:51:24.8516532Z [0;36m(Worker_DP1_TP3_EP11 pid=481)[0;0m INFO 02-26 18:51:24 [layer.py:475] [EP Rank 11/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-26T18:51:24.8549918Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-26 18:51:24 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T18:51:24.9659008Z [0;36m(Worker_DP0_TP7_EP7 pid=896)[0;0m [2026-02-26 18:51:24] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T18:51:25.0794260Z [0;36m(Worker_DP0_TP7_EP7 pid=896)[0;0m INFO 02-26 18:51:25 [layer.py:475] [EP Rank 7/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-26T18:51:25.3947857Z [0;36m(Worker_DP1_TP1_EP9 pid=261)[0;0m [2026-02-26 18:51:25] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T18:51:25.4258914Z [0;36m(Worker_DP1_TP6_EP14 pid=795)[0;0m [2026-02-26 18:51:25] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T18:51:25.4969868Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m [2026-02-26 18:51:25] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T18:51:25.5097870Z [0;36m(Worker_DP1_TP1_EP9 pid=261)[0;0m INFO 02-26 18:51:25 [layer.py:475] [EP Rank 9/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-26T18:51:25.5351020Z [0;36m(Worker_DP1_TP6_EP14 pid=795)[0;0m INFO 02-26 18:51:25 [layer.py:475] [EP Rank 14/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-26T18:51:25.5977512Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-26 18:51:25 [layer.py:475] [EP Rank 10/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-26T18:51:26.1251276Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T18:51:26.1256873Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m   return func(*args, **kwargs)
2026-02-26T18:51:26.1486359Z [0;36m(Worker_DP1_TP4_EP12 pid=587)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T18:51:26.1493886Z [0;36m(Worker_DP1_TP4_EP12 pid=587)[0;0m   return func(*args, **kwargs)
2026-02-26T18:51:26.1585242Z [0;36m(Worker_DP1_TP7_EP15 pid=899)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T18:51:26.1593786Z [0;36m(Worker_DP1_TP7_EP15 pid=899)[0;0m   return func(*args, **kwargs)
2026-02-26T18:51:26.1629623Z [0;36m(Worker_DP0_TP1_EP1 pid=258)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T18:51:26.1638548Z [0;36m(Worker_DP0_TP1_EP1 pid=258)[0;0m   return func(*args, **kwargs)
2026-02-26T18:51:26.1922493Z [0;36m(Worker_DP1_TP3_EP11 pid=481)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T18:51:26.1930382Z [0;36m(Worker_DP1_TP3_EP11 pid=481)[0;0m   return func(*args, **kwargs)
2026-02-26T18:51:26.1948742Z [0;36m(Worker_DP0_TP3_EP3 pid=480)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T18:51:26.1957823Z [0;36m(Worker_DP0_TP3_EP3 pid=480)[0;0m   return func(*args, **kwargs)
2026-02-26T18:51:26.2044807Z [0;36m(Worker_DP1_TP5_EP13 pid=691)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T18:51:26.2053174Z [0;36m(Worker_DP1_TP5_EP13 pid=691)[0;0m   return func(*args, **kwargs)
2026-02-26T18:51:26.2100300Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T18:51:26.2108998Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m   return func(*args, **kwargs)
2026-02-26T18:51:26.2127544Z [0;36m(Worker_DP0_TP7_EP7 pid=896)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T18:51:26.2136140Z [0;36m(Worker_DP0_TP7_EP7 pid=896)[0;0m   return func(*args, **kwargs)
2026-02-26T18:51:26.2205904Z [0;36m(Worker_DP1_TP4_EP12 pid=587)[0;0m INFO 02-26 18:51:26 [fused_moe.py:293] [EP Rank 12/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-26T18:51:26.2242736Z [0;36m(Worker_DP0_TP1_EP1 pid=258)[0;0m INFO 02-26 18:51:26 [fused_moe.py:293] [EP Rank 1/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-26T18:51:26.2290370Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-26 18:51:26 [fused_moe.py:293] [EP Rank 8/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-26T18:51:26.2303860Z [0;36m(Worker_DP1_TP1_EP9 pid=261)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T18:51:26.2310108Z [0;36m(Worker_DP1_TP1_EP9 pid=261)[0;0m   return func(*args, **kwargs)
2026-02-26T18:51:26.2464938Z [0;36m(Worker_DP1_TP6_EP14 pid=795)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T18:51:26.2472085Z [0;36m(Worker_DP1_TP6_EP14 pid=795)[0;0m   return func(*args, **kwargs)
2026-02-26T18:51:26.2487273Z [0;36m(Worker_DP1_TP7_EP15 pid=899)[0;0m INFO 02-26 18:51:26 [fused_moe.py:293] [EP Rank 15/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-26T18:51:26.2583851Z [0;36m(Worker_DP0_TP2_EP2 pid=377)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T18:51:26.2591658Z [0;36m(Worker_DP0_TP2_EP2 pid=377)[0;0m   return func(*args, **kwargs)
2026-02-26T18:51:26.2602583Z [0;36m(Worker_DP0_TP3_EP3 pid=480)[0;0m INFO 02-26 18:51:26 [fused_moe.py:293] [EP Rank 3/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-26T18:51:26.2613167Z [0;36m(Worker_DP1_TP3_EP11 pid=481)[0;0m INFO 02-26 18:51:26 [fused_moe.py:293] [EP Rank 11/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-26T18:51:26.2671133Z [0;36m(Worker_DP0_TP6_EP6 pid=792)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T18:51:26.2683810Z [0;36m(Worker_DP0_TP6_EP6 pid=792)[0;0m   return func(*args, **kwargs)
2026-02-26T18:51:26.2696347Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-26 18:51:26 [fused_moe.py:293] [EP Rank 0/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-26T18:51:26.2719377Z [0;36m(Worker_DP1_TP5_EP13 pid=691)[0;0m INFO 02-26 18:51:26 [fused_moe.py:293] [EP Rank 13/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-26T18:51:26.2785808Z [0;36m(Worker_DP0_TP7_EP7 pid=896)[0;0m INFO 02-26 18:51:26 [fused_moe.py:293] [EP Rank 7/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-26T18:51:26.2835743Z [0;36m(Worker_DP0_TP4_EP4 pid=584)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T18:51:26.2844595Z [0;36m(Worker_DP0_TP4_EP4 pid=584)[0;0m   return func(*args, **kwargs)
2026-02-26T18:51:26.2938169Z [0;36m(Worker_DP1_TP1_EP9 pid=261)[0;0m INFO 02-26 18:51:26 [fused_moe.py:293] [EP Rank 9/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-26T18:51:26.3074228Z [0;36m(Worker_DP1_TP6_EP14 pid=795)[0;0m INFO 02-26 18:51:26 [fused_moe.py:293] [EP Rank 14/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-26T18:51:26.3115608Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T18:51:26.3124025Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m   return func(*args, **kwargs)
2026-02-26T18:51:26.3137735Z [0;36m(Worker_DP0_TP2_EP2 pid=377)[0;0m INFO 02-26 18:51:26 [fused_moe.py:293] [EP Rank 2/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-26T18:51:26.3264497Z [0;36m(Worker_DP0_TP6_EP6 pid=792)[0;0m INFO 02-26 18:51:26 [fused_moe.py:293] [EP Rank 6/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-26T18:51:26.3359236Z [0;36m(Worker_DP0_TP5_EP5 pid=688)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T18:51:26.3367225Z [0;36m(Worker_DP0_TP5_EP5 pid=688)[0;0m   return func(*args, **kwargs)
2026-02-26T18:51:26.3422436Z [0;36m(Worker_DP0_TP4_EP4 pid=584)[0;0m INFO 02-26 18:51:26 [fused_moe.py:293] [EP Rank 4/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-26T18:51:26.3703556Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-26 18:51:26 [fused_moe.py:293] [EP Rank 10/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-26T18:51:26.3987620Z [0;36m(Worker_DP0_TP5_EP5 pid=688)[0;0m INFO 02-26 18:51:26 [fused_moe.py:293] [EP Rank 5/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-26T18:51:26.5730445Z [0;36m(Worker_DP1_TP5_EP13 pid=691)[0;0m INFO 02-26 18:51:26 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T18:51:26.5737465Z [0;36m(Worker_DP1_TP4_EP12 pid=587)[0;0m INFO 02-26 18:51:26 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T18:51:26.5754502Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-26 18:51:26 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T18:51:26.5767821Z [0;36m(Worker_DP0_TP3_EP3 pid=480)[0;0m INFO 02-26 18:51:26 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T18:51:26.5781293Z [0;36m(Worker_DP1_TP7_EP15 pid=899)[0;0m INFO 02-26 18:51:26 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T18:51:26.5791736Z [0;36m(Worker_DP0_TP1_EP1 pid=258)[0;0m INFO 02-26 18:51:26 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T18:51:26.5928506Z [0;36m(Worker_DP0_TP7_EP7 pid=896)[0;0m INFO 02-26 18:51:26 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T18:51:26.6192595Z [0;36m(Worker_DP1_TP3_EP11 pid=481)[0;0m INFO 02-26 18:51:26 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T18:51:26.6431234Z [0;36m(Worker_DP0_TP4_EP4 pid=584)[0;0m INFO 02-26 18:51:26 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T18:51:26.6459197Z [0;36m(Worker_DP0_TP2_EP2 pid=377)[0;0m INFO 02-26 18:51:26 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T18:51:26.6487456Z [0;36m(Worker_DP1_TP6_EP14 pid=795)[0;0m INFO 02-26 18:51:26 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T18:51:26.6663321Z [0;36m(Worker_DP1_TP1_EP9 pid=261)[0;0m INFO 02-26 18:51:26 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T18:51:26.6664218Z [0;36m(Worker_DP0_TP6_EP6 pid=792)[0;0m INFO 02-26 18:51:26 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T18:51:26.6911231Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-26 18:51:26 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T18:51:26.7104644Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-26 18:51:26 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T18:51:26.7188157Z [0;36m(Worker_DP0_TP5_EP5 pid=688)[0;0m INFO 02-26 18:51:26 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T18:51:29.8219762Z [0;36m(Worker_DP0_TP1_EP1 pid=258)[0;0m INFO 02-26 18:51:29 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T18:51:29.9139716Z [0;36m(Worker_DP0_TP4_EP4 pid=584)[0;0m INFO 02-26 18:51:29 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T18:51:29.9214625Z [0;36m(Worker_DP1_TP4_EP12 pid=587)[0;0m INFO 02-26 18:51:29 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T18:51:29.9340464Z [0;36m(Worker_DP1_TP5_EP13 pid=691)[0;0m INFO 02-26 18:51:29 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T18:51:29.9360285Z [0;36m(Worker_DP1_TP3_EP11 pid=481)[0;0m INFO 02-26 18:51:29 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T18:51:29.9492781Z [0;36m(Worker_DP0_TP3_EP3 pid=480)[0;0m INFO 02-26 18:51:29 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T18:51:30.0281594Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-26 18:51:30 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T18:51:30.0502861Z [0;36m(Worker_DP1_TP7_EP15 pid=899)[0;0m INFO 02-26 18:51:30 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T18:51:30.0713962Z [0;36m(Worker_DP1_TP6_EP14 pid=795)[0;0m INFO 02-26 18:51:30 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T18:51:30.0960126Z [0;36m(Worker_DP0_TP5_EP5 pid=688)[0;0m INFO 02-26 18:51:30 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T18:51:30.1205823Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-26 18:51:30 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T18:51:30.1272755Z [0;36m(Worker_DP0_TP2_EP2 pid=377)[0;0m INFO 02-26 18:51:30 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T18:51:30.1484949Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-26 18:51:30 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T18:51:30.1598090Z [0;36m(Worker_DP0_TP6_EP6 pid=792)[0;0m INFO 02-26 18:51:30 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T18:51:30.1784324Z [0;36m(Worker_DP1_TP1_EP9 pid=261)[0;0m INFO 02-26 18:51:30 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T18:51:30.1784937Z [0;36m(Worker_DP0_TP7_EP7 pid=896)[0;0m INFO 02-26 18:51:30 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T18:51:30.1860695Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:51:30.1861079Z Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
2026-02-26T18:51:31.4592199Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:51:31.4592700Z Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:01<03:26,  1.28s/it]
2026-02-26T18:51:34.8535934Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:51:34.8536516Z Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:04<06:46,  2.52s/it]
2026-02-26T18:51:36.0973382Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:51:36.0974133Z Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:05<05:10,  1.94s/it]
2026-02-26T18:51:37.7328650Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:51:37.7329119Z Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:07<04:49,  1.82s/it]
2026-02-26T18:51:41.2307424Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:51:41.2307884Z Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:11<06:23,  2.42s/it]
2026-02-26T18:51:44.1015471Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:51:44.1015930Z Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:13<06:44,  2.58s/it]
2026-02-26T18:51:46.8313706Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:51:46.8314667Z Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:16<06:49,  2.63s/it]
2026-02-26T18:51:49.4767770Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:51:49.4768389Z Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:19<06:48,  2.63s/it]
2026-02-26T18:51:50.9440900Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:51:50.9441476Z Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:20<05:49,  2.27s/it]
2026-02-26T18:51:52.4666739Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:51:52.4667427Z Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:22<05:11,  2.04s/it]
2026-02-26T18:51:53.9645102Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:51:53.9645659Z Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:23<04:44,  1.87s/it]
2026-02-26T18:51:55.5235952Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:51:55.5236614Z Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:25<04:28,  1.78s/it]
2026-02-26T18:51:57.2288583Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:51:57.2289136Z Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:27<04:23,  1.76s/it]
2026-02-26T18:52:00.7381062Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:00.7381630Z Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:30<05:40,  2.29s/it]
2026-02-26T18:52:03.2968041Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:03.2968473Z Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:33<05:50,  2.37s/it]
2026-02-26T18:52:05.6228501Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:05.6229060Z Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:35<05:46,  2.36s/it]
2026-02-26T18:52:08.2016897Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:08.2017399Z Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:38<05:53,  2.42s/it]
2026-02-26T18:52:10.7274496Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:10.7275063Z Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:40<05:55,  2.45s/it]
2026-02-26T18:52:11.9784694Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:11.9785429Z Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:41<05:01,  2.09s/it]
2026-02-26T18:52:13.4998780Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:13.4999333Z Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:43<04:34,  1.92s/it]
2026-02-26T18:52:16.8590168Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:16.8590643Z Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:46<05:34,  2.35s/it]
2026-02-26T18:52:18.0628466Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:18.0629114Z Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:47<04:43,  2.01s/it]
2026-02-26T18:52:19.5893127Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:19.5893627Z Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:49<04:20,  1.86s/it]
2026-02-26T18:52:21.1139454Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:21.1139914Z Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:50<04:04,  1.76s/it]
2026-02-26T18:52:24.8560002Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:24.8560440Z Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:54<05:25,  2.36s/it]
2026-02-26T18:52:26.0696489Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:26.0696935Z Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:55<04:35,  2.01s/it]
2026-02-26T18:52:27.4308307Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:27.4308787Z Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:57<04:07,  1.82s/it]
2026-02-26T18:52:30.5007013Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:30.5007479Z Loading safetensors checkpoint shards:  17% Completed | 28/163 [01:00<04:56,  2.19s/it]
2026-02-26T18:52:32.1013321Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:32.1013886Z Loading safetensors checkpoint shards:  18% Completed | 29/163 [01:01<04:30,  2.02s/it]
2026-02-26T18:52:33.6244628Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:33.6245115Z Loading safetensors checkpoint shards:  18% Completed | 30/163 [01:03<04:08,  1.87s/it]
2026-02-26T18:52:35.1624686Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:35.1625159Z Loading safetensors checkpoint shards:  19% Completed | 31/163 [01:04<03:53,  1.77s/it]
2026-02-26T18:52:36.9931259Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:36.9931890Z Loading safetensors checkpoint shards:  20% Completed | 32/163 [01:06<03:54,  1.79s/it]
2026-02-26T18:52:38.8459050Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:38.8459521Z Loading safetensors checkpoint shards:  20% Completed | 33/163 [01:08<03:54,  1.81s/it]
2026-02-26T18:52:40.4299941Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:40.4300408Z Loading safetensors checkpoint shards:  21% Completed | 34/163 [01:10<03:44,  1.74s/it]
2026-02-26T18:52:41.9855493Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:41.9856106Z Loading safetensors checkpoint shards:  21% Completed | 35/163 [01:11<03:35,  1.68s/it]
2026-02-26T18:52:46.5692924Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:46.5693394Z Loading safetensors checkpoint shards:  22% Completed | 36/163 [01:16<05:24,  2.55s/it]
2026-02-26T18:52:47.4725845Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:47.4726318Z Loading safetensors checkpoint shards:  23% Completed | 37/163 [01:17<04:19,  2.06s/it]
2026-02-26T18:52:48.5592872Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:48.5593375Z Loading safetensors checkpoint shards:  23% Completed | 38/163 [01:18<03:40,  1.77s/it]
2026-02-26T18:52:50.1570372Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:50.1570893Z Loading safetensors checkpoint shards:  24% Completed | 39/163 [01:19<03:32,  1.72s/it]
2026-02-26T18:52:54.4394175Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:54.4394659Z Loading safetensors checkpoint shards:  25% Completed | 40/163 [01:24<05:05,  2.49s/it]
2026-02-26T18:52:55.7459427Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:55.7459934Z Loading safetensors checkpoint shards:  25% Completed | 41/163 [01:25<04:20,  2.13s/it]
2026-02-26T18:52:57.3691897Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:52:57.3692530Z Loading safetensors checkpoint shards:  26% Completed | 42/163 [01:27<03:59,  1.98s/it]
2026-02-26T18:53:01.1152267Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:01.1152885Z Loading safetensors checkpoint shards:  26% Completed | 43/163 [01:30<05:01,  2.51s/it]
2026-02-26T18:53:02.3924251Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:02.3924720Z Loading safetensors checkpoint shards:  27% Completed | 44/163 [01:32<04:14,  2.14s/it]
2026-02-26T18:53:03.8835923Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:03.8836406Z Loading safetensors checkpoint shards:  28% Completed | 45/163 [01:33<03:49,  1.95s/it]
2026-02-26T18:53:05.4228355Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:05.4228839Z Loading safetensors checkpoint shards:  28% Completed | 46/163 [01:35<03:33,  1.82s/it]
2026-02-26T18:53:11.9602507Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:11.9602972Z Loading safetensors checkpoint shards:  29% Completed | 47/163 [01:41<06:15,  3.24s/it]
2026-02-26T18:53:14.2426032Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:14.2426561Z Loading safetensors checkpoint shards:  29% Completed | 48/163 [01:44<05:39,  2.95s/it]
2026-02-26T18:53:14.4192935Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:14.4193635Z Loading safetensors checkpoint shards:  30% Completed | 49/163 [01:44<04:01,  2.12s/it]
2026-02-26T18:53:16.2947826Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:16.2948635Z Loading safetensors checkpoint shards:  31% Completed | 50/163 [01:46<03:51,  2.05s/it]
2026-02-26T18:53:18.8693369Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:18.8693999Z Loading safetensors checkpoint shards:  31% Completed | 51/163 [01:48<04:06,  2.20s/it]
2026-02-26T18:53:20.3214251Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:20.3214701Z Loading safetensors checkpoint shards:  32% Completed | 52/163 [01:50<03:39,  1.98s/it]
2026-02-26T18:53:23.4560383Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:23.4560864Z Loading safetensors checkpoint shards:  33% Completed | 53/163 [01:53<04:15,  2.33s/it]
2026-02-26T18:53:24.6681345Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:24.6681856Z Loading safetensors checkpoint shards:  33% Completed | 54/163 [01:54<03:37,  1.99s/it]
2026-02-26T18:53:26.1651625Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:26.1652196Z Loading safetensors checkpoint shards:  34% Completed | 55/163 [01:55<03:19,  1.84s/it]
2026-02-26T18:53:27.6063393Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:27.6063931Z Loading safetensors checkpoint shards:  34% Completed | 56/163 [01:57<03:04,  1.72s/it]
2026-02-26T18:53:29.1179088Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:29.1179551Z Loading safetensors checkpoint shards:  35% Completed | 57/163 [01:58<02:55,  1.66s/it]
2026-02-26T18:53:30.4851628Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:30.4852263Z Loading safetensors checkpoint shards:  36% Completed | 58/163 [02:00<02:45,  1.57s/it]
2026-02-26T18:53:34.3612406Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:34.3612869Z Loading safetensors checkpoint shards:  36% Completed | 59/163 [02:04<03:55,  2.26s/it]
2026-02-26T18:53:35.6360767Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:35.6361264Z Loading safetensors checkpoint shards:  37% Completed | 60/163 [02:05<03:22,  1.97s/it]
2026-02-26T18:53:37.1535182Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:37.1535826Z Loading safetensors checkpoint shards:  37% Completed | 61/163 [02:06<03:06,  1.83s/it]
2026-02-26T18:53:40.5775637Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:40.5776215Z Loading safetensors checkpoint shards:  38% Completed | 62/163 [02:10<03:53,  2.31s/it]
2026-02-26T18:53:41.8699277Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:41.8699794Z Loading safetensors checkpoint shards:  39% Completed | 63/163 [02:11<03:20,  2.00s/it]
2026-02-26T18:53:43.3337285Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:43.3337728Z Loading safetensors checkpoint shards:  39% Completed | 64/163 [02:13<03:02,  1.84s/it]
2026-02-26T18:53:46.8810378Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:46.8810919Z Loading safetensors checkpoint shards:  40% Completed | 65/163 [02:16<03:50,  2.35s/it]
2026-02-26T18:53:49.3507185Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:49.3507627Z Loading safetensors checkpoint shards:  40% Completed | 66/163 [02:19<03:51,  2.39s/it]
2026-02-26T18:53:50.4821423Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:50.4821915Z Loading safetensors checkpoint shards:  41% Completed | 67/163 [02:20<03:12,  2.01s/it]
2026-02-26T18:53:53.6015482Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:53.6016340Z Loading safetensors checkpoint shards:  42% Completed | 68/163 [02:23<03:42,  2.34s/it]
2026-02-26T18:53:54.8403155Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:54.8403752Z Loading safetensors checkpoint shards:  42% Completed | 69/163 [02:24<03:09,  2.01s/it]
2026-02-26T18:53:56.2963844Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:56.2964300Z Loading safetensors checkpoint shards:  43% Completed | 70/163 [02:26<02:51,  1.85s/it]
2026-02-26T18:53:59.4109894Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:53:59.4110557Z Loading safetensors checkpoint shards:  44% Completed | 71/163 [02:29<03:24,  2.23s/it]
2026-02-26T18:54:02.4506314Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:02.4507403Z Loading safetensors checkpoint shards:  44% Completed | 72/163 [02:32<03:44,  2.47s/it]
2026-02-26T18:54:03.9589651Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:03.9590251Z Loading safetensors checkpoint shards:  45% Completed | 73/163 [02:33<03:16,  2.18s/it]
2026-02-26T18:54:05.5122604Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:05.5123204Z Loading safetensors checkpoint shards:  45% Completed | 74/163 [02:35<02:57,  1.99s/it]
2026-02-26T18:54:07.1096435Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:07.1097062Z Loading safetensors checkpoint shards:  46% Completed | 75/163 [02:36<02:44,  1.87s/it]
2026-02-26T18:54:08.7840522Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:08.7840982Z Loading safetensors checkpoint shards:  47% Completed | 76/163 [02:38<02:37,  1.81s/it]
2026-02-26T18:54:12.5780756Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:12.5781326Z Loading safetensors checkpoint shards:  47% Completed | 77/163 [02:42<03:27,  2.41s/it]
2026-02-26T18:54:13.7316868Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:13.7317417Z Loading safetensors checkpoint shards:  48% Completed | 78/163 [02:43<02:52,  2.03s/it]
2026-02-26T18:54:15.2600817Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:15.2601314Z Loading safetensors checkpoint shards:  48% Completed | 79/163 [02:45<02:37,  1.88s/it]
2026-02-26T18:54:18.3772092Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:18.3772730Z Loading safetensors checkpoint shards:  49% Completed | 80/163 [02:48<03:06,  2.25s/it]
2026-02-26T18:54:21.4140818Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:21.4141415Z Loading safetensors checkpoint shards:  50% Completed | 81/163 [02:51<03:23,  2.49s/it]
2026-02-26T18:54:22.3984011Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:22.3984546Z Loading safetensors checkpoint shards:  50% Completed | 82/163 [02:52<02:44,  2.04s/it]
2026-02-26T18:54:25.5300669Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:25.5301292Z Loading safetensors checkpoint shards:  51% Completed | 83/163 [02:55<03:09,  2.37s/it]
2026-02-26T18:54:26.9448966Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:26.9449645Z Loading safetensors checkpoint shards:  52% Completed | 84/163 [02:56<02:44,  2.08s/it]
2026-02-26T18:54:30.0108553Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:30.0109027Z Loading safetensors checkpoint shards:  52% Completed | 85/163 [02:59<03:05,  2.38s/it]
2026-02-26T18:54:31.2942963Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:31.2943511Z Loading safetensors checkpoint shards:  53% Completed | 86/163 [03:01<02:37,  2.05s/it]
2026-02-26T18:54:33.3521466Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:33.3521932Z Loading safetensors checkpoint shards:  53% Completed | 87/163 [03:03<02:35,  2.05s/it]
2026-02-26T18:54:36.5243419Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:36.5243881Z Loading safetensors checkpoint shards:  54% Completed | 88/163 [03:06<02:59,  2.39s/it]
2026-02-26T18:54:37.8532202Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:37.8532711Z Loading safetensors checkpoint shards:  55% Completed | 89/163 [03:07<02:33,  2.07s/it]
2026-02-26T18:54:41.0278461Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:41.0278926Z Loading safetensors checkpoint shards:  55% Completed | 90/163 [03:10<02:55,  2.40s/it]
2026-02-26T18:54:42.0911238Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:42.0911860Z Loading safetensors checkpoint shards:  56% Completed | 91/163 [03:11<02:23,  2.00s/it]
2026-02-26T18:54:45.4997376Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:45.4997832Z Loading safetensors checkpoint shards:  56% Completed | 92/163 [03:15<02:51,  2.42s/it]
2026-02-26T18:54:46.6154699Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:46.6155352Z Loading safetensors checkpoint shards:  57% Completed | 93/163 [03:16<02:22,  2.03s/it]
2026-02-26T18:54:48.0380802Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:48.0381304Z Loading safetensors checkpoint shards:  58% Completed | 94/163 [03:17<02:07,  1.85s/it]
2026-02-26T18:54:49.7088407Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:49.7088893Z Loading safetensors checkpoint shards:  58% Completed | 95/163 [03:19<02:02,  1.79s/it]
2026-02-26T18:54:53.5405769Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:53.5406244Z Loading safetensors checkpoint shards:  59% Completed | 96/163 [03:23<02:41,  2.41s/it]
2026-02-26T18:54:54.6181323Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:54.6181782Z Loading safetensors checkpoint shards:  60% Completed | 97/163 [03:24<02:12,  2.01s/it]
2026-02-26T18:54:56.1133172Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:56.1133647Z Loading safetensors checkpoint shards:  60% Completed | 98/163 [03:25<02:00,  1.85s/it]
2026-02-26T18:54:57.5866024Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:57.5866568Z Loading safetensors checkpoint shards:  61% Completed | 99/163 [03:27<01:51,  1.74s/it]
2026-02-26T18:54:59.1917674Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:54:59.1918162Z Loading safetensors checkpoint shards:  61% Completed | 100/163 [03:29<01:47,  1.70s/it]
2026-02-26T18:55:00.5284764Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:00.5285251Z Loading safetensors checkpoint shards:  62% Completed | 101/163 [03:30<01:38,  1.59s/it]
2026-02-26T18:55:03.9996327Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:03.9996813Z Loading safetensors checkpoint shards:  63% Completed | 102/163 [03:33<02:11,  2.15s/it]
2026-02-26T18:55:06.6709425Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:06.6709892Z Loading safetensors checkpoint shards:  63% Completed | 103/163 [03:36<02:18,  2.31s/it]
2026-02-26T18:55:09.6957998Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:09.6958625Z Loading safetensors checkpoint shards:  64% Completed | 104/163 [03:39<02:28,  2.52s/it]
2026-02-26T18:55:10.6288436Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:10.6288913Z Loading safetensors checkpoint shards:  64% Completed | 105/163 [03:40<01:58,  2.05s/it]
2026-02-26T18:55:13.8008514Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:13.8008980Z Loading safetensors checkpoint shards:  65% Completed | 106/163 [03:43<02:15,  2.38s/it]
2026-02-26T18:55:15.0029028Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:15.0029446Z Loading safetensors checkpoint shards:  66% Completed | 107/163 [03:44<01:53,  2.03s/it]
2026-02-26T18:55:17.7293701Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:17.7294208Z Loading safetensors checkpoint shards:  66% Completed | 108/163 [03:47<02:03,  2.24s/it]
2026-02-26T18:55:19.1020926Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:19.1021386Z Loading safetensors checkpoint shards:  67% Completed | 109/163 [03:48<01:46,  1.98s/it]
2026-02-26T18:55:21.9550220Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:21.9550710Z Loading safetensors checkpoint shards:  67% Completed | 110/163 [03:51<01:58,  2.24s/it]
2026-02-26T18:55:25.0039278Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:25.0039753Z Loading safetensors checkpoint shards:  68% Completed | 111/163 [03:54<02:09,  2.48s/it]
2026-02-26T18:55:27.2789316Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:27.2789848Z Loading safetensors checkpoint shards:  69% Completed | 112/163 [03:57<02:03,  2.42s/it]
2026-02-26T18:55:29.8872376Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:29.8873119Z Loading safetensors checkpoint shards:  69% Completed | 113/163 [03:59<02:03,  2.48s/it]
2026-02-26T18:55:31.2737814Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:31.2738410Z Loading safetensors checkpoint shards:  70% Completed | 114/163 [04:01<01:45,  2.15s/it]
2026-02-26T18:55:32.8066973Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:32.8067906Z Loading safetensors checkpoint shards:  71% Completed | 115/163 [04:02<01:34,  1.96s/it]
2026-02-26T18:55:35.9214475Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:35.9215067Z Loading safetensors checkpoint shards:  71% Completed | 116/163 [04:05<01:48,  2.31s/it]
2026-02-26T18:55:37.2269422Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:37.2269992Z Loading safetensors checkpoint shards:  72% Completed | 117/163 [04:07<01:32,  2.01s/it]
2026-02-26T18:55:40.0422712Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:40.0423173Z Loading safetensors checkpoint shards:  72% Completed | 118/163 [04:09<01:41,  2.25s/it]
2026-02-26T18:55:41.3321723Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:41.3322372Z Loading safetensors checkpoint shards:  73% Completed | 119/163 [04:11<01:25,  1.95s/it]
2026-02-26T18:55:42.7801267Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:42.7801738Z Loading safetensors checkpoint shards:  74% Completed | 120/163 [04:12<01:17,  1.81s/it]
2026-02-26T18:55:44.3858300Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:44.3858779Z Loading safetensors checkpoint shards:  74% Completed | 121/163 [04:14<01:13,  1.75s/it]
2026-02-26T18:55:45.9883347Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:45.9883890Z Loading safetensors checkpoint shards:  75% Completed | 122/163 [04:15<01:09,  1.71s/it]
2026-02-26T18:55:50.1770905Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:50.1771559Z Loading safetensors checkpoint shards:  75% Completed | 123/163 [04:19<01:38,  2.45s/it]
2026-02-26T18:55:50.9887829Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:50.9888375Z Loading safetensors checkpoint shards:  76% Completed | 124/163 [04:20<01:16,  1.96s/it]
2026-02-26T18:55:54.0166383Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:54.0166986Z Loading safetensors checkpoint shards:  77% Completed | 125/163 [04:23<01:26,  2.28s/it]
2026-02-26T18:55:55.3555717Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:55.3556415Z Loading safetensors checkpoint shards:  77% Completed | 126/163 [04:25<01:13,  2.00s/it]
2026-02-26T18:55:56.8161811Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:55:56.8162520Z Loading safetensors checkpoint shards:  78% Completed | 127/163 [04:26<01:06,  1.84s/it]
2026-02-26T18:56:00.1807149Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:00.1807766Z Loading safetensors checkpoint shards:  79% Completed | 128/163 [04:29<01:20,  2.29s/it]
2026-02-26T18:56:02.8894250Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:02.8894807Z Loading safetensors checkpoint shards:  79% Completed | 129/163 [04:32<01:22,  2.42s/it]
2026-02-26T18:56:04.1641114Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:04.1641607Z Loading safetensors checkpoint shards:  80% Completed | 130/163 [04:33<01:08,  2.08s/it]
2026-02-26T18:56:05.6799758Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:05.6800253Z Loading safetensors checkpoint shards:  80% Completed | 131/163 [04:35<01:01,  1.91s/it]
2026-02-26T18:56:07.1824019Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:07.1824492Z Loading safetensors checkpoint shards:  81% Completed | 132/163 [04:36<00:55,  1.79s/it]
2026-02-26T18:56:11.6835577Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:11.6836680Z Loading safetensors checkpoint shards:  82% Completed | 133/163 [04:41<01:18,  2.60s/it]
2026-02-26T18:56:12.9068148Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:12.9068770Z Loading safetensors checkpoint shards:  82% Completed | 134/163 [04:42<01:03,  2.19s/it]
2026-02-26T18:56:16.1291466Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:16.1292213Z Loading safetensors checkpoint shards:  83% Completed | 135/163 [04:45<01:09,  2.50s/it]
2026-02-26T18:56:17.2357149Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:17.2357734Z Loading safetensors checkpoint shards:  83% Completed | 136/163 [04:47<00:56,  2.08s/it]
2026-02-26T18:56:18.8430746Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:18.8431238Z Loading safetensors checkpoint shards:  84% Completed | 137/163 [04:48<00:50,  1.94s/it]
2026-02-26T18:56:22.2680725Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:22.2681194Z Loading safetensors checkpoint shards:  85% Completed | 138/163 [04:52<00:59,  2.38s/it]
2026-02-26T18:56:24.5026693Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:24.5027171Z Loading safetensors checkpoint shards:  85% Completed | 139/163 [04:54<00:56,  2.34s/it]
2026-02-26T18:56:25.8300989Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:25.8301468Z Loading safetensors checkpoint shards:  86% Completed | 140/163 [04:55<00:46,  2.04s/it]
2026-02-26T18:56:27.2472638Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:27.2473075Z Loading safetensors checkpoint shards:  87% Completed | 141/163 [04:57<00:40,  1.85s/it]
2026-02-26T18:56:28.9790963Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:28.9791494Z Loading safetensors checkpoint shards:  87% Completed | 142/163 [04:58<00:38,  1.81s/it]
2026-02-26T18:56:32.2705661Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:32.2706232Z Loading safetensors checkpoint shards:  88% Completed | 143/163 [05:02<00:45,  2.26s/it]
2026-02-26T18:56:34.4304465Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:34.4304930Z Loading safetensors checkpoint shards:  88% Completed | 144/163 [05:04<00:42,  2.23s/it]
2026-02-26T18:56:36.1173313Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:36.1173933Z Loading safetensors checkpoint shards:  89% Completed | 145/163 [05:05<00:37,  2.07s/it]
2026-02-26T18:56:38.7598428Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:38.7598844Z Loading safetensors checkpoint shards:  90% Completed | 146/163 [05:08<00:38,  2.24s/it]
2026-02-26T18:56:40.8746923Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:40.8747384Z Loading safetensors checkpoint shards:  90% Completed | 147/163 [05:10<00:35,  2.20s/it]
2026-02-26T18:56:42.3544020Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:42.3544505Z Loading safetensors checkpoint shards:  91% Completed | 148/163 [05:12<00:29,  1.98s/it]
2026-02-26T18:56:43.7949785Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:43.7950276Z Loading safetensors checkpoint shards:  91% Completed | 149/163 [05:13<00:25,  1.82s/it]
2026-02-26T18:56:46.6730034Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:46.6730484Z Loading safetensors checkpoint shards:  93% Completed | 151/163 [05:16<00:19,  1.65s/it]
2026-02-26T18:56:48.2075424Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:48.2075946Z Loading safetensors checkpoint shards:  93% Completed | 152/163 [05:18<00:17,  1.62s/it]
2026-02-26T18:56:49.7212821Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:49.7213286Z Loading safetensors checkpoint shards:  94% Completed | 153/163 [05:19<00:15,  1.59s/it]
2026-02-26T18:56:52.0943646Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:52.0944114Z Loading safetensors checkpoint shards:  94% Completed | 154/163 [05:21<00:16,  1.80s/it]
2026-02-26T18:56:52.9210516Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:52.9211022Z Loading safetensors checkpoint shards:  95% Completed | 155/163 [05:22<00:12,  1.53s/it]
2026-02-26T18:56:54.4621125Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:54.4621622Z Loading safetensors checkpoint shards:  96% Completed | 156/163 [05:24<00:10,  1.53s/it]
2026-02-26T18:56:56.1128164Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:56.1128742Z Loading safetensors checkpoint shards:  96% Completed | 157/163 [05:25<00:09,  1.57s/it]
2026-02-26T18:56:59.7360717Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:56:59.7361304Z Loading safetensors checkpoint shards:  97% Completed | 158/163 [05:29<00:10,  2.17s/it]
2026-02-26T18:57:01.0267395Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:57:01.0268193Z Loading safetensors checkpoint shards:  98% Completed | 159/163 [05:30<00:07,  1.91s/it]
2026-02-26T18:57:02.6159413Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:57:02.6159898Z Loading safetensors checkpoint shards:  98% Completed | 160/163 [05:32<00:05,  1.82s/it]
2026-02-26T18:57:04.2901039Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:57:04.2901491Z Loading safetensors checkpoint shards:  99% Completed | 161/163 [05:34<00:03,  1.77s/it]
2026-02-26T18:57:08.0966750Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:57:08.0967270Z Loading safetensors checkpoint shards:  99% Completed | 162/163 [05:37<00:02,  2.38s/it]
2026-02-26T18:57:09.4327587Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:57:09.4328044Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [05:39<00:00,  2.07s/it]
2026-02-26T18:57:09.4350141Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:57:09.4350556Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [05:39<00:00,  2.08s/it]
2026-02-26T18:57:09.4360588Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:57:09.4563835Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-26 18:57:09 [default_loader.py:291] Loading weights took 339.27 seconds
2026-02-26T18:57:10.2300402Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-26 18:57:10 [default_loader.py:291] Loading weights took 340.01 seconds
2026-02-26T18:57:23.7949695Z INFO 02-26 18:57:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:57:23.7959235Z INFO 02-26 18:57:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:57:23.7971048Z INFO 02-26 18:57:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:57:23.7993010Z INFO 02-26 18:57:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:57:23.7993405Z INFO 02-26 18:57:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:57:23.8002285Z INFO 02-26 18:57:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:57:23.8011630Z INFO 02-26 18:57:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:57:23.8021448Z INFO 02-26 18:57:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:57:23.8031820Z INFO 02-26 18:57:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:57:23.8041779Z INFO 02-26 18:57:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:57:23.8051523Z INFO 02-26 18:57:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:57:23.8062112Z INFO 02-26 18:57:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:57:23.8072322Z INFO 02-26 18:57:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:57:23.8082324Z INFO 02-26 18:57:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:57:23.8091832Z INFO 02-26 18:57:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:57:23.8101259Z INFO 02-26 18:57:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:57:23.8112741Z INFO 02-26 18:57:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:57:23.8121590Z INFO 02-26 18:57:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:57:23.8131458Z INFO 02-26 18:57:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:57:23.8141795Z INFO 02-26 18:57:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:57:23.8151501Z INFO 02-26 18:57:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:57:23.8161692Z INFO 02-26 18:57:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:57:23.8170713Z INFO 02-26 18:57:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:57:23.8180368Z INFO 02-26 18:57:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:57:23.8189634Z INFO 02-26 18:57:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:57:23.8199437Z INFO 02-26 18:57:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:57:23.8209466Z INFO 02-26 18:57:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:57:23.8218875Z INFO 02-26 18:57:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:57:23.8228562Z INFO 02-26 18:57:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:57:23.8238468Z INFO 02-26 18:57:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:57:23.8247234Z INFO 02-26 18:57:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:57:23.8257929Z INFO 02-26 18:57:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:57:23.8266978Z INFO 02-26 18:57:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:57:23.8277325Z INFO 02-26 18:57:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:57:23.8286687Z INFO 02-26 18:57:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:57:23.8296388Z INFO 02-26 18:57:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:57:23.8305973Z INFO 02-26 18:57:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:57:23.8315834Z INFO 02-26 18:57:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:57:23.8325786Z INFO 02-26 18:57:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:57:23.8335568Z INFO 02-26 18:57:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:57:23.8345028Z INFO 02-26 18:57:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:57:23.8354794Z INFO 02-26 18:57:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:57:23.8364655Z INFO 02-26 18:57:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:57:23.8374545Z INFO 02-26 18:57:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:57:23.8383840Z INFO 02-26 18:57:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:57:23.8393637Z INFO 02-26 18:57:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:57:23.8403329Z INFO 02-26 18:57:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:57:23.8412797Z INFO 02-26 18:57:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:57:23.8423081Z INFO 02-26 18:57:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:57:23.8433067Z INFO 02-26 18:57:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:57:23.8443906Z INFO 02-26 18:57:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:57:23.8454000Z INFO 02-26 18:57:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:57:23.8463071Z INFO 02-26 18:57:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:57:23.8473003Z INFO 02-26 18:57:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:57:23.8483066Z INFO 02-26 18:57:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:57:23.8496278Z INFO 02-26 18:57:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:57:23.9424533Z INFO 02-26 18:57:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:57:23.9432363Z INFO 02-26 18:57:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:57:23.9441728Z INFO 02-26 18:57:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:57:23.9450982Z INFO 02-26 18:57:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T18:57:23.9460584Z INFO 02-26 18:57:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T18:57:23.9470399Z INFO 02-26 18:57:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T18:57:23.9518751Z INFO 02-26 18:57:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:57:23.9528384Z INFO 02-26 18:57:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T18:57:49.5232391Z [0;36m(Worker_DP0_TP3_EP3 pid=480)[0;0m WARNING 02-26 18:57:49 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T18:57:49.5242424Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m WARNING 02-26 18:57:49 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T18:57:49.5253809Z [0;36m(Worker_DP0_TP2_EP2 pid=377)[0;0m WARNING 02-26 18:57:49 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T18:57:49.5364205Z [0;36m(Worker_DP1_TP3_EP11 pid=481)[0;0m WARNING 02-26 18:57:49 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T18:57:49.5611165Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m WARNING 02-26 18:57:49 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T18:57:49.5619846Z [0;36m(Worker_DP0_TP1_EP1 pid=258)[0;0m WARNING 02-26 18:57:49 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T18:57:49.5628963Z [0;36m(Worker_DP0_TP4_EP4 pid=584)[0;0m WARNING 02-26 18:57:49 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T18:57:49.5748078Z [0;36m(Worker_DP0_TP5_EP5 pid=688)[0;0m WARNING 02-26 18:57:49 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T18:57:49.5757680Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m WARNING 02-26 18:57:49 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T18:57:49.5875248Z [0;36m(Worker_DP1_TP1_EP9 pid=261)[0;0m WARNING 02-26 18:57:49 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T18:57:49.5885116Z [0;36m(Worker_DP0_TP3_EP3 pid=480)[0;0m INFO 02-26 18:57:49 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T18:57:49.5893946Z [0;36m(Worker_DP0_TP2_EP2 pid=377)[0;0m INFO 02-26 18:57:49 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T18:57:49.5903891Z [0;36m(Worker_DP1_TP6_EP14 pid=795)[0;0m WARNING 02-26 18:57:49 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T18:57:49.5913870Z [0;36m(Worker_DP0_TP6_EP6 pid=792)[0;0m WARNING 02-26 18:57:49 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T18:57:49.5924214Z [0;36m(Worker_DP0_TP7_EP7 pid=896)[0;0m WARNING 02-26 18:57:49 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T18:57:49.5953838Z [0;36m(Worker_DP1_TP5_EP13 pid=691)[0;0m WARNING 02-26 18:57:49 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T18:57:49.5964162Z [0;36m(Worker_DP1_TP4_EP12 pid=587)[0;0m WARNING 02-26 18:57:49 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T18:57:49.6015978Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-26 18:57:49 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T18:57:49.6077329Z [0;36m(Worker_DP1_TP3_EP11 pid=481)[0;0m INFO 02-26 18:57:49 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T18:57:49.6086303Z [0;36m(Worker_DP0_TP1_EP1 pid=258)[0;0m INFO 02-26 18:57:49 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T18:57:49.6131959Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-26 18:57:49 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T18:57:49.6158838Z [0;36m(Worker_DP0_TP4_EP4 pid=584)[0;0m INFO 02-26 18:57:49 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T18:57:49.6185736Z [0;36m(Worker_DP1_TP7_EP15 pid=899)[0;0m WARNING 02-26 18:57:49 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T18:57:49.6211858Z [0;36m(Worker_DP0_TP5_EP5 pid=688)[0;0m INFO 02-26 18:57:49 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T18:57:49.6293182Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-26 18:57:49 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T18:57:49.6379606Z [0;36m(Worker_DP1_TP1_EP9 pid=261)[0;0m INFO 02-26 18:57:49 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T18:57:49.6432943Z [0;36m(Worker_DP0_TP7_EP7 pid=896)[0;0m INFO 02-26 18:57:49 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T18:57:49.6457157Z [0;36m(Worker_DP1_TP6_EP14 pid=795)[0;0m INFO 02-26 18:57:49 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T18:57:49.6479901Z [0;36m(Worker_DP1_TP5_EP13 pid=691)[0;0m INFO 02-26 18:57:49 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T18:57:49.6568537Z [0;36m(Worker_DP0_TP6_EP6 pid=792)[0;0m INFO 02-26 18:57:49 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T18:57:49.6578347Z [0;36m(Worker_DP1_TP4_EP12 pid=587)[0;0m INFO 02-26 18:57:49 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T18:57:49.6768906Z [0;36m(Worker_DP1_TP7_EP15 pid=899)[0;0m INFO 02-26 18:57:49 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T18:57:49.7942770Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:57:49.7944998Z Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
2026-02-26T18:57:50.5181520Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:57:50.5182263Z Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<01:57,  1.38it/s]
2026-02-26T18:57:51.3031008Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:57:51.3031550Z Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:01<02:01,  1.32it/s]
2026-02-26T18:57:52.1686016Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:57:52.1686436Z Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:02<02:09,  1.24it/s]
2026-02-26T18:57:53.0148110Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:57:53.0148602Z Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:03<02:10,  1.21it/s]
2026-02-26T18:57:53.7834617Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:57:53.7835111Z Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:03<02:06,  1.24it/s]
2026-02-26T18:57:54.3866272Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:57:54.3867166Z Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:04<01:55,  1.36it/s]
2026-02-26T18:57:54.9799051Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:57:54.9799512Z Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:05<01:47,  1.45it/s]
2026-02-26T18:57:55.4903353Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:57:55.4903824Z Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:05<01:37,  1.58it/s]
2026-02-26T18:57:56.2413301Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:57:56.2413801Z Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:06<01:43,  1.49it/s]
2026-02-26T18:57:57.2853738Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:57:57.2854179Z Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:07<02:00,  1.27it/s]
2026-02-26T18:57:58.4339783Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:57:58.4340336Z Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:08<02:16,  1.12it/s]
2026-02-26T18:57:59.5609625Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:57:59.5610290Z Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:09<02:25,  1.04it/s]
2026-02-26T18:58:00.7526620Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:00.7527245Z Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:10<02:35,  1.03s/it]
2026-02-26T18:58:01.9012690Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:01.9013304Z Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:12<02:39,  1.07s/it]
2026-02-26T18:58:02.9245302Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:02.9245929Z Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:13<02:36,  1.06s/it]
2026-02-26T18:58:03.9392970Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:03.9393494Z Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:14<02:33,  1.04s/it]
2026-02-26T18:58:04.9545570Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:04.9546162Z Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:15<02:31,  1.03s/it]
2026-02-26T18:58:05.9168509Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:05.9168987Z Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:16<02:26,  1.01s/it]
2026-02-26T18:58:07.0140889Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:07.0141320Z Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:17<02:29,  1.04s/it]
2026-02-26T18:58:08.1380638Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:08.1381154Z Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:18<02:32,  1.06s/it]
2026-02-26T18:58:09.2618410Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:09.2618871Z Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:19<02:33,  1.08s/it]
2026-02-26T18:58:10.4174949Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:10.4175448Z Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:20<02:35,  1.10s/it]
2026-02-26T18:58:11.5091093Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:11.5091591Z Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:21<02:34,  1.10s/it]
2026-02-26T18:58:12.6054937Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:12.6055491Z Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:22<02:32,  1.10s/it]
2026-02-26T18:58:13.6701365Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:13.6701873Z Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:23<02:30,  1.09s/it]
2026-02-26T18:58:14.9284239Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:14.9284848Z Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:25<02:36,  1.14s/it]
2026-02-26T18:58:16.1355832Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:17.2205773Z Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:26<02:37,  1.16s/it]
2026-02-26T18:58:17.2206970Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:17.2207489Z Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:27<02:33,  1.14s/it]
2026-02-26T18:58:18.3459923Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:18.3460436Z Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:28<02:31,  1.13s/it]
2026-02-26T18:58:19.4375632Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:19.4376089Z Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:29<02:29,  1.12s/it]
2026-02-26T18:58:20.6214533Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:20.6215468Z Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:30<02:30,  1.14s/it]
2026-02-26T18:58:21.7754163Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:21.7754642Z Loading safetensors checkpoint shards:  20% Completed | 32/163 [00:31<02:29,  1.14s/it]
2026-02-26T18:58:22.9485152Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:22.9485630Z Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:33<02:29,  1.15s/it]
2026-02-26T18:58:24.0798707Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:24.0799136Z Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:34<02:27,  1.15s/it]
2026-02-26T18:58:25.1954646Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:25.1955119Z Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:35<02:25,  1.14s/it]
2026-02-26T18:58:26.2138787Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:26.2139222Z Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:36<02:19,  1.10s/it]
2026-02-26T18:58:27.3540024Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:27.3540596Z Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:37<02:20,  1.11s/it]
2026-02-26T18:58:28.4877480Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:28.4878081Z Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:38<02:19,  1.12s/it]
2026-02-26T18:58:29.7104848Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:29.7105270Z Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:39<02:22,  1.15s/it]
2026-02-26T18:58:30.8150780Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:30.8151242Z Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:41<02:19,  1.14s/it]
2026-02-26T18:58:31.9509309Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:31.9509753Z Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:42<02:18,  1.14s/it]
2026-02-26T18:58:33.1029388Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:33.1029942Z Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:43<02:18,  1.14s/it]
2026-02-26T18:58:34.1551287Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:34.1551733Z Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:44<02:13,  1.11s/it]
2026-02-26T18:58:35.2458987Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:35.2459530Z Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:45<02:11,  1.11s/it]
2026-02-26T18:58:36.4762529Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:36.4762977Z Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:46<02:15,  1.14s/it]
2026-02-26T18:58:37.6971016Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:37.6971522Z Loading safetensors checkpoint shards:  28% Completed | 46/163 [00:47<02:16,  1.17s/it]
2026-02-26T18:58:38.4582482Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:38.4583101Z Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:48<02:01,  1.04s/it]
2026-02-26T18:58:39.6301420Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:39.6301973Z Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:49<01:35,  1.20it/s]
2026-02-26T18:58:40.7990659Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:40.7991321Z Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:51<01:43,  1.09it/s]
2026-02-26T18:58:42.0340981Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:42.0341695Z Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:52<01:51,  1.00it/s]
2026-02-26T18:58:43.1243879Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:43.1244432Z Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:53<01:53,  1.02s/it]
2026-02-26T18:58:44.1627998Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:44.1628623Z Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:54<01:53,  1.03s/it]
2026-02-26T18:58:45.2355129Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:45.2356103Z Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:55<01:53,  1.04s/it]
2026-02-26T18:58:46.4526406Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:46.4526838Z Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:56<01:57,  1.09s/it]
2026-02-26T18:58:47.6322286Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:47.6322780Z Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:57<01:59,  1.12s/it]
2026-02-26T18:58:48.8433920Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:48.8434385Z Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:59<02:01,  1.15s/it]
2026-02-26T18:58:49.9700313Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:49.9700807Z Loading safetensors checkpoint shards:  36% Completed | 58/163 [01:00<01:59,  1.14s/it]
2026-02-26T18:58:51.0656413Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:51.0656882Z Loading safetensors checkpoint shards:  36% Completed | 59/163 [01:01<01:57,  1.13s/it]
2026-02-26T18:58:52.2175125Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:52.2175624Z Loading safetensors checkpoint shards:  37% Completed | 60/163 [01:02<01:56,  1.13s/it]
2026-02-26T18:58:53.3432523Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:53.3433013Z Loading safetensors checkpoint shards:  37% Completed | 61/163 [01:03<01:55,  1.13s/it]
2026-02-26T18:58:54.4307361Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:54.4307837Z Loading safetensors checkpoint shards:  38% Completed | 62/163 [01:04<01:52,  1.12s/it]
2026-02-26T18:58:55.5956871Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:55.5957412Z Loading safetensors checkpoint shards:  39% Completed | 63/163 [01:05<01:53,  1.13s/it]
2026-02-26T18:58:56.8387045Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:56.8387788Z Loading safetensors checkpoint shards:  39% Completed | 64/163 [01:07<01:55,  1.17s/it]
2026-02-26T18:58:57.8656156Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:57.8656647Z Loading safetensors checkpoint shards:  40% Completed | 65/163 [01:08<01:50,  1.12s/it]
2026-02-26T18:58:58.9670008Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:58:58.9670497Z Loading safetensors checkpoint shards:  40% Completed | 66/163 [01:09<01:48,  1.12s/it]
2026-02-26T18:59:00.0310584Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:00.0311032Z Loading safetensors checkpoint shards:  41% Completed | 67/163 [01:10<01:45,  1.10s/it]
2026-02-26T18:59:01.0831115Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:01.0831618Z Loading safetensors checkpoint shards:  42% Completed | 68/163 [01:11<01:43,  1.09s/it]
2026-02-26T18:59:02.2220659Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:02.2221113Z Loading safetensors checkpoint shards:  42% Completed | 69/163 [01:12<01:43,  1.10s/it]
2026-02-26T18:59:03.4628649Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:03.4629174Z Loading safetensors checkpoint shards:  43% Completed | 70/163 [01:13<01:46,  1.14s/it]
2026-02-26T18:59:04.6491592Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:04.6492165Z Loading safetensors checkpoint shards:  44% Completed | 71/163 [01:14<01:46,  1.16s/it]
2026-02-26T18:59:05.7536464Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:05.7537310Z Loading safetensors checkpoint shards:  44% Completed | 72/163 [01:15<01:43,  1.14s/it]
2026-02-26T18:59:06.8860254Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:06.8860745Z Loading safetensors checkpoint shards:  45% Completed | 73/163 [01:17<01:42,  1.14s/it]
2026-02-26T18:59:08.0149836Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:08.0150380Z Loading safetensors checkpoint shards:  45% Completed | 74/163 [01:18<01:41,  1.14s/it]
2026-02-26T18:59:09.1416492Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:09.1417036Z Loading safetensors checkpoint shards:  46% Completed | 75/163 [01:19<01:39,  1.13s/it]
2026-02-26T18:59:10.3263381Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:10.3263996Z Loading safetensors checkpoint shards:  47% Completed | 76/163 [01:20<01:39,  1.15s/it]
2026-02-26T18:59:11.3790926Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:11.3791421Z Loading safetensors checkpoint shards:  47% Completed | 77/163 [01:21<01:36,  1.12s/it]
2026-02-26T18:59:12.5503761Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:12.5504194Z Loading safetensors checkpoint shards:  48% Completed | 78/163 [01:22<01:36,  1.14s/it]
2026-02-26T18:59:13.7277954Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:13.7278413Z Loading safetensors checkpoint shards:  48% Completed | 79/163 [01:23<01:36,  1.15s/it]
2026-02-26T18:59:14.9596432Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:14.9596897Z Loading safetensors checkpoint shards:  49% Completed | 80/163 [01:25<01:37,  1.17s/it]
2026-02-26T18:59:16.0713252Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:16.0713725Z Loading safetensors checkpoint shards:  50% Completed | 81/163 [01:26<01:34,  1.15s/it]
2026-02-26T18:59:17.2183855Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:17.2184347Z Loading safetensors checkpoint shards:  50% Completed | 82/163 [01:27<01:33,  1.15s/it]
2026-02-26T18:59:18.3963491Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:18.3963971Z Loading safetensors checkpoint shards:  51% Completed | 83/163 [01:28<01:32,  1.16s/it]
2026-02-26T18:59:19.6215602Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:19.6216196Z Loading safetensors checkpoint shards:  52% Completed | 84/163 [01:29<01:33,  1.18s/it]
2026-02-26T18:59:20.8331868Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:20.8332640Z Loading safetensors checkpoint shards:  52% Completed | 85/163 [01:31<01:32,  1.19s/it]
2026-02-26T18:59:22.0106093Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:22.0106673Z Loading safetensors checkpoint shards:  53% Completed | 86/163 [01:32<01:31,  1.19s/it]
2026-02-26T18:59:23.2438408Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:23.2439125Z Loading safetensors checkpoint shards:  53% Completed | 87/163 [01:33<01:31,  1.20s/it]
2026-02-26T18:59:24.3933443Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:24.3934053Z Loading safetensors checkpoint shards:  54% Completed | 88/163 [01:34<01:28,  1.18s/it]
2026-02-26T18:59:25.5476164Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:25.5476747Z Loading safetensors checkpoint shards:  55% Completed | 89/163 [01:35<01:26,  1.18s/it]
2026-02-26T18:59:26.6324421Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:26.6324847Z Loading safetensors checkpoint shards:  55% Completed | 90/163 [01:36<01:23,  1.15s/it]
2026-02-26T18:59:27.7026504Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:27.7027007Z Loading safetensors checkpoint shards:  56% Completed | 91/163 [01:37<01:20,  1.12s/it]
2026-02-26T18:59:28.8001612Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:28.8002214Z Loading safetensors checkpoint shards:  56% Completed | 92/163 [01:39<01:19,  1.12s/it]
2026-02-26T18:59:29.9376128Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:29.9376587Z Loading safetensors checkpoint shards:  57% Completed | 93/163 [01:40<01:18,  1.12s/it]
2026-02-26T18:59:31.1098712Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:31.1099243Z Loading safetensors checkpoint shards:  58% Completed | 94/163 [01:41<01:18,  1.14s/it]
2026-02-26T18:59:32.2092423Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:32.2092917Z Loading safetensors checkpoint shards:  58% Completed | 95/163 [01:42<01:16,  1.13s/it]
2026-02-26T18:59:33.3780765Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:33.3781189Z Loading safetensors checkpoint shards:  59% Completed | 96/163 [01:43<01:16,  1.14s/it]
2026-02-26T18:59:34.5348677Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:34.5349790Z Loading safetensors checkpoint shards:  60% Completed | 97/163 [01:44<01:15,  1.14s/it]
2026-02-26T18:59:35.6836442Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:35.6836916Z Loading safetensors checkpoint shards:  60% Completed | 98/163 [01:45<01:14,  1.15s/it]
2026-02-26T18:59:36.8030487Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:36.8031152Z Loading safetensors checkpoint shards:  61% Completed | 99/163 [01:47<01:12,  1.14s/it]
2026-02-26T18:59:38.0950741Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:38.0951261Z Loading safetensors checkpoint shards:  61% Completed | 100/163 [01:48<01:14,  1.18s/it]
2026-02-26T18:59:39.0903941Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:39.0904376Z Loading safetensors checkpoint shards:  62% Completed | 101/163 [01:49<01:09,  1.13s/it]
2026-02-26T18:59:40.1921625Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:40.1922163Z Loading safetensors checkpoint shards:  63% Completed | 102/163 [01:50<01:08,  1.12s/it]
2026-02-26T18:59:41.3427222Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:41.3427859Z Loading safetensors checkpoint shards:  63% Completed | 103/163 [01:51<01:07,  1.13s/it]
2026-02-26T18:59:42.5729495Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:42.5730021Z Loading safetensors checkpoint shards:  64% Completed | 104/163 [01:52<01:08,  1.16s/it]
2026-02-26T18:59:43.8760871Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:43.8761325Z Loading safetensors checkpoint shards:  64% Completed | 105/163 [01:54<01:09,  1.20s/it]
2026-02-26T18:59:45.0795922Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:45.0796454Z Loading safetensors checkpoint shards:  65% Completed | 106/163 [01:55<01:08,  1.20s/it]
2026-02-26T18:59:47.7207450Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:47.7208101Z Loading safetensors checkpoint shards:  66% Completed | 107/163 [01:57<01:31,  1.63s/it]
2026-02-26T18:59:48.8601946Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:48.8602549Z Loading safetensors checkpoint shards:  66% Completed | 108/163 [01:59<01:21,  1.49s/it]
2026-02-26T18:59:50.0418709Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:50.0419198Z Loading safetensors checkpoint shards:  67% Completed | 109/163 [02:00<01:15,  1.39s/it]
2026-02-26T18:59:51.2990017Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:51.2990729Z Loading safetensors checkpoint shards:  67% Completed | 110/163 [02:01<01:11,  1.35s/it]
2026-02-26T18:59:52.4687527Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:52.4688109Z Loading safetensors checkpoint shards:  68% Completed | 111/163 [02:02<01:07,  1.30s/it]
2026-02-26T18:59:53.5875264Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:53.5875805Z Loading safetensors checkpoint shards:  69% Completed | 112/163 [02:03<01:03,  1.24s/it]
2026-02-26T18:59:54.7299327Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:54.7299960Z Loading safetensors checkpoint shards:  69% Completed | 113/163 [02:04<01:00,  1.21s/it]
2026-02-26T18:59:54.8616765Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:54.8617252Z Loading safetensors checkpoint shards:  71% Completed | 116/163 [02:05<00:26,  1.80it/s]
2026-02-26T18:59:54.9688345Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:54.9689081Z Loading safetensors checkpoint shards:  73% Completed | 119/163 [02:05<00:14,  3.09it/s]
2026-02-26T18:59:55.0708684Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:55.0709104Z Loading safetensors checkpoint shards:  75% Completed | 122/163 [02:05<00:08,  4.73it/s]
2026-02-26T18:59:55.1738779Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:55.1739351Z Loading safetensors checkpoint shards:  77% Completed | 125/163 [02:05<00:05,  6.75it/s]
2026-02-26T18:59:55.2781976Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:55.2782515Z Loading safetensors checkpoint shards:  79% Completed | 128/163 [02:05<00:03,  9.12it/s]
2026-02-26T18:59:55.3807189Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:55.3808004Z Loading safetensors checkpoint shards:  81% Completed | 132/163 [02:05<00:02, 13.01it/s]
2026-02-26T18:59:55.4820094Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:55.4820570Z Loading safetensors checkpoint shards:  83% Completed | 135/163 [02:05<00:01, 15.59it/s]
2026-02-26T18:59:55.5837886Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:55.5838461Z Loading safetensors checkpoint shards:  85% Completed | 138/163 [02:05<00:01, 18.12it/s]
2026-02-26T18:59:55.6847537Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:55.6847969Z Loading safetensors checkpoint shards:  87% Completed | 141/163 [02:05<00:01, 20.50it/s]
2026-02-26T18:59:55.8179741Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:55.8180238Z Loading safetensors checkpoint shards:  89% Completed | 145/163 [02:06<00:00, 23.17it/s]
2026-02-26T18:59:55.9181897Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:55.9182552Z Loading safetensors checkpoint shards:  91% Completed | 148/163 [02:06<00:00, 24.72it/s]
2026-02-26T18:59:59.3798172Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:59.3798805Z Loading safetensors checkpoint shards:  93% Completed | 151/163 [02:09<00:04,  2.79it/s]
2026-02-26T18:59:59.4826082Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:59.4826544Z Loading safetensors checkpoint shards:  94% Completed | 154/163 [02:09<00:02,  3.78it/s]
2026-02-26T18:59:59.5857500Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:59.5857956Z Loading safetensors checkpoint shards:  96% Completed | 157/163 [02:09<00:01,  5.07it/s]
2026-02-26T18:59:59.6891030Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:59.6891667Z Loading safetensors checkpoint shards:  98% Completed | 160/163 [02:09<00:00,  6.71it/s]
2026-02-26T18:59:59.7761671Z [0;36m(Worker_DP1_TP6_EP14 pid=795)[0;0m INFO 02-26 18:59:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T18:59:59.7886533Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:59.7886990Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [02:09<00:00,  1.25it/s]
2026-02-26T18:59:59.7894946Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T18:59:59.8126718Z [0;36m(Worker_DP0_TP6_EP6 pid=792)[0;0m INFO 02-26 18:59:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T18:59:59.8154638Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-26 18:59:59 [default_loader.py:291] Loading weights took 130.02 seconds
2026-02-26T18:59:59.8304143Z [0;36m(Worker_DP0_TP2_EP2 pid=377)[0;0m INFO 02-26 18:59:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T18:59:59.8362273Z [0;36m(Worker_DP0_TP1_EP1 pid=258)[0;0m INFO 02-26 18:59:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T18:59:59.8527258Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-26 18:59:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T18:59:59.8666993Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-26 18:59:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T18:59:59.8699032Z [0;36m(Worker_DP1_TP7_EP15 pid=899)[0;0m INFO 02-26 18:59:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T18:59:59.8735753Z [0;36m(Worker_DP1_TP1_EP9 pid=261)[0;0m INFO 02-26 18:59:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T18:59:59.8795214Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-26 18:59:59 [default_loader.py:291] Loading weights took 130.17 seconds
2026-02-26T18:59:59.8819641Z [0;36m(Worker_DP0_TP3_EP3 pid=480)[0;0m INFO 02-26 18:59:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T18:59:59.8875330Z [0;36m(Worker_DP1_TP5_EP13 pid=691)[0;0m INFO 02-26 18:59:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T18:59:59.9120799Z [0;36m(Worker_DP0_TP4_EP4 pid=584)[0;0m INFO 02-26 18:59:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T18:59:59.9146405Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-26 18:59:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T18:59:59.9204320Z [0;36m(Worker_DP0_TP7_EP7 pid=896)[0;0m INFO 02-26 18:59:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T18:59:59.9445458Z [0;36m(Worker_DP0_TP5_EP5 pid=688)[0;0m INFO 02-26 18:59:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T18:59:59.9916174Z [0;36m(Worker_DP1_TP4_EP12 pid=587)[0;0m INFO 02-26 18:59:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T19:00:00.0382712Z [0;36m(Worker_DP1_TP3_EP11 pid=481)[0;0m INFO 02-26 19:00:00 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T19:00:00.6755846Z [0;36m(Worker_DP0_TP3_EP3 pid=480)[0;0m INFO 02-26 19:00:00 [model_runner_v1.py:2353] Loading model weights took 30.8108 GB
2026-02-26T19:00:01.0322338Z [0;36m(Worker_DP0_TP7_EP7 pid=896)[0;0m INFO 02-26 19:00:01 [model_runner_v1.py:2353] Loading model weights took 30.8108 GB
2026-02-26T19:00:01.4874142Z [0;36m(Worker_DP0_TP5_EP5 pid=688)[0;0m INFO 02-26 19:00:01 [model_runner_v1.py:2353] Loading model weights took 30.8108 GB
2026-02-26T19:00:01.6095604Z [0;36m(Worker_DP0_TP2_EP2 pid=377)[0;0m INFO 02-26 19:00:01 [model_runner_v1.py:2353] Loading model weights took 30.8108 GB
2026-02-26T19:00:01.6834469Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-26 19:00:01 [model_runner_v1.py:2353] Loading model weights took 30.8108 GB
2026-02-26T19:00:01.7055786Z [0;36m(Worker_DP0_TP6_EP6 pid=792)[0;0m INFO 02-26 19:00:01 [model_runner_v1.py:2353] Loading model weights took 30.8108 GB
2026-02-26T19:00:01.7551170Z [0;36m(Worker_DP1_TP6_EP14 pid=795)[0;0m INFO 02-26 19:00:01 [model_runner_v1.py:2353] Loading model weights took 30.8108 GB
2026-02-26T19:00:01.9199278Z [0;36m(Worker_DP1_TP5_EP13 pid=691)[0;0m INFO 02-26 19:00:01 [model_runner_v1.py:2353] Loading model weights took 30.8108 GB
2026-02-26T19:00:01.9987479Z [0;36m(Worker_DP0_TP1_EP1 pid=258)[0;0m INFO 02-26 19:00:01 [model_runner_v1.py:2353] Loading model weights took 30.8108 GB
2026-02-26T19:00:02.0620513Z [0;36m(Worker_DP1_TP1_EP9 pid=261)[0;0m INFO 02-26 19:00:02 [model_runner_v1.py:2353] Loading model weights took 30.8108 GB
2026-02-26T19:00:02.0705140Z [0;36m(Worker_DP1_TP7_EP15 pid=899)[0;0m INFO 02-26 19:00:02 [model_runner_v1.py:2353] Loading model weights took 30.8108 GB
2026-02-26T19:00:02.1651825Z [0;36m(Worker_DP1_TP3_EP11 pid=481)[0;0m INFO 02-26 19:00:02 [model_runner_v1.py:2353] Loading model weights took 30.8108 GB
2026-02-26T19:00:02.2919574Z [0;36m(Worker_DP0_TP4_EP4 pid=584)[0;0m INFO 02-26 19:00:02 [model_runner_v1.py:2353] Loading model weights took 30.8108 GB
2026-02-26T19:00:02.3114490Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-26 19:00:02 [model_runner_v1.py:2353] Loading model weights took 30.8108 GB
2026-02-26T19:00:02.4108876Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-26 19:00:02 [model_runner_v1.py:2353] Loading model weights took 30.8108 GB
2026-02-26T19:00:02.6342474Z [0;36m(Worker_DP1_TP4_EP12 pid=587)[0;0m INFO 02-26 19:00:02 [model_runner_v1.py:2353] Loading model weights took 30.8108 GB
2026-02-26T19:00:07.9602214Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-26 19:00:07 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/3cf8167aee/rank_0_0/backbone for vLLM's torch.compile
2026-02-26T19:00:07.9693391Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-26 19:00:07 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/3cf8167aee/rank_0_1/backbone for vLLM's torch.compile
2026-02-26T19:00:07.9724863Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-26 19:00:07 [backends.py:865] Dynamo bytecode transform time: 4.87 s
2026-02-26T19:00:07.9819657Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-26 19:00:07 [backends.py:865] Dynamo bytecode transform time: 4.88 s
2026-02-26T19:00:15.0065346Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T19:00:15.0066844Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m   warnings.warn(
2026-02-26T19:00:15.0145422Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T19:00:15.0153287Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m   warnings.warn(
2026-02-26T19:00:15.0194422Z [0;36m(Worker_DP1_TP6_EP14 pid=795)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T19:00:15.0209152Z [0;36m(Worker_DP1_TP6_EP14 pid=795)[0;0m   warnings.warn(
2026-02-26T19:00:15.0258257Z [0;36m(Worker_DP1_TP4_EP12 pid=587)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T19:00:15.0274392Z [0;36m(Worker_DP1_TP4_EP12 pid=587)[0;0m   warnings.warn(
2026-02-26T19:00:15.0288772Z [0;36m(Worker_DP0_TP3_EP3 pid=480)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T19:00:15.0298752Z [0;36m(Worker_DP0_TP3_EP3 pid=480)[0;0m   warnings.warn(
2026-02-26T19:00:15.0320767Z [0;36m(Worker_DP1_TP5_EP13 pid=691)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T19:00:15.0328433Z [0;36m(Worker_DP1_TP5_EP13 pid=691)[0;0m   warnings.warn(
2026-02-26T19:00:15.0430495Z [0;36m(Worker_DP0_TP4_EP4 pid=584)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T19:00:15.0438301Z [0;36m(Worker_DP0_TP4_EP4 pid=584)[0;0m   warnings.warn(
2026-02-26T19:00:15.0450332Z [0;36m(Worker_DP0_TP2_EP2 pid=377)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T19:00:15.0458775Z [0;36m(Worker_DP0_TP2_EP2 pid=377)[0;0m   warnings.warn(
2026-02-26T19:00:15.0471052Z [0;36m(Worker_DP0_TP1_EP1 pid=258)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T19:00:15.0480504Z [0;36m(Worker_DP0_TP1_EP1 pid=258)[0;0m   warnings.warn(
2026-02-26T19:00:15.0491656Z [0;36m(Worker_DP0_TP5_EP5 pid=688)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T19:00:15.0500359Z [0;36m(Worker_DP0_TP5_EP5 pid=688)[0;0m   warnings.warn(
2026-02-26T19:00:15.0564061Z [0;36m(Worker_DP1_TP1_EP9 pid=261)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T19:00:15.0575828Z [0;36m(Worker_DP1_TP1_EP9 pid=261)[0;0m   warnings.warn(
2026-02-26T19:00:15.0631065Z [0;36m(Worker_DP0_TP6_EP6 pid=792)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T19:00:15.0640017Z [0;36m(Worker_DP0_TP6_EP6 pid=792)[0;0m   warnings.warn(
2026-02-26T19:00:15.0841941Z [0;36m(Worker_DP0_TP7_EP7 pid=896)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T19:00:15.0852165Z [0;36m(Worker_DP0_TP7_EP7 pid=896)[0;0m   warnings.warn(
2026-02-26T19:00:15.0869422Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T19:00:15.0937638Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m   warnings.warn(
2026-02-26T19:00:15.1032965Z [0;36m(Worker_DP1_TP3_EP11 pid=481)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T19:00:15.1044828Z [0;36m(Worker_DP1_TP3_EP11 pid=481)[0;0m   warnings.warn(
2026-02-26T19:00:15.1585912Z [0;36m(Worker_DP1_TP7_EP15 pid=899)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T19:00:15.1593114Z [0;36m(Worker_DP1_TP7_EP15 pid=899)[0;0m   warnings.warn(
2026-02-26T19:00:27.7778013Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-26 19:00:27 [backends.py:319] Compiling a graph for compile range (1, 4096) takes 12.78 s
2026-02-26T19:00:27.7787112Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-26 19:00:27 [monitor.py:34] torch.compile takes 17.65 s in total
2026-02-26T19:00:28.2996054Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-26 19:00:28 [backends.py:319] Compiling a graph for compile range (1, 4096) takes 13.21 s
2026-02-26T19:00:28.3009693Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-26 19:00:28 [monitor.py:34] torch.compile takes 18.10 s in total
2026-02-26T19:00:35.1257776Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-26 19:00:35 [worker.py:359] Available KV cache memory: 16.98 GiB
2026-02-26T19:00:35.2315829Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-26 19:00:35 [worker.py:359] Available KV cache memory: 16.98 GiB
2026-02-26T19:00:35.7880512Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-26 19:00:35 [kv_cache_utils.py:1307] GPU KV cache size: 208,768 tokens
2026-02-26T19:00:35.7894043Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-26 19:00:35 [kv_cache_utils.py:1312] Maximum concurrency for 68,000 tokens per request: 3.07x
2026-02-26T19:00:35.9260103Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-26 19:00:35 [kv_cache_utils.py:1307] GPU KV cache size: 208,768 tokens
2026-02-26T19:00:35.9267342Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-26 19:00:35 [kv_cache_utils.py:1312] Maximum concurrency for 68,000 tokens per request: 3.07x
2026-02-26T19:00:55.6758065Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m 
2026-02-26T19:00:55.6758933Z Capturing CUDA graphs (decode, FULL):   0%|          | 0/6 [00:00<?, ?it/s][rank4]:[W226 19:00:55.628287716 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T19:00:55.6779503Z [rank6]:[W226 19:00:55.628916390 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T19:00:55.6791091Z [rank5]:[W226 19:00:55.629347733 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T19:00:55.6803281Z [rank7]:[W226 19:00:55.630019218 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T19:00:55.6814688Z [rank2]:[W226 19:00:55.630440080 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T19:00:55.6825466Z [rank0]:[W226 19:00:55.631078445 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T19:00:55.6836130Z [rank1]:[W226 19:00:55.631425237 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T19:00:55.6847283Z [rank3]:[W226 19:00:55.631506177 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T19:00:55.6857549Z [rank12]:[W226 19:00:55.638604695 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T19:00:55.6868833Z [rank9]:[W226 19:00:55.639704262 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T19:00:55.6893356Z [rank10]:[W226 19:00:55.640837180 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T19:00:55.6903881Z [rank8]:[W226 19:00:55.640996301 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T19:00:55.6915071Z [rank13]:[W226 19:00:55.641575435 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T19:00:55.6924933Z [rank14]:[W226 19:00:55.642242719 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T19:00:55.6935418Z [rank11]:[W226 19:00:55.642390730 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T19:00:55.6945051Z [rank15]:[W226 19:00:55.642394670 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T19:00:56.1037411Z 
2026-02-26T19:00:56.1038974Z Capturing CUDA graphs (decode, FULL):  17%|â–ˆâ–‹        | 1/6 [00:17<01:27, 17.53s/it][rank15]:[W226 19:00:56.056919864 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T19:00:56.1044725Z [rank4]:[W226 19:00:56.056923954 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T19:00:56.1054643Z [rank3]:[W226 19:00:56.057183876 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T19:00:56.1064640Z [rank10]:[W226 19:00:56.057563168 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T19:00:56.1074781Z [rank13]:[W226 19:00:56.057591518 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T19:00:56.1085309Z [rank12]:[W226 19:00:56.057992061 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T19:00:56.1094969Z [rank7]:[W226 19:00:56.058030761 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T19:00:56.1104861Z [rank11]:[W226 19:00:56.058291413 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T19:00:56.1115182Z [rank5]:[W226 19:00:56.058330083 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T19:00:56.1126143Z [rank6]:[W226 19:00:56.058464444 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T19:00:56.1136253Z [rank9]:[W226 19:00:56.058791036 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T19:00:56.1146102Z [rank2]:[W226 19:00:56.059187559 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T19:00:56.1156346Z [rank0]:[W226 19:00:56.059535761 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T19:00:56.1167651Z [rank8]:[W226 19:00:56.059535701 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T19:00:56.1177550Z [rank14]:[W226 19:00:56.060256316 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T19:00:56.1187187Z [rank1]:[W226 19:00:56.060298766 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T19:01:13.6719982Z 
2026-02-26T19:01:13.6721042Z Capturing CUDA graphs (decode, FULL):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:20<00:35,  8.96s/it]
2026-02-26T19:01:13.6722221Z Capturing CUDA graphs (decode, FULL):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:23<00:18,  6.25s/it]
2026-02-26T19:01:13.6722770Z Capturing CUDA graphs (decode, FULL):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:26<00:09,  4.95s/it]
2026-02-26T19:01:13.6723258Z Capturing CUDA graphs (decode, FULL):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:29<00:04,  4.25s/it]
2026-02-26T19:01:13.6723799Z Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:35<00:00,  4.85s/it]
2026-02-26T19:01:13.6724256Z Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:35<00:00,  5.92s/it]
2026-02-26T19:01:14.1303767Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-26 19:01:14 [gpu_model_runner.py:5051] Graph capturing finished in 36 secs, took 0.33 GiB
2026-02-26T19:01:14.1386700Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-26 19:01:14 [gpu_model_runner.py:5051] Graph capturing finished in 36 secs, took 0.33 GiB
2026-02-26T19:01:14.1783154Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-26 19:01:14 [core.py:272] init engine (profile, create kv cache, warmup model) took 71.54 seconds
2026-02-26T19:01:14.1886533Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-26 19:01:14 [core.py:272] init engine (profile, create kv cache, warmup model) took 71.78 seconds
2026-02-26T19:01:15.1017948Z INFO 02-26 19:01:15 [coordinator.py:200] All engine subscriptions received by DP coordinator
2026-02-26T19:01:15.1063128Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 19:01:15 [loggers.py:1219] AsyncLLM created with api_server_count more than 1; disabling stats logging to avoid incomplete stats.
2026-02-26T19:01:15.1064056Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 19:01:15 [loggers.py:1219] AsyncLLM created with api_server_count more than 1; disabling stats logging to avoid incomplete stats.
2026-02-26T19:01:15.1065016Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 19:01:15 [loggers.py:1219] AsyncLLM created with api_server_count more than 1; disabling stats logging to avoid incomplete stats.
2026-02-26T19:01:15.1068834Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 19:01:15 [loggers.py:1219] AsyncLLM created with api_server_count more than 1; disabling stats logging to avoid incomplete stats.
2026-02-26T19:01:15.1079800Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-26 19:01:15 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-26T19:01:15.1087762Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-26 19:01:15 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-26T19:01:15.1097549Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-26 19:01:15 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T19:01:15.1107303Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-26 19:01:15 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T19:01:15.1117735Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-26 19:01:15 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T19:01:15.1127910Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-26 19:01:15 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T19:01:15.1138871Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-26 19:01:15 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T19:01:15.1150258Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-26 19:01:15 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T19:01:15.1159514Z INFO 02-26 19:01:15 [utils.py:249] Waiting for API servers to complete ...
2026-02-26T19:01:15.1170065Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-26 19:01:15 [platform.py:323] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-26T19:01:15.1179635Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-26 19:01:15 [platform.py:323] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-26T19:01:15.1188580Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-26 19:01:15 [platform.py:340] [91m
2026-02-26T19:01:15.1199486Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             **********************************************************************************
2026-02-26T19:01:15.1212380Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             * WARNING: You have enabled the *full graph* feature.
2026-02-26T19:01:15.1227155Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             * This is an early experimental stage and may involve various unknown issues.
2026-02-26T19:01:15.1238506Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-26T19:01:15.1248978Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-26T19:01:15.1280386Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-26T19:01:15.1281279Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             * batch size for graph capture.
2026-02-26T19:01:15.1286434Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             * For more details, please refer to:
2026-02-26T19:01:15.1294038Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-26T19:01:15.1302770Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             **********************************************************************************[0m
2026-02-26T19:01:15.1327099Z [0;36m(EngineCore_DP0 pid=161)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             
2026-02-26T19:01:15.1340890Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-26 19:01:15 [platform.py:340] [91m
2026-02-26T19:01:15.1341886Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             **********************************************************************************
2026-02-26T19:01:15.1348371Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             * WARNING: You have enabled the *full graph* feature.
2026-02-26T19:01:15.1357968Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             * This is an early experimental stage and may involve various unknown issues.
2026-02-26T19:01:15.1363730Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-26T19:01:15.1392874Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-26T19:01:15.1412522Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-26T19:01:15.1422272Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             * batch size for graph capture.
2026-02-26T19:01:15.1433250Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             * For more details, please refer to:
2026-02-26T19:01:15.1444977Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-26T19:01:15.1453721Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             **********************************************************************************[0m
2026-02-26T19:01:15.1463200Z [0;36m(EngineCore_DP1 pid=180)[0;0m WARNING 02-26 19:01:15 [platform.py:340]             
2026-02-26T19:01:15.1473411Z [0;36m(EngineCore_DP1 pid=180)[0;0m INFO 02-26 19:01:15 [platform.py:448] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-26T19:01:15.1483501Z [0;36m(EngineCore_DP0 pid=161)[0;0m INFO 02-26 19:01:15 [platform.py:448] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-26T19:01:15.2928617Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [api_server.py:665] Supported tasks: ['generate']
2026-02-26T19:01:15.2946592Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [api_server.py:665] Supported tasks: ['generate']
2026-02-26T19:01:15.2956246Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [api_server.py:665] Supported tasks: ['generate']
2026-02-26T19:01:15.2965620Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [api_server.py:665] Supported tasks: ['generate']
2026-02-26T19:01:15.3010759Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 19:01:15 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'top_p': 0.95}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
2026-02-26T19:01:15.3012418Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 19:01:15 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'top_p': 0.95}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
2026-02-26T19:01:15.3014554Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 19:01:15 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'top_p': 0.95}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
2026-02-26T19:01:15.3025178Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 19:01:15 [model.py:1371] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'top_p': 0.95}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
2026-02-26T19:01:15.3033999Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [serving.py:177] Warming up chat template processing...
2026-02-26T19:01:15.3047127Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [serving.py:177] Warming up chat template processing...
2026-02-26T19:01:15.3093270Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [serving.py:177] Warming up chat template processing...
2026-02-26T19:01:15.3093828Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [serving.py:177] Warming up chat template processing...
2026-02-26T19:01:15.3341130Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [serving.py:212] Chat template warmup completed in 29.5ms
2026-02-26T19:01:15.3341780Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [serving.py:212] Chat template warmup completed in 29.8ms
2026-02-26T19:01:15.3342461Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [serving.py:212] Chat template warmup completed in 29.5ms
2026-02-26T19:01:15.3346418Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [serving.py:212] Chat template warmup completed in 30.4ms
2026-02-26T19:01:15.3366982Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [api_server.py:946] Starting vLLM API server 2 on http://0.0.0.0:8080
2026-02-26T19:01:15.3377010Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8080
2026-02-26T19:01:15.3386859Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [api_server.py:946] Starting vLLM API server 3 on http://0.0.0.0:8080
2026-02-26T19:01:15.3396737Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:38] Available routes are:
2026-02-26T19:01:15.3407302Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:38] Available routes are:
2026-02-26T19:01:15.3416088Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
2026-02-26T19:01:15.3428249Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:38] Available routes are:
2026-02-26T19:01:15.3435544Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
2026-02-26T19:01:15.3445427Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /docs, Methods: GET, HEAD
2026-02-26T19:01:15.3455400Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
2026-02-26T19:01:15.3464466Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /docs, Methods: GET, HEAD
2026-02-26T19:01:15.3474604Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
2026-02-26T19:01:15.3485044Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
2026-02-26T19:01:15.3494094Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /docs, Methods: HEAD, GET
2026-02-26T19:01:15.3504026Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
2026-02-26T19:01:15.3513366Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
2026-02-26T19:01:15.3523501Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
2026-02-26T19:01:15.3533369Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
2026-02-26T19:01:15.3542278Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
2026-02-26T19:01:15.3552747Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
2026-02-26T19:01:15.3562647Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
2026-02-26T19:01:15.3572120Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
2026-02-26T19:01:15.3581635Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
2026-02-26T19:01:15.3591099Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /tokenize, Methods: POST
2026-02-26T19:01:15.3601402Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
2026-02-26T19:01:15.3610970Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /tokenize, Methods: POST
2026-02-26T19:01:15.3620922Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /detokenize, Methods: POST
2026-02-26T19:01:15.3630374Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /tokenize, Methods: POST
2026-02-26T19:01:15.3640660Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /detokenize, Methods: POST
2026-02-26T19:01:15.3649643Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
2026-02-26T19:01:15.3659332Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
2026-02-26T19:01:15.3669070Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /pause, Methods: POST
2026-02-26T19:01:15.3679109Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /detokenize, Methods: POST
2026-02-26T19:01:15.3688999Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /pause, Methods: POST
2026-02-26T19:01:15.3698385Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /resume, Methods: POST
2026-02-26T19:01:15.3707934Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
2026-02-26T19:01:15.3718221Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /resume, Methods: POST
2026-02-26T19:01:15.3728028Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /is_paused, Methods: GET
2026-02-26T19:01:15.3737884Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /pause, Methods: POST
2026-02-26T19:01:15.3747724Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /is_paused, Methods: GET
2026-02-26T19:01:15.3757866Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /resume, Methods: POST
2026-02-26T19:01:15.3767906Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /metrics, Methods: GET
2026-02-26T19:01:15.3778072Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /is_paused, Methods: GET
2026-02-26T19:01:15.3787510Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /metrics, Methods: GET
2026-02-26T19:01:15.3797354Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /health, Methods: GET
2026-02-26T19:01:15.3809784Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /metrics, Methods: GET
2026-02-26T19:01:15.3817666Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
2026-02-26T19:01:15.3827175Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /health, Methods: GET
2026-02-26T19:01:15.3837128Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /health, Methods: GET
2026-02-26T19:01:15.3847552Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
2026-02-26T19:01:15.3857155Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
2026-02-26T19:01:15.3866180Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
2026-02-26T19:01:15.3876315Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/responses, Methods: POST
2026-02-26T19:01:15.3886675Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
2026-02-26T19:01:15.3896420Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
2026-02-26T19:01:15.3908954Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
2026-02-26T19:01:15.3915907Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/responses, Methods: POST
2026-02-26T19:01:15.3926369Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/responses, Methods: POST
2026-02-26T19:01:15.3935760Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
2026-02-26T19:01:15.3945729Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
2026-02-26T19:01:15.3955598Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
2026-02-26T19:01:15.3966195Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
2026-02-26T19:01:15.3975721Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
2026-02-26T19:01:15.4150609Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
2026-02-26T19:01:15.4151231Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
2026-02-26T19:01:15.4151885Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
2026-02-26T19:01:15.4155766Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
2026-02-26T19:01:15.4156383Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/completions, Methods: POST
2026-02-26T19:01:15.4156976Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
2026-02-26T19:01:15.4157578Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
2026-02-26T19:01:15.4158162Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/completions/render, Methods: POST
2026-02-26T19:01:15.4158831Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/completions, Methods: POST
2026-02-26T19:01:15.4159393Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/completions, Methods: POST
2026-02-26T19:01:15.4159902Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/messages, Methods: POST
2026-02-26T19:01:15.4160507Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/completions/render, Methods: POST
2026-02-26T19:01:15.4161106Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/completions/render, Methods: POST
2026-02-26T19:01:15.4161620Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/models, Methods: GET
2026-02-26T19:01:15.4162350Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/messages, Methods: POST
2026-02-26T19:01:15.4166875Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/messages, Methods: POST
2026-02-26T19:01:15.4176479Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/models, Methods: GET
2026-02-26T19:01:15.4187556Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /load, Methods: GET
2026-02-26T19:01:15.4211719Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/models, Methods: GET
2026-02-26T19:01:15.4221061Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /version, Methods: GET
2026-02-26T19:01:15.4221612Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /load, Methods: GET
2026-02-26T19:01:15.4226581Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /load, Methods: GET
2026-02-26T19:01:15.4236207Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /ping, Methods: GET
2026-02-26T19:01:15.4247534Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /version, Methods: GET
2026-02-26T19:01:15.4257097Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /version, Methods: GET
2026-02-26T19:01:15.4267763Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /ping, Methods: POST
2026-02-26T19:01:15.4277454Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /ping, Methods: GET
2026-02-26T19:01:15.4290525Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /ping, Methods: GET
2026-02-26T19:01:15.4301423Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /invocations, Methods: POST
2026-02-26T19:01:15.4310634Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /ping, Methods: POST
2026-02-26T19:01:15.4320740Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /ping, Methods: POST
2026-02-26T19:01:15.4330197Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /classify, Methods: POST
2026-02-26T19:01:15.4340012Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /invocations, Methods: POST
2026-02-26T19:01:15.4353069Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /invocations, Methods: POST
2026-02-26T19:01:15.4365046Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/embeddings, Methods: POST
2026-02-26T19:01:15.4373935Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /classify, Methods: POST
2026-02-26T19:01:15.4384913Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /classify, Methods: POST
2026-02-26T19:01:15.4395314Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /score, Methods: POST
2026-02-26T19:01:15.4406020Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/embeddings, Methods: POST
2026-02-26T19:01:15.4415415Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/embeddings, Methods: POST
2026-02-26T19:01:15.4424771Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/score, Methods: POST
2026-02-26T19:01:15.4434911Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /score, Methods: POST
2026-02-26T19:01:15.4445199Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /score, Methods: POST
2026-02-26T19:01:15.4454424Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /rerank, Methods: POST
2026-02-26T19:01:15.4466789Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/score, Methods: POST
2026-02-26T19:01:15.4474799Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/score, Methods: POST
2026-02-26T19:01:15.4484837Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/rerank, Methods: POST
2026-02-26T19:01:15.4494368Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /rerank, Methods: POST
2026-02-26T19:01:15.4504526Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /rerank, Methods: POST
2026-02-26T19:01:15.4513905Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v2/rerank, Methods: POST
2026-02-26T19:01:15.4523883Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/rerank, Methods: POST
2026-02-26T19:01:15.4533914Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/rerank, Methods: POST
2026-02-26T19:01:15.4543705Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /pooling, Methods: POST
2026-02-26T19:01:15.4553395Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v2/rerank, Methods: POST
2026-02-26T19:01:15.4562776Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v2/rerank, Methods: POST
2026-02-26T19:01:15.4572392Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /pooling, Methods: POST
2026-02-26T19:01:15.4582494Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /pooling, Methods: POST
2026-02-26T19:01:15.4592068Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [api_server.py:946] Starting vLLM API server 1 on http://0.0.0.0:8080
2026-02-26T19:01:15.4601588Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:38] Available routes are:
2026-02-26T19:01:15.4611475Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
2026-02-26T19:01:15.4620937Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /docs, Methods: HEAD, GET
2026-02-26T19:01:15.4629762Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
2026-02-26T19:01:15.4640197Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
2026-02-26T19:01:15.4650648Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
2026-02-26T19:01:15.4660659Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
2026-02-26T19:01:15.4670169Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /tokenize, Methods: POST
2026-02-26T19:01:15.4680086Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /detokenize, Methods: POST
2026-02-26T19:01:15.4689794Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
2026-02-26T19:01:15.4699953Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /pause, Methods: POST
2026-02-26T19:01:15.4709005Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /resume, Methods: POST
2026-02-26T19:01:15.4718451Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /is_paused, Methods: GET
2026-02-26T19:01:15.4728685Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /metrics, Methods: GET
2026-02-26T19:01:15.4738204Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /health, Methods: GET
2026-02-26T19:01:15.4748127Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
2026-02-26T19:01:15.4758039Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
2026-02-26T19:01:15.4774007Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/responses, Methods: POST
2026-02-26T19:01:15.4783269Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
2026-02-26T19:01:15.4793084Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
2026-02-26T19:01:15.4802409Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
2026-02-26T19:01:15.4812442Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
2026-02-26T19:01:15.4822223Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/completions, Methods: POST
2026-02-26T19:01:15.4831908Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/completions/render, Methods: POST
2026-02-26T19:01:15.4841835Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/messages, Methods: POST
2026-02-26T19:01:15.4851665Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/models, Methods: GET
2026-02-26T19:01:15.4861081Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /load, Methods: GET
2026-02-26T19:01:15.4870622Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /version, Methods: GET
2026-02-26T19:01:15.4881067Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /ping, Methods: GET
2026-02-26T19:01:15.4890870Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /ping, Methods: POST
2026-02-26T19:01:15.4900775Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /invocations, Methods: POST
2026-02-26T19:01:15.4910979Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /classify, Methods: POST
2026-02-26T19:01:15.4922115Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/embeddings, Methods: POST
2026-02-26T19:01:15.4931257Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /score, Methods: POST
2026-02-26T19:01:15.4941153Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/score, Methods: POST
2026-02-26T19:01:15.4950634Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /rerank, Methods: POST
2026-02-26T19:01:15.4960844Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v1/rerank, Methods: POST
2026-02-26T19:01:15.4971732Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /v2/rerank, Methods: POST
2026-02-26T19:01:15.4981599Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:01:15 [launcher.py:46] Route: /pooling, Methods: POST
2026-02-26T19:01:15.4993414Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 19:01:15 [warnings.py:110] /usr/local/python3.11.14/lib/python3.11/site-packages/websockets/legacy/__init__.py:6: DeprecationWarning: websockets.legacy is deprecated; see https://websockets.readthedocs.io/en/stable/howto/upgrade.html for upgrade instructions
2026-02-26T19:01:15.5002929Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 19:01:15 [warnings.py:110] /usr/local/python3.11.14/lib/python3.11/site-packages/websockets/legacy/__init__.py:6: DeprecationWarning: websockets.legacy is deprecated; see https://websockets.readthedocs.io/en/stable/howto/upgrade.html for upgrade instructions
2026-02-26T19:01:15.5013787Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 19:01:15 [warnings.py:110] /usr/local/python3.11.14/lib/python3.11/site-packages/websockets/legacy/__init__.py:6: DeprecationWarning: websockets.legacy is deprecated; see https://websockets.readthedocs.io/en/stable/howto/upgrade.html for upgrade instructions
2026-02-26T19:01:15.5023944Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 19:01:15 [warnings.py:110] /usr/local/python3.11.14/lib/python3.11/site-packages/websockets/legacy/__init__.py:6: DeprecationWarning: websockets.legacy is deprecated; see https://websockets.readthedocs.io/en/stable/howto/upgrade.html for upgrade instructions
2026-02-26T19:01:15.5034637Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 19:01:15 [warnings.py:110] /usr/local/python3.11.14/lib/python3.11/site-packages/uvicorn/protocols/websockets/websockets_impl.py:17: DeprecationWarning: websockets.server.WebSocketServerProtocol is deprecated
2026-02-26T19:01:15.5045719Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 19:01:15 [warnings.py:110] /usr/local/python3.11.14/lib/python3.11/site-packages/uvicorn/protocols/websockets/websockets_impl.py:17: DeprecationWarning: websockets.server.WebSocketServerProtocol is deprecated
2026-02-26T19:01:15.5099255Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 19:01:15 [warnings.py:110] /usr/local/python3.11.14/lib/python3.11/site-packages/uvicorn/protocols/websockets/websockets_impl.py:17: DeprecationWarning: websockets.server.WebSocketServerProtocol is deprecated
2026-02-26T19:01:15.5100622Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 19:01:15 [warnings.py:110] /usr/local/python3.11.14/lib/python3.11/site-packages/uvicorn/protocols/websockets/websockets_impl.py:17: DeprecationWarning: websockets.server.WebSocketServerProtocol is deprecated
2026-02-26T19:01:15.5101628Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     Started server process [194]
2026-02-26T19:01:15.5102167Z [0;36m(ApiServer_1 pid=192)[0;0m INFO:     Started server process [192]
2026-02-26T19:01:15.5102624Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     Waiting for application startup.
2026-02-26T19:01:15.5105306Z [0;36m(ApiServer_2 pid=193)[0;0m INFO:     Started server process [193]
2026-02-26T19:01:15.5115128Z [0;36m(ApiServer_0 pid=191)[0;0m INFO:     Started server process [191]
2026-02-26T19:01:15.5125525Z [0;36m(ApiServer_1 pid=192)[0;0m INFO:     Waiting for application startup.
2026-02-26T19:01:15.5135103Z [0;36m(ApiServer_2 pid=193)[0;0m INFO:     Waiting for application startup.
2026-02-26T19:01:15.5145148Z [0;36m(ApiServer_0 pid=191)[0;0m INFO:     Waiting for application startup.
2026-02-26T19:01:15.7072327Z [0;36m(ApiServer_0 pid=191)[0;0m INFO:     Application startup complete.
2026-02-26T19:01:15.7980587Z [0;36m(ApiServer_2 pid=193)[0;0m INFO:     Application startup complete.
2026-02-26T19:01:15.9665182Z [0;36m(ApiServer_1 pid=192)[0;0m INFO:     Application startup complete.
2026-02-26T19:01:16.0674045Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     Application startup complete.
2026-02-26T19:01:18.2960189Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.222:49362 - "GET /health HTTP/1.1" 200 OK
2026-02-26T19:01:18.2988089Z [2026-02-26 19:01:18] INFO conftest.py:390: [READY] Node 10.0.0.222 is ready.
2026-02-26T19:01:18.3443755Z 2026-02-26 19:01:18,342 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-26T19:01:18.3808242Z 2026-02-26 19:01:18,379 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-26T19:01:20.1664888Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.76:42002 - "GET /health HTTP/1.1" 200 OK
2026-02-26T19:01:20.1731176Z [0;36m(ApiServer_1 pid=192)[0;0m INFO:     10.0.0.76:42014 - "GET /health HTTP/1.1" 200 OK
2026-02-26T19:01:25.1771786Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.76:38882 - "GET /health HTTP/1.1" 200 OK
2026-02-26T19:01:30.1879418Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.76:38890 - "GET /health HTTP/1.1" 200 OK
2026-02-26T19:01:35.1515945Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 19:01:35 [warnings.py:110] /vllm-workspace/vllm/vllm/entrypoints/openai/engine/serving.py:1067: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
2026-02-26T19:01:35.1525292Z [0;36m(ApiServer_3 pid=194)[0;0m WARNING 02-26 19:01:35 [warnings.py:110] /vllm-workspace/vllm/vllm/entrypoints/utils.py:218: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
2026-02-26T19:01:35.1535700Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 19:01:35 [warnings.py:110] /vllm-workspace/vllm/vllm/entrypoints/openai/engine/serving.py:1067: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
2026-02-26T19:01:35.1546267Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 19:01:35 [warnings.py:110] /vllm-workspace/vllm/vllm/entrypoints/utils.py:218: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
2026-02-26T19:01:35.1556955Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 19:01:35 [warnings.py:110] /vllm-workspace/vllm/vllm/entrypoints/openai/engine/serving.py:1067: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
2026-02-26T19:01:35.1567836Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 19:01:35 [warnings.py:110] /vllm-workspace/vllm/vllm/entrypoints/utils.py:218: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
2026-02-26T19:01:35.1602786Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 19:01:35 [warnings.py:110] /vllm-workspace/vllm/vllm/entrypoints/openai/engine/serving.py:1067: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
2026-02-26T19:01:35.1613121Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 19:01:35 [warnings.py:110] /vllm-workspace/vllm/vllm/entrypoints/utils.py:218: DeprecationWarning: max_tokens is deprecated in favor of the max_completion_tokens field
2026-02-26T19:01:35.1830080Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.76:35934 - "GET /health HTTP/1.1" 200 OK
2026-02-26T19:01:40.1882757Z ................[0;36m(ApiServer_1 pid=192)[0;0m INFO:     10.0.0.76:35950 - "GET /health HTTP/1.1" 200 OK
2026-02-26T19:01:45.1926976Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.76:38224 - "GET /health HTTP/1.1" 200 OK
2026-02-26T19:01:50.1970971Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.76:38232 - "GET /health HTTP/1.1" 200 OK
2026-02-26T19:01:55.2010693Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.76:50160 - "GET /health HTTP/1.1" 200 OK
2026-02-26T19:02:00.1628900Z [0;36m(Worker_DP0_TP3_EP3 pid=480)[0;0m INFO 02-26 19:02:00 [acl_graph.py:185] Replaying aclgraph
2026-02-26T19:02:00.1639459Z [0;36m(Worker_DP0_TP5_EP5 pid=688)[0;0m INFO 02-26 19:02:00 [acl_graph.py:185] Replaying aclgraph
2026-02-26T19:02:00.1649809Z [0;36m(Worker_DP0_TP6_EP6 pid=792)[0;0m INFO 02-26 19:02:00 [acl_graph.py:185] Replaying aclgraph
2026-02-26T19:02:00.1660422Z [0;36m(Worker_DP1_TP5_EP13 pid=691)[0;0m INFO 02-26 19:02:00 [acl_graph.py:185] Replaying aclgraph
2026-02-26T19:02:00.1670450Z [0;36m(Worker_DP1_TP3_EP11 pid=481)[0;0m INFO 02-26 19:02:00 [acl_graph.py:185] Replaying aclgraph
2026-02-26T19:02:00.1680889Z [0;36m(Worker_DP1_TP6_EP14 pid=795)[0;0m INFO 02-26 19:02:00 [acl_graph.py:185] Replaying aclgraph
2026-02-26T19:02:00.1690236Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-26 19:02:00 [acl_graph.py:185] Replaying aclgraph
2026-02-26T19:02:00.1699623Z [0;36m(Worker_DP0_TP1_EP1 pid=258)[0;0m INFO 02-26 19:02:00 [acl_graph.py:185] Replaying aclgraph
2026-02-26T19:02:00.1709430Z [0;36m(Worker_DP1_TP1_EP9 pid=261)[0;0m INFO 02-26 19:02:00 [acl_graph.py:185] Replaying aclgraph
2026-02-26T19:02:00.1719611Z [0;36m(Worker_DP1_TP7_EP15 pid=899)[0;0m INFO 02-26 19:02:00 [acl_graph.py:185] Replaying aclgraph
2026-02-26T19:02:00.1730257Z [0;36m(Worker_DP0_TP7_EP7 pid=896)[0;0m INFO 02-26 19:02:00 [acl_graph.py:185] Replaying aclgraph
2026-02-26T19:02:00.1740507Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-26 19:02:00 [acl_graph.py:185] Replaying aclgraph
2026-02-26T19:02:00.1750137Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-26 19:02:00 [acl_graph.py:185] Replaying aclgraph
2026-02-26T19:02:00.1761408Z [0;36m(Worker_DP0_TP4_EP4 pid=584)[0;0m INFO 02-26 19:02:00 [acl_graph.py:185] Replaying aclgraph
2026-02-26T19:02:00.1780299Z [0;36m(Worker_DP1_TP4_EP12 pid=587)[0;0m INFO 02-26 19:02:00 [acl_graph.py:185] Replaying aclgraph
2026-02-26T19:02:00.1790113Z [0;36m(Worker_DP0_TP2_EP2 pid=377)[0;0m INFO 02-26 19:02:00 [acl_graph.py:185] Replaying aclgraph
2026-02-26T19:02:00.2084424Z [0;36m(ApiServer_1 pid=192)[0;0m INFO:     10.0.0.76:50162 - "GET /health HTTP/1.1" 200 OK
2026-02-26T19:02:03.4077522Z [0;36m(ApiServer_1 pid=192)[0;0m INFO:     10.0.0.222:46402 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:03.4103661Z [0;36m(ApiServer_1 pid=192)[0;0m INFO:     10.0.0.222:46588 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:03.5117291Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.222:46414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:04.2608553Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.222:46400 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:04.2635489Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.222:46602 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:04.4853856Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.222:46612 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:04.6557480Z [0;36m(ApiServer_1 pid=192)[0;0m INFO:     10.0.0.222:46422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:05.0521425Z [0;36m(ApiServer_1 pid=192)[0;0m INFO:     10.0.0.222:46432 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:05.1501482Z [0;36m(ApiServer_0 pid=191)[0;0m INFO:     10.0.0.222:46506 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:05.2083593Z [0;36m(ApiServer_1 pid=192)[0;0m INFO:     10.0.0.76:41630 - "GET /health HTTP/1.1" 200 OK
2026-02-26T19:02:05.2493467Z [0;36m(ApiServer_2 pid=193)[0;0m INFO:     10.0.0.222:46618 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:05.3498258Z [0;36m(ApiServer_2 pid=193)[0;0m INFO:     10.0.0.222:46658 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:05.7522555Z [0;36m(ApiServer_0 pid=191)[0;0m INFO:     10.0.0.222:46490 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:06.0498638Z [0;36m(ApiServer_2 pid=193)[0;0m INFO:     10.0.0.222:46562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:06.1512522Z [0;36m(ApiServer_0 pid=191)[0;0m INFO:     10.0.0.222:46546 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:06.4969422Z [0;36m(ApiServer_2 pid=193)[0;0m INFO:     10.0.0.222:46548 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:06.5177684Z [0;36m(ApiServer_1 pid=192)[0;0m INFO:     10.0.0.222:46674 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:06.9917218Z [0;36m(ApiServer_0 pid=191)[0;0m INFO:     10.0.0.222:46476 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:07.2938948Z [0;36m(ApiServer_2 pid=193)[0;0m INFO:     10.0.0.222:46412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:07.7038133Z [0;36m(ApiServer_2 pid=193)[0;0m INFO:     10.0.0.222:46642 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:10.2124940Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.76:41646 - "GET /health HTTP/1.1" 200 OK
2026-02-26T19:02:15.2307885Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.76:44088 - "GET /health HTTP/1.1" 200 OK
2026-02-26T19:02:16.3048249Z [0;36m(ApiServer_1 pid=192)[0;0m INFO:     10.0.0.222:46702 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:16.3983461Z [0;36m(ApiServer_0 pid=191)[0;0m INFO:     10.0.0.222:46534 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:16.3995812Z [0;36m(ApiServer_0 pid=191)[0;0m INFO:     10.0.0.222:46518 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:17.1874113Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.222:46686 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:17.3769376Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.222:46630 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:17.5682776Z [0;36m(ApiServer_1 pid=192)[0;0m INFO:     10.0.0.222:46628 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:20.2202846Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.76:44098 - "GET /health HTTP/1.1" 200 OK
2026-02-26T19:02:20.2817440Z [0;36m(ApiServer_0 pid=191)[0;0m INFO:     10.0.0.222:46446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:21.3611674Z [0;36m(ApiServer_2 pid=193)[0;0m INFO:     10.0.0.222:46426 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:21.4503225Z [0;36m(ApiServer_2 pid=193)[0;0m INFO:     10.0.0.222:46570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:22.5347868Z [0;36m(ApiServer_1 pid=192)[0;0m INFO:     10.0.0.222:46462 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:24.6025934Z [0;36m(ApiServer_0 pid=191)[0;0m INFO:     10.0.0.222:46512 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:25.2235899Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.76:40188 - "GET /health HTTP/1.1" 200 OK
2026-02-26T19:02:25.7194989Z [0;36m(ApiServer_2 pid=193)[0;0m INFO:     10.0.0.222:46582 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:27.5044026Z [0;36m(ApiServer_0 pid=191)[0;0m INFO:     10.0.0.222:46508 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2026-02-26T19:02:30.2267885Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.76:40204 - "GET /health HTTP/1.1" 200 OK
2026-02-26T19:02:35.2300995Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.76:58278 - "GET /health HTTP/1.1" 200 OK
2026-02-26T19:02:40.2337843Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     10.0.0.76:58282 - "GET /health HTTP/1.1" 200 OK
2026-02-26T19:02:40.8378715Z INFO 02-26 19:02:40 [utils.py:288] Received KeyboardInterrupt, shutting down API servers...
2026-02-26T19:02:40.8383928Z INFO 02-26 19:02:40 [utils.py:293] Terminating remaining processes ...
2026-02-26T19:02:40.9479660Z [0;36m(ApiServer_3 pid=194)[0;0m INFO 02-26 19:02:40 [launcher.py:110] Shutting down FastAPI HTTP server.
2026-02-26T19:02:40.9523329Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     Shutting down
2026-02-26T19:02:41.0038378Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 19:02:41 [launcher.py:104] port 8080 is used by process psutil.Process(pid=194, name='VLLM::APIServer_3', status='sleeping', started='18:49:48') launched with command:
2026-02-26T19:02:41.0045969Z [0;36m(ApiServer_0 pid=191)[0;0m WARNING 02-26 19:02:41 [launcher.py:104] VLLM::APIServer_3                                                                                                                                    
2026-02-26T19:02:41.0055394Z [0;36m(ApiServer_0 pid=191)[0;0m INFO 02-26 19:02:41 [launcher.py:110] Shutting down FastAPI HTTP server.
2026-02-26T19:02:41.0064586Z [0;36m(ApiServer_0 pid=191)[0;0m INFO:     Shutting down
2026-02-26T19:02:41.0090831Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 19:02:41 [launcher.py:104] port 8080 is used by process psutil.Process(pid=194, name='VLLM::APIServer_3', status='sleeping', started='18:49:48') launched with command:
2026-02-26T19:02:41.0100682Z [0;36m(ApiServer_1 pid=192)[0;0m WARNING 02-26 19:02:41 [launcher.py:104] VLLM::APIServer_3                                                                                                                                    
2026-02-26T19:02:41.0110822Z [0;36m(ApiServer_1 pid=192)[0;0m INFO 02-26 19:02:41 [launcher.py:110] Shutting down FastAPI HTTP server.
2026-02-26T19:02:41.0124733Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 19:02:41 [launcher.py:104] port 8080 is used by process psutil.Process(pid=194, name='VLLM::APIServer_3', status='sleeping', started='18:49:48') launched with command:
2026-02-26T19:02:41.0132315Z [0;36m(ApiServer_2 pid=193)[0;0m WARNING 02-26 19:02:41 [launcher.py:104] VLLM::APIServer_3                                                                                                                                    
2026-02-26T19:02:41.0141066Z [0;36m(ApiServer_2 pid=193)[0;0m INFO 02-26 19:02:41 [launcher.py:110] Shutting down FastAPI HTTP server.
2026-02-26T19:02:41.0150622Z [0;36m(ApiServer_1 pid=192)[0;0m INFO:     Shutting down
2026-02-26T19:02:41.0161614Z [0;36m(ApiServer_2 pid=193)[0;0m INFO:     Shutting down
2026-02-26T19:02:41.0500841Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     Waiting for application shutdown.
2026-02-26T19:02:41.0515253Z [0;36m(ApiServer_3 pid=194)[0;0m INFO:     Application shutdown complete.
2026-02-26T19:02:41.1041618Z [0;36m(ApiServer_0 pid=191)[0;0m INFO:     Waiting for application shutdown.
2026-02-26T19:02:41.1064123Z [0;36m(ApiServer_0 pid=191)[0;0m INFO:     Application shutdown complete.
2026-02-26T19:02:41.1096900Z [0;36m(ApiServer_1 pid=192)[0;0m INFO:     Waiting for application shutdown.
2026-02-26T19:02:41.1119930Z [0;36m(ApiServer_1 pid=192)[0;0m INFO:     Application shutdown complete.
2026-02-26T19:02:41.1130156Z [0;36m(ApiServer_2 pid=193)[0;0m INFO:     Waiting for application shutdown.
2026-02-26T19:02:41.1140443Z [0;36m(ApiServer_2 pid=193)[0;0m INFO:     Application shutdown complete.
2026-02-26T19:02:41.1429830Z [0;36m(ApiServer_3 pid=194)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-26T19:02:41.1583934Z [0;36m(ApiServer_2 pid=193)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-26T19:02:41.1671145Z [0;36m(ApiServer_1 pid=192)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-26T19:02:41.1696049Z [0;36m(ApiServer_0 pid=191)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-26T19:02:42.2933720Z [0;36m(Worker_DP1_TP3_EP11 pid=481)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T19:02:42.2941496Z [0;36m(Worker_DP0_TP3_EP3 pid=480)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T19:02:42.2952141Z [0;36m(Worker_DP0_TP4_EP4 pid=584)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T19:02:42.2962304Z [0;36m(Worker_DP1_TP7_EP15 pid=899)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T19:02:42.2972416Z [0;36m(Worker_DP0_TP5_EP5 pid=688)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T19:02:42.2982560Z [0;36m(Worker_DP0_TP2_EP2 pid=377)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T19:02:42.2993318Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T19:02:42.3003184Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T19:02:42.3013540Z [0;36m(Worker_DP0_TP1_EP1 pid=258)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T19:02:42.3023343Z [0;36m(Worker_DP0_TP6_EP6 pid=792)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T19:02:42.3033813Z [0;36m(Worker_DP1_TP4_EP12 pid=587)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T19:02:42.3045067Z [0;36m(Worker_DP1_TP1_EP9 pid=261)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T19:02:42.3057797Z [0;36m(Worker_DP0_TP7_EP7 pid=896)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T19:02:42.3068372Z [0;36m(Worker_DP1_TP6_EP14 pid=795)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T19:02:42.3079811Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T19:02:42.3137062Z [0;36m(Worker_DP1_TP3_EP11 pid=481)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T19:02:42.3145270Z [0;36m(Worker_DP0_TP4_EP4 pid=584)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T19:02:42.3153852Z [0;36m(Worker_DP0_TP3_EP3 pid=480)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T19:02:42.3164834Z [0;36m(Worker_DP1_TP7_EP15 pid=899)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T19:02:42.3174896Z [0;36m(Worker_DP1_TP5_EP13 pid=691)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T19:02:42.3187145Z [0;36m(Worker_DP1_TP4_EP12 pid=587)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T19:02:42.3200123Z [0;36m(Worker_DP0_TP2_EP2 pid=377)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T19:02:42.3213580Z [0;36m(Worker_DP1_TP0_EP8 pid=208)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T19:02:42.3223904Z [0;36m(Worker_DP0_TP0_EP0 pid=209)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T19:02:42.3234220Z [0;36m(Worker_DP0_TP5_EP5 pid=688)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T19:02:42.3245134Z [0;36m(Worker_DP0_TP1_EP1 pid=258)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T19:02:42.3255256Z [0;36m(Worker_DP0_TP6_EP6 pid=792)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T19:02:42.3265074Z [0;36m(Worker_DP0_TP7_EP7 pid=896)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T19:02:42.3276113Z [0;36m(Worker_DP1_TP1_EP9 pid=261)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T19:02:42.3286107Z [0;36m(Worker_DP1_TP2_EP10 pid=376)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T19:02:42.3296560Z [0;36m(Worker_DP1_TP6_EP14 pid=795)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T19:02:42.3306471Z [0;36m(Worker_DP1_TP5_EP13 pid=691)[0;0m INFO 02-26 19:02:42 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T19:02:47.7909045Z sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-26T19:02:48.8388589Z The request config is
2026-02-26T19:02:48.8395161Z  from ais_bench.benchmark.models import VLLMCustomAPIChat
2026-02-26T19:02:48.8406045Z from ais_bench.benchmark.utils.model_postprocessors import extract_non_reasoning_content
2026-02-26T19:02:48.8414820Z 
2026-02-26T19:02:48.8424252Z models = [
2026-02-26T19:02:48.8434259Z     dict(
2026-02-26T19:02:48.8445535Z         attr="service",
2026-02-26T19:02:48.8455208Z         type=VLLMCustomAPIChat,
2026-02-26T19:02:48.8465267Z         abbr='vllm-api-general-chat',
2026-02-26T19:02:48.8475841Z         path="",
2026-02-26T19:02:48.8485582Z         model="vllm-ascend/DeepSeek-V3.2-W8A8",
2026-02-26T19:02:48.8495446Z         request_rate = 0,
2026-02-26T19:02:48.8507089Z         retry = 2,
2026-02-26T19:02:48.8515479Z         host_ip = "10.0.0.222",
2026-02-26T19:02:48.8525110Z         host_port = 8080,
2026-02-26T19:02:48.8534958Z         max_out_len = 4096,
2026-02-26T19:02:48.8544298Z         batch_size = 64,
2026-02-26T19:02:48.8553997Z         trust_remote_code=False,
2026-02-26T19:02:48.8564765Z         generation_kwargs = dict(
2026-02-26T19:02:48.8574233Z             temperature = 0.6,
2026-02-26T19:02:48.8583757Z             ignore_eos = False,
2026-02-26T19:02:48.8593675Z             #top_k = 10,
2026-02-26T19:02:48.8603874Z             top_p = 0.95,
2026-02-26T19:02:48.8613096Z             #seed = None,
2026-02-26T19:02:48.8622361Z             #repetition_penalty = 1.03,
2026-02-26T19:02:48.8631587Z         ),
2026-02-26T19:02:48.8641475Z         pred_postprocessor=dict(type=extract_non_reasoning_content)
2026-02-26T19:02:48.8650915Z     )
2026-02-26T19:02:48.8660145Z ]
2026-02-26T19:02:48.8670277Z 
2026-02-26T19:02:48.8680454Z running aisbench cmd: ais_bench --models vllm_api_general_chat_custom --datasets gsm8k_gen_0_shot_cot_chat_prompt
2026-02-26T19:02:48.8691095Z 02/26 19:01:32 - AISBench - INFO - Loading gsm8k_gen_0_shot_cot_chat_prompt: /vllm-workspace/vllm-ascend/benchmark/ais_bench/benchmark/configs/./datasets/gsm8k/gsm8k_gen_0_shot_cot_chat_prompt.py
2026-02-26T19:02:48.8701410Z 02/26 19:01:32 - AISBench - INFO - Loading vllm_api_general_chat_custom: /vllm-workspace/vllm-ascend/benchmark/ais_bench/benchmark/configs/./models/vllm_api/vllm_api_general_chat_custom.py
2026-02-26T19:02:48.8711435Z 02/26 19:01:32 - AISBench - INFO - Loading example: /vllm-workspace/vllm-ascend/benchmark/ais_bench/benchmark/configs/./summarizers/example.py
2026-02-26T19:02:48.8720843Z 02/26 19:01:33 - AISBench - INFO - Current exp folder: outputs/default/20260226_190133
2026-02-26T19:02:48.8730550Z 02/26 19:01:35 - AISBench - INFO - Starting inference tasks...
2026-02-26T19:02:48.8741456Z 02/26 19:01:35 - AISBench - INFO - Partitioned into 1 tasks.
2026-02-26T19:02:48.8750571Z 02/26 19:01:35 - AISBench - INFO - Continuous batch enable! All the logs and processes for each task should be checked in each infer/.out file.
2026-02-26T19:02:48.8760260Z Launch OpenICLInfer[vllm-api-general-chat/gsm8k] on CPU
2026-02-26T19:02:48.8770449Z Launch OpenICLInfer[vllm-api-general-chat/gsm8k] on CPU
2026-02-26T19:02:48.8779916Z 02/26 19:02:27 - AISBench - INFO - Inference tasks completed.
2026-02-26T19:02:48.8790196Z 02/26 19:02:29 - AISBench - INFO - Starting evaluation tasks...
2026-02-26T19:02:48.8801266Z 02/26 19:02:29 - AISBench - INFO - Partitioned into 1 tasks.
2026-02-26T19:02:48.8810967Z launch OpenICLEval[vllm-api-general-chat/gsm8k] on CPU
2026-02-26T19:02:48.8821415Z 02/26 19:02:40 - AISBench - INFO - Evaluation tasks completed.
2026-02-26T19:02:48.8830039Z 02/26 19:02:40 - AISBench - INFO - Summarizing evaluation results...
2026-02-26T19:02:48.8840369Z dataset    version    metric    mode      vllm-api-general-chat
2026-02-26T19:02:48.8851462Z ---------  ---------  --------  ------  -----------------------
2026-02-26T19:02:48.8860814Z gsm8k      7cd45e     accuracy  gen                       96.88
2026-02-26T19:02:48.8871000Z 02/26 19:02:40 - AISBench - INFO - write summary to /vllm-workspace/vllm-ascend/outputs/default/20260226_190133/summary/summary_20260226_190133.txt
2026-02-26T19:02:48.8881123Z 02/26 19:02:40 - AISBench - INFO - write csv to /vllm-workspace/vllm-ascend/outputs/default/20260226_190133/summary/summary_20260226_190133.csv
2026-02-26T19:02:48.9084081Z /usr/local/python3.11.14/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked shared_memory objects to clean up at shutdown
2026-02-26T19:02:48.9099526Z   warnings.warn('resource_tracker: There appear to be %d '
2026-02-26T19:02:50.2204347Z PASSED
2026-02-26T19:02:50.2211101Z 
2026-02-26T19:02:50.2221390Z =============================== warnings summary ===============================
2026-02-26T19:02:50.2230650Z <frozen importlib._bootstrap>:241
2026-02-26T19:02:50.2240611Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute
2026-02-26T19:02:50.2249566Z 
2026-02-26T19:02:50.2258699Z <frozen importlib._bootstrap>:241
2026-02-26T19:02:50.2269185Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute
2026-02-26T19:02:50.2278752Z 
2026-02-26T19:02:50.2288265Z ../../usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647
2026-02-26T19:02:50.2299484Z   /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T19:02:50.2307971Z     warnings.warn(
2026-02-26T19:02:50.2317263Z 
2026-02-26T19:02:50.2328071Z ../../usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8
2026-02-26T19:02:50.2339313Z   /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
2026-02-26T19:02:50.2346907Z     import pkg_resources
2026-02-26T19:02:50.2356486Z 
2026-02-26T19:02:50.2366723Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node
2026-02-26T19:02:50.2376054Z   /usr/local/python3.11.14/lib/python3.11/site-packages/pandas/core/dtypes/cast.py:1641: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.
2026-02-26T19:02:50.2385953Z   See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)
2026-02-26T19:02:50.2395025Z     return np.find_common_type(types, [])
2026-02-26T19:02:50.2405066Z 
2026-02-26T19:02:50.2415280Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2026-02-26T19:02:50.2425034Z ================== 1 passed, 5 warnings in 821.67s (0:13:41) ===================
2026-02-26T19:02:50.4999320Z sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-26T19:02:51.8815788Z [0;32mâœ“ All tests passed![0m
2026-02-26T19:02:52.1318497Z Cleaning up background log streams...
2026-02-26T19:02:52.6530587Z ##[group]Run actions/upload-artifact@v6
2026-02-26T19:02:52.6530875Z with:
2026-02-26T19:02:52.6531162Z   name: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs
2026-02-26T19:02:52.6531476Z   path: /tmp/vllm*_logs.txt
2026-02-26T19:02:52.6531714Z   retention-days: 7
2026-02-26T19:02:52.6532158Z   if-no-files-found: warn
2026-02-26T19:02:52.6532375Z   compression-level: 6
2026-02-26T19:02:52.6532628Z   overwrite: false
2026-02-26T19:02:52.6532872Z   include-hidden-files: false
2026-02-26T19:02:52.6533080Z env:
2026-02-26T19:02:52.6533359Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-26T19:02:52.6533665Z ##[endgroup]
2026-02-26T19:02:52.6563066Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-26T19:02:52.6564003Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-26T19:02:52.6564375Z ##[endgroup]
2026-02-26T19:02:53.0180133Z (node:973) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-26T19:02:53.0180898Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-26T19:02:53.9634483Z With the provided path, there will be 1 file uploaded
2026-02-26T19:02:53.9638118Z Artifact name is valid!
2026-02-26T19:02:53.9638348Z Root directory input is valid!
2026-02-26T19:02:54.9091202Z Beginning upload of artifact content to blob storage
2026-02-26T19:02:56.5710147Z Uploaded bytes 13868
2026-02-26T19:02:56.8321506Z Finished uploading artifact content to blob storage!
2026-02-26T19:02:56.8322102Z SHA256 digest of uploaded artifact zip is fcd8e3f22fb4257a330fab60205bbe2f75019ba40a63d1fe7ffb8011383cc086
2026-02-26T19:02:56.8322507Z Finalizing artifact upload
2026-02-26T19:02:57.7881016Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs.zip successfully finalized. Artifact ID 5678270724
2026-02-26T19:02:57.7881731Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs has been successfully uploaded! Final size is 13868 bytes. Artifact ID is 5678270724
2026-02-26T19:02:57.7886192Z Artifact download URL: https://github.com/vllm-project/vllm-ascend/actions/runs/22451755861/artifacts/5678270724
2026-02-26T19:02:58.2695221Z ##[group]Run kubectl get pods -n "$NAMESPACE" --ignore-not-found=true
2026-02-26T19:02:58.2695615Z [36;1mkubectl get pods -n "$NAMESPACE" --ignore-not-found=true[0m
2026-02-26T19:02:58.2695956Z [36;1mkubectl delete -f ./lws.yaml --ignore-not-found=true || true[0m
2026-02-26T19:02:58.2696301Z shell: bash -el {0}
2026-02-26T19:02:58.2696463Z env:
2026-02-26T19:02:58.2696765Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-26T19:02:58.2697037Z ##[endgroup]
2026-02-26T19:02:58.2783493Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-26T19:02:58.2784126Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-26T19:02:58.2784353Z ##[endgroup]
2026-02-26T19:02:58.6387173Z (node:1087) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-26T19:02:58.6387861Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-26T19:02:59.2906459Z NAME                                             READY   STATUS    RESTARTS     AGE
2026-02-26T19:02:59.2906873Z linux-aarch64-a3-0-n4cwm-runner-2b4j6            1/1     Running   0            15m
2026-02-26T19:02:59.2907281Z linux-aarch64-a3-0-n4cwm-runner-2b4j6-workflow   1/1     Running   0            15m
2026-02-26T19:02:59.2907635Z vllm-0                                           1/1     Running   1 (8s ago)   14m
2026-02-26T19:02:59.2907921Z vllm-0-1                                         1/1     Running   1 (2s ago)   14m
2026-02-26T19:02:59.3663711Z leaderworkerset.leaderworkerset.x-k8s.io "vllm" deleted from vllm-project namespace
2026-02-26T19:02:59.4757712Z service "vllm-leader" deleted from vllm-project namespace
2026-02-26T19:03:00.0173540Z Post job cleanup.
2026-02-26T19:03:00.0195391Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-26T19:03:00.0196153Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-26T19:03:00.0196432Z ##[endgroup]
2026-02-26T19:03:00.3802530Z (node:1212) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-26T19:03:00.3803240Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-26T19:03:01.0511003Z [command]/usr/bin/git version
2026-02-26T19:03:01.0683483Z git version 2.34.1
2026-02-26T19:03:01.0716572Z Copying '/root/.gitconfig' to '/__w/_temp/6c66c909-8fd1-4778-a7cd-22f9114c2feb/.gitconfig'
2026-02-26T19:03:01.0727181Z Temporarily overriding HOME='/__w/_temp/6c66c909-8fd1-4778-a7cd-22f9114c2feb' before making global git config changes
2026-02-26T19:03:01.0727738Z Adding repository directory to the temporary git global config as a safe directory
2026-02-26T19:03:01.0730821Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-26T19:03:01.0770535Z Removing SSH command configuration
2026-02-26T19:03:01.0775266Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-26T19:03:01.0827671Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-26T19:03:01.1329036Z Removing HTTP extra header
2026-02-26T19:03:01.1329605Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-26T19:03:01.1355655Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-26T19:03:01.1536198Z Removing includeIf entries pointing to credentials config files
2026-02-26T19:03:01.1539770Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-26T19:03:01.1557993Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-26T19:03:01.1558316Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-26T19:03:01.1558611Z includeif.gitdir:/github/workspace/.git.path
2026-02-26T19:03:01.1558875Z includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-26T19:03:01.1565315Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-26T19:03:01.1582316Z /__w/_temp/git-credentials-ec556a8e-7e51-48a1-8ee7-65884d12efa2.config
2026-02-26T19:03:01.1589894Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-ec556a8e-7e51-48a1-8ee7-65884d12efa2.config
2026-02-26T19:03:01.1621664Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-26T19:03:01.1639723Z /__w/_temp/git-credentials-ec556a8e-7e51-48a1-8ee7-65884d12efa2.config
2026-02-26T19:03:01.1646388Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-ec556a8e-7e51-48a1-8ee7-65884d12efa2.config
2026-02-26T19:03:01.1676135Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git.path
2026-02-26T19:03:01.1692996Z /github/runner_temp/git-credentials-ec556a8e-7e51-48a1-8ee7-65884d12efa2.config
2026-02-26T19:03:01.1699402Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-ec556a8e-7e51-48a1-8ee7-65884d12efa2.config
2026-02-26T19:03:01.1729756Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-26T19:03:01.1746962Z /github/runner_temp/git-credentials-ec556a8e-7e51-48a1-8ee7-65884d12efa2.config
2026-02-26T19:03:01.1753218Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-ec556a8e-7e51-48a1-8ee7-65884d12efa2.config
2026-02-26T19:03:01.1786729Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-26T19:03:01.1968302Z Removing credentials config '/__w/_temp/git-credentials-ec556a8e-7e51-48a1-8ee7-65884d12efa2.config'
2026-02-26T19:03:20.0306902Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-26T19:03:20.0307721Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-26T19:03:20.0307959Z ##[endgroup]
2026-02-26T19:03:20.4988701Z Cleaning up orphan processes
