# Run ID: 22451755861
# Commit: ed175d6d92ada7fbb344da7f99d000f644a8cd23
# Job: single-node (deepseek3_2-w8a8, linux-aarch64-a3-16, tests/e2e/nightly/single_node/models/test_dee... / tests/e2e/nightly/single_node/models/test_deepseek_v3_2_w8a8.py
# Date: 2026-02-26
============================================================

ï»¿2026-02-26T21:16:44.0162882Z Current runner version: '2.330.0'
2026-02-26T21:16:44.0167329Z Runner name: 'linux-aarch64-a3-16-jl5bl-runner-xxk27'
2026-02-26T21:16:44.0168010Z Runner group name: 'Default'
2026-02-26T21:16:44.0168714Z Machine name: 'linux-aarch64-a3-16-jl5bl-runner-xxk27'
2026-02-26T21:16:44.0172412Z ##[group]GITHUB_TOKEN Permissions
2026-02-26T21:16:44.0174411Z Actions: write
2026-02-26T21:16:44.0174841Z ArtifactMetadata: write
2026-02-26T21:16:44.0175286Z Attestations: write
2026-02-26T21:16:44.0175713Z Checks: write
2026-02-26T21:16:44.0176087Z Contents: write
2026-02-26T21:16:44.0176533Z Deployments: write
2026-02-26T21:16:44.0176943Z Discussions: write
2026-02-26T21:16:44.0177306Z Issues: write
2026-02-26T21:16:44.0177697Z Metadata: read
2026-02-26T21:16:44.0178050Z Models: read
2026-02-26T21:16:44.0178478Z Packages: write
2026-02-26T21:16:44.0178827Z Pages: write
2026-02-26T21:16:44.0179230Z PullRequests: write
2026-02-26T21:16:44.0179612Z RepositoryProjects: write
2026-02-26T21:16:44.0180200Z SecurityEvents: write
2026-02-26T21:16:44.0180737Z Statuses: write
2026-02-26T21:16:44.0181175Z ##[endgroup]
2026-02-26T21:16:44.0183034Z Secret source: Actions
2026-02-26T21:16:44.0183562Z Prepare workflow directory
2026-02-26T21:16:44.0844917Z Prepare all required actions
2026-02-26T21:16:44.0898047Z Uses: vllm-project/vllm-ascend/.github/workflows/_e2e_nightly_single_node.yaml@refs/heads/main (ed175d6d92ada7fbb344da7f99d000f644a8cd23)
2026-02-26T21:16:44.0902490Z ##[group] Inputs
2026-02-26T21:16:44.0903029Z   runner: linux-aarch64-a3-16
2026-02-26T21:16:44.0903885Z   image: swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3
2026-02-26T21:16:44.0904954Z   tests: tests/e2e/nightly/single_node/models/test_deepseek_v3_2_w8a8.py
2026-02-26T21:16:44.0905646Z   name: deepseek3_2-w8a8
2026-02-26T21:16:44.0906168Z ##[endgroup]
2026-02-26T21:16:44.0907370Z Complete job name: single-node (deepseek3_2-w8a8, linux-aarch64-a3-16, tests/e2e/nightly/single_node/models/test_dee... / tests/e2e/nightly/single_node/models/test_deepseek_v3_2_w8a8.py
2026-02-26T21:16:44.1411134Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-26T21:16:44.1418264Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-26T21:16:44.1419055Z ##[endgroup]
2026-02-26T21:17:16.0477161Z (node:53) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-26T21:17:16.0478715Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-26T21:17:16.5527960Z ##[group]Run npu-smi info
2026-02-26T21:17:16.5528388Z [36;1mnpu-smi info[0m
2026-02-26T21:17:16.5528904Z [36;1mcat /usr/local/Ascend/ascend-toolkit/latest/"$(uname -i)"-linux/ascend_toolkit_install.info[0m
2026-02-26T21:17:16.5529701Z shell: bash -el {0}
2026-02-26T21:17:16.5529969Z env:
2026-02-26T21:17:16.5530235Z   HF_HUB_OFFLINE: 1
2026-02-26T21:17:16.5530459Z   VLLM_USE_MODELSCOPE: true
2026-02-26T21:17:16.5530765Z ##[endgroup]
2026-02-26T21:17:16.5663156Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-26T21:17:16.5664287Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-26T21:17:16.5664686Z ##[endgroup]
2026-02-26T21:17:16.9668475Z (node:79) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-26T21:17:16.9669312Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-26T21:17:19.6789949Z +------------------------------------------------------------------------------------------------+
2026-02-26T21:17:19.6790670Z | npu-smi 25.5.0                   Version: 25.5.0                                               |
2026-02-26T21:17:19.6791362Z +---------------------------+---------------+----------------------------------------------------+
2026-02-26T21:17:19.6791875Z | NPU   Name                | Health        | Power(W)    Temp(C)           Hugepages-Usage(page)|
2026-02-26T21:17:19.6792953Z | Chip  Phy-ID              | Bus-Id        | AICore(%)   Memory-Usage(MB)  HBM-Usage(MB)        |
2026-02-26T21:17:19.6793432Z +===========================+===============+====================================================+
2026-02-26T21:17:19.6793853Z | 0     Ascend910           | OK            | 168.0       34                0    / 0             |
2026-02-26T21:17:19.6794318Z | 0     0                   | 0000:9D:00.0  | 0           0    / 0          3171 / 65536         |
2026-02-26T21:17:19.6794809Z +------------------------------------------------------------------------------------------------+
2026-02-26T21:17:19.6795432Z | 0     Ascend910           | OK            | -           34                0    / 0             |
2026-02-26T21:17:19.6795892Z | 1     1                   | 0000:9F:00.0  | 0           0    / 0          2889 / 65536         |
2026-02-26T21:17:19.6796327Z +===========================+===============+====================================================+
2026-02-26T21:17:19.6796779Z | 1     Ascend910           | OK            | 165.7       34                0    / 0             |
2026-02-26T21:17:19.6797225Z | 0     2                   | 0000:99:00.0  | 0           0    / 0          3155 / 65536         |
2026-02-26T21:17:19.6797670Z +------------------------------------------------------------------------------------------------+
2026-02-26T21:17:19.6798202Z | 1     Ascend910           | OK            | -           36                0    / 0             |
2026-02-26T21:17:19.6798654Z | 1     3                   | 0000:9B:00.0  | 0           0    / 0          2894 / 65536         |
2026-02-26T21:17:19.6799028Z +===========================+===============+====================================================+
2026-02-26T21:17:19.6799474Z | 2     Ascend910           | OK            | 166.4       36                0    / 0             |
2026-02-26T21:17:19.6799873Z | 0     4                   | 0000:95:00.0  | 0           0    / 0          3162 / 65536         |
2026-02-26T21:17:19.6800321Z +------------------------------------------------------------------------------------------------+
2026-02-26T21:17:19.6800904Z | 2     Ascend910           | OK            | -           34                0    / 0             |
2026-02-26T21:17:19.6801307Z | 1     5                   | 0000:97:00.0  | 0           0    / 0          2882 / 65536         |
2026-02-26T21:17:19.6801714Z +===========================+===============+====================================================+
2026-02-26T21:17:19.6802378Z | 3     Ascend910           | OK            | 154.0       35                0    / 0             |
2026-02-26T21:17:19.6802831Z | 0     6                   | 0000:91:00.0  | 0           0    / 0          3162 / 65536         |
2026-02-26T21:17:19.6803282Z +------------------------------------------------------------------------------------------------+
2026-02-26T21:17:19.6803738Z | 3     Ascend910           | OK            | -           34                0    / 0             |
2026-02-26T21:17:19.6804182Z | 1     7                   | 0000:93:00.0  | 0           0    / 0          2886 / 65536         |
2026-02-26T21:17:19.6804540Z +===========================+===============+====================================================+
2026-02-26T21:17:19.6804995Z | 4     Ascend910           | OK            | 164.0       36                0    / 0             |
2026-02-26T21:17:19.6805525Z | 0     8                   | 0000:8D:00.0  | 0           0    / 0          3162 / 65536         |
2026-02-26T21:17:19.6806005Z +------------------------------------------------------------------------------------------------+
2026-02-26T21:17:19.6806478Z | 4     Ascend910           | OK            | -           33                0    / 0             |
2026-02-26T21:17:19.6806870Z | 1     9                   | 0000:8F:00.0  | 0           0    / 0          2881 / 65536         |
2026-02-26T21:17:19.6807258Z +===========================+===============+====================================================+
2026-02-26T21:17:19.6807777Z | 5     Ascend910           | OK            | 168.9       36                0    / 0             |
2026-02-26T21:17:19.6808181Z | 0     10                  | 0000:89:00.0  | 0           0    / 0          3145 / 65536         |
2026-02-26T21:17:19.6808621Z +------------------------------------------------------------------------------------------------+
2026-02-26T21:17:19.6809060Z | 5     Ascend910           | OK            | -           36                0    / 0             |
2026-02-26T21:17:19.6809492Z | 1     11                  | 0000:8B:00.0  | 0           0    / 0          2890 / 65536         |
2026-02-26T21:17:19.9832504Z +===========================+===============+====================================================+
2026-02-26T21:17:19.9833044Z | 6     Ascend910           | OK            | 163.6       34                0    / 0             |
2026-02-26T21:17:19.9833469Z | 0     12                  | 0000:85:00.0  | 0           0    / 0          3163 / 65536         |
2026-02-26T21:17:19.9833939Z +------------------------------------------------------------------------------------------------+
2026-02-26T21:17:19.9834322Z | 6     Ascend910           | OK            | -           34                0    / 0             |
2026-02-26T21:17:19.9834818Z | 1     13                  | 0000:87:00.0  | 0           0    / 0          2882 / 65536         |
2026-02-26T21:17:19.9835171Z +===========================+===============+====================================================+
2026-02-26T21:17:19.9835568Z | 7     Ascend910           | OK            | 164.3       33                0    / 0             |
2026-02-26T21:17:19.9835960Z | 0     14                  | 0000:81:00.0  | 0           0    / 0          3144 / 65536         |
2026-02-26T21:17:19.9836332Z +------------------------------------------------------------------------------------------------+
2026-02-26T21:17:19.9836764Z | 7     Ascend910           | OK            | -           36                0    / 0             |
2026-02-26T21:17:19.9837123Z | 1     15                  | 0000:83:00.0  | 0           0    / 0          2891 / 65536         |
2026-02-26T21:17:19.9837472Z +===========================+===============+====================================================+
2026-02-26T21:17:19.9838156Z +---------------------------+---------------+----------------------------------------------------+
2026-02-26T21:17:19.9838593Z | NPU     Chip              | Process id    | Process name             | Process memory(MB)      |
2026-02-26T21:17:19.9839175Z +===========================+===============+====================================================+
2026-02-26T21:17:19.9839562Z | No running processes found in NPU 0                                                            |
2026-02-26T21:17:19.9839928Z +===========================+===============+====================================================+
2026-02-26T21:17:19.9840315Z | No running processes found in NPU 1                                                            |
2026-02-26T21:17:19.9840782Z +===========================+===============+====================================================+
2026-02-26T21:17:19.9841184Z | No running processes found in NPU 2                                                            |
2026-02-26T21:17:19.9841599Z +===========================+===============+====================================================+
2026-02-26T21:17:19.9841938Z | No running processes found in NPU 3                                                            |
2026-02-26T21:17:19.9842589Z +===========================+===============+====================================================+
2026-02-26T21:17:19.9842948Z | No running processes found in NPU 4                                                            |
2026-02-26T21:17:19.9843314Z +===========================+===============+====================================================+
2026-02-26T21:17:19.9843704Z | No running processes found in NPU 5                                                            |
2026-02-26T21:17:19.9844160Z +===========================+===============+====================================================+
2026-02-26T21:17:19.9844526Z | No running processes found in NPU 6                                                            |
2026-02-26T21:17:19.9844959Z +===========================+===============+====================================================+
2026-02-26T21:17:19.9845361Z | No running processes found in NPU 7                                                            |
2026-02-26T21:17:19.9845761Z +===========================+===============+====================================================+
2026-02-26T21:17:19.9882389Z package_name=Ascend-cann-toolkit
2026-02-26T21:17:19.9882707Z version=8.5.0
2026-02-26T21:17:19.9882913Z innerversion=V100R001C25SPC001B232
2026-02-26T21:17:19.9883318Z compatible_version=[V100R001C15],[V100R001C18],[V100R001C19],[V100R001C20],[V100R001C21],[V100R001C23]
2026-02-26T21:17:19.9883710Z arch=aarch64
2026-02-26T21:17:19.9883889Z os=linux
2026-02-26T21:17:19.9884124Z path=/usr/local/Ascend/cann-8.5.0
2026-02-26T21:17:20.4083307Z ##[group]Run echo "Installed vLLM-related Python packages:"
2026-02-26T21:17:20.4083805Z [36;1mecho "Installed vLLM-related Python packages:"[0m
2026-02-26T21:17:20.4084162Z [36;1mpip list | grep vllm || echo "No vllm packages found."[0m
2026-02-26T21:17:20.4084457Z [36;1m[0m
2026-02-26T21:17:20.4084669Z [36;1mecho ""[0m
2026-02-26T21:17:20.4084876Z [36;1mecho "============================"[0m
2026-02-26T21:17:20.4085204Z [36;1mecho "vLLM Git information"[0m
2026-02-26T21:17:20.4085473Z [36;1mecho "============================"[0m
2026-02-26T21:17:20.4085707Z [36;1mcd vllm[0m
2026-02-26T21:17:20.4085938Z [36;1mif [ -d .git ]; then[0m
2026-02-26T21:17:20.4086208Z [36;1m  echo "Branch:      $(git rev-parse --abbrev-ref HEAD)"[0m
2026-02-26T21:17:20.4086555Z [36;1m  echo "Commit hash: $(git rev-parse HEAD)"[0m
2026-02-26T21:17:20.4086900Z [36;1m  echo "Author:      $(git log -1 --pretty=format:'%an <%ae>')"[0m
2026-02-26T21:17:20.4087382Z [36;1m  echo "Date:        $(git log -1 --pretty=format:'%ad' --date=iso)"[0m
2026-02-26T21:17:20.4087751Z [36;1m  echo "Message:     $(git log -1 --pretty=format:'%s')"[0m
2026-02-26T21:17:20.4088090Z [36;1m  echo "Tags:        $(git tag --points-at HEAD || echo 'None')"[0m
2026-02-26T21:17:20.4088447Z [36;1m  echo "Remote:      $(git remote -v | head -n1)"[0m
2026-02-26T21:17:20.4088691Z [36;1m  echo ""[0m
2026-02-26T21:17:20.4088921Z [36;1melse[0m
2026-02-26T21:17:20.4089140Z [36;1m  echo "No .git directory found in vllm"[0m
2026-02-26T21:17:20.4089399Z [36;1mfi[0m
2026-02-26T21:17:20.4089614Z [36;1mcd ..[0m
2026-02-26T21:17:20.4089795Z [36;1m[0m
2026-02-26T21:17:20.4089981Z [36;1mecho ""[0m
2026-02-26T21:17:20.4090193Z [36;1mecho "============================"[0m
2026-02-26T21:17:20.4090517Z [36;1mecho "vLLM-Ascend Git information"[0m
2026-02-26T21:17:20.4090857Z [36;1mecho "============================"[0m
2026-02-26T21:17:20.4091149Z [36;1mcd vllm-ascend[0m
2026-02-26T21:17:20.4091388Z [36;1mif [ -d .git ]; then[0m
2026-02-26T21:17:20.4091683Z [36;1m  echo "Branch:      $(git rev-parse --abbrev-ref HEAD)"[0m
2026-02-26T21:17:20.4092118Z [36;1m  echo "Commit hash: $(git rev-parse HEAD)"[0m
2026-02-26T21:17:20.4092444Z [36;1m  echo "Author:      $(git log -1 --pretty=format:'%an <%ae>')"[0m
2026-02-26T21:17:20.4092902Z [36;1m  echo "Date:        $(git log -1 --pretty=format:'%ad' --date=iso)"[0m
2026-02-26T21:17:20.4093244Z [36;1m  echo "Message:     $(git log -1 --pretty=format:'%s')"[0m
2026-02-26T21:17:20.4093615Z [36;1m  echo "Tags:        $(git tag --points-at HEAD || echo 'None')"[0m
2026-02-26T21:17:20.4093964Z [36;1m  echo "Remote:      $(git remote -v | head -n1)"[0m
2026-02-26T21:17:20.4094212Z [36;1m  echo ""[0m
2026-02-26T21:17:20.4094481Z [36;1melse[0m
2026-02-26T21:17:20.4094702Z [36;1m  echo "No .git directory found in vllm-ascend"[0m
2026-02-26T21:17:20.4094972Z [36;1mfi[0m
2026-02-26T21:17:20.4095157Z [36;1mcd ..[0m
2026-02-26T21:17:20.4095676Z shell: bash -el {0}
2026-02-26T21:17:20.4095949Z env:
2026-02-26T21:17:20.4096148Z   HF_HUB_OFFLINE: 1
2026-02-26T21:17:20.4096375Z   VLLM_USE_MODELSCOPE: true
2026-02-26T21:17:20.4096578Z ##[endgroup]
2026-02-26T21:17:20.4231546Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-26T21:17:20.4232576Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-26T21:17:20.4232888Z ##[endgroup]
2026-02-26T21:17:20.7988168Z (node:112) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-26T21:17:20.7988957Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-26T21:17:21.5930769Z Installed vLLM-related Python packages:
2026-02-26T21:17:22.8554106Z ais_bench_benchmark               3.0.20250930                /vllm-workspace/vllm-ascend/benchmark
2026-02-26T21:17:22.8554605Z vllm                              0.15.0+empty                /vllm-workspace/vllm
2026-02-26T21:17:22.8555376Z vllm_ascend                       0.14.0rc2.dev198+g3953dcf78 /vllm-workspace/vllm-ascend
2026-02-26T21:17:22.8555775Z 
2026-02-26T21:17:22.8555862Z ============================
2026-02-26T21:17:22.8556092Z vLLM Git information
2026-02-26T21:17:22.8556315Z ============================
2026-02-26T21:17:22.8819184Z Branch:      HEAD
2026-02-26T21:17:22.8839474Z Commit hash: f176443446f659dbab5315e056e605d8984fd976
2026-02-26T21:17:22.8931554Z Author:      TJian <tunjian.tan@embeddedllm.com>
2026-02-26T21:17:22.9003174Z Date:        2026-01-29 14:45:42 +0800
2026-02-26T21:17:22.9003788Z Message:     [Release] [CI] Optim release pipeline (#33156)
2026-02-26T21:17:22.9004191Z Tags:        v0.15.0
2026-02-26T21:17:22.9027995Z Remote:      origin	https://gh-proxy.test.osinfra.cn/https://github.com/vllm-project/vllm.git (fetch)
2026-02-26T21:17:22.9028432Z 
2026-02-26T21:17:22.9028435Z 
2026-02-26T21:17:22.9028549Z ============================
2026-02-26T21:17:22.9028905Z vLLM-Ascend Git information
2026-02-26T21:17:22.9029133Z ============================
2026-02-26T21:17:22.9100089Z Branch:      main
2026-02-26T21:17:22.9120946Z Commit hash: 3953dcf784da3471eb99795da67803fb06f40cc9
2026-02-26T21:17:22.9213665Z Author:      Cao Yi <slightwindsec@gmail.com>
2026-02-26T21:17:22.9238775Z Date:        2026-02-26 10:59:25 +0800
2026-02-26T21:17:22.9260666Z Message:     [Feature][Quant] Auto-detect quantization format from model files (#6645)
2026-02-26T21:17:22.9597156Z Tags:        
2026-02-26T21:17:22.9615538Z Remote:      origin	https://gh-proxy.test.osinfra.cn/https://github.com/vllm-project/vllm-ascend (fetch)
2026-02-26T21:17:22.9615958Z 
2026-02-26T21:17:23.3600285Z ##[group]Run apt-get update && apt-get -y install clang-15
2026-02-26T21:17:23.3600741Z [36;1mapt-get update && apt-get -y install clang-15[0m
2026-02-26T21:17:23.3601154Z [36;1mupdate-alternatives --install /usr/bin/clang clang /usr/bin/clang-15 20[0m
2026-02-26T21:17:23.3601817Z [36;1mupdate-alternatives --install /usr/bin/clang++ clang++ /usr/bin/clang++-15 20[0m
2026-02-26T21:17:23.3602471Z shell: bash -l {0}
2026-02-26T21:17:23.3602689Z env:
2026-02-26T21:17:23.3602913Z   HF_HUB_OFFLINE: 1
2026-02-26T21:17:23.3603115Z   VLLM_USE_MODELSCOPE: true
2026-02-26T21:17:23.3603418Z ##[endgroup]
2026-02-26T21:17:23.3696542Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-26T21:17:23.3697482Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-26T21:17:23.3697769Z ##[endgroup]
2026-02-26T21:17:23.7295893Z (node:154) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-26T21:17:23.7296780Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-26T21:17:25.1594134Z Get:1 http://ports.ubuntu.com/ubuntu-ports jammy InRelease [270 kB]
2026-02-26T21:17:26.4042753Z Get:2 http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease [128 kB]
2026-02-26T21:17:26.7120174Z Get:3 http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease [127 kB]
2026-02-26T21:17:27.0192989Z Get:4 http://ports.ubuntu.com/ubuntu-ports jammy-security InRelease [129 kB]
2026-02-26T21:17:27.3281763Z Get:5 http://ports.ubuntu.com/ubuntu-ports jammy/restricted arm64 Packages [24.2 kB]
2026-02-26T21:17:27.3527437Z Get:6 http://ports.ubuntu.com/ubuntu-ports jammy/universe arm64 Packages [17.2 MB]
2026-02-26T21:17:29.2518477Z Get:7 http://ports.ubuntu.com/ubuntu-ports jammy/multiverse arm64 Packages [224 kB]
2026-02-26T21:17:29.2604308Z Get:8 http://ports.ubuntu.com/ubuntu-ports jammy/main arm64 Packages [1758 kB]
2026-02-26T21:17:29.3781257Z Get:9 http://ports.ubuntu.com/ubuntu-ports jammy-updates/multiverse arm64 Packages [47.7 kB]
2026-02-26T21:17:29.3801544Z Get:10 http://ports.ubuntu.com/ubuntu-ports jammy-updates/universe arm64 Packages [1659 kB]
2026-02-26T21:17:29.5135385Z Get:11 http://ports.ubuntu.com/ubuntu-ports jammy-updates/restricted arm64 Packages [6683 kB]
2026-02-26T21:17:29.9979804Z Get:12 http://ports.ubuntu.com/ubuntu-ports jammy-updates/main arm64 Packages [3890 kB]
2026-02-26T21:17:30.2772877Z Get:13 http://ports.ubuntu.com/ubuntu-ports jammy-backports/universe arm64 Packages [35.3 kB]
2026-02-26T21:17:30.2787842Z Get:14 http://ports.ubuntu.com/ubuntu-ports jammy-backports/main arm64 Packages [83.5 kB]
2026-02-26T21:17:30.2849139Z Get:15 http://ports.ubuntu.com/ubuntu-ports jammy-security/multiverse arm64 Packages [41.2 kB]
2026-02-26T21:17:30.2882499Z Get:16 http://ports.ubuntu.com/ubuntu-ports jammy-security/restricted arm64 Packages [6473 kB]
2026-02-26T21:17:30.7575218Z Get:17 http://ports.ubuntu.com/ubuntu-ports jammy-security/universe arm64 Packages [1355 kB]
2026-02-26T21:17:30.8593142Z Get:18 http://ports.ubuntu.com/ubuntu-ports jammy-security/main arm64 Packages [3554 kB]
2026-02-26T21:17:31.2602325Z Fetched 43.7 MB in 7s (6625 kB/s)
2026-02-26T21:17:32.1551133Z Reading package lists...
2026-02-26T21:17:33.1112890Z Reading package lists...
2026-02-26T21:17:33.3645215Z Building dependency tree...
2026-02-26T21:17:33.3653327Z Reading state information...
2026-02-26T21:17:33.6953436Z clang-15 is already the newest version (1:15.0.7-0ubuntu0.22.04.3).
2026-02-26T21:17:33.6954094Z The following packages were automatically installed and are no longer required:
2026-02-26T21:17:33.6954733Z   autoconf automake autotools-dev file javascript-common libboost-atomic-dev
2026-02-26T21:17:33.6955263Z   libboost-atomic1.74-dev libboost-atomic1.74.0 libboost-chrono-dev
2026-02-26T21:17:33.6955705Z   libboost-chrono1.74-dev libboost-chrono1.74.0 libboost-container-dev
2026-02-26T21:17:33.6956497Z   libboost-container1.74-dev libboost-container1.74.0 libboost-context-dev
2026-02-26T21:17:33.6956951Z   libboost-context1.74-dev libboost-context1.74.0 libboost-coroutine-dev
2026-02-26T21:17:33.6957436Z   libboost-coroutine1.74-dev libboost-coroutine1.74.0 libboost-date-time-dev
2026-02-26T21:17:33.6957880Z   libboost-date-time1.74-dev libboost-date-time1.74.0 libboost-dev
2026-02-26T21:17:33.6958292Z   libboost-exception-dev libboost-exception1.74-dev libboost-fiber-dev
2026-02-26T21:17:33.6958726Z   libboost-fiber1.74-dev libboost-fiber1.74.0 libboost-filesystem-dev
2026-02-26T21:17:33.6959170Z   libboost-filesystem1.74-dev libboost-filesystem1.74.0 libboost-graph-dev
2026-02-26T21:17:33.6959575Z   libboost-graph-parallel-dev libboost-graph-parallel1.74-dev
2026-02-26T21:17:33.6960017Z   libboost-graph-parallel1.74.0 libboost-graph1.74-dev libboost-graph1.74.0
2026-02-26T21:17:33.6960461Z   libboost-iostreams-dev libboost-iostreams1.74-dev libboost-iostreams1.74.0
2026-02-26T21:17:33.6960912Z   libboost-locale-dev libboost-locale1.74-dev libboost-locale1.74.0
2026-02-26T21:17:33.6961310Z   libboost-log-dev libboost-log1.74-dev libboost-log1.74.0 libboost-math-dev
2026-02-26T21:17:33.6961755Z   libboost-math1.74-dev libboost-math1.74.0 libboost-mpi1.74.0
2026-02-26T21:17:33.6962285Z   libboost-nowide-dev libboost-nowide1.74-dev libboost-nowide1.74.0
2026-02-26T21:17:33.6962799Z   libboost-numpy-dev libboost-numpy1.74-dev libboost-numpy1.74.0
2026-02-26T21:17:33.6963240Z   libboost-program-options-dev libboost-program-options1.74-dev
2026-02-26T21:17:33.6963648Z   libboost-program-options1.74.0 libboost-python-dev libboost-python1.74-dev
2026-02-26T21:17:33.6964084Z   libboost-python1.74.0 libboost-random-dev libboost-random1.74-dev
2026-02-26T21:17:33.6964573Z   libboost-random1.74.0 libboost-regex-dev libboost-regex1.74-dev
2026-02-26T21:17:33.6964925Z   libboost-regex1.74.0 libboost-serialization-dev
2026-02-26T21:17:33.6965310Z   libboost-serialization1.74-dev libboost-serialization1.74.0
2026-02-26T21:17:33.6965671Z   libboost-stacktrace-dev libboost-stacktrace1.74-dev
2026-02-26T21:17:33.6966064Z   libboost-stacktrace1.74.0 libboost-system-dev libboost-system1.74-dev
2026-02-26T21:17:33.6966484Z   libboost-system1.74.0 libboost-test-dev libboost-test1.74-dev
2026-02-26T21:17:33.6966859Z   libboost-test1.74.0 libboost-thread-dev libboost-thread1.74-dev
2026-02-26T21:17:33.6967265Z   libboost-thread1.74.0 libboost-timer-dev libboost-timer1.74-dev
2026-02-26T21:17:33.6967888Z   libboost-timer1.74.0 libboost-tools-dev libboost-type-erasure-dev
2026-02-26T21:17:33.6968396Z   libboost-type-erasure1.74-dev libboost-type-erasure1.74.0 libboost-wave-dev
2026-02-26T21:17:33.6968791Z   libboost-wave1.74-dev libboost-wave1.74.0 libboost1.74-dev
2026-02-26T21:17:33.6969198Z   libboost1.74-tools-dev libcaf-openmpi-3 libcoarrays-dev libevent-2.1-7
2026-02-26T21:17:33.6969657Z   libevent-core-2.1-7 libevent-dev libevent-extra-2.1-7 libevent-openssl-2.1-7
2026-02-26T21:17:33.6970067Z   libevent-pthreads-2.1-7 libhwloc-dev libjs-jquery libjs-jquery-ui
2026-02-26T21:17:33.6970501Z   libjs-sphinxdoc libjs-underscore libltdl-dev libltdl7 libmagic-mgc libmagic1
2026-02-26T21:17:33.6970913Z   libopenmpi3 libpmix-dev libpmix2 libsigsegv2 libtool libucx0 m4
2026-02-26T21:17:33.6973472Z   openmpi-common python3-dev python3-distutils python3-lib2to3 python3.10-dev
2026-02-26T21:17:33.6974939Z Use 'apt autoremove' to remove them.
2026-02-26T21:17:33.7286039Z 0 upgraded, 0 newly installed, 0 to remove and 12 not upgraded.
2026-02-26T21:17:34.1871125Z ##[group]Run # ignore test_dispatch_ffn_combine until the test is fixed
2026-02-26T21:17:34.1871629Z [36;1m# ignore test_dispatch_ffn_combine until the test is fixed[0m
2026-02-26T21:17:34.1872248Z [36;1mpytest -sv tests/e2e/nightly/single_node/models/test_deepseek_v3_2_w8a8.py \[0m
2026-02-26T21:17:34.1872705Z [36;1m--ignore=tests/e2e/nightly/single_node/ops/singlecard_ops/test_fused_moe.py[0m
2026-02-26T21:17:34.1873234Z shell: bash -el {0}
2026-02-26T21:17:34.1873428Z env:
2026-02-26T21:17:34.1873637Z   HF_HUB_OFFLINE: 1
2026-02-26T21:17:34.1873883Z   VLLM_USE_MODELSCOPE: true
2026-02-26T21:17:34.1874222Z   VLLM_WORKER_MULTIPROC_METHOD: spawn
2026-02-26T21:17:34.1874594Z   VLLM_CI_RUNNER: linux-aarch64-a3-16
2026-02-26T21:17:34.1874892Z   BENCHMARK_HOME: /vllm-workspace/vllm-ascend/benchmark
2026-02-26T21:17:34.1875187Z ##[endgroup]
2026-02-26T21:17:34.1960257Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-26T21:17:34.1961124Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-26T21:17:34.1961392Z ##[endgroup]
2026-02-26T21:17:34.5514936Z (node:211) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-26T21:17:34.5515896Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-26T21:17:50.0515970Z INFO 02-26 21:17:50 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:17:50.0516484Z INFO 02-26 21:17:50 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:17:50.0517083Z INFO 02-26 21:17:50 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:17:50.0999541Z INFO 02-26 21:17:50 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:17:57.5422922Z ============================= test session starts ==============================
2026-02-26T21:17:57.5423917Z platform linux -- Python 3.11.14, pytest-8.4.2, pluggy-1.6.0 -- /usr/local/python3.11.14/bin/python3
2026-02-26T21:17:57.5424382Z cachedir: .pytest_cache
2026-02-26T21:17:57.5424626Z rootdir: /vllm-workspace/vllm-ascend
2026-02-26T21:17:57.5424909Z configfile: pyproject.toml
2026-02-26T21:17:57.5425215Z plugins: asyncio-1.3.0, cov-7.0.0, mock-3.15.1, anyio-4.12.1
2026-02-26T21:17:57.5425729Z asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
2026-02-26T21:17:58.4144119Z collecting ... collected 1 item
2026-02-26T21:17:58.4144342Z 
2026-02-26T21:17:58.4160152Z [2026-02-26 21:17:58] INFO conftest.py:241: Starting server with command: vllm serve vllm-ascend/DeepSeek-V3.2-W8A8 --enable-expert-parallel --tensor-parallel-size 8 --data-parallel-size 2 --port 39027 --max-model-len 8192 --max-num-batched-tokens 8192 --max-num-seqs 4 --trust-remote-code --quantization ascend --gpu-memory-utilization 0.98 --compilation-config {"cudagraph_capture_sizes":[8, 16, 24, 32, 40, 48], "cudagraph_mode":"FULL_DECODE_ONLY"} --speculative-config {"num_speculative_tokens": 3, "method":"deepseek_mtp"} --additional-config {"layer_sharding": ["q_b_proj", "o_proj"]} --reasoning-parser deepseek_v3 --tokenizer_mode deepseek_v32
2026-02-26T21:18:02.6643608Z tests/e2e/nightly/single_node/models/test_deepseek_v3_2_w8a8.py::test_models[2-8-vllm-ascend/DeepSeek-V3.2-W8A8] INFO 02-26 21:18:02 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:18:02.6644394Z INFO 02-26 21:18:02 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:18:02.6644966Z INFO 02-26 21:18:02 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:18:02.6706144Z INFO 02-26 21:18:02 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:18:08.6719086Z 2026-02-26 21:18:08,670 - 982 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:18:08.7019613Z INFO 02-26 21:18:08 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:18:08.8403112Z INFO 02-26 21:18:08 [serve.py:100] Defaulting api_server_count to data_parallel_size (2).
2026-02-26T21:18:08.8403689Z INFO 02-26 21:18:08 [utils.py:325] 
2026-02-26T21:18:08.8478957Z INFO 02-26 21:18:08 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
2026-02-26T21:18:08.8479603Z INFO 02-26 21:18:08 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
2026-02-26T21:18:08.8480092Z INFO 02-26 21:18:08 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   vllm-ascend/DeepSeek-V3.2-W8A8
2026-02-26T21:18:08.8480569Z INFO 02-26 21:18:08 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
2026-02-26T21:18:08.8480876Z INFO 02-26 21:18:08 [utils.py:325] 
2026-02-26T21:18:08.8484840Z INFO 02-26 21:18:08 [utils.py:261] non-default args: {'model_tag': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'api_server_count': 2, 'port': 39027, 'model': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'tokenizer_mode': 'deepseek_v32', 'trust_remote_code': True, 'max_model_len': 8192, 'quantization': 'ascend', 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 8, 'data_parallel_size': 2, 'enable_expert_parallel': True, 'gpu_memory_utilization': 0.98, 'max_num_batched_tokens': 8192, 'max_num_seqs': 4, 'speculative_config': {'num_speculative_tokens': 3, 'method': 'deepseek_mtp'}, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [8, 16, 24, 32, 40, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}, 'additional_config': {'layer_sharding': ['q_b_proj', 'o_proj']}}
2026-02-26T21:18:08.8900864Z 2026-02-26 21:18:08,889 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-26T21:18:08.8981441Z INFO 02-26 21:18:08 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-26T21:18:08.8987847Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:08.8988573Z [2026-02-26 21:18:08] WARNING configuration_utils.py:697: The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:08.9045249Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:08.9045909Z [2026-02-26 21:18:08] WARNING configuration_utils.py:697: The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:08.9055127Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:08.9055787Z [2026-02-26 21:18:08] WARNING configuration_utils.py:697: The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:08.9063505Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T21:18:08.9064737Z [2026-02-26 21:18:08] WARNING configuration_utils.py:635: You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T21:18:08.9277720Z INFO 02-26 21:18:08 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-26T21:18:08.9278245Z INFO 02-26 21:18:08 [model.py:1561] Using max model len 8192
2026-02-26T21:18:09.2465282Z WARNING 02-26 21:18:09 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-26T21:18:09.2466219Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:09.2466896Z [2026-02-26 21:18:09] WARNING configuration_utils.py:697: The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:09.2475963Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:09.2476641Z [2026-02-26 21:18:09] WARNING configuration_utils.py:697: The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:09.2484103Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T21:18:09.2485038Z [2026-02-26 21:18:09] WARNING configuration_utils.py:635: You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T21:18:09.2575929Z INFO 02-26 21:18:09 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-26T21:18:09.2577283Z INFO 02-26 21:18:09 [model.py:1561] Using max model len 163840
2026-02-26T21:18:09.2582175Z WARNING 02-26 21:18:09 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-26T21:18:09.2583280Z INFO 02-26 21:18:09 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.
2026-02-26T21:18:09.8412695Z INFO 02-26 21:18:09 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-26T21:18:09.8413336Z INFO 02-26 21:18:09 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-26T21:18:09.8425460Z WARNING 02-26 21:18:09 [platform.py:735] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-26T21:18:09.8426338Z WARNING 02-26 21:18:09 [platform.py:776] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-26T21:18:09.8426993Z INFO 02-26 21:18:09 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:18:09.8427351Z INFO 02-26 21:18:09 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:18:09.8428226Z INFO 02-26 21:18:09 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:18:09.8429365Z INFO 02-26 21:18:09 [platform.py:323] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-26T21:18:09.8430112Z WARNING 02-26 21:18:09 [platform.py:340] [91m
2026-02-26T21:18:09.8430490Z WARNING 02-26 21:18:09 [platform.py:340]             **********************************************************************************
2026-02-26T21:18:09.8431012Z WARNING 02-26 21:18:09 [platform.py:340]             * WARNING: You have enabled the *full graph* feature.
2026-02-26T21:18:09.8431538Z WARNING 02-26 21:18:09 [platform.py:340]             * This is an early experimental stage and may involve various unknown issues.
2026-02-26T21:18:09.8432296Z WARNING 02-26 21:18:09 [platform.py:340]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-26T21:18:09.8433163Z WARNING 02-26 21:18:09 [platform.py:340]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-26T21:18:09.8433733Z WARNING 02-26 21:18:09 [platform.py:340]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-26T21:18:09.8434318Z WARNING 02-26 21:18:09 [platform.py:340]             * batch size for graph capture.
2026-02-26T21:18:09.8434730Z WARNING 02-26 21:18:09 [platform.py:340]             * For more details, please refer to:
2026-02-26T21:18:09.8435329Z WARNING 02-26 21:18:09 [platform.py:340]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-26T21:18:09.8436010Z WARNING 02-26 21:18:09 [platform.py:340]             **********************************************************************************[0m
2026-02-26T21:18:09.8436406Z WARNING 02-26 21:18:09 [platform.py:340]             
2026-02-26T21:18:09.8436807Z INFO 02-26 21:18:09 [platform.py:448] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-26T21:18:09.8454917Z INFO 02-26 21:18:09 [utils.py:851] Started DP Coordinator process (PID: 995)
2026-02-26T21:18:14.5222901Z INFO 02-26 21:18:14 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:18:14.5223343Z INFO 02-26 21:18:14 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:18:14.5223930Z INFO 02-26 21:18:14 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:18:14.5296159Z INFO 02-26 21:18:14 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:18:14.6320544Z INFO 02-26 21:18:14 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:18:14.6321148Z INFO 02-26 21:18:14 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:18:14.6321642Z INFO 02-26 21:18:14 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:18:14.6400671Z INFO 02-26 21:18:14 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:18:24.2784823Z INFO 02-26 21:18:24 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:18:24.2785371Z INFO 02-26 21:18:24 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:18:24.2785903Z INFO 02-26 21:18:24 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:18:24.2855112Z INFO 02-26 21:18:24 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:18:29.4149527Z INFO 02-26 21:18:29 [utils.py:218] Started 2 API server processes
2026-02-26T21:18:29.6031476Z [0;36m(EngineCore_DP1 pid=1017)[0;0m 2026-02-26 21:18:29,601 - 1017 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:18:29.6047444Z [0;36m(EngineCore_DP0 pid=998)[0;0m 2026-02-26 21:18:29,603 - 998 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:18:29.6064334Z [0;36m(EngineCore_DP1 pid=1017)[0;0m INFO 02-26 21:18:29 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:18:29.6083204Z [0;36m(EngineCore_DP0 pid=998)[0;0m INFO 02-26 21:18:29 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:18:29.6097030Z [0;36m(EngineCore_DP0 pid=998)[0;0m INFO 02-26 21:18:29 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', speculative_config=SpeculativeConfig(method='mtp', model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', num_spec_tokens=3), tokenizer='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', skip_tokenizer_init=False, tokenizer_mode=deepseek_v32, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=2, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=bfloat16, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [8, 16, 24, 32, 40, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 48, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
2026-02-26T21:18:34.2661648Z INFO 02-26 21:18:34 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:18:34.2662248Z INFO 02-26 21:18:34 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:18:34.2662772Z INFO 02-26 21:18:34 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:18:34.2739964Z INFO 02-26 21:18:34 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:18:34.3919963Z INFO 02-26 21:18:34 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:18:34.3920525Z INFO 02-26 21:18:34 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:18:34.3921145Z INFO 02-26 21:18:34 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:18:34.4000988Z INFO 02-26 21:18:34 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:18:34.8007548Z INFO 02-26 21:18:34 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:18:34.8008055Z INFO 02-26 21:18:34 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:18:34.8008525Z INFO 02-26 21:18:34 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:18:34.8094804Z INFO 02-26 21:18:34 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:18:34.8281495Z INFO 02-26 21:18:34 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:18:34.8281878Z INFO 02-26 21:18:34 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:18:34.8282493Z INFO 02-26 21:18:34 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:18:34.8362709Z INFO 02-26 21:18:34 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:18:39.9316078Z 2026-02-26 21:18:39,930 - 1044 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:18:39.9376357Z INFO 02-26 21:18:39 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:18:40.0399507Z 2026-02-26 21:18:40,038 - 1043 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:18:40.0459337Z INFO 02-26 21:18:40 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:18:40.2546974Z [0;36m(ApiServer_0 pid=1028)[0;0m 2026-02-26 21:18:40,253 - 1028 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:18:40.2697280Z [0;36m(ApiServer_0 pid=1028)[0;0m INFO 02-26 21:18:40 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:18:40.2967662Z [0;36m(ApiServer_0 pid=1028)[0;0m 2026-02-26 21:18:40,295 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-26T21:18:40.3036797Z [0;36m(ApiServer_0 pid=1028)[0;0m INFO 02-26 21:18:40 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-26T21:18:40.3311959Z [0;36m(ApiServer_1 pid=1029)[0;0m 2026-02-26 21:18:40,330 - 1029 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:18:40.3466466Z [0;36m(ApiServer_1 pid=1029)[0;0m INFO 02-26 21:18:40 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:18:40.3660966Z [0;36m(ApiServer_1 pid=1029)[0;0m 2026-02-26 21:18:40,360 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-26T21:18:40.3685146Z [0;36m(ApiServer_1 pid=1029)[0;0m INFO 02-26 21:18:40 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-26T21:18:40.4055912Z [0;36m(ApiServer_0 pid=1028)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:40.4056885Z [0;36m(ApiServer_0 pid=1028)[0;0m [2026-02-26 21:18:40] WARNING configuration_utils.py:697: The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:40.4076456Z [0;36m(ApiServer_0 pid=1028)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:40.4077354Z [0;36m(ApiServer_0 pid=1028)[0;0m [2026-02-26 21:18:40] WARNING configuration_utils.py:697: The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:40.4086530Z [0;36m(ApiServer_0 pid=1028)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:40.4087408Z [0;36m(ApiServer_0 pid=1028)[0;0m [2026-02-26 21:18:40] WARNING configuration_utils.py:697: The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:40.4095276Z [0;36m(ApiServer_0 pid=1028)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T21:18:40.4097169Z [0;36m(ApiServer_0 pid=1028)[0;0m [2026-02-26 21:18:40] WARNING configuration_utils.py:635: You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T21:18:40.4194317Z [0;36m(ApiServer_0 pid=1028)[0;0m INFO 02-26 21:18:40 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-26T21:18:40.4196474Z [0;36m(ApiServer_0 pid=1028)[0;0m INFO 02-26 21:18:40 [model.py:1561] Using max model len 8192
2026-02-26T21:18:40.4696385Z [0;36m(ApiServer_1 pid=1029)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:40.4697322Z [0;36m(ApiServer_1 pid=1029)[0;0m [2026-02-26 21:18:40] WARNING configuration_utils.py:697: The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:40.4707739Z [0;36m(ApiServer_1 pid=1029)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:40.4708790Z [0;36m(ApiServer_1 pid=1029)[0;0m [2026-02-26 21:18:40] WARNING configuration_utils.py:697: The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:40.4719247Z [0;36m(ApiServer_1 pid=1029)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:40.4720122Z [0;36m(ApiServer_1 pid=1029)[0;0m [2026-02-26 21:18:40] WARNING configuration_utils.py:697: The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:40.4727328Z [0;36m(ApiServer_1 pid=1029)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T21:18:40.4728486Z [0;36m(ApiServer_1 pid=1029)[0;0m [2026-02-26 21:18:40] WARNING configuration_utils.py:635: You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T21:18:40.4794678Z [0;36m(ApiServer_1 pid=1029)[0;0m INFO 02-26 21:18:40 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-26T21:18:40.4799026Z [0;36m(ApiServer_1 pid=1029)[0;0m INFO 02-26 21:18:40 [model.py:1561] Using max model len 8192
2026-02-26T21:18:40.5272304Z [0;36m(ApiServer_0 pid=1028)[0;0m WARNING 02-26 21:18:40 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-26T21:18:40.5275129Z [0;36m(ApiServer_0 pid=1028)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:40.5276024Z [0;36m(ApiServer_0 pid=1028)[0;0m [2026-02-26 21:18:40] WARNING configuration_utils.py:697: The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:40.5284637Z [0;36m(ApiServer_0 pid=1028)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:40.5285742Z [0;36m(ApiServer_0 pid=1028)[0;0m [2026-02-26 21:18:40] WARNING configuration_utils.py:697: The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:40.5291950Z [0;36m(ApiServer_0 pid=1028)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T21:18:40.5293290Z [0;36m(ApiServer_0 pid=1028)[0;0m [2026-02-26 21:18:40] WARNING configuration_utils.py:635: You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T21:18:40.5362250Z [0;36m(ApiServer_0 pid=1028)[0;0m INFO 02-26 21:18:40 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-26T21:18:40.5364261Z [0;36m(ApiServer_0 pid=1028)[0;0m INFO 02-26 21:18:40 [model.py:1561] Using max model len 163840
2026-02-26T21:18:40.5365492Z [0;36m(ApiServer_0 pid=1028)[0;0m WARNING 02-26 21:18:40 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-26T21:18:40.5366560Z [0;36m(ApiServer_0 pid=1028)[0;0m INFO 02-26 21:18:40 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.
2026-02-26T21:18:40.5874262Z [0;36m(ApiServer_1 pid=1029)[0;0m WARNING 02-26 21:18:40 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-26T21:18:40.5876317Z [0;36m(ApiServer_1 pid=1029)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:40.5877232Z [0;36m(ApiServer_1 pid=1029)[0;0m [2026-02-26 21:18:40] WARNING configuration_utils.py:697: The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:40.5885686Z [0;36m(ApiServer_1 pid=1029)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:40.5886572Z [0;36m(ApiServer_1 pid=1029)[0;0m [2026-02-26 21:18:40] WARNING configuration_utils.py:697: The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-26T21:18:40.5894192Z [0;36m(ApiServer_1 pid=1029)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T21:18:40.5895372Z [0;36m(ApiServer_1 pid=1029)[0;0m [2026-02-26 21:18:40] WARNING configuration_utils.py:635: You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-26T21:18:40.5958667Z [0;36m(ApiServer_1 pid=1029)[0;0m INFO 02-26 21:18:40 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-26T21:18:40.5960706Z [0;36m(ApiServer_1 pid=1029)[0;0m INFO 02-26 21:18:40 [model.py:1561] Using max model len 163840
2026-02-26T21:18:40.5961666Z [0;36m(ApiServer_1 pid=1029)[0;0m WARNING 02-26 21:18:40 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-26T21:18:40.5964152Z [0;36m(ApiServer_1 pid=1029)[0;0m INFO 02-26 21:18:40 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.
2026-02-26T21:18:40.9885713Z [0;36m(ApiServer_0 pid=1028)[0;0m INFO 02-26 21:18:40 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-26T21:18:40.9886473Z [0;36m(ApiServer_0 pid=1028)[0;0m INFO 02-26 21:18:40 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-26T21:18:40.9887520Z [0;36m(ApiServer_0 pid=1028)[0;0m WARNING 02-26 21:18:40 [platform.py:735] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-26T21:18:40.9888659Z [0;36m(ApiServer_0 pid=1028)[0;0m WARNING 02-26 21:18:40 [platform.py:776] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-26T21:18:40.9889457Z [0;36m(ApiServer_0 pid=1028)[0;0m INFO 02-26 21:18:40 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:18:40.9890116Z [0;36m(ApiServer_0 pid=1028)[0;0m INFO 02-26 21:18:40 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:18:40.9891228Z [0;36m(ApiServer_0 pid=1028)[0;0m INFO 02-26 21:18:40 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:18:40.9892806Z [0;36m(ApiServer_0 pid=1028)[0;0m INFO 02-26 21:18:40 [platform.py:323] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-26T21:18:40.9893990Z [0;36m(ApiServer_0 pid=1028)[0;0m WARNING 02-26 21:18:40 [platform.py:340] [91m
2026-02-26T21:18:40.9894576Z [0;36m(ApiServer_0 pid=1028)[0;0m WARNING 02-26 21:18:40 [platform.py:340]             **********************************************************************************
2026-02-26T21:18:40.9895279Z [0;36m(ApiServer_0 pid=1028)[0;0m WARNING 02-26 21:18:40 [platform.py:340]             * WARNING: You have enabled the *full graph* feature.
2026-02-26T21:18:40.9896057Z [0;36m(ApiServer_0 pid=1028)[0;0m WARNING 02-26 21:18:40 [platform.py:340]             * This is an early experimental stage and may involve various unknown issues.
2026-02-26T21:18:40.9896816Z [0;36m(ApiServer_0 pid=1028)[0;0m WARNING 02-26 21:18:40 [platform.py:340]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-26T21:18:40.9897632Z [0;36m(ApiServer_0 pid=1028)[0;0m WARNING 02-26 21:18:40 [platform.py:340]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-26T21:18:40.9898438Z [0;36m(ApiServer_0 pid=1028)[0;0m WARNING 02-26 21:18:40 [platform.py:340]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-26T21:18:40.9899113Z [0;36m(ApiServer_0 pid=1028)[0;0m WARNING 02-26 21:18:40 [platform.py:340]             * batch size for graph capture.
2026-02-26T21:18:40.9899729Z [0;36m(ApiServer_0 pid=1028)[0;0m WARNING 02-26 21:18:40 [platform.py:340]             * For more details, please refer to:
2026-02-26T21:18:40.9900541Z [0;36m(ApiServer_0 pid=1028)[0;0m WARNING 02-26 21:18:40 [platform.py:340]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-26T21:18:40.9901305Z [0;36m(ApiServer_0 pid=1028)[0;0m WARNING 02-26 21:18:40 [platform.py:340]             **********************************************************************************[0m
2026-02-26T21:18:40.9902146Z [0;36m(ApiServer_0 pid=1028)[0;0m WARNING 02-26 21:18:40 [platform.py:340]             
2026-02-26T21:18:40.9902803Z [0;36m(ApiServer_0 pid=1028)[0;0m INFO 02-26 21:18:40 [platform.py:448] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-26T21:18:41.0268655Z [0;36m(ApiServer_1 pid=1029)[0;0m INFO 02-26 21:18:41 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-26T21:18:41.0269436Z [0;36m(ApiServer_1 pid=1029)[0;0m INFO 02-26 21:18:41 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-26T21:18:41.0270333Z [0;36m(ApiServer_1 pid=1029)[0;0m WARNING 02-26 21:18:41 [platform.py:735] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-26T21:18:41.0271455Z [0;36m(ApiServer_1 pid=1029)[0;0m WARNING 02-26 21:18:41 [platform.py:776] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-26T21:18:41.0272429Z [0;36m(ApiServer_1 pid=1029)[0;0m INFO 02-26 21:18:41 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:18:41.0273065Z [0;36m(ApiServer_1 pid=1029)[0;0m INFO 02-26 21:18:41 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:18:41.0274198Z [0;36m(ApiServer_1 pid=1029)[0;0m INFO 02-26 21:18:41 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:18:41.0275611Z [0;36m(ApiServer_1 pid=1029)[0;0m INFO 02-26 21:18:41 [platform.py:323] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-26T21:18:41.0276388Z [0;36m(ApiServer_1 pid=1029)[0;0m WARNING 02-26 21:18:41 [platform.py:340] [91m
2026-02-26T21:18:41.0277235Z [0;36m(ApiServer_1 pid=1029)[0;0m WARNING 02-26 21:18:41 [platform.py:340]             **********************************************************************************
2026-02-26T21:18:41.0277973Z [0;36m(ApiServer_1 pid=1029)[0;0m WARNING 02-26 21:18:41 [platform.py:340]             * WARNING: You have enabled the *full graph* feature.
2026-02-26T21:18:41.0278691Z [0;36m(ApiServer_1 pid=1029)[0;0m WARNING 02-26 21:18:41 [platform.py:340]             * This is an early experimental stage and may involve various unknown issues.
2026-02-26T21:18:41.0279501Z [0;36m(ApiServer_1 pid=1029)[0;0m WARNING 02-26 21:18:41 [platform.py:340]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-26T21:18:41.0280302Z [0;36m(ApiServer_1 pid=1029)[0;0m WARNING 02-26 21:18:41 [platform.py:340]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-26T21:18:41.0281069Z [0;36m(ApiServer_1 pid=1029)[0;0m WARNING 02-26 21:18:41 [platform.py:340]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-26T21:18:41.0281795Z [0;36m(ApiServer_1 pid=1029)[0;0m WARNING 02-26 21:18:41 [platform.py:340]             * batch size for graph capture.
2026-02-26T21:18:41.0282617Z [0;36m(ApiServer_1 pid=1029)[0;0m WARNING 02-26 21:18:41 [platform.py:340]             * For more details, please refer to:
2026-02-26T21:18:41.0283406Z [0;36m(ApiServer_1 pid=1029)[0;0m WARNING 02-26 21:18:41 [platform.py:340]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-26T21:18:41.0284225Z [0;36m(ApiServer_1 pid=1029)[0;0m WARNING 02-26 21:18:41 [platform.py:340]             **********************************************************************************[0m
2026-02-26T21:18:41.0284831Z [0;36m(ApiServer_1 pid=1029)[0;0m WARNING 02-26 21:18:41 [platform.py:340]             
2026-02-26T21:18:41.0285379Z [0;36m(ApiServer_1 pid=1029)[0;0m INFO 02-26 21:18:41 [platform.py:448] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-26T21:18:41.9409109Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T21:18:41.9410439Z   warnings.warn(
2026-02-26T21:18:41.9486711Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T21:18:41.9487552Z   warnings.warn(
2026-02-26T21:18:44.7078718Z INFO 02-26 21:18:44 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:18:44.7079224Z INFO 02-26 21:18:44 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:18:44.7079729Z INFO 02-26 21:18:44 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:18:44.7157912Z INFO 02-26 21:18:44 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:18:44.8037961Z INFO 02-26 21:18:44 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:18:44.8038513Z INFO 02-26 21:18:44 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:18:44.8039011Z INFO 02-26 21:18:44 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:18:44.8116348Z INFO 02-26 21:18:44 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:18:45.4612827Z INFO 02-26 21:18:45 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:18:45.4613283Z INFO 02-26 21:18:45 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:18:45.4614539Z INFO 02-26 21:18:45 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:18:45.4615611Z INFO 02-26 21:18:45 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:18:45.4615965Z INFO 02-26 21:18:45 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:18:45.4616781Z INFO 02-26 21:18:45 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:18:45.8102278Z INFO 02-26 21:18:45 [parallel_state.py:1212] world_size=16 rank=8 local_rank=0 distributed_init_method=tcp://127.0.0.1:52755 backend=hccl
2026-02-26T21:18:45.8203670Z INFO 02-26 21:18:45 [parallel_state.py:1212] world_size=16 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:52755 backend=hccl
2026-02-26T21:18:49.6636396Z 2026-02-26 21:18:49,662 - 1079 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:18:49.6692643Z INFO 02-26 21:18:49 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:18:49.8565772Z 2026-02-26 21:18:49,855 - 1077 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:18:49.8626165Z INFO 02-26 21:18:49 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:18:50.9928066Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T21:18:50.9929367Z   warnings.warn(
2026-02-26T21:18:51.1624148Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T21:18:51.1625063Z   warnings.warn(
2026-02-26T21:18:53.0956732Z INFO 02-26 21:18:53 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:18:53.0957177Z INFO 02-26 21:18:53 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:18:53.0958043Z INFO 02-26 21:18:53 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:18:53.2721074Z INFO 02-26 21:18:53 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:18:53.2721468Z INFO 02-26 21:18:53 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:18:53.2722514Z INFO 02-26 21:18:53 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:18:54.0841342Z INFO 02-26 21:18:54 [parallel_state.py:1212] world_size=16 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:52755 backend=hccl
2026-02-26T21:18:54.1791482Z INFO 02-26 21:18:54 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:18:54.1792304Z INFO 02-26 21:18:54 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:18:54.1792960Z INFO 02-26 21:18:54 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:18:54.1863534Z INFO 02-26 21:18:54 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:18:54.2624707Z INFO 02-26 21:18:54 [parallel_state.py:1212] world_size=16 rank=9 local_rank=1 distributed_init_method=tcp://127.0.0.1:52755 backend=hccl
2026-02-26T21:18:54.2875978Z INFO 02-26 21:18:54 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:18:54.2876597Z INFO 02-26 21:18:54 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:18:54.2877128Z INFO 02-26 21:18:54 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:18:54.2942793Z INFO 02-26 21:18:54 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:18:59.0829943Z 2026-02-26 21:18:59,081 - 1186 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:18:59.0886720Z INFO 02-26 21:18:59 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:18:59.3499030Z 2026-02-26 21:18:59,348 - 1183 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:18:59.3557239Z INFO 02-26 21:18:59 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:19:00.4298293Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T21:19:00.4299231Z   warnings.warn(
2026-02-26T21:19:00.6431561Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T21:19:00.6432616Z   warnings.warn(
2026-02-26T21:19:02.4985152Z INFO 02-26 21:19:02 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:19:02.4985669Z INFO 02-26 21:19:02 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:19:02.4986513Z INFO 02-26 21:19:02 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:19:02.8534770Z INFO 02-26 21:19:02 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:19:02.8535222Z INFO 02-26 21:19:02 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:19:02.8537334Z INFO 02-26 21:19:02 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:19:03.2604956Z INFO 02-26 21:19:03 [parallel_state.py:1212] world_size=16 rank=2 local_rank=2 distributed_init_method=tcp://127.0.0.1:52755 backend=hccl
2026-02-26T21:19:03.6377417Z INFO 02-26 21:19:03 [parallel_state.py:1212] world_size=16 rank=10 local_rank=2 distributed_init_method=tcp://127.0.0.1:52755 backend=hccl
2026-02-26T21:19:03.8401251Z INFO 02-26 21:19:03 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:19:03.8401804Z INFO 02-26 21:19:03 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:19:03.8402433Z INFO 02-26 21:19:03 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:19:03.8474867Z INFO 02-26 21:19:03 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:19:04.0125025Z INFO 02-26 21:19:04 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:19:04.0125501Z INFO 02-26 21:19:04 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:19:04.0126025Z INFO 02-26 21:19:04 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:19:04.0202954Z INFO 02-26 21:19:04 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:19:08.8643894Z 2026-02-26 21:19:08,863 - 1287 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:19:08.8700654Z INFO 02-26 21:19:08 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:19:09.2541960Z 2026-02-26 21:19:09,252 - 1290 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:19:09.2600562Z INFO 02-26 21:19:09 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:19:10.1520364Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T21:19:10.1521379Z   warnings.warn(
2026-02-26T21:19:10.5727514Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T21:19:10.5728624Z   warnings.warn(
2026-02-26T21:19:12.2337606Z INFO 02-26 21:19:12 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:19:12.2338168Z INFO 02-26 21:19:12 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:19:12.2339044Z INFO 02-26 21:19:12 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:19:12.7512233Z INFO 02-26 21:19:12 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:19:12.7512701Z INFO 02-26 21:19:12 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:19:12.7513589Z INFO 02-26 21:19:12 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:19:13.0407795Z INFO 02-26 21:19:13 [parallel_state.py:1212] world_size=16 rank=3 local_rank=3 distributed_init_method=tcp://127.0.0.1:52755 backend=hccl
2026-02-26T21:19:13.2298550Z INFO 02-26 21:19:13 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:19:13.2299157Z INFO 02-26 21:19:13 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:19:13.2299692Z INFO 02-26 21:19:13 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:19:13.2370914Z INFO 02-26 21:19:13 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:19:13.5356129Z INFO 02-26 21:19:13 [parallel_state.py:1212] world_size=16 rank=11 local_rank=3 distributed_init_method=tcp://127.0.0.1:52755 backend=hccl
2026-02-26T21:19:13.8839167Z INFO 02-26 21:19:13 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:19:13.8839713Z INFO 02-26 21:19:13 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:19:13.8840222Z INFO 02-26 21:19:13 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:19:13.8910705Z INFO 02-26 21:19:13 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:19:18.1133811Z 2026-02-26 21:19:18,112 - 1391 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:19:18.1193524Z INFO 02-26 21:19:18 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:19:18.8503660Z 2026-02-26 21:19:18,849 - 1394 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:19:18.8563387Z INFO 02-26 21:19:18 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:19:19.4078733Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T21:19:19.4079649Z   warnings.warn(
2026-02-26T21:19:20.1602915Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T21:19:20.1603907Z   warnings.warn(
2026-02-26T21:19:21.4355772Z INFO 02-26 21:19:21 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:19:21.4356197Z INFO 02-26 21:19:21 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:19:21.4357103Z INFO 02-26 21:19:21 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:19:22.1806477Z INFO 02-26 21:19:22 [parallel_state.py:1212] world_size=16 rank=4 local_rank=4 distributed_init_method=tcp://127.0.0.1:52755 backend=hccl
2026-02-26T21:19:22.3169870Z INFO 02-26 21:19:22 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:19:22.3170349Z INFO 02-26 21:19:22 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:19:22.3171438Z INFO 02-26 21:19:22 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:19:22.6575632Z INFO 02-26 21:19:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:19:22.6576227Z INFO 02-26 21:19:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:19:22.6576735Z INFO 02-26 21:19:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:19:22.6647701Z INFO 02-26 21:19:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:19:23.1079850Z INFO 02-26 21:19:23 [parallel_state.py:1212] world_size=16 rank=12 local_rank=4 distributed_init_method=tcp://127.0.0.1:52755 backend=hccl
2026-02-26T21:19:23.3866446Z INFO 02-26 21:19:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:19:23.3867087Z INFO 02-26 21:19:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:19:23.3867712Z INFO 02-26 21:19:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:19:23.3938516Z INFO 02-26 21:19:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:19:27.6598785Z 2026-02-26 21:19:27,658 - 1495 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:19:27.6655486Z INFO 02-26 21:19:27 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:19:28.6459025Z 2026-02-26 21:19:28,644 - 1498 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:19:28.6519810Z INFO 02-26 21:19:28 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:19:28.9356863Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T21:19:28.9357852Z   warnings.warn(
2026-02-26T21:19:29.9642481Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T21:19:29.9643414Z   warnings.warn(
2026-02-26T21:19:31.0584018Z INFO 02-26 21:19:31 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:19:31.0584448Z INFO 02-26 21:19:31 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:19:31.0585473Z INFO 02-26 21:19:31 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:19:31.8457804Z INFO 02-26 21:19:31 [parallel_state.py:1212] world_size=16 rank=5 local_rank=5 distributed_init_method=tcp://127.0.0.1:52755 backend=hccl
2026-02-26T21:19:31.9156741Z INFO 02-26 21:19:31 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:19:31.9157209Z INFO 02-26 21:19:31 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:19:31.9157727Z INFO 02-26 21:19:31 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:19:31.9231124Z INFO 02-26 21:19:31 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:19:32.0881975Z INFO 02-26 21:19:32 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:19:32.0882906Z INFO 02-26 21:19:32 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:19:32.0883879Z INFO 02-26 21:19:32 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:19:32.8942107Z INFO 02-26 21:19:32 [parallel_state.py:1212] world_size=16 rank=13 local_rank=5 distributed_init_method=tcp://127.0.0.1:52755 backend=hccl
2026-02-26T21:19:33.1240925Z INFO 02-26 21:19:33 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:19:33.1241437Z INFO 02-26 21:19:33 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:19:33.1241908Z INFO 02-26 21:19:33 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:19:33.1318752Z INFO 02-26 21:19:33 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:19:36.6957421Z 2026-02-26 21:19:36,694 - 1599 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:19:36.7014342Z INFO 02-26 21:19:36 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:19:37.9985438Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T21:19:37.9986367Z   warnings.warn(
2026-02-26T21:19:38.2055887Z 2026-02-26 21:19:38,204 - 1602 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:19:38.2116973Z INFO 02-26 21:19:38 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:19:39.7603242Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T21:19:39.7604279Z   warnings.warn(
2026-02-26T21:19:40.0789014Z INFO 02-26 21:19:40 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:19:40.0789432Z INFO 02-26 21:19:40 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:19:40.0790296Z INFO 02-26 21:19:40 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:19:40.9101730Z INFO 02-26 21:19:40 [parallel_state.py:1212] world_size=16 rank=6 local_rank=6 distributed_init_method=tcp://127.0.0.1:52755 backend=hccl
2026-02-26T21:19:41.2294368Z INFO 02-26 21:19:41 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:19:41.2294897Z INFO 02-26 21:19:41 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:19:41.2295439Z INFO 02-26 21:19:41 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:19:41.2367994Z INFO 02-26 21:19:41 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:19:41.9160499Z INFO 02-26 21:19:41 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:19:41.9160940Z INFO 02-26 21:19:41 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:19:41.9162380Z INFO 02-26 21:19:41 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:19:42.6995484Z INFO 02-26 21:19:42 [parallel_state.py:1212] world_size=16 rank=14 local_rank=6 distributed_init_method=tcp://127.0.0.1:52755 backend=hccl
2026-02-26T21:19:42.8424577Z INFO 02-26 21:19:42 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:19:42.8425043Z INFO 02-26 21:19:42 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:19:42.8425633Z INFO 02-26 21:19:42 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:19:42.8504627Z INFO 02-26 21:19:42 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:19:46.4775798Z 2026-02-26 21:19:46,475 - 1703 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:19:46.4824062Z INFO 02-26 21:19:46 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:19:47.7764543Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T21:19:47.7765451Z   warnings.warn(
2026-02-26T21:19:47.8931873Z 2026-02-26 21:19:47,892 - 1710 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-26T21:19:47.8989976Z INFO 02-26 21:19:47 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-26T21:19:49.1707345Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T21:19:49.1708320Z   warnings.warn(
2026-02-26T21:19:49.8331123Z INFO 02-26 21:19:49 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:19:49.8331661Z INFO 02-26 21:19:49 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:19:49.8332660Z INFO 02-26 21:19:49 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:19:50.5908903Z INFO 02-26 21:19:50 [parallel_state.py:1212] world_size=16 rank=7 local_rank=7 distributed_init_method=tcp://127.0.0.1:52755 backend=hccl
2026-02-26T21:19:51.2039530Z INFO 02-26 21:19:51 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:19:51.2040046Z INFO 02-26 21:19:51 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:19:51.2040944Z INFO 02-26 21:19:51 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:19:51.9976385Z INFO 02-26 21:19:51 [parallel_state.py:1212] world_size=16 rank=15 local_rank=7 distributed_init_method=tcp://127.0.0.1:52755 backend=hccl
2026-02-26T21:19:52.0182155Z [Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.0188929Z [Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.0190283Z [Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.0190832Z [Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.0192311Z [Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.0192920Z [Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.0193666Z [Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.0859888Z [Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.0897523Z [Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.0898117Z [Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.0899659Z [Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.0978749Z [Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.0979350Z [Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.0980022Z [Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.0980517Z [Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.1538874Z [Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.1640355Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T21:19:52.1640816Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T21:19:52.1641286Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T21:19:52.1641750Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T21:19:52.1643392Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T21:19:52.1643889Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T21:19:52.1644371Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T21:19:52.1645096Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T21:19:52.1724110Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1733054Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1733546Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1733973Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1734445Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1734929Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1735362Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1735875Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1736330Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1756867Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1757341Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1757763Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1758226Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1758666Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1759121Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1759704Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1761766Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1779447Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1779950Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1780384Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1780862Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1781282Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1781749Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.1782364Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2356794Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T21:19:52.2357328Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T21:19:52.2357771Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T21:19:52.2358231Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T21:19:52.2358684Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T21:19:52.2359168Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T21:19:52.2359633Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T21:19:52.2360198Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-26T21:19:52.2428589Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2439389Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2439827Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2440288Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2440755Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2441177Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2441635Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2442236Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2448176Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2458025Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2458502Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2461154Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2461646Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2462346Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2462765Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2463957Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2473336Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2473810Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2475483Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2481148Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2481648Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2482243Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2482709Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2483297Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-26T21:19:52.2491136Z [Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.2491609Z [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.2492211Z [Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.2492636Z [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.2498620Z [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.2499046Z [Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.2499491Z [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.2499946Z [Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.2505056Z [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.2505519Z [Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.2505988Z [Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.2506526Z [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.2506990Z [Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.2507404Z [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.2540220Z [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.2540691Z [Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.2622386Z [Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.2622888Z [Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.2623356Z [Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.2624702Z [Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.2625182Z [Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.2626401Z [Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.2626892Z [Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.2627383Z [Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.2627814Z [Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.2628408Z INFO 02-26 21:19:52 [parallel_state.py:1423] rank 0 in world size 16 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
2026-02-26T21:19:52.2628974Z [Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.2629541Z [Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.2630019Z [Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.2630545Z INFO 02-26 21:19:52 [parallel_state.py:1423] rank 3 in world size 16 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
2026-02-26T21:19:52.2631114Z [Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.2631662Z INFO 02-26 21:19:52 [parallel_state.py:1423] rank 2 in world size 16 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
2026-02-26T21:19:52.2632397Z [Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.2632868Z [Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.2633389Z INFO 02-26 21:19:52 [parallel_state.py:1423] rank 4 in world size 16 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 4, EP rank 4
2026-02-26T21:19:52.2633976Z [Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.2634535Z INFO 02-26 21:19:52 [parallel_state.py:1423] rank 1 in world size 16 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
2026-02-26T21:19:52.2635122Z INFO 02-26 21:19:52 [parallel_state.py:1423] rank 8 in world size 16 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 0, EP rank 8
2026-02-26T21:19:52.2635762Z INFO 02-26 21:19:52 [parallel_state.py:1423] rank 11 in world size 16 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 3, EP rank 11
2026-02-26T21:19:52.2636396Z INFO 02-26 21:19:52 [parallel_state.py:1423] rank 13 in world size 16 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 5, EP rank 13
2026-02-26T21:19:52.2636980Z INFO 02-26 21:19:52 [parallel_state.py:1423] rank 7 in world size 16 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 7, EP rank 7
2026-02-26T21:19:52.2637697Z INFO 02-26 21:19:52 [parallel_state.py:1423] rank 15 in world size 16 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 7, EP rank 15
2026-02-26T21:19:52.2704163Z INFO 02-26 21:19:52 [parallel_state.py:1423] rank 9 in world size 16 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 1, EP rank 9
2026-02-26T21:19:52.2704801Z INFO 02-26 21:19:52 [parallel_state.py:1423] rank 5 in world size 16 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 5, EP rank 5
2026-02-26T21:19:52.2705428Z INFO 02-26 21:19:52 [parallel_state.py:1423] rank 6 in world size 16 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 6, EP rank 6
2026-02-26T21:19:52.2706094Z INFO 02-26 21:19:52 [parallel_state.py:1423] rank 10 in world size 16 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 2, EP rank 10
2026-02-26T21:19:52.2706753Z INFO 02-26 21:19:52 [parallel_state.py:1423] rank 12 in world size 16 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 4, EP rank 12
2026-02-26T21:19:52.2707382Z INFO 02-26 21:19:52 [parallel_state.py:1423] rank 14 in world size 16 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 6, EP rank 14
2026-02-26T21:19:52.3573938Z [Gloo] Rank 0 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.4024135Z [Gloo] Rank 7 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.4024759Z [Gloo] Rank 9 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.4025205Z [Gloo] Rank 15 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.4047127Z [Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.4047715Z [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.4105275Z [Gloo] Rank 5 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.4105987Z [Gloo] Rank 3 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.4176840Z [Gloo] Rank 1 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.4178031Z [Gloo] Rank 6 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.4178525Z [Gloo] Rank 8 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.4179927Z [Gloo] Rank 10 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.4180666Z [Gloo] Rank 13 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.4181111Z [Gloo] Rank 14 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.4181589Z [Gloo] Rank 12 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.4183154Z [Gloo] Rank 2 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.4184837Z [Gloo] Rank 11 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.4188532Z [Gloo] Rank 4 is connected to 15 peer ranks. Expected number of connected peer ranks is : 15
2026-02-26T21:19:52.4198193Z [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.4198622Z [Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.4199873Z [Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.4200331Z [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.4206381Z [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.4206882Z [Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.4207465Z [Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.4207914Z [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.4208389Z [Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.4208812Z [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.4210123Z [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.4210611Z [Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.4211037Z [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:52.4211480Z [Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2026-02-26T21:19:55.5446277Z INFO 02-26 21:19:55 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T21:19:55.5447038Z INFO 02-26 21:19:55 [cpu_binding.py:302] NPU7: main=[282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317]  acl=[318]  release=[[319]]
2026-02-26T21:19:55.6009460Z INFO 02-26 21:19:55 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:55.6027279Z INFO 02-26 21:19:55 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:55.6570274Z WARNING 02-26 21:19:55 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:55.7158708Z WARNING 02-26 21:19:55 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:55.7190794Z [0;36m(Worker_DP0_TP7_EP7 pid=1703)[0;0m INFO 02-26 21:19:55 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T21:19:56.0383707Z [0;36m(Worker_DP0_TP7_EP7 pid=1703)[0;0m [2026-02-26 21:19:56] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T21:19:56.1313083Z INFO 02-26 21:19:56 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T21:19:56.1313808Z INFO 02-26 21:19:56 [cpu_binding.py:302] NPU15: main=[282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317]  acl=[318]  release=[[319]]
2026-02-26T21:19:56.1847104Z INFO 02-26 21:19:56 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:56.1870290Z INFO 02-26 21:19:56 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:56.1984591Z INFO 02-26 21:19:56 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T21:19:56.1985307Z INFO 02-26 21:19:56 [cpu_binding.py:302] NPU8: main=[2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37]  acl=[38]  release=[[39]]
2026-02-26T21:19:56.2171167Z [0;36m(Worker_DP0_TP7_EP7 pid=1703)[0;0m INFO 02-26 21:19:56 [layer.py:475] [EP Rank 7/16] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119, 8->120, 9->121, 10->122, 11->123, 12->124, 13->125, 14->126, 15->127.
2026-02-26T21:19:56.2254099Z WARNING 02-26 21:19:56 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:56.2539816Z WARNING 02-26 21:19:56 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:56.2555038Z INFO 02-26 21:19:56 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:56.2577650Z [0;36m(Worker_DP1_TP7_EP15 pid=1710)[0;0m INFO 02-26 21:19:56 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T21:19:56.2957765Z WARNING 02-26 21:19:56 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:56.3239422Z WARNING 02-26 21:19:56 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:56.3287233Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m INFO 02-26 21:19:56 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T21:19:56.3940374Z INFO 02-26 21:19:56 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T21:19:56.3941086Z INFO 02-26 21:19:56 [cpu_binding.py:302] NPU0: main=[2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37]  acl=[38]  release=[[39]]
2026-02-26T21:19:56.4526765Z INFO 02-26 21:19:56 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:56.4838180Z WARNING 02-26 21:19:56 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:56.5107455Z WARNING 02-26 21:19:56 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:56.5147019Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m INFO 02-26 21:19:56 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T21:19:56.8709020Z [0;36m(Worker_DP1_TP7_EP15 pid=1710)[0;0m [2026-02-26 21:19:56] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T21:19:56.9447273Z INFO 02-26 21:19:56 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T21:19:56.9449412Z INFO 02-26 21:19:56 [cpu_binding.py:302] NPU13: main=[202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237]  acl=[238]  release=[[239]]
2026-02-26T21:19:56.9839003Z [0;36m(Worker_DP1_TP7_EP15 pid=1710)[0;0m INFO 02-26 21:19:56 [layer.py:475] [EP Rank 15/16] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/256. Experts local to global index map: 0->240, 1->241, 2->242, 3->243, 4->244, 5->245, 6->246, 7->247, 8->248, 9->249, 10->250, 11->251, 12->252, 13->253, 14->254, 15->255.
2026-02-26T21:19:57.0077818Z INFO 02-26 21:19:57 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:57.0105435Z INFO 02-26 21:19:57 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:57.0581718Z INFO 02-26 21:19:57 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T21:19:57.0582367Z INFO 02-26 21:19:57 [cpu_binding.py:302] NPU9: main=[42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77]  acl=[78]  release=[[79]]
2026-02-26T21:19:57.0583092Z WARNING 02-26 21:19:57 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:57.0978006Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m [2026-02-26 21:19:57] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T21:19:57.0982344Z WARNING 02-26 21:19:57 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:57.1027037Z [0;36m(Worker_DP1_TP5_EP13 pid=1498)[0;0m INFO 02-26 21:19:57 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T21:19:57.1182565Z INFO 02-26 21:19:57 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:57.1202145Z INFO 02-26 21:19:57 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:57.1355074Z INFO 02-26 21:19:57 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T21:19:57.1355871Z INFO 02-26 21:19:57 [cpu_binding.py:302] NPU6: main=[242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277]  acl=[278]  release=[[279]]
2026-02-26T21:19:57.1447089Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m [2026-02-26 21:19:57] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T21:19:57.1564436Z WARNING 02-26 21:19:57 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:57.1826217Z INFO 02-26 21:19:57 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:57.1843522Z INFO 02-26 21:19:57 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:57.1887311Z WARNING 02-26 21:19:57 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:57.1925836Z [0;36m(Worker_DP1_TP1_EP9 pid=1079)[0;0m INFO 02-26 21:19:57 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T21:19:57.2415860Z WARNING 02-26 21:19:57 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:57.2705582Z WARNING 02-26 21:19:57 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:57.2736530Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m INFO 02-26 21:19:57 [layer.py:475] [EP Rank 8/16] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/256. Experts local to global index map: 0->128, 1->129, 2->130, 3->131, 4->132, 5->133, 6->134, 7->135, 8->136, 9->137, 10->138, 11->139, 12->140, 13->141, 14->142, 15->143.
2026-02-26T21:19:57.2739863Z [0;36m(Worker_DP0_TP6_EP6 pid=1599)[0;0m INFO 02-26 21:19:57 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T21:19:57.3145222Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m INFO 02-26 21:19:57 [layer.py:475] [EP Rank 0/16] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7, 8->8, 9->9, 10->10, 11->11, 12->12, 13->13, 14->14, 15->15.
2026-02-26T21:19:57.5692872Z INFO 02-26 21:19:57 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T21:19:57.5693712Z INFO 02-26 21:19:57 [cpu_binding.py:302] NPU11: main=[122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157]  acl=[158]  release=[[159]]
2026-02-26T21:19:57.6291265Z INFO 02-26 21:19:57 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:57.6325661Z INFO 02-26 21:19:57 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:57.6380505Z INFO 02-26 21:19:57 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T21:19:57.6381220Z INFO 02-26 21:19:57 [cpu_binding.py:302] NPU12: main=[162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197]  acl=[198]  release=[[199]]
2026-02-26T21:19:57.6926187Z WARNING 02-26 21:19:57 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:57.6977423Z INFO 02-26 21:19:57 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:57.7004404Z INFO 02-26 21:19:57 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:57.7208899Z WARNING 02-26 21:19:57 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:57.7245740Z [0;36m(Worker_DP1_TP3_EP11 pid=1290)[0;0m INFO 02-26 21:19:57 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T21:19:57.7281622Z [0;36m(Worker_DP1_TP5_EP13 pid=1498)[0;0m [2026-02-26 21:19:57] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T21:19:57.7414402Z WARNING 02-26 21:19:57 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:57.7696478Z WARNING 02-26 21:19:57 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:57.7730873Z [0;36m(Worker_DP1_TP4_EP12 pid=1394)[0;0m INFO 02-26 21:19:57 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T21:19:57.8228000Z [0;36m(Worker_DP1_TP1_EP9 pid=1079)[0;0m [2026-02-26 21:19:57] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T21:19:57.8355002Z [0;36m(Worker_DP0_TP6_EP6 pid=1599)[0;0m [2026-02-26 21:19:57] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T21:19:57.8474653Z [0;36m(Worker_DP1_TP5_EP13 pid=1498)[0;0m INFO 02-26 21:19:57 [layer.py:475] [EP Rank 13/16] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/256. Experts local to global index map: 0->208, 1->209, 2->210, 3->211, 4->212, 5->213, 6->214, 7->215, 8->216, 9->217, 10->218, 11->219, 12->220, 13->221, 14->222, 15->223.
2026-02-26T21:19:57.8475885Z INFO 02-26 21:19:57 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T21:19:57.8476546Z INFO 02-26 21:19:57 [cpu_binding.py:302] NPU4: main=[162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197]  acl=[198]  release=[[199]]
2026-02-26T21:19:57.9115118Z INFO 02-26 21:19:57 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:57.9136028Z INFO 02-26 21:19:57 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:57.9378134Z INFO 02-26 21:19:57 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T21:19:57.9378865Z INFO 02-26 21:19:57 [cpu_binding.py:302] NPU3: main=[122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157]  acl=[158]  release=[[159]]
2026-02-26T21:19:57.9429839Z [0;36m(Worker_DP1_TP1_EP9 pid=1079)[0;0m INFO 02-26 21:19:57 [layer.py:475] [EP Rank 9/16] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/256. Experts local to global index map: 0->144, 1->145, 2->146, 3->147, 4->148, 5->149, 6->150, 7->151, 8->152, 9->153, 10->154, 11->155, 12->156, 13->157, 14->158, 15->159.
2026-02-26T21:19:57.9551469Z [0;36m(Worker_DP0_TP6_EP6 pid=1599)[0;0m INFO 02-26 21:19:57 [layer.py:475] [EP Rank 6/16] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103, 8->104, 9->105, 10->106, 11->107, 12->108, 13->109, 14->110, 15->111.
2026-02-26T21:19:57.9703084Z WARNING 02-26 21:19:57 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:58.0042407Z INFO 02-26 21:19:58 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:58.0051258Z WARNING 02-26 21:19:58 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:58.0068077Z INFO 02-26 21:19:58 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:58.0103506Z [0;36m(Worker_DP0_TP4_EP4 pid=1391)[0;0m INFO 02-26 21:19:58 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T21:19:58.0408082Z WARNING 02-26 21:19:58 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:58.0663547Z WARNING 02-26 21:19:58 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:58.0682201Z [0;36m(Worker_DP0_TP3_EP3 pid=1287)[0;0m INFO 02-26 21:19:58 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T21:19:58.1531815Z INFO 02-26 21:19:58 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T21:19:58.1532619Z INFO 02-26 21:19:58 [cpu_binding.py:302] NPU2: main=[82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117]  acl=[118]  release=[[119]]
2026-02-26T21:19:58.2066108Z INFO 02-26 21:19:58 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:58.2088284Z INFO 02-26 21:19:58 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:58.2411079Z WARNING 02-26 21:19:58 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:58.2648604Z WARNING 02-26 21:19:58 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:58.2675948Z [0;36m(Worker_DP0_TP2_EP2 pid=1186)[0;0m INFO 02-26 21:19:58 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T21:19:58.3706300Z [0;36m(Worker_DP1_TP3_EP11 pid=1290)[0;0m [2026-02-26 21:19:58] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T21:19:58.4340515Z [0;36m(Worker_DP1_TP4_EP12 pid=1394)[0;0m [2026-02-26 21:19:58] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T21:19:58.4377248Z INFO 02-26 21:19:58 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T21:19:58.4377822Z INFO 02-26 21:19:58 [cpu_binding.py:302] NPU1: main=[42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77]  acl=[78]  release=[[79]]
2026-02-26T21:19:58.4452723Z [0;36m(Worker_DP0_TP3_EP3 pid=1287)[0;0m [2026-02-26 21:19:58] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T21:19:58.4816160Z [0;36m(Worker_DP1_TP3_EP11 pid=1290)[0;0m INFO 02-26 21:19:58 [layer.py:475] [EP Rank 11/16] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/256. Experts local to global index map: 0->176, 1->177, 2->178, 3->179, 4->180, 5->181, 6->182, 7->183, 8->184, 9->185, 10->186, 11->187, 12->188, 13->189, 14->190, 15->191.
2026-02-26T21:19:58.4965619Z INFO 02-26 21:19:58 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:58.4985014Z INFO 02-26 21:19:58 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:58.5020771Z INFO 02-26 21:19:58 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T21:19:58.5021667Z INFO 02-26 21:19:58 [cpu_binding.py:302] NPU10: main=[82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117]  acl=[118]  release=[[119]]
2026-02-26T21:19:58.5346088Z WARNING 02-26 21:19:58 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:58.5546230Z INFO 02-26 21:19:58 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:58.5565653Z INFO 02-26 21:19:58 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:58.5572394Z [0;36m(Worker_DP1_TP4_EP12 pid=1394)[0;0m INFO 02-26 21:19:58 [layer.py:475] [EP Rank 12/16] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/256. Experts local to global index map: 0->192, 1->193, 2->194, 3->195, 4->196, 5->197, 6->198, 7->199, 8->200, 9->201, 10->202, 11->203, 12->204, 13->205, 14->206, 15->207.
2026-02-26T21:19:58.5574168Z [0;36m(Worker_DP0_TP3_EP3 pid=1287)[0;0m INFO 02-26 21:19:58 [layer.py:475] [EP Rank 3/16] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55, 8->56, 9->57, 10->58, 11->59, 12->60, 13->61, 14->62, 15->63.
2026-02-26T21:19:58.5612610Z WARNING 02-26 21:19:58 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:58.5648735Z [0;36m(Worker_DP0_TP1_EP1 pid=1077)[0;0m INFO 02-26 21:19:58 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T21:19:58.5924917Z WARNING 02-26 21:19:58 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:58.5946480Z [0;36m(Worker_DP0_TP2_EP2 pid=1186)[0;0m [2026-02-26 21:19:58] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T21:19:58.6182273Z WARNING 02-26 21:19:58 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:58.6211116Z [0;36m(Worker_DP1_TP2_EP10 pid=1183)[0;0m INFO 02-26 21:19:58 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T21:19:58.7044306Z [0;36m(Worker_DP0_TP2_EP2 pid=1186)[0;0m INFO 02-26 21:19:58 [layer.py:475] [EP Rank 2/16] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39, 8->40, 9->41, 10->42, 11->43, 12->44, 13->45, 14->46, 15->47.
2026-02-26T21:19:58.7506664Z INFO 02-26 21:19:58 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T21:19:58.7507512Z INFO 02-26 21:19:58 [cpu_binding.py:302] NPU5: main=[202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237]  acl=[238]  release=[[239]]
2026-02-26T21:19:58.7970603Z INFO 02-26 21:19:58 [cpu_binding.py:297] The CPU allocation plan is as follows:
2026-02-26T21:19:58.7971303Z INFO 02-26 21:19:58 [cpu_binding.py:302] NPU14: main=[242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277]  acl=[278]  release=[[279]]
2026-02-26T21:19:58.8100052Z INFO 02-26 21:19:58 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:58.8122327Z INFO 02-26 21:19:58 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:58.8414221Z [0;36m(Worker_DP0_TP4_EP4 pid=1391)[0;0m [2026-02-26 21:19:58] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T21:19:58.8450221Z INFO 02-26 21:19:58 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:58.8470625Z INFO 02-26 21:19:58 [cpu_binding.py:306] The 'migratepages' command is not available, skipping memory binding.
2026-02-26T21:19:58.8504472Z WARNING 02-26 21:19:58 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:58.8778029Z WARNING 02-26 21:19:58 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:58.8813703Z [0;36m(Worker_DP0_TP5_EP5 pid=1495)[0;0m INFO 02-26 21:19:58 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T21:19:58.9007596Z WARNING 02-26 21:19:58 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:58.9298186Z WARNING 02-26 21:19:58 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-26T21:19:58.9316530Z [0;36m(Worker_DP1_TP2_EP10 pid=1183)[0;0m [2026-02-26 21:19:58] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T21:19:58.9332587Z [0;36m(Worker_DP1_TP6_EP14 pid=1602)[0;0m INFO 02-26 21:19:58 [model_runner_v1.py:2337] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-26T21:19:58.9562646Z [0;36m(Worker_DP0_TP4_EP4 pid=1391)[0;0m INFO 02-26 21:19:58 [layer.py:475] [EP Rank 4/16] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71, 8->72, 9->73, 10->74, 11->75, 12->76, 13->77, 14->78, 15->79.
2026-02-26T21:19:59.0354327Z [0;36m(Worker_DP1_TP2_EP10 pid=1183)[0;0m INFO 02-26 21:19:59 [layer.py:475] [EP Rank 10/16] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/256. Experts local to global index map: 0->160, 1->161, 2->162, 3->163, 4->164, 5->165, 6->166, 7->167, 8->168, 9->169, 10->170, 11->171, 12->172, 13->173, 14->174, 15->175.
2026-02-26T21:19:59.1493435Z [0;36m(Worker_DP0_TP1_EP1 pid=1077)[0;0m [2026-02-26 21:19:59] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T21:19:59.2563416Z [0;36m(Worker_DP0_TP1_EP1 pid=1077)[0;0m INFO 02-26 21:19:59 [layer.py:475] [EP Rank 1/16] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23, 8->24, 9->25, 10->26, 11->27, 12->28, 13->29, 14->30, 15->31.
2026-02-26T21:19:59.4781207Z [0;36m(Worker_DP0_TP5_EP5 pid=1495)[0;0m [2026-02-26 21:19:59] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T21:19:59.5243221Z [0;36m(Worker_DP1_TP6_EP14 pid=1602)[0;0m [2026-02-26 21:19:59] INFO modelslim_config.py:296: Using the vLLM Ascend modelslim Quantization now!
2026-02-26T21:19:59.5887736Z [0;36m(Worker_DP0_TP5_EP5 pid=1495)[0;0m INFO 02-26 21:19:59 [layer.py:475] [EP Rank 5/16] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87, 8->88, 9->89, 10->90, 11->91, 12->92, 13->93, 14->94, 15->95.
2026-02-26T21:19:59.6340566Z [0;36m(Worker_DP1_TP6_EP14 pid=1602)[0;0m INFO 02-26 21:19:59 [layer.py:475] [EP Rank 14/16] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/256. Experts local to global index map: 0->224, 1->225, 2->226, 3->227, 4->228, 5->229, 6->230, 7->231, 8->232, 9->233, 10->234, 11->235, 12->236, 13->237, 14->238, 15->239.
2026-02-26T21:20:00.1044749Z [0;36m(Worker_DP0_TP1_EP1 pid=1077)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T21:20:00.1046061Z [0;36m(Worker_DP0_TP1_EP1 pid=1077)[0;0m   return func(*args, **kwargs)
2026-02-26T21:20:00.1522451Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T21:20:00.1523874Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m   return func(*args, **kwargs)
2026-02-26T21:20:00.1548742Z [0;36m(Worker_DP1_TP5_EP13 pid=1498)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T21:20:00.1550043Z [0;36m(Worker_DP1_TP5_EP13 pid=1498)[0;0m   return func(*args, **kwargs)
2026-02-26T21:20:00.1614602Z [0;36m(Worker_DP0_TP1_EP1 pid=1077)[0;0m INFO 02-26 21:20:00 [fused_moe.py:293] [EP Rank 1/16] Expert parallelism is enabled. Local/global number of experts: 16/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23, 8->24, 9->25, 10->26, 11->27, 12->28, 13->29, 14->30, 15->31.
2026-02-26T21:20:00.1787641Z [0;36m(Worker_DP1_TP3_EP11 pid=1290)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T21:20:00.1789070Z [0;36m(Worker_DP1_TP3_EP11 pid=1290)[0;0m   return func(*args, **kwargs)
2026-02-26T21:20:00.1991139Z [0;36m(Worker_DP0_TP5_EP5 pid=1495)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T21:20:00.1992379Z [0;36m(Worker_DP0_TP5_EP5 pid=1495)[0;0m   return func(*args, **kwargs)
2026-02-26T21:20:00.2176293Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m INFO 02-26 21:20:00 [fused_moe.py:293] [EP Rank 0/16] Expert parallelism is enabled. Local/global number of experts: 16/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7, 8->8, 9->9, 10->10, 11->11, 12->12, 13->13, 14->14, 15->15.
2026-02-26T21:20:00.2366990Z [0;36m(Worker_DP1_TP3_EP11 pid=1290)[0;0m INFO 02-26 21:20:00 [fused_moe.py:293] [EP Rank 11/16] Expert parallelism is enabled. Local/global number of experts: 16/256. Experts local to global index map: 0->176, 1->177, 2->178, 3->179, 4->180, 5->181, 6->182, 7->183, 8->184, 9->185, 10->186, 11->187, 12->188, 13->189, 14->190, 15->191.
2026-02-26T21:20:00.2457046Z [0;36m(Worker_DP1_TP1_EP9 pid=1079)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T21:20:00.2458571Z [0;36m(Worker_DP1_TP1_EP9 pid=1079)[0;0m   return func(*args, **kwargs)
2026-02-26T21:20:00.2459855Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T21:20:00.2460971Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m   return func(*args, **kwargs)
2026-02-26T21:20:00.2656432Z [0;36m(Worker_DP1_TP5_EP13 pid=1498)[0;0m INFO 02-26 21:20:00 [fused_moe.py:293] [EP Rank 13/16] Expert parallelism is enabled. Local/global number of experts: 16/256. Experts local to global index map: 0->208, 1->209, 2->210, 3->211, 4->212, 5->213, 6->214, 7->215, 8->216, 9->217, 10->218, 11->219, 12->220, 13->221, 14->222, 15->223.
2026-02-26T21:20:00.2675230Z [0;36m(Worker_DP0_TP3_EP3 pid=1287)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T21:20:00.2676421Z [0;36m(Worker_DP0_TP3_EP3 pid=1287)[0;0m   return func(*args, **kwargs)
2026-02-26T21:20:00.2685856Z [0;36m(Worker_DP1_TP4_EP12 pid=1394)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T21:20:00.2687000Z [0;36m(Worker_DP1_TP4_EP12 pid=1394)[0;0m   return func(*args, **kwargs)
2026-02-26T21:20:00.2688401Z [0;36m(Worker_DP0_TP2_EP2 pid=1186)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T21:20:00.2689530Z [0;36m(Worker_DP0_TP2_EP2 pid=1186)[0;0m   return func(*args, **kwargs)
2026-02-26T21:20:00.2801819Z [0;36m(Worker_DP0_TP5_EP5 pid=1495)[0;0m INFO 02-26 21:20:00 [fused_moe.py:293] [EP Rank 5/16] Expert parallelism is enabled. Local/global number of experts: 16/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87, 8->88, 9->89, 10->90, 11->91, 12->92, 13->93, 14->94, 15->95.
2026-02-26T21:20:00.2817122Z [0;36m(Worker_DP0_TP4_EP4 pid=1391)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T21:20:00.2818377Z [0;36m(Worker_DP0_TP4_EP4 pid=1391)[0;0m   return func(*args, **kwargs)
2026-02-26T21:20:00.3030827Z [0;36m(Worker_DP1_TP1_EP9 pid=1079)[0;0m INFO 02-26 21:20:00 [fused_moe.py:293] [EP Rank 9/16] Expert parallelism is enabled. Local/global number of experts: 16/256. Experts local to global index map: 0->144, 1->145, 2->146, 3->147, 4->148, 5->149, 6->150, 7->151, 8->152, 9->153, 10->154, 11->155, 12->156, 13->157, 14->158, 15->159.
2026-02-26T21:20:00.3032842Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m INFO 02-26 21:20:00 [fused_moe.py:293] [EP Rank 8/16] Expert parallelism is enabled. Local/global number of experts: 16/256. Experts local to global index map: 0->128, 1->129, 2->130, 3->131, 4->132, 5->133, 6->134, 7->135, 8->136, 9->137, 10->138, 11->139, 12->140, 13->141, 14->142, 15->143.
2026-02-26T21:20:00.3178077Z [0;36m(Worker_DP1_TP6_EP14 pid=1602)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T21:20:00.3179365Z [0;36m(Worker_DP1_TP6_EP14 pid=1602)[0;0m   return func(*args, **kwargs)
2026-02-26T21:20:00.3204957Z [0;36m(Worker_DP1_TP7_EP15 pid=1710)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T21:20:00.3206200Z [0;36m(Worker_DP1_TP7_EP15 pid=1710)[0;0m   return func(*args, **kwargs)
2026-02-26T21:20:00.3230593Z [0;36m(Worker_DP0_TP3_EP3 pid=1287)[0;0m INFO 02-26 21:20:00 [fused_moe.py:293] [EP Rank 3/16] Expert parallelism is enabled. Local/global number of experts: 16/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55, 8->56, 9->57, 10->58, 11->59, 12->60, 13->61, 14->62, 15->63.
2026-02-26T21:20:00.3235019Z [0;36m(Worker_DP0_TP2_EP2 pid=1186)[0;0m INFO 02-26 21:20:00 [fused_moe.py:293] [EP Rank 2/16] Expert parallelism is enabled. Local/global number of experts: 16/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39, 8->40, 9->41, 10->42, 11->43, 12->44, 13->45, 14->46, 15->47.
2026-02-26T21:20:00.3308974Z [0;36m(Worker_DP1_TP4_EP12 pid=1394)[0;0m INFO 02-26 21:20:00 [fused_moe.py:293] [EP Rank 12/16] Expert parallelism is enabled. Local/global number of experts: 16/256. Experts local to global index map: 0->192, 1->193, 2->194, 3->195, 4->196, 5->197, 6->198, 7->199, 8->200, 9->201, 10->202, 11->203, 12->204, 13->205, 14->206, 15->207.
2026-02-26T21:20:00.3453348Z [0;36m(Worker_DP0_TP4_EP4 pid=1391)[0;0m INFO 02-26 21:20:00 [fused_moe.py:293] [EP Rank 4/16] Expert parallelism is enabled. Local/global number of experts: 16/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71, 8->72, 9->73, 10->74, 11->75, 12->76, 13->77, 14->78, 15->79.
2026-02-26T21:20:00.3462628Z [0;36m(Worker_DP1_TP2_EP10 pid=1183)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T21:20:00.3463848Z [0;36m(Worker_DP1_TP2_EP10 pid=1183)[0;0m   return func(*args, **kwargs)
2026-02-26T21:20:00.3688357Z [0;36m(Worker_DP0_TP7_EP7 pid=1703)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T21:20:00.3689609Z [0;36m(Worker_DP0_TP7_EP7 pid=1703)[0;0m   return func(*args, **kwargs)
2026-02-26T21:20:00.3777420Z [0;36m(Worker_DP0_TP6_EP6 pid=1599)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-26T21:20:00.3778600Z [0;36m(Worker_DP0_TP6_EP6 pid=1599)[0;0m   return func(*args, **kwargs)
2026-02-26T21:20:00.3779861Z [0;36m(Worker_DP1_TP6_EP14 pid=1602)[0;0m INFO 02-26 21:20:00 [fused_moe.py:293] [EP Rank 14/16] Expert parallelism is enabled. Local/global number of experts: 16/256. Experts local to global index map: 0->224, 1->225, 2->226, 3->227, 4->228, 5->229, 6->230, 7->231, 8->232, 9->233, 10->234, 11->235, 12->236, 13->237, 14->238, 15->239.
2026-02-26T21:20:00.4021947Z [0;36m(Worker_DP1_TP2_EP10 pid=1183)[0;0m INFO 02-26 21:20:00 [fused_moe.py:293] [EP Rank 10/16] Expert parallelism is enabled. Local/global number of experts: 16/256. Experts local to global index map: 0->160, 1->161, 2->162, 3->163, 4->164, 5->165, 6->166, 7->167, 8->168, 9->169, 10->170, 11->171, 12->172, 13->173, 14->174, 15->175.
2026-02-26T21:20:00.4241143Z [0;36m(Worker_DP0_TP7_EP7 pid=1703)[0;0m INFO 02-26 21:20:00 [fused_moe.py:293] [EP Rank 7/16] Expert parallelism is enabled. Local/global number of experts: 16/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119, 8->120, 9->121, 10->122, 11->123, 12->124, 13->125, 14->126, 15->127.
2026-02-26T21:20:00.4294183Z [0;36m(Worker_DP1_TP7_EP15 pid=1710)[0;0m INFO 02-26 21:20:00 [fused_moe.py:293] [EP Rank 15/16] Expert parallelism is enabled. Local/global number of experts: 16/256. Experts local to global index map: 0->240, 1->241, 2->242, 3->243, 4->244, 5->245, 6->246, 7->247, 8->248, 9->249, 10->250, 11->251, 12->252, 13->253, 14->254, 15->255.
2026-02-26T21:20:00.4866973Z [0;36m(Worker_DP0_TP6_EP6 pid=1599)[0;0m INFO 02-26 21:20:00 [fused_moe.py:293] [EP Rank 6/16] Expert parallelism is enabled. Local/global number of experts: 16/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103, 8->104, 9->105, 10->106, 11->107, 12->108, 13->109, 14->110, 15->111.
2026-02-26T21:20:00.6192612Z [0;36m(Worker_DP1_TP4_EP12 pid=1394)[0;0m INFO 02-26 21:20:00 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T21:20:00.6414932Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m INFO 02-26 21:20:00 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T21:20:00.6415941Z [0;36m(Worker_DP0_TP4_EP4 pid=1391)[0;0m INFO 02-26 21:20:00 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T21:20:00.6422640Z [0;36m(Worker_DP1_TP1_EP9 pid=1079)[0;0m INFO 02-26 21:20:00 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T21:20:00.6613626Z [0;36m(Worker_DP1_TP6_EP14 pid=1602)[0;0m INFO 02-26 21:20:00 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T21:20:00.6623280Z [0;36m(Worker_DP0_TP3_EP3 pid=1287)[0;0m INFO 02-26 21:20:00 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T21:20:00.6636122Z [0;36m(Worker_DP0_TP2_EP2 pid=1186)[0;0m INFO 02-26 21:20:00 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T21:20:00.6789467Z [0;36m(Worker_DP1_TP2_EP10 pid=1183)[0;0m INFO 02-26 21:20:00 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T21:20:00.7201059Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m INFO 02-26 21:20:00 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T21:20:00.7206617Z [0;36m(Worker_DP0_TP7_EP7 pid=1703)[0;0m INFO 02-26 21:20:00 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T21:20:00.7239869Z [0;36m(Worker_DP1_TP7_EP15 pid=1710)[0;0m INFO 02-26 21:20:00 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T21:20:00.7859722Z [0;36m(Worker_DP0_TP6_EP6 pid=1599)[0;0m INFO 02-26 21:20:00 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T21:20:00.7884672Z [0;36m(Worker_DP0_TP1_EP1 pid=1077)[0;0m INFO 02-26 21:20:00 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T21:20:00.7919000Z [0;36m(Worker_DP0_TP5_EP5 pid=1495)[0;0m INFO 02-26 21:20:00 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T21:20:00.8139466Z [0;36m(Worker_DP1_TP5_EP13 pid=1498)[0;0m INFO 02-26 21:20:00 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T21:20:00.8264331Z [0;36m(Worker_DP1_TP3_EP11 pid=1290)[0;0m INFO 02-26 21:20:00 [fused_moe.py:533] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-26T21:20:05.8668012Z [0;36m(Worker_DP1_TP4_EP12 pid=1394)[0;0m INFO 02-26 21:20:05 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T21:20:05.8668837Z [0;36m(Worker_DP1_TP2_EP10 pid=1183)[0;0m INFO 02-26 21:20:05 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T21:20:05.8669627Z [0;36m(Worker_DP0_TP4_EP4 pid=1391)[0;0m INFO 02-26 21:20:05 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T21:20:05.9183999Z [0;36m(Worker_DP0_TP7_EP7 pid=1703)[0;0m INFO 02-26 21:20:05 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T21:20:05.9186291Z [0;36m(Worker_DP1_TP7_EP15 pid=1710)[0;0m INFO 02-26 21:20:05 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T21:20:05.9360533Z [0;36m(Worker_DP1_TP5_EP13 pid=1498)[0;0m INFO 02-26 21:20:05 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T21:20:05.9800548Z [0;36m(Worker_DP1_TP6_EP14 pid=1602)[0;0m INFO 02-26 21:20:05 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T21:20:05.9955049Z [0;36m(Worker_DP0_TP5_EP5 pid=1495)[0;0m INFO 02-26 21:20:05 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T21:20:06.0106303Z [0;36m(Worker_DP1_TP1_EP9 pid=1079)[0;0m INFO 02-26 21:20:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T21:20:06.0207919Z [0;36m(Worker_DP0_TP2_EP2 pid=1186)[0;0m INFO 02-26 21:20:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T21:20:06.0378352Z [0;36m(Worker_DP0_TP6_EP6 pid=1599)[0;0m INFO 02-26 21:20:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T21:20:06.0937366Z [0;36m(Worker_DP0_TP3_EP3 pid=1287)[0;0m INFO 02-26 21:20:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T21:20:06.1083501Z [0;36m(Worker_DP1_TP3_EP11 pid=1290)[0;0m INFO 02-26 21:20:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T21:20:06.1603311Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m INFO 02-26 21:20:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T21:20:06.3579883Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m INFO 02-26 21:20:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T21:20:06.3592090Z [0;36m(Worker_DP0_TP1_EP1 pid=1077)[0;0m INFO 02-26 21:20:06 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-26T21:20:06.4207985Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:20:06.4208412Z Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
2026-02-26T21:20:08.9954471Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:20:08.9954936Z Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:02<06:57,  2.57s/it]
2026-02-26T21:20:23.5594655Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:20:23.5595346Z Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:17<25:49,  9.63s/it]
2026-02-26T21:20:25.4510031Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:20:25.4510567Z Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:19<16:15,  6.09s/it]
2026-02-26T21:20:28.3988856Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:20:28.3989400Z Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:21<12:51,  4.85s/it]
2026-02-26T21:20:37.2079593Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:20:37.2080074Z Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:30<16:32,  6.28s/it]
2026-02-26T21:20:47.7709603Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:20:47.7710092Z Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:41<20:14,  7.74s/it]
2026-02-26T21:20:55.4029706Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:20:55.4030145Z Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:48<20:01,  7.70s/it]
2026-02-26T21:21:00.4786316Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:21:00.4786843Z Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:54<17:44,  6.87s/it]
2026-02-26T21:21:04.0196464Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:21:04.0197001Z Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:57<14:57,  5.83s/it]
2026-02-26T21:21:08.2048079Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:21:08.2048578Z Loading safetensors checkpoint shards:   6% Completed | 10/163 [01:01<13:33,  5.32s/it]
2026-02-26T21:21:12.9334276Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:21:12.9334864Z Loading safetensors checkpoint shards:   7% Completed | 11/163 [01:06<13:01,  5.14s/it]
2026-02-26T21:21:17.1091588Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:21:17.1092141Z Loading safetensors checkpoint shards:   7% Completed | 12/163 [01:10<12:11,  4.85s/it]
2026-02-26T21:21:21.0460446Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:21:21.0461001Z Loading safetensors checkpoint shards:   8% Completed | 13/163 [01:14<11:25,  4.57s/it]
2026-02-26T21:21:36.2458687Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:21:36.2459622Z Loading safetensors checkpoint shards:   9% Completed | 14/163 [01:29<19:19,  7.78s/it]
2026-02-26T21:21:51.8886242Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:21:51.8886770Z Loading safetensors checkpoint shards:   9% Completed | 15/163 [01:45<25:02, 10.15s/it]
2026-02-26T21:22:06.2239811Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:22:06.2240261Z Loading safetensors checkpoint shards:  10% Completed | 16/163 [01:59<27:57, 11.41s/it]
2026-02-26T21:22:19.1909866Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:22:19.1910508Z Loading safetensors checkpoint shards:  10% Completed | 17/163 [02:12<28:54, 11.88s/it]
2026-02-26T21:22:35.0107592Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:22:35.0108136Z Loading safetensors checkpoint shards:  11% Completed | 18/163 [02:28<31:34, 13.06s/it]
2026-02-26T21:22:35.1967482Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:22:35.1967964Z Loading safetensors checkpoint shards:  12% Completed | 19/163 [02:28<22:04,  9.20s/it]
2026-02-26T21:22:35.3487286Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:22:35.3487777Z Loading safetensors checkpoint shards:  12% Completed | 20/163 [02:28<15:26,  6.48s/it]
2026-02-26T21:22:51.5126176Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:22:51.5126662Z Loading safetensors checkpoint shards:  13% Completed | 21/163 [02:45<22:12,  9.39s/it]
2026-02-26T21:22:51.6998215Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:22:51.6998676Z Loading safetensors checkpoint shards:  13% Completed | 22/163 [02:45<15:34,  6.63s/it]
2026-02-26T21:22:51.8550537Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:22:51.8551011Z Loading safetensors checkpoint shards:  14% Completed | 23/163 [02:45<10:55,  4.68s/it]
2026-02-26T21:22:52.0121792Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:22:52.0122356Z Loading safetensors checkpoint shards:  15% Completed | 24/163 [02:45<07:42,  3.33s/it]
2026-02-26T21:23:07.2103962Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:23:07.2104870Z Loading safetensors checkpoint shards:  15% Completed | 25/163 [03:00<15:50,  6.89s/it]
2026-02-26T21:23:10.0977738Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:23:10.0978226Z Loading safetensors checkpoint shards:  16% Completed | 26/163 [03:03<12:59,  5.69s/it]
2026-02-26T21:23:13.6351387Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:23:13.6351849Z Loading safetensors checkpoint shards:  17% Completed | 27/163 [03:07<11:25,  5.04s/it]
2026-02-26T21:23:30.0608368Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:23:30.0608796Z Loading safetensors checkpoint shards:  17% Completed | 28/163 [03:23<19:01,  8.46s/it]
2026-02-26T21:23:31.0595008Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:23:31.0595492Z Loading safetensors checkpoint shards:  18% Completed | 29/163 [03:24<13:53,  6.22s/it]
2026-02-26T21:23:35.1427400Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:23:35.1427930Z Loading safetensors checkpoint shards:  18% Completed | 30/163 [03:28<12:21,  5.58s/it]
2026-02-26T21:23:39.5006529Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:23:39.5007014Z Loading safetensors checkpoint shards:  19% Completed | 31/163 [03:33<11:28,  5.21s/it]
2026-02-26T21:23:43.8459239Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:23:43.8459710Z Loading safetensors checkpoint shards:  20% Completed | 32/163 [03:37<10:48,  4.95s/it]
2026-02-26T21:23:48.4110340Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:23:48.4110962Z Loading safetensors checkpoint shards:  20% Completed | 33/163 [03:41<10:28,  4.84s/it]
2026-02-26T21:23:56.3355092Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:23:56.3355619Z Loading safetensors checkpoint shards:  21% Completed | 34/163 [03:49<12:23,  5.76s/it]
2026-02-26T21:23:59.7554207Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:23:59.7554664Z Loading safetensors checkpoint shards:  21% Completed | 35/163 [03:53<10:47,  5.06s/it]
2026-02-26T21:24:17.9351326Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:24:17.9351846Z Loading safetensors checkpoint shards:  22% Completed | 36/163 [04:11<19:02,  9.00s/it]
2026-02-26T21:24:18.4547017Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:24:18.4547509Z Loading safetensors checkpoint shards:  23% Completed | 37/163 [04:12<13:33,  6.45s/it]
2026-02-26T21:24:23.6631259Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:24:23.6631706Z Loading safetensors checkpoint shards:  23% Completed | 38/163 [04:17<12:39,  6.08s/it]
2026-02-26T21:24:25.5110554Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:24:25.5111139Z Loading safetensors checkpoint shards:  24% Completed | 39/163 [04:19<09:56,  4.81s/it]
2026-02-26T21:24:42.9446637Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:24:42.9447243Z Loading safetensors checkpoint shards:  25% Completed | 40/163 [04:36<17:37,  8.60s/it]
2026-02-26T21:24:43.1372104Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:24:43.1372660Z Loading safetensors checkpoint shards:  25% Completed | 41/163 [04:36<12:21,  6.08s/it]
2026-02-26T21:24:45.2965069Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:24:45.2965536Z Loading safetensors checkpoint shards:  26% Completed | 42/163 [04:38<09:53,  4.90s/it]
2026-02-26T21:25:05.6115907Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:25:05.6117864Z Loading safetensors checkpoint shards:  26% Completed | 43/163 [04:59<19:03,  9.53s/it]
2026-02-26T21:25:05.8095896Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:25:05.8096385Z Loading safetensors checkpoint shards:  27% Completed | 44/163 [04:59<13:20,  6.73s/it]
2026-02-26T21:25:05.9647055Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:25:05.9647576Z Loading safetensors checkpoint shards:  28% Completed | 45/163 [04:59<09:21,  4.76s/it]
2026-02-26T21:25:10.0473126Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:25:10.0473591Z Loading safetensors checkpoint shards:  28% Completed | 46/163 [05:03<08:52,  4.55s/it]
2026-02-26T21:25:55.9412908Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:25:55.9413423Z Loading safetensors checkpoint shards:  29% Completed | 47/163 [05:49<32:46, 16.96s/it]
2026-02-26T21:25:58.1255200Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:25:58.1255881Z Loading safetensors checkpoint shards:  29% Completed | 48/163 [05:51<24:00, 12.52s/it]
2026-02-26T21:26:00.8517652Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:26:00.8518222Z Loading safetensors checkpoint shards:  30% Completed | 49/163 [05:54<18:12,  9.58s/it]
2026-02-26T21:26:13.8794479Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:26:13.8795019Z Loading safetensors checkpoint shards:  31% Completed | 50/163 [06:07<19:59, 10.62s/it]
2026-02-26T21:26:28.3478738Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:26:28.3479218Z Loading safetensors checkpoint shards:  31% Completed | 51/163 [06:21<21:58, 11.77s/it]
2026-02-26T21:26:28.5396219Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:26:28.5396752Z Loading safetensors checkpoint shards:  32% Completed | 52/163 [06:22<15:21,  8.30s/it]
2026-02-26T21:26:42.0298688Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:26:42.0299219Z Loading safetensors checkpoint shards:  33% Completed | 53/163 [06:35<18:04,  9.86s/it]
2026-02-26T21:26:42.2178907Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:26:42.2179348Z Loading safetensors checkpoint shards:  33% Completed | 54/163 [06:35<12:38,  6.96s/it]
2026-02-26T21:26:42.3731694Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:26:42.3732237Z Loading safetensors checkpoint shards:  34% Completed | 55/163 [06:35<08:50,  4.92s/it]
2026-02-26T21:26:44.9887800Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:26:44.9888408Z Loading safetensors checkpoint shards:  34% Completed | 56/163 [06:38<07:32,  4.23s/it]
2026-02-26T21:26:48.6179679Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:26:48.6180794Z Loading safetensors checkpoint shards:  35% Completed | 57/163 [06:42<07:08,  4.05s/it]
2026-02-26T21:26:52.0688694Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:26:52.0689162Z Loading safetensors checkpoint shards:  36% Completed | 58/163 [06:45<06:46,  3.87s/it]
2026-02-26T21:27:05.6658502Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:27:05.6659007Z Loading safetensors checkpoint shards:  36% Completed | 59/163 [06:59<11:45,  6.79s/it]
2026-02-26T21:27:07.4691267Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:27:07.4691726Z Loading safetensors checkpoint shards:  37% Completed | 60/163 [07:01<09:05,  5.29s/it]
2026-02-26T21:27:11.8342677Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:27:11.8343186Z Loading safetensors checkpoint shards:  37% Completed | 61/163 [07:05<08:31,  5.01s/it]
2026-02-26T21:27:34.0450198Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:27:34.0450700Z Loading safetensors checkpoint shards:  38% Completed | 62/163 [07:27<17:07, 10.17s/it]
2026-02-26T21:27:37.2913655Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:27:37.2914211Z Loading safetensors checkpoint shards:  39% Completed | 63/163 [07:30<13:29,  8.09s/it]
2026-02-26T21:27:40.5868957Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:27:40.5869573Z Loading safetensors checkpoint shards:  39% Completed | 64/163 [07:34<10:58,  6.66s/it]
2026-02-26T21:27:56.9337072Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:27:56.9337521Z Loading safetensors checkpoint shards:  40% Completed | 65/163 [07:50<15:37,  9.56s/it]
2026-02-26T21:28:10.0798708Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:28:10.0799184Z Loading safetensors checkpoint shards:  40% Completed | 66/163 [08:03<17:11, 10.64s/it]
2026-02-26T21:28:10.2690716Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:28:10.2691350Z Loading safetensors checkpoint shards:  41% Completed | 67/163 [08:03<12:00,  7.50s/it]
2026-02-26T21:28:24.5431943Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:28:24.5432864Z Loading safetensors checkpoint shards:  42% Completed | 68/163 [08:18<15:05,  9.53s/it]
2026-02-26T21:28:24.7333270Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:28:24.7333712Z Loading safetensors checkpoint shards:  42% Completed | 69/163 [08:18<10:32,  6.73s/it]
2026-02-26T21:28:26.7026205Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:28:26.7026708Z Loading safetensors checkpoint shards:  43% Completed | 70/163 [08:20<08:13,  5.30s/it]
2026-02-26T21:28:37.5188161Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:28:37.5188668Z Loading safetensors checkpoint shards:  44% Completed | 71/163 [08:31<10:40,  6.96s/it]
2026-02-26T21:28:41.6439155Z [0;36m(ApiServer_1 pid=1029)[0;0m Process ApiServer_1:
2026-02-26T21:28:41.6543570Z [0;36m(ApiServer_0 pid=1028)[0;0m Process ApiServer_0:
2026-02-26T21:28:41.6695252Z [0;36m(ApiServer_0 pid=1028)[0;0m Traceback (most recent call last):
2026-02-26T21:28:41.6695737Z [0;36m(ApiServer_1 pid=1029)[0;0m Traceback (most recent call last):
2026-02-26T21:28:41.6696533Z [0;36m(ApiServer_0 pid=1028)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-26T21:28:41.6697193Z [0;36m(ApiServer_0 pid=1028)[0;0m     self.run()
2026-02-26T21:28:41.6697819Z [0;36m(ApiServer_1 pid=1029)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-26T21:28:41.6698550Z [0;36m(ApiServer_0 pid=1028)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-26T21:28:41.6699097Z [0;36m(ApiServer_1 pid=1029)[0;0m     self.run()
2026-02-26T21:28:41.6699484Z [0;36m(ApiServer_0 pid=1028)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-26T21:28:41.6700110Z [0;36m(ApiServer_0 pid=1028)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-26T21:28:41.6700827Z [0;36m(ApiServer_1 pid=1029)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-26T21:28:41.6701797Z [0;36m(ApiServer_0 pid=1028)[0;0m     uvloop.run(
2026-02-26T21:28:41.6702299Z [0;36m(ApiServer_1 pid=1029)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-26T21:28:41.6702900Z [0;36m(ApiServer_0 pid=1028)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-26T21:28:41.6703641Z [0;36m(ApiServer_1 pid=1029)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-26T21:28:41.6704178Z [0;36m(ApiServer_0 pid=1028)[0;0m     return runner.run(wrapper())
2026-02-26T21:28:41.6704612Z [0;36m(ApiServer_1 pid=1029)[0;0m     uvloop.run(
2026-02-26T21:28:41.6704986Z [0;36m(ApiServer_0 pid=1028)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-26T21:28:41.6705492Z [0;36m(ApiServer_0 pid=1028)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-26T21:28:41.6706186Z [0;36m(ApiServer_1 pid=1029)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-26T21:28:41.6706825Z [0;36m(ApiServer_0 pid=1028)[0;0m     return self._loop.run_until_complete(task)
2026-02-26T21:28:41.6707277Z [0;36m(ApiServer_1 pid=1029)[0;0m     return runner.run(wrapper())
2026-02-26T21:28:41.6707789Z [0;36m(ApiServer_0 pid=1028)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-26T21:28:41.6708175Z [0;36m(ApiServer_1 pid=1029)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-26T21:28:41.6708685Z [0;36m(ApiServer_0 pid=1028)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-26T21:28:41.6709285Z [0;36m(ApiServer_1 pid=1029)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-26T21:28:41.6709846Z [0;36m(ApiServer_1 pid=1029)[0;0m     return self._loop.run_until_complete(task)
2026-02-26T21:28:41.6710415Z [0;36m(ApiServer_1 pid=1029)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-26T21:28:41.6710971Z [0;36m(ApiServer_0 pid=1028)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-26T21:28:41.6711531Z [0;36m(ApiServer_0 pid=1028)[0;0m     return await main
2026-02-26T21:28:41.6712077Z [0;36m(ApiServer_1 pid=1029)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-26T21:28:41.6712573Z [0;36m(ApiServer_0 pid=1028)[0;0m            ^^^^^^^^^^
2026-02-26T21:28:41.6713146Z [0;36m(ApiServer_1 pid=1029)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-26T21:28:41.6713852Z [0;36m(ApiServer_0 pid=1028)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-26T21:28:41.6714467Z [0;36m(ApiServer_1 pid=1029)[0;0m     return await main
2026-02-26T21:28:41.6714857Z [0;36m(ApiServer_0 pid=1028)[0;0m     async with build_async_engine_client(
2026-02-26T21:28:41.6715301Z [0;36m(ApiServer_1 pid=1029)[0;0m            ^^^^^^^^^^
2026-02-26T21:28:41.6715808Z [0;36m(ApiServer_0 pid=1028)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-26T21:28:41.6716603Z [0;36m(ApiServer_1 pid=1029)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-26T21:28:41.6717178Z [0;36m(ApiServer_0 pid=1028)[0;0m     return await anext(self.gen)
2026-02-26T21:28:41.6717562Z [0;36m(ApiServer_1 pid=1029)[0;0m     async with build_async_engine_client(
2026-02-26T21:28:41.6718006Z [0;36m(ApiServer_0 pid=1028)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-26T21:28:41.6718550Z [0;36m(ApiServer_1 pid=1029)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-26T21:28:41.6719230Z [0;36m(ApiServer_0 pid=1028)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-26T21:28:41.6719964Z [0;36m(ApiServer_0 pid=1028)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-26T21:28:41.6720430Z [0;36m(ApiServer_1 pid=1029)[0;0m     return await anext(self.gen)
2026-02-26T21:28:41.6720861Z [0;36m(ApiServer_1 pid=1029)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-26T21:28:41.6721407Z [0;36m(ApiServer_0 pid=1028)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-26T21:28:41.6721902Z [0;36m(ApiServer_0 pid=1028)[0;0m     return await anext(self.gen)
2026-02-26T21:28:41.6722617Z [0;36m(ApiServer_1 pid=1029)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-26T21:28:41.6723216Z [0;36m(ApiServer_0 pid=1028)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-26T21:28:41.6723637Z [0;36m(ApiServer_1 pid=1029)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-26T21:28:41.6724225Z [0;36m(ApiServer_1 pid=1029)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-26T21:28:41.6724967Z [0;36m(ApiServer_0 pid=1028)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-26T21:28:41.6725591Z [0;36m(ApiServer_1 pid=1029)[0;0m     return await anext(self.gen)
2026-02-26T21:28:41.6726016Z [0;36m(ApiServer_0 pid=1028)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-26T21:28:41.6726476Z [0;36m(ApiServer_1 pid=1029)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-26T21:28:41.6726893Z [0;36m(ApiServer_0 pid=1028)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-26T21:28:41.6727588Z [0;36m(ApiServer_1 pid=1029)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-26T21:28:41.6728441Z [0;36m(ApiServer_0 pid=1028)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-26T21:28:41.6728984Z [0;36m(ApiServer_1 pid=1029)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-26T21:28:41.6729348Z [0;36m(ApiServer_0 pid=1028)[0;0m     return cls(
2026-02-26T21:28:41.6729740Z [0;36m(ApiServer_1 pid=1029)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-26T21:28:41.6730071Z [0;36m(ApiServer_0 pid=1028)[0;0m            ^^^^
2026-02-26T21:28:41.6730576Z [0;36m(ApiServer_1 pid=1029)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-26T21:28:41.6731232Z [0;36m(ApiServer_0 pid=1028)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-26T21:28:41.6731672Z [0;36m(ApiServer_1 pid=1029)[0;0m     return cls(
2026-02-26T21:28:41.6732201Z [0;36m(ApiServer_0 pid=1028)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-26T21:28:41.6732631Z [0;36m(ApiServer_1 pid=1029)[0;0m            ^^^^
2026-02-26T21:28:41.6733041Z [0;36m(ApiServer_0 pid=1028)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-26T21:28:41.6733710Z [0;36m(ApiServer_0 pid=1028)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-26T21:28:41.6734344Z [0;36m(ApiServer_1 pid=1029)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-26T21:28:41.6734881Z [0;36m(ApiServer_0 pid=1028)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-26T21:28:41.6735431Z [0;36m(ApiServer_1 pid=1029)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-26T21:28:41.6735953Z [0;36m(ApiServer_0 pid=1028)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-26T21:28:41.6736530Z [0;36m(ApiServer_1 pid=1029)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-26T21:28:41.6737080Z [0;36m(ApiServer_0 pid=1028)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-26T21:28:41.6737849Z [0;36m(ApiServer_1 pid=1029)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-26T21:28:41.6738352Z [0;36m(ApiServer_0 pid=1028)[0;0m     super().__init__(
2026-02-26T21:28:41.6738760Z [0;36m(ApiServer_1 pid=1029)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-26T21:28:41.6739187Z [0;36m(ApiServer_1 pid=1029)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-26T21:28:41.6739704Z [0;36m(ApiServer_0 pid=1028)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-26T21:28:41.6740444Z [0;36m(ApiServer_1 pid=1029)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-26T21:28:41.6740893Z [0;36m(ApiServer_0 pid=1028)[0;0m     super().__init__(
2026-02-26T21:28:41.6741262Z [0;36m(ApiServer_1 pid=1029)[0;0m     super().__init__(
2026-02-26T21:28:41.6741779Z [0;36m(ApiServer_0 pid=1028)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-26T21:28:41.6742568Z [0;36m(ApiServer_1 pid=1029)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-26T21:28:41.6743074Z [0;36m(ApiServer_0 pid=1028)[0;0m     super().__init__(
2026-02-26T21:28:41.6743392Z [0;36m(ApiServer_1 pid=1029)[0;0m     super().__init__(
2026-02-26T21:28:41.6743872Z [0;36m(ApiServer_0 pid=1028)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-26T21:28:41.6744520Z [0;36m(ApiServer_1 pid=1029)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-26T21:28:41.6744988Z [0;36m(ApiServer_0 pid=1028)[0;0m     raise TimeoutError(
2026-02-26T21:28:41.6745329Z [0;36m(ApiServer_1 pid=1029)[0;0m     super().__init__(
2026-02-26T21:28:41.6745808Z [0;36m(ApiServer_1 pid=1029)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-26T21:28:41.6746536Z [0;36m(ApiServer_1 pid=1029)[0;0m     raise TimeoutError(
2026-02-26T21:28:41.6747060Z [0;36m(ApiServer_0 pid=1028)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-26T21:28:41.6747704Z [0;36m(ApiServer_1 pid=1029)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-26T21:28:42.0783262Z [0;36m(ApiServer_1 pid=1029)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-26T21:28:42.0787612Z [0;36m(ApiServer_0 pid=1028)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-26T21:28:51.3608303Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:28:51.3608725Z Loading safetensors checkpoint shards:  44% Completed | 72/163 [08:44<13:41,  9.02s/it]
2026-02-26T21:28:53.0356188Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:28:53.0356869Z Loading safetensors checkpoint shards:  45% Completed | 73/163 [08:46<10:13,  6.82s/it]
2026-02-26T21:28:57.2559711Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:29:01.2421221Z Loading safetensors checkpoint shards:  45% Completed | 74/163 [08:50<08:57,  6.04s/it]
2026-02-26T21:29:01.2422099Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:29:01.2422469Z Loading safetensors checkpoint shards:  46% Completed | 75/163 [08:54<07:57,  5.42s/it]
2026-02-26T21:29:05.1231895Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:29:05.1232462Z Loading safetensors checkpoint shards:  47% Completed | 76/163 [08:58<07:11,  4.96s/it]
2026-02-26T21:29:23.0170989Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:29:23.0171447Z Loading safetensors checkpoint shards:  47% Completed | 77/163 [09:16<12:40,  8.84s/it]
2026-02-26T21:29:23.2231146Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:29:23.2231707Z Loading safetensors checkpoint shards:  48% Completed | 78/163 [09:16<08:51,  6.25s/it]
2026-02-26T21:29:24.6963511Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:29:24.6964038Z Loading safetensors checkpoint shards:  48% Completed | 79/163 [09:18<06:44,  4.82s/it]
2026-02-26T21:29:43.0314859Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:29:43.0315332Z Loading safetensors checkpoint shards:  49% Completed | 80/163 [09:36<12:16,  8.87s/it]
2026-02-26T21:29:58.0134148Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:29:58.0134742Z Loading safetensors checkpoint shards:  50% Completed | 81/163 [09:51<14:37, 10.71s/it]
2026-02-26T21:29:58.2036807Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:29:58.2037640Z Loading safetensors checkpoint shards:  50% Completed | 82/163 [09:51<10:11,  7.55s/it]
2026-02-26T21:30:04.1200826Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:30:04.1201325Z Loading safetensors checkpoint shards:  51% Completed | 83/163 [09:57<09:24,  7.06s/it]
2026-02-26T21:30:07.6552699Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:30:07.6553217Z Loading safetensors checkpoint shards:  52% Completed | 84/163 [10:01<07:54,  6.00s/it]
2026-02-26T21:30:24.9814422Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:30:24.9815112Z Loading safetensors checkpoint shards:  52% Completed | 85/163 [10:18<12:13,  9.40s/it]
2026-02-26T21:30:25.1896763Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:30:25.1897376Z Loading safetensors checkpoint shards:  53% Completed | 86/163 [10:18<08:31,  6.64s/it]
2026-02-26T21:30:30.2762184Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:30:30.2762782Z Loading safetensors checkpoint shards:  53% Completed | 87/163 [10:23<07:49,  6.17s/it]
2026-02-26T21:30:45.3763978Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:30:45.3764441Z Loading safetensors checkpoint shards:  54% Completed | 88/163 [10:38<11:04,  8.85s/it]
2026-02-26T21:30:46.5501396Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:30:46.5501871Z Loading safetensors checkpoint shards:  55% Completed | 89/163 [10:40<08:04,  6.55s/it]
2026-02-26T21:31:02.7619560Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:31:02.7620045Z Loading safetensors checkpoint shards:  55% Completed | 90/163 [10:56<11:29,  9.45s/it]
2026-02-26T21:31:07.6446514Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:31:07.6447027Z Loading safetensors checkpoint shards:  56% Completed | 91/163 [11:01<09:41,  8.08s/it]
2026-02-26T21:31:20.7704030Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:31:20.7704566Z Loading safetensors checkpoint shards:  56% Completed | 92/163 [11:14<11:21,  9.59s/it]
2026-02-26T21:31:20.9597632Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:31:20.9598019Z Loading safetensors checkpoint shards:  57% Completed | 93/163 [11:14<07:54,  6.77s/it]
2026-02-26T21:31:21.1185942Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:31:21.1186470Z Loading safetensors checkpoint shards:  58% Completed | 94/163 [11:14<05:30,  4.79s/it]
2026-02-26T21:31:24.7238804Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:31:24.7239342Z Loading safetensors checkpoint shards:  58% Completed | 95/163 [11:18<05:01,  4.43s/it]
2026-02-26T21:31:42.2034338Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:31:44.4168059Z Loading safetensors checkpoint shards:  59% Completed | 96/163 [11:35<09:19,  8.35s/it]
2026-02-26T21:31:44.4168787Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:31:44.4169226Z Loading safetensors checkpoint shards:  60% Completed | 97/163 [11:37<07:09,  6.51s/it]
2026-02-26T21:31:47.8455002Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:31:47.8455437Z Loading safetensors checkpoint shards:  60% Completed | 98/163 [11:41<06:02,  5.58s/it]
2026-02-26T21:31:51.1402133Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:31:51.1402712Z Loading safetensors checkpoint shards:  61% Completed | 99/163 [11:44<05:13,  4.90s/it]
2026-02-26T21:31:54.3364889Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:31:54.3365808Z Loading safetensors checkpoint shards:  61% Completed | 100/163 [11:47<04:36,  4.39s/it]
2026-02-26T21:31:57.2301965Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:31:57.2302574Z Loading safetensors checkpoint shards:  62% Completed | 101/163 [11:50<04:04,  3.94s/it]
2026-02-26T21:32:14.1750570Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:32:14.1751157Z Loading safetensors checkpoint shards:  63% Completed | 102/163 [12:07<07:58,  7.84s/it]
2026-02-26T21:32:27.7768579Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:32:27.7769137Z Loading safetensors checkpoint shards:  63% Completed | 103/163 [12:21<09:34,  9.57s/it]
2026-02-26T21:32:41.3385805Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:32:41.3386430Z Loading safetensors checkpoint shards:  64% Completed | 104/163 [12:34<10:35, 10.77s/it]
2026-02-26T21:32:41.5241635Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:32:41.5242249Z Loading safetensors checkpoint shards:  64% Completed | 105/163 [12:35<07:20,  7.59s/it]
2026-02-26T21:32:52.7569767Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:32:52.7570273Z Loading safetensors checkpoint shards:  65% Completed | 106/163 [12:46<08:15,  8.68s/it]
2026-02-26T21:32:52.8734645Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:32:52.8735201Z Loading safetensors checkpoint shards:  66% Completed | 107/163 [12:46<05:42,  6.11s/it]
2026-02-26T21:33:02.8116915Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:33:02.8117364Z Loading safetensors checkpoint shards:  66% Completed | 108/163 [12:56<06:39,  7.26s/it]
2026-02-26T21:33:03.0039166Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:33:03.0039661Z Loading safetensors checkpoint shards:  67% Completed | 109/163 [12:56<04:37,  5.14s/it]
2026-02-26T21:33:16.2524128Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:33:16.2524542Z Loading safetensors checkpoint shards:  67% Completed | 110/163 [13:09<06:41,  7.57s/it]
2026-02-26T21:33:27.3385822Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:33:27.3386641Z Loading safetensors checkpoint shards:  68% Completed | 111/163 [13:20<07:28,  8.63s/it]
2026-02-26T21:33:32.9231960Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:33:32.9232540Z Loading safetensors checkpoint shards:  69% Completed | 112/163 [13:26<06:33,  7.71s/it]
2026-02-26T21:33:42.8141144Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:33:42.8141630Z Loading safetensors checkpoint shards:  69% Completed | 113/163 [13:36<06:58,  8.37s/it]
2026-02-26T21:33:45.5094918Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:33:45.5095531Z Loading safetensors checkpoint shards:  70% Completed | 114/163 [13:39<05:26,  6.67s/it]
2026-02-26T21:33:49.0248091Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:34:07.2884086Z Loading safetensors checkpoint shards:  71% Completed | 115/163 [13:42<04:34,  5.72s/it]
2026-02-26T21:34:07.2884766Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:34:07.2885157Z Loading safetensors checkpoint shards:  71% Completed | 116/163 [14:00<07:25,  9.48s/it]
2026-02-26T21:34:10.4468907Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:34:10.4469397Z Loading safetensors checkpoint shards:  72% Completed | 117/163 [14:04<05:48,  7.59s/it]
2026-02-26T21:34:25.1926945Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:34:25.1927457Z Loading safetensors checkpoint shards:  72% Completed | 118/163 [14:18<07:18,  9.73s/it]
2026-02-26T21:34:25.3863943Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:34:25.3864461Z Loading safetensors checkpoint shards:  73% Completed | 119/163 [14:18<05:02,  6.87s/it]
2026-02-26T21:34:27.9633028Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:34:27.9633653Z Loading safetensors checkpoint shards:  74% Completed | 120/163 [14:21<04:00,  5.58s/it]
2026-02-26T21:34:31.6171951Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:34:31.6172700Z Loading safetensors checkpoint shards:  74% Completed | 121/163 [14:25<03:30,  5.00s/it]
2026-02-26T21:34:35.5383505Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:34:35.5384011Z Loading safetensors checkpoint shards:  75% Completed | 122/163 [14:29<03:11,  4.68s/it]
2026-02-26T21:34:53.2456786Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:34:53.2457275Z Loading safetensors checkpoint shards:  75% Completed | 123/163 [14:46<05:43,  8.59s/it]
2026-02-26T21:34:53.4429848Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:34:53.4430451Z Loading safetensors checkpoint shards:  76% Completed | 124/163 [14:47<03:56,  6.07s/it]
2026-02-26T21:35:07.2864878Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:35:07.2865361Z Loading safetensors checkpoint shards:  77% Completed | 125/163 [15:00<05:19,  8.40s/it]
2026-02-26T21:35:08.1451117Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:35:08.1451545Z Loading safetensors checkpoint shards:  77% Completed | 126/163 [15:01<03:47,  6.14s/it]
2026-02-26T21:35:11.6493082Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:35:11.6493629Z Loading safetensors checkpoint shards:  78% Completed | 127/163 [15:05<03:12,  5.35s/it]
2026-02-26T21:35:29.5220025Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:35:29.5220526Z Loading safetensors checkpoint shards:  79% Completed | 128/163 [15:23<05:18,  9.11s/it]
2026-02-26T21:35:40.3938233Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:35:40.3938847Z Loading safetensors checkpoint shards:  79% Completed | 129/163 [15:33<05:27,  9.64s/it]
2026-02-26T21:35:40.5767660Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:35:40.5768109Z Loading safetensors checkpoint shards:  80% Completed | 130/163 [15:34<03:44,  6.80s/it]
2026-02-26T21:35:40.7265147Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:35:40.7265586Z Loading safetensors checkpoint shards:  80% Completed | 131/163 [15:34<02:33,  4.80s/it]
2026-02-26T21:35:44.0140524Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:35:44.0141058Z Loading safetensors checkpoint shards:  81% Completed | 132/163 [15:37<02:14,  4.35s/it]
2026-02-26T21:36:00.2416727Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:36:00.2417211Z Loading safetensors checkpoint shards:  82% Completed | 133/163 [15:53<03:57,  7.91s/it]
2026-02-26T21:36:01.4581711Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:36:01.4582323Z Loading safetensors checkpoint shards:  82% Completed | 134/163 [15:55<02:51,  5.90s/it]
2026-02-26T21:36:17.0361122Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:36:17.0361600Z Loading safetensors checkpoint shards:  83% Completed | 135/163 [16:10<04:06,  8.81s/it]
2026-02-26T21:36:19.2148624Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:36:19.2149108Z Loading safetensors checkpoint shards:  83% Completed | 136/163 [16:12<03:04,  6.82s/it]
2026-02-26T21:36:22.6729216Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:36:22.6729678Z Loading safetensors checkpoint shards:  84% Completed | 137/163 [16:16<02:31,  5.81s/it]
2026-02-26T21:36:34.4346812Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:36:34.4347470Z Loading safetensors checkpoint shards:  85% Completed | 138/163 [16:28<03:09,  7.60s/it]
2026-02-26T21:36:43.1844514Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:36:43.1845023Z Loading safetensors checkpoint shards:  85% Completed | 139/163 [16:36<03:10,  7.94s/it]
2026-02-26T21:36:43.3769855Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:36:43.3770432Z Loading safetensors checkpoint shards:  86% Completed | 140/163 [16:36<02:09,  5.62s/it]
2026-02-26T21:36:50.2159088Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:36:50.3867899Z Loading safetensors checkpoint shards:  87% Completed | 141/163 [16:43<02:11,  5.98s/it]
2026-02-26T21:36:50.3868710Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:36:50.3869077Z Loading safetensors checkpoint shards:  87% Completed | 142/163 [16:43<01:29,  4.24s/it]
2026-02-26T21:37:02.2321234Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:37:02.2322508Z Loading safetensors checkpoint shards:  88% Completed | 143/163 [16:55<02:10,  6.52s/it]
2026-02-26T21:37:11.9502508Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:37:11.9503087Z Loading safetensors checkpoint shards:  88% Completed | 144/163 [17:05<02:22,  7.48s/it]
2026-02-26T21:37:20.5040377Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:37:20.5040913Z Loading safetensors checkpoint shards:  89% Completed | 145/163 [17:14<02:20,  7.80s/it]
2026-02-26T21:37:32.9698807Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:37:32.9699413Z Loading safetensors checkpoint shards:  90% Completed | 146/163 [17:26<02:36,  9.20s/it]
2026-02-26T21:37:33.1706267Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:37:33.1706703Z Loading safetensors checkpoint shards:  90% Completed | 147/163 [17:26<01:44,  6.50s/it]
2026-02-26T21:37:33.3439888Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:37:33.3440456Z Loading safetensors checkpoint shards:  91% Completed | 148/163 [17:26<01:09,  4.60s/it]
2026-02-26T21:37:33.5039410Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:37:33.5039855Z Loading safetensors checkpoint shards:  91% Completed | 149/163 [17:27<00:45,  3.27s/it]
2026-02-26T21:37:46.6581426Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:37:46.6581952Z Loading safetensors checkpoint shards:  93% Completed | 151/163 [17:40<00:57,  4.80s/it]
2026-02-26T21:37:46.8373027Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:37:46.8373552Z Loading safetensors checkpoint shards:  93% Completed | 152/163 [17:40<00:40,  3.65s/it]
2026-02-26T21:37:46.9953391Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:37:46.9953950Z Loading safetensors checkpoint shards:  94% Completed | 153/163 [17:40<00:27,  2.74s/it]
2026-02-26T21:37:57.1032822Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:37:57.1033293Z Loading safetensors checkpoint shards:  94% Completed | 154/163 [17:50<00:42,  4.74s/it]
2026-02-26T21:37:57.2761441Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:37:57.2762301Z Loading safetensors checkpoint shards:  95% Completed | 155/163 [17:50<00:27,  3.46s/it]
2026-02-26T21:37:57.4304427Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:37:57.4304908Z Loading safetensors checkpoint shards:  96% Completed | 156/163 [17:51<00:17,  2.52s/it]
2026-02-26T21:37:58.4360822Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:37:58.4361332Z Loading safetensors checkpoint shards:  96% Completed | 157/163 [17:52<00:12,  2.08s/it]
2026-02-26T21:38:10.0239992Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:38:10.0240556Z Loading safetensors checkpoint shards:  97% Completed | 158/163 [18:03<00:24,  4.86s/it]
2026-02-26T21:38:11.7882238Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:38:11.7882892Z Loading safetensors checkpoint shards:  98% Completed | 159/163 [18:05<00:15,  3.95s/it]
2026-02-26T21:38:13.8555216Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:38:13.8555834Z Loading safetensors checkpoint shards:  98% Completed | 160/163 [18:07<00:10,  3.39s/it]
2026-02-26T21:38:17.1570952Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:38:17.1571429Z Loading safetensors checkpoint shards:  99% Completed | 161/163 [18:10<00:06,  3.36s/it]
2026-02-26T21:38:27.2875321Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:38:27.2875757Z Loading safetensors checkpoint shards:  99% Completed | 162/163 [18:20<00:05,  5.38s/it]
2026-02-26T21:38:30.7006787Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:38:30.7007313Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [18:24<00:00,  4.79s/it]
2026-02-26T21:38:30.7007826Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:38:30.7008234Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [18:24<00:00,  6.77s/it]
2026-02-26T21:38:30.7008671Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:38:30.7175915Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m INFO 02-26 21:38:30 [default_loader.py:291] Loading weights took 1104.29 seconds
2026-02-26T21:38:43.6017860Z INFO 02-26 21:38:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:38:43.6018504Z INFO 02-26 21:38:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:38:43.6019135Z INFO 02-26 21:38:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:38:43.6019516Z INFO 02-26 21:38:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:38:43.6020039Z INFO 02-26 21:38:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:38:43.6020605Z INFO 02-26 21:38:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:38:43.6021221Z INFO 02-26 21:38:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:38:43.6021725Z INFO 02-26 21:38:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:38:43.6022250Z INFO 02-26 21:38:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:38:43.6022786Z INFO 02-26 21:38:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:38:43.6023237Z INFO 02-26 21:38:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:38:43.6023669Z INFO 02-26 21:38:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:38:43.6024244Z INFO 02-26 21:38:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:38:43.6024682Z INFO 02-26 21:38:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:38:43.6025185Z INFO 02-26 21:38:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:38:43.6025667Z INFO 02-26 21:38:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:38:43.6026475Z INFO 02-26 21:38:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:38:43.6026906Z INFO 02-26 21:38:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:38:43.6027238Z INFO 02-26 21:38:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:38:43.6027717Z INFO 02-26 21:38:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:38:43.6028302Z INFO 02-26 21:38:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:38:43.6168165Z INFO 02-26 21:38:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:38:43.6168580Z INFO 02-26 21:38:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:38:43.6168960Z INFO 02-26 21:38:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:38:43.6169353Z INFO 02-26 21:38:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:38:43.6169712Z INFO 02-26 21:38:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:38:43.6170096Z INFO 02-26 21:38:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:38:43.6170466Z INFO 02-26 21:38:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:38:43.6720271Z INFO 02-26 21:38:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:38:43.6720802Z INFO 02-26 21:38:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:38:43.6721373Z INFO 02-26 21:38:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:38:43.6786198Z INFO 02-26 21:38:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:38:51.2102222Z INFO 02-26 21:38:51 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:38:51.2102726Z INFO 02-26 21:38:51 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:38:51.2103795Z INFO 02-26 21:38:51 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:38:51.2179555Z INFO 02-26 21:38:51 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:38:52.1287851Z INFO 02-26 21:38:52 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:38:52.1288406Z INFO 02-26 21:38:52 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:38:52.1288887Z INFO 02-26 21:38:52 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:38:52.1359560Z INFO 02-26 21:38:52 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:38:53.3059888Z INFO 02-26 21:38:53 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:38:53.3060473Z INFO 02-26 21:38:53 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:38:53.3061042Z INFO 02-26 21:38:53 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:38:53.3132694Z INFO 02-26 21:38:53 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:38:54.0113791Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m INFO 02-26 21:38:54 [default_loader.py:291] Loading weights took 1127.79 seconds
2026-02-26T21:38:59.3487935Z INFO 02-26 21:38:59 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:38:59.3488518Z INFO 02-26 21:38:59 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:38:59.3489066Z INFO 02-26 21:38:59 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:38:59.3577970Z INFO 02-26 21:38:59 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:39:01.6807049Z INFO 02-26 21:39:01 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:39:01.6807601Z INFO 02-26 21:39:01 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:39:01.6808551Z INFO 02-26 21:39:01 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:39:01.6896644Z INFO 02-26 21:39:01 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:39:03.1082560Z INFO 02-26 21:39:03 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:39:03.1083143Z INFO 02-26 21:39:03 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:39:03.1083736Z INFO 02-26 21:39:03 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:39:03.1178212Z INFO 02-26 21:39:03 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:39:05.3983273Z INFO 02-26 21:39:05 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:39:05.3983876Z INFO 02-26 21:39:05 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:39:05.3984442Z INFO 02-26 21:39:05 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:39:05.4078069Z INFO 02-26 21:39:05 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:39:06.4576806Z INFO 02-26 21:39:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-26T21:39:06.4577354Z INFO 02-26 21:39:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-26T21:39:06.4577921Z INFO 02-26 21:39:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-26T21:39:06.4670650Z INFO 02-26 21:39:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-26T21:39:07.1770217Z [0;36m(Worker_DP1_TP6_EP14 pid=1602)[0;0m WARNING 02-26 21:39:07 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T21:39:07.1796250Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m WARNING 02-26 21:39:07 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T21:39:07.1980228Z [0;36m(Worker_DP1_TP5_EP13 pid=1498)[0;0m WARNING 02-26 21:39:07 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T21:39:07.2207350Z [0;36m(Worker_DP0_TP1_EP1 pid=1077)[0;0m WARNING 02-26 21:39:07 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T21:39:07.2211757Z [0;36m(Worker_DP1_TP4_EP12 pid=1394)[0;0m WARNING 02-26 21:39:07 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T21:39:07.2368145Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m INFO 02-26 21:39:07 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T21:39:07.2372497Z [0;36m(Worker_DP1_TP6_EP14 pid=1602)[0;0m INFO 02-26 21:39:07 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T21:39:07.2562569Z [0;36m(Worker_DP1_TP5_EP13 pid=1498)[0;0m INFO 02-26 21:39:07 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T21:39:07.2784024Z [0;36m(Worker_DP1_TP4_EP12 pid=1394)[0;0m INFO 02-26 21:39:07 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T21:39:07.2786449Z [0;36m(Worker_DP0_TP1_EP1 pid=1077)[0;0m INFO 02-26 21:39:07 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T21:39:07.2903498Z [0;36m(Worker_DP1_TP2_EP10 pid=1183)[0;0m WARNING 02-26 21:39:07 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T21:39:07.2991110Z [0;36m(Worker_DP0_TP2_EP2 pid=1186)[0;0m WARNING 02-26 21:39:07 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T21:39:07.3208160Z [0;36m(Worker_DP0_TP3_EP3 pid=1287)[0;0m WARNING 02-26 21:39:07 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T21:39:07.3363671Z [0;36m(Worker_DP0_TP7_EP7 pid=1703)[0;0m WARNING 02-26 21:39:07 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T21:39:07.3420505Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:07.3420917Z Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
2026-02-26T21:39:07.3428632Z [0;36m(Worker_DP1_TP7_EP15 pid=1710)[0;0m WARNING 02-26 21:39:07 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T21:39:07.3437262Z [0;36m(Worker_DP1_TP3_EP11 pid=1290)[0;0m WARNING 02-26 21:39:07 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T21:39:07.3461800Z [0;36m(Worker_DP1_TP2_EP10 pid=1183)[0;0m INFO 02-26 21:39:07 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T21:39:07.3564139Z [0;36m(Worker_DP0_TP2_EP2 pid=1186)[0;0m INFO 02-26 21:39:07 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T21:39:07.3765743Z [0;36m(Worker_DP0_TP3_EP3 pid=1287)[0;0m INFO 02-26 21:39:07 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T21:39:07.3943618Z [0;36m(Worker_DP0_TP7_EP7 pid=1703)[0;0m INFO 02-26 21:39:07 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T21:39:07.4054558Z [0;36m(Worker_DP1_TP7_EP15 pid=1710)[0;0m INFO 02-26 21:39:07 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T21:39:07.4576679Z [0;36m(Worker_DP1_TP3_EP11 pid=1290)[0;0m INFO 02-26 21:39:07 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T21:39:08.7732054Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:08.7732638Z Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:01<03:51,  1.43s/it]
2026-02-26T21:39:10.1308965Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:10.1309401Z Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:02<03:43,  1.39s/it]
2026-02-26T21:39:11.6115720Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:11.6116576Z Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:04<03:48,  1.43s/it]
2026-02-26T21:39:13.0790516Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:13.0790964Z Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:05<03:49,  1.45s/it]
2026-02-26T21:39:14.5090274Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:14.5090864Z Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:07<03:47,  1.44s/it]
2026-02-26T21:39:15.9046831Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:15.9047274Z Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:08<03:43,  1.42s/it]
2026-02-26T21:39:17.5495963Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:17.5496666Z Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:10<03:53,  1.50s/it]
2026-02-26T21:39:18.9259421Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:18.9260064Z Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:11<03:46,  1.46s/it]
2026-02-26T21:39:20.5571747Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:20.5572427Z Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:13<03:52,  1.51s/it]
2026-02-26T21:39:21.9441964Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:21.9442522Z Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:14<03:45,  1.47s/it]
2026-02-26T21:39:23.3409465Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:23.3409944Z Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:15<03:40,  1.45s/it]
2026-02-26T21:39:24.7471652Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:24.7472316Z Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:17<03:36,  1.44s/it]
2026-02-26T21:39:26.1913475Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:26.1913967Z Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:18<03:35,  1.44s/it]
2026-02-26T21:39:27.6804057Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:27.6804804Z Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:20<03:36,  1.45s/it]
2026-02-26T21:39:29.1533103Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:29.1533642Z Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:21<03:36,  1.46s/it]
2026-02-26T21:39:30.5163882Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:30.5164527Z Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:23<03:30,  1.43s/it]
2026-02-26T21:39:32.0480198Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:32.0480837Z Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:24<03:33,  1.46s/it]
2026-02-26T21:39:33.6109713Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:33.6110275Z Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:26<03:36,  1.49s/it]
2026-02-26T21:39:35.2805501Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:35.2806048Z Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:27<03:42,  1.54s/it]
2026-02-26T21:39:36.6987491Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:36.6987978Z Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:29<03:35,  1.51s/it]
2026-02-26T21:39:37.9855764Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:37.9856298Z Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:30<03:24,  1.44s/it]
2026-02-26T21:39:39.4999181Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:39.4999653Z Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:32<03:26,  1.46s/it]
2026-02-26T21:39:40.8957132Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:40.8957590Z Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:33<03:21,  1.44s/it]
2026-02-26T21:39:42.2689972Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:42.2690530Z Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:34<03:17,  1.42s/it]
2026-02-26T21:39:43.5937513Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:43.5938053Z Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:36<03:12,  1.39s/it]
2026-02-26T21:39:45.1161965Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:46.7644040Z Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:37<03:16,  1.43s/it]
2026-02-26T21:39:46.7644714Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:46.7645112Z Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:39<03:23,  1.50s/it]
2026-02-26T21:39:48.3390699Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:48.3391118Z Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:40<03:25,  1.52s/it]
2026-02-26T21:39:49.7887191Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:49.7887661Z Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:42<03:20,  1.50s/it]
2026-02-26T21:39:51.2110205Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:51.2110756Z Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:43<03:16,  1.48s/it]
2026-02-26T21:39:52.5356362Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:52.5356847Z Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:45<03:08,  1.43s/it]
2026-02-26T21:39:53.9320625Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:53.9321074Z Loading safetensors checkpoint shards:  20% Completed | 32/163 [00:46<03:06,  1.42s/it]
2026-02-26T21:39:55.3505775Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:55.3506333Z Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:48<03:04,  1.42s/it]
2026-02-26T21:39:56.7034722Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:56.7035372Z Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:49<03:00,  1.40s/it]
2026-02-26T21:39:58.0048476Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:58.0049004Z Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:50<02:55,  1.37s/it]
2026-02-26T21:39:59.3910836Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:39:59.3911393Z Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:52<02:54,  1.38s/it]
2026-02-26T21:40:00.6503074Z [0;36m(Worker_DP0_TP4_EP4 pid=1391)[0;0m WARNING 02-26 21:40:00 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T21:40:00.7026809Z [0;36m(Worker_DP0_TP4_EP4 pid=1391)[0;0m INFO 02-26 21:40:00 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T21:40:00.7485876Z [0;36m(Worker_DP0_TP6_EP6 pid=1599)[0;0m WARNING 02-26 21:40:00 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T21:40:00.8050791Z [0;36m(Worker_DP0_TP6_EP6 pid=1599)[0;0m INFO 02-26 21:40:00 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T21:40:00.8103340Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m WARNING 02-26 21:40:00 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T21:40:00.8483136Z [0;36m(Worker_DP0_TP5_EP5 pid=1495)[0;0m WARNING 02-26 21:40:00 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T21:40:00.8676514Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m INFO 02-26 21:40:00 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T21:40:00.8918382Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:00.8918830Z Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:53<02:58,  1.41s/it]
2026-02-26T21:40:00.9031347Z [0;36m(Worker_DP0_TP5_EP5 pid=1495)[0;0m INFO 02-26 21:40:00 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T21:40:00.9295678Z [0;36m(Worker_DP1_TP1_EP9 pid=1079)[0;0m WARNING 02-26 21:40:00 [sfa_v1.py:497] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-26T21:40:00.9977634Z [0;36m(Worker_DP1_TP1_EP9 pid=1079)[0;0m INFO 02-26 21:40:00 [model_runner_v1.py:2344] Loading drafter model...
2026-02-26T21:40:02.3781610Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:02.3782211Z Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:55<02:59,  1.43s/it]
2026-02-26T21:40:04.2445505Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:04.2446020Z Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:56<03:13,  1.56s/it]
2026-02-26T21:40:05.6969035Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:05.6969772Z Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:58<03:08,  1.53s/it]
2026-02-26T21:40:07.5340916Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:07.5341478Z Loading safetensors checkpoint shards:  25% Completed | 41/163 [01:00<03:17,  1.62s/it]
2026-02-26T21:40:09.1967238Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:09.1967789Z Loading safetensors checkpoint shards:  26% Completed | 42/163 [01:01<03:17,  1.63s/it]
2026-02-26T21:40:10.8102214Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:10.8102879Z Loading safetensors checkpoint shards:  26% Completed | 43/163 [01:03<03:15,  1.63s/it]
2026-02-26T21:40:12.4750163Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:12.4750643Z Loading safetensors checkpoint shards:  27% Completed | 44/163 [01:05<03:15,  1.64s/it]
2026-02-26T21:40:14.1629959Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:14.1630447Z Loading safetensors checkpoint shards:  28% Completed | 45/163 [01:06<03:15,  1.65s/it]
2026-02-26T21:40:15.8890892Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:15.8891361Z Loading safetensors checkpoint shards:  28% Completed | 46/163 [01:08<03:16,  1.68s/it]
2026-02-26T21:40:17.0590871Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:17.0591390Z Loading safetensors checkpoint shards:  29% Completed | 47/163 [01:09<02:56,  1.52s/it]
2026-02-26T21:40:18.7721752Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:18.7722725Z Loading safetensors checkpoint shards:  30% Completed | 49/163 [01:11<02:18,  1.22s/it]
2026-02-26T21:40:20.2080033Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:20.2080749Z Loading safetensors checkpoint shards:  31% Completed | 50/163 [01:12<02:23,  1.27s/it]
2026-02-26T21:40:21.8302792Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:21.8303363Z Loading safetensors checkpoint shards:  31% Completed | 51/163 [01:14<02:32,  1.36s/it]
2026-02-26T21:40:23.3700438Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:23.3701127Z Loading safetensors checkpoint shards:  32% Completed | 52/163 [01:16<02:36,  1.41s/it]
2026-02-26T21:40:24.7776276Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:24.7776758Z Loading safetensors checkpoint shards:  33% Completed | 53/163 [01:17<02:35,  1.41s/it]
2026-02-26T21:40:26.3256282Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:26.3256756Z Loading safetensors checkpoint shards:  33% Completed | 54/163 [01:18<02:37,  1.45s/it]
2026-02-26T21:40:27.7951078Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:27.7951555Z Loading safetensors checkpoint shards:  34% Completed | 55/163 [01:20<02:37,  1.46s/it]
2026-02-26T21:40:29.2651627Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:29.2652181Z Loading safetensors checkpoint shards:  34% Completed | 56/163 [01:21<02:36,  1.46s/it]
2026-02-26T21:40:30.7956511Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:30.7956968Z Loading safetensors checkpoint shards:  35% Completed | 57/163 [01:23<02:36,  1.48s/it]
2026-02-26T21:40:32.4754458Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:32.4754933Z Loading safetensors checkpoint shards:  36% Completed | 58/163 [01:25<02:41,  1.54s/it]
2026-02-26T21:40:33.9178516Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:33.9178992Z Loading safetensors checkpoint shards:  36% Completed | 59/163 [01:26<02:37,  1.51s/it]
2026-02-26T21:40:35.3631111Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:35.3631867Z Loading safetensors checkpoint shards:  37% Completed | 60/163 [01:28<02:33,  1.49s/it]
2026-02-26T21:40:36.8406245Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:38.4531333Z Loading safetensors checkpoint shards:  37% Completed | 61/163 [01:29<02:31,  1.49s/it]
2026-02-26T21:40:38.4532202Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:38.4532622Z Loading safetensors checkpoint shards:  38% Completed | 62/163 [01:31<02:33,  1.52s/it]
2026-02-26T21:40:39.9118261Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:41.4307562Z Loading safetensors checkpoint shards:  39% Completed | 63/163 [01:32<02:30,  1.50s/it]
2026-02-26T21:40:41.4308278Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:41.4308709Z Loading safetensors checkpoint shards:  39% Completed | 64/163 [01:34<02:29,  1.51s/it]
2026-02-26T21:40:42.8611860Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:42.8612506Z Loading safetensors checkpoint shards:  40% Completed | 65/163 [01:35<02:25,  1.49s/it]
2026-02-26T21:40:44.5168145Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:44.5168603Z Loading safetensors checkpoint shards:  40% Completed | 66/163 [01:37<02:29,  1.54s/it]
2026-02-26T21:40:46.0415195Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:46.0415626Z Loading safetensors checkpoint shards:  41% Completed | 67/163 [01:38<02:27,  1.53s/it]
2026-02-26T21:40:47.4275960Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:47.4276487Z Loading safetensors checkpoint shards:  42% Completed | 68/163 [01:40<02:21,  1.49s/it]
2026-02-26T21:40:48.8858346Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:48.8858994Z Loading safetensors checkpoint shards:  42% Completed | 69/163 [01:41<02:19,  1.48s/it]
2026-02-26T21:40:50.3645233Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:50.3645948Z Loading safetensors checkpoint shards:  43% Completed | 70/163 [01:43<02:17,  1.48s/it]
2026-02-26T21:40:51.7128215Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:51.7129170Z Loading safetensors checkpoint shards:  44% Completed | 71/163 [01:44<02:12,  1.44s/it]
2026-02-26T21:40:53.1632717Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:53.1633133Z Loading safetensors checkpoint shards:  44% Completed | 72/163 [01:45<02:11,  1.44s/it]
2026-02-26T21:40:54.6525838Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:54.6526318Z Loading safetensors checkpoint shards:  45% Completed | 73/163 [01:47<02:11,  1.46s/it]
2026-02-26T21:40:56.1336156Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:56.1336656Z Loading safetensors checkpoint shards:  45% Completed | 74/163 [01:48<02:10,  1.46s/it]
2026-02-26T21:40:57.6183921Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:57.6184404Z Loading safetensors checkpoint shards:  46% Completed | 75/163 [01:50<02:09,  1.47s/it]
2026-02-26T21:40:59.1546489Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:40:59.1547124Z Loading safetensors checkpoint shards:  47% Completed | 76/163 [01:51<02:09,  1.49s/it]
2026-02-26T21:41:00.5908231Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:00.5908932Z Loading safetensors checkpoint shards:  47% Completed | 77/163 [01:53<02:06,  1.47s/it]
2026-02-26T21:41:02.1424223Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:02.1424678Z Loading safetensors checkpoint shards:  48% Completed | 78/163 [01:54<02:07,  1.50s/it]
2026-02-26T21:41:03.7631109Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:03.7631741Z Loading safetensors checkpoint shards:  48% Completed | 79/163 [01:56<02:08,  1.53s/it]
2026-02-26T21:41:05.2065838Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:05.2066321Z Loading safetensors checkpoint shards:  49% Completed | 80/163 [01:57<02:05,  1.51s/it]
2026-02-26T21:41:06.7837266Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:06.7837902Z Loading safetensors checkpoint shards:  50% Completed | 81/163 [01:59<02:05,  1.53s/it]
2026-02-26T21:41:08.2650499Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:08.2651056Z Loading safetensors checkpoint shards:  50% Completed | 82/163 [02:00<02:02,  1.51s/it]
2026-02-26T21:41:09.9204597Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:09.9205041Z Loading safetensors checkpoint shards:  51% Completed | 83/163 [02:02<02:04,  1.56s/it]
2026-02-26T21:41:11.5949030Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:11.5949606Z Loading safetensors checkpoint shards:  52% Completed | 84/163 [02:04<02:05,  1.59s/it]
2026-02-26T21:41:13.0758110Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:13.0758627Z Loading safetensors checkpoint shards:  52% Completed | 85/163 [02:05<02:01,  1.56s/it]
2026-02-26T21:41:14.5837684Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:14.5838313Z Loading safetensors checkpoint shards:  53% Completed | 86/163 [02:07<01:58,  1.54s/it]
2026-02-26T21:41:16.1000511Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:16.1000975Z Loading safetensors checkpoint shards:  53% Completed | 87/163 [02:08<01:56,  1.54s/it]
2026-02-26T21:41:17.6414299Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:17.6414920Z Loading safetensors checkpoint shards:  54% Completed | 88/163 [02:10<01:55,  1.54s/it]
2026-02-26T21:41:19.0432309Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:19.0432954Z Loading safetensors checkpoint shards:  55% Completed | 89/163 [02:11<01:50,  1.50s/it]
2026-02-26T21:41:20.3877606Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:20.3878098Z Loading safetensors checkpoint shards:  55% Completed | 90/163 [02:13<01:45,  1.45s/it]
2026-02-26T21:41:21.7912236Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:21.7912721Z Loading safetensors checkpoint shards:  56% Completed | 91/163 [02:14<01:43,  1.44s/it]
2026-02-26T21:41:23.1707831Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:23.1708721Z Loading safetensors checkpoint shards:  56% Completed | 92/163 [02:15<01:40,  1.42s/it]
2026-02-26T21:41:24.6390200Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:24.6390704Z Loading safetensors checkpoint shards:  57% Completed | 93/163 [02:17<01:40,  1.43s/it]
2026-02-26T21:41:26.1097234Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:26.1097693Z Loading safetensors checkpoint shards:  58% Completed | 94/163 [02:18<01:39,  1.45s/it]
2026-02-26T21:41:27.4725056Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:27.4725660Z Loading safetensors checkpoint shards:  58% Completed | 95/163 [02:20<01:36,  1.42s/it]
2026-02-26T21:41:28.7667599Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:28.7668125Z Loading safetensors checkpoint shards:  59% Completed | 96/163 [02:21<01:32,  1.38s/it]
2026-02-26T21:41:30.0681368Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:30.0682222Z Loading safetensors checkpoint shards:  60% Completed | 97/163 [02:22<01:29,  1.36s/it]
2026-02-26T21:41:31.4117694Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:31.4118140Z Loading safetensors checkpoint shards:  60% Completed | 98/163 [02:24<01:28,  1.35s/it]
2026-02-26T21:41:32.7586283Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:32.7586705Z Loading safetensors checkpoint shards:  61% Completed | 99/163 [02:25<01:26,  1.35s/it]
2026-02-26T21:41:34.0380430Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:34.0380977Z Loading safetensors checkpoint shards:  61% Completed | 100/163 [02:26<01:23,  1.33s/it]
2026-02-26T21:41:35.6296724Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:35.6297217Z Loading safetensors checkpoint shards:  62% Completed | 101/163 [02:28<01:27,  1.41s/it]
2026-02-26T21:41:37.0911203Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:37.0911811Z Loading safetensors checkpoint shards:  63% Completed | 102/163 [02:29<01:26,  1.42s/it]
2026-02-26T21:41:38.3957037Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:38.3957668Z Loading safetensors checkpoint shards:  63% Completed | 103/163 [02:31<01:23,  1.39s/it]
2026-02-26T21:41:39.6761606Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:39.6762457Z Loading safetensors checkpoint shards:  64% Completed | 104/163 [02:32<01:20,  1.36s/it]
2026-02-26T21:41:41.0152097Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:41.0152687Z Loading safetensors checkpoint shards:  64% Completed | 105/163 [02:33<01:18,  1.35s/it]
2026-02-26T21:41:42.2968391Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:42.2968927Z Loading safetensors checkpoint shards:  65% Completed | 106/163 [02:34<01:15,  1.33s/it]
2026-02-26T21:41:45.3756540Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:45.3756987Z Loading safetensors checkpoint shards:  66% Completed | 107/163 [02:38<01:43,  1.85s/it]
2026-02-26T21:41:46.6584009Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:46.6584572Z Loading safetensors checkpoint shards:  66% Completed | 108/163 [02:39<01:32,  1.68s/it]
2026-02-26T21:41:47.8977043Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:47.8977522Z Loading safetensors checkpoint shards:  67% Completed | 109/163 [02:40<01:23,  1.55s/it]
2026-02-26T21:41:49.1255867Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:49.1256384Z Loading safetensors checkpoint shards:  67% Completed | 110/163 [02:41<01:17,  1.45s/it]
2026-02-26T21:41:50.3980598Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:50.3981106Z Loading safetensors checkpoint shards:  68% Completed | 111/163 [02:43<01:12,  1.40s/it]
2026-02-26T21:41:51.6694881Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:51.6695366Z Loading safetensors checkpoint shards:  69% Completed | 112/163 [02:44<01:09,  1.36s/it]
2026-02-26T21:41:53.1255508Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:53.1256074Z Loading safetensors checkpoint shards:  69% Completed | 113/163 [02:45<01:09,  1.39s/it]
2026-02-26T21:41:54.4845864Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:54.4846720Z Loading safetensors checkpoint shards:  70% Completed | 114/163 [02:47<01:07,  1.38s/it]
2026-02-26T21:41:55.7671952Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:55.7672512Z Loading safetensors checkpoint shards:  71% Completed | 115/163 [02:48<01:04,  1.35s/it]
2026-02-26T21:41:57.0690406Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:57.0690888Z Loading safetensors checkpoint shards:  71% Completed | 116/163 [02:49<01:02,  1.33s/it]
2026-02-26T21:41:58.3939203Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:58.3939658Z Loading safetensors checkpoint shards:  72% Completed | 117/163 [02:51<01:01,  1.33s/it]
2026-02-26T21:41:59.5688034Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:41:59.5688517Z Loading safetensors checkpoint shards:  72% Completed | 118/163 [02:52<00:57,  1.29s/it]
2026-02-26T21:42:00.8453108Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:00.8453644Z Loading safetensors checkpoint shards:  73% Completed | 119/163 [02:53<00:56,  1.28s/it]
2026-02-26T21:42:02.1442506Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:02.1443007Z Loading safetensors checkpoint shards:  74% Completed | 120/163 [02:54<00:55,  1.29s/it]
2026-02-26T21:42:03.5247770Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:03.5248220Z Loading safetensors checkpoint shards:  74% Completed | 121/163 [02:56<00:55,  1.32s/it]
2026-02-26T21:42:04.9273178Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:04.9273777Z Loading safetensors checkpoint shards:  75% Completed | 122/163 [02:57<00:55,  1.34s/it]
2026-02-26T21:42:06.1811779Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:06.1812314Z Loading safetensors checkpoint shards:  75% Completed | 123/163 [02:58<00:52,  1.32s/it]
2026-02-26T21:42:07.4804074Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:07.4805040Z Loading safetensors checkpoint shards:  76% Completed | 124/163 [03:00<00:51,  1.31s/it]
2026-02-26T21:42:08.8029647Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:08.8030249Z Loading safetensors checkpoint shards:  77% Completed | 125/163 [03:01<00:49,  1.31s/it]
2026-02-26T21:42:10.1304815Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:10.1305285Z Loading safetensors checkpoint shards:  77% Completed | 126/163 [03:02<00:48,  1.32s/it]
2026-02-26T21:42:11.4304341Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:11.4304829Z Loading safetensors checkpoint shards:  78% Completed | 127/163 [03:04<00:47,  1.31s/it]
2026-02-26T21:42:12.6693717Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:12.6694180Z Loading safetensors checkpoint shards:  79% Completed | 128/163 [03:05<00:45,  1.29s/it]
2026-02-26T21:42:14.0039278Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:14.0039771Z Loading safetensors checkpoint shards:  80% Completed | 130/163 [03:06<00:33,  1.00s/it]
2026-02-26T21:42:15.3900013Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:15.3900793Z Loading safetensors checkpoint shards:  80% Completed | 131/163 [03:08<00:35,  1.10s/it]
2026-02-26T21:42:16.6548358Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:16.6548965Z Loading safetensors checkpoint shards:  81% Completed | 132/163 [03:09<00:35,  1.14s/it]
2026-02-26T21:42:17.9087354Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:17.9087980Z Loading safetensors checkpoint shards:  82% Completed | 133/163 [03:10<00:35,  1.17s/it]
2026-02-26T21:42:19.2036863Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:19.2037409Z Loading safetensors checkpoint shards:  82% Completed | 134/163 [03:11<00:34,  1.21s/it]
2026-02-26T21:42:20.3929957Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:20.3930582Z Loading safetensors checkpoint shards:  83% Completed | 135/163 [03:13<00:33,  1.20s/it]
2026-02-26T21:42:21.6411041Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:21.6411922Z Loading safetensors checkpoint shards:  83% Completed | 136/163 [03:14<00:32,  1.22s/it]
2026-02-26T21:42:22.9139529Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:22.9139998Z Loading safetensors checkpoint shards:  84% Completed | 137/163 [03:15<00:32,  1.23s/it]
2026-02-26T21:42:24.1413267Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:24.1413739Z Loading safetensors checkpoint shards:  85% Completed | 138/163 [03:16<00:30,  1.23s/it]
2026-02-26T21:42:25.3259017Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:25.3259531Z Loading safetensors checkpoint shards:  85% Completed | 139/163 [03:17<00:29,  1.22s/it]
2026-02-26T21:42:26.6166484Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:26.6166960Z Loading safetensors checkpoint shards:  86% Completed | 140/163 [03:19<00:28,  1.24s/it]
2026-02-26T21:42:27.9249336Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:27.9249896Z Loading safetensors checkpoint shards:  87% Completed | 141/163 [03:20<00:27,  1.26s/it]
2026-02-26T21:42:29.1837288Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:29.1837968Z Loading safetensors checkpoint shards:  87% Completed | 142/163 [03:21<00:26,  1.26s/it]
2026-02-26T21:42:30.4622680Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:30.4623393Z Loading safetensors checkpoint shards:  88% Completed | 143/163 [03:23<00:25,  1.27s/it]
2026-02-26T21:42:31.7548816Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:31.7549398Z Loading safetensors checkpoint shards:  88% Completed | 144/163 [03:24<00:24,  1.27s/it]
2026-02-26T21:42:33.0471812Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:33.0472358Z Loading safetensors checkpoint shards:  89% Completed | 145/163 [03:25<00:23,  1.28s/it]
2026-02-26T21:42:34.2832938Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:34.2833405Z Loading safetensors checkpoint shards:  90% Completed | 146/163 [03:26<00:21,  1.27s/it]
2026-02-26T21:42:35.5470953Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:35.5471473Z Loading safetensors checkpoint shards:  90% Completed | 147/163 [03:28<00:20,  1.27s/it]
2026-02-26T21:42:36.8956884Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:36.8957367Z Loading safetensors checkpoint shards:  91% Completed | 148/163 [03:29<00:19,  1.29s/it]
2026-02-26T21:42:38.1661188Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:38.1661623Z Loading safetensors checkpoint shards:  91% Completed | 149/163 [03:30<00:17,  1.28s/it]
2026-02-26T21:42:42.0928454Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:42.0928939Z Loading safetensors checkpoint shards:  92% Completed | 150/163 [03:34<00:26,  2.08s/it]
2026-02-26T21:42:43.3201059Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:43.3201531Z Loading safetensors checkpoint shards:  93% Completed | 151/163 [03:35<00:21,  1.82s/it]
2026-02-26T21:42:44.6367203Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:44.6367757Z Loading safetensors checkpoint shards:  93% Completed | 152/163 [03:37<00:18,  1.67s/it]
2026-02-26T21:42:45.8990785Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:45.8991230Z Loading safetensors checkpoint shards:  94% Completed | 153/163 [03:38<00:15,  1.55s/it]
2026-02-26T21:42:47.1849655Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:47.1850144Z Loading safetensors checkpoint shards:  94% Completed | 154/163 [03:39<00:13,  1.47s/it]
2026-02-26T21:42:48.4364297Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:48.4364807Z Loading safetensors checkpoint shards:  95% Completed | 155/163 [03:41<00:11,  1.40s/it]
2026-02-26T21:42:49.6991522Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:49.6992074Z Loading safetensors checkpoint shards:  96% Completed | 156/163 [03:42<00:09,  1.36s/it]
2026-02-26T21:42:51.0160795Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:51.0161295Z Loading safetensors checkpoint shards:  96% Completed | 157/163 [03:43<00:08,  1.35s/it]
2026-02-26T21:42:52.1720957Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:52.1721512Z Loading safetensors checkpoint shards:  97% Completed | 158/163 [03:44<00:06,  1.29s/it]
2026-02-26T21:42:53.5304038Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:53.5304495Z Loading safetensors checkpoint shards:  98% Completed | 159/163 [03:46<00:05,  1.31s/it]
2026-02-26T21:42:54.8249823Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:54.8346908Z Loading safetensors checkpoint shards:  98% Completed | 160/163 [03:47<00:03,  1.31s/it]
2026-02-26T21:42:56.1635511Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:56.1635884Z Loading safetensors checkpoint shards:  99% Completed | 161/163 [03:48<00:02,  1.32s/it]
2026-02-26T21:42:57.4091612Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:57.4092103Z Loading safetensors checkpoint shards:  99% Completed | 162/163 [03:50<00:01,  1.29s/it]
2026-02-26T21:42:58.7558123Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:58.7558569Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [03:51<00:00,  1.31s/it]
2026-02-26T21:42:58.7558925Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:58.7559234Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [03:51<00:00,  1.42s/it]
2026-02-26T21:42:58.7559577Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:42:58.8425637Z [0;36m(Worker_DP1_TP4_EP12 pid=1394)[0;0m INFO 02-26 21:42:58 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T21:42:58.9795623Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m INFO 02-26 21:42:58 [default_loader.py:291] Loading weights took 178.03 seconds
2026-02-26T21:42:58.9961843Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m INFO 02-26 21:42:58 [default_loader.py:291] Loading weights took 231.65 seconds
2026-02-26T21:42:59.0386727Z [0;36m(Worker_DP0_TP2_EP2 pid=1186)[0;0m INFO 02-26 21:42:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T21:42:59.0395097Z [0;36m(Worker_DP1_TP6_EP14 pid=1602)[0;0m INFO 02-26 21:42:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T21:42:59.0395916Z [0;36m(Worker_DP0_TP7_EP7 pid=1703)[0;0m INFO 02-26 21:42:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T21:42:59.0405173Z [0;36m(Worker_DP1_TP1_EP9 pid=1079)[0;0m INFO 02-26 21:42:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T21:42:59.0406195Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m INFO 02-26 21:42:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T21:42:59.0408202Z [0;36m(Worker_DP1_TP5_EP13 pid=1498)[0;0m INFO 02-26 21:42:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T21:42:59.0417611Z [0;36m(Worker_DP1_TP3_EP11 pid=1290)[0;0m INFO 02-26 21:42:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T21:42:59.0428008Z [0;36m(Worker_DP0_TP4_EP4 pid=1391)[0;0m INFO 02-26 21:42:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T21:42:59.0533288Z [0;36m(Worker_DP0_TP3_EP3 pid=1287)[0;0m INFO 02-26 21:42:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T21:42:59.0545701Z [0;36m(Worker_DP0_TP1_EP1 pid=1077)[0;0m INFO 02-26 21:42:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T21:42:59.0555723Z [0;36m(Worker_DP0_TP6_EP6 pid=1599)[0;0m INFO 02-26 21:42:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T21:42:59.0565220Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m INFO 02-26 21:42:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T21:42:59.0565998Z [0;36m(Worker_DP1_TP7_EP15 pid=1710)[0;0m INFO 02-26 21:42:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T21:42:59.0576092Z [0;36m(Worker_DP0_TP5_EP5 pid=1495)[0;0m INFO 02-26 21:42:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T21:42:59.0679549Z [0;36m(Worker_DP1_TP2_EP10 pid=1183)[0;0m INFO 02-26 21:42:59 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-26T21:42:59.8182943Z [0;36m(Worker_DP0_TP7_EP7 pid=1703)[0;0m INFO 02-26 21:42:59 [model_runner_v1.py:2353] Loading model weights took 50.2984 GB
2026-02-26T21:42:59.8644855Z [0;36m(Worker_DP0_TP2_EP2 pid=1186)[0;0m INFO 02-26 21:42:59 [model_runner_v1.py:2353] Loading model weights took 50.2984 GB
2026-02-26T21:42:59.8861516Z [0;36m(Worker_DP1_TP2_EP10 pid=1183)[0;0m INFO 02-26 21:42:59 [model_runner_v1.py:2353] Loading model weights took 50.2984 GB
2026-02-26T21:42:59.8962456Z [0;36m(Worker_DP0_TP4_EP4 pid=1391)[0;0m INFO 02-26 21:42:59 [model_runner_v1.py:2353] Loading model weights took 50.2984 GB
2026-02-26T21:43:00.6007832Z [0;36m(Worker_DP1_TP4_EP12 pid=1394)[0;0m INFO 02-26 21:43:00 [model_runner_v1.py:2353] Loading model weights took 50.2984 GB
2026-02-26T21:43:00.7108309Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m INFO 02-26 21:43:00 [model_runner_v1.py:2353] Loading model weights took 50.2984 GB
2026-02-26T21:43:00.7609091Z [0;36m(Worker_DP1_TP6_EP14 pid=1602)[0;0m INFO 02-26 21:43:00 [model_runner_v1.py:2353] Loading model weights took 50.2984 GB
2026-02-26T21:43:00.8297704Z [0;36m(Worker_DP0_TP1_EP1 pid=1077)[0;0m INFO 02-26 21:43:00 [model_runner_v1.py:2353] Loading model weights took 50.2984 GB
2026-02-26T21:43:00.8365628Z [0;36m(Worker_DP0_TP6_EP6 pid=1599)[0;0m INFO 02-26 21:43:00 [model_runner_v1.py:2353] Loading model weights took 50.2984 GB
2026-02-26T21:43:00.8511492Z [0;36m(Worker_DP0_TP3_EP3 pid=1287)[0;0m INFO 02-26 21:43:00 [model_runner_v1.py:2353] Loading model weights took 50.2984 GB
2026-02-26T21:43:00.8816805Z [0;36m(Worker_DP0_TP5_EP5 pid=1495)[0;0m INFO 02-26 21:43:00 [model_runner_v1.py:2353] Loading model weights took 50.2984 GB
2026-02-26T21:43:00.9112417Z [0;36m(Worker_DP1_TP7_EP15 pid=1710)[0;0m INFO 02-26 21:43:00 [model_runner_v1.py:2353] Loading model weights took 50.2984 GB
2026-02-26T21:43:00.9336367Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m INFO 02-26 21:43:00 [model_runner_v1.py:2353] Loading model weights took 50.2984 GB
2026-02-26T21:43:00.9360619Z [0;36m(Worker_DP1_TP5_EP13 pid=1498)[0;0m INFO 02-26 21:43:00 [model_runner_v1.py:2353] Loading model weights took 50.2984 GB
2026-02-26T21:43:00.9537447Z [0;36m(Worker_DP1_TP1_EP9 pid=1079)[0;0m INFO 02-26 21:43:00 [model_runner_v1.py:2353] Loading model weights took 50.2984 GB
2026-02-26T21:43:01.2273567Z [0;36m(Worker_DP1_TP3_EP11 pid=1290)[0;0m INFO 02-26 21:43:01 [model_runner_v1.py:2353] Loading model weights took 50.2984 GB
2026-02-26T21:43:06.4186576Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m INFO 02-26 21:43:06 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/0ec9909fdc/rank_0_0/backbone for vLLM's torch.compile
2026-02-26T21:43:06.4339630Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m INFO 02-26 21:43:06 [backends.py:865] Dynamo bytecode transform time: 4.56 s
2026-02-26T21:43:06.8264078Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m INFO 02-26 21:43:06 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/0ec9909fdc/rank_0_1/backbone for vLLM's torch.compile
2026-02-26T21:43:06.8381662Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m INFO 02-26 21:43:06 [backends.py:865] Dynamo bytecode transform time: 4.97 s
2026-02-26T21:43:13.2989377Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T21:43:13.2990535Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m   warnings.warn(
2026-02-26T21:43:13.3144538Z [0;36m(Worker_DP0_TP6_EP6 pid=1599)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T21:43:13.3145646Z [0;36m(Worker_DP0_TP6_EP6 pid=1599)[0;0m   warnings.warn(
2026-02-26T21:43:13.3507795Z [0;36m(Worker_DP0_TP7_EP7 pid=1703)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T21:43:13.3508894Z [0;36m(Worker_DP0_TP7_EP7 pid=1703)[0;0m   warnings.warn(
2026-02-26T21:43:13.3820338Z [0;36m(Worker_DP0_TP2_EP2 pid=1186)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T21:43:13.3821385Z [0;36m(Worker_DP0_TP2_EP2 pid=1186)[0;0m   warnings.warn(
2026-02-26T21:43:13.3857490Z [0;36m(Worker_DP1_TP2_EP10 pid=1183)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T21:43:13.3858845Z [0;36m(Worker_DP1_TP2_EP10 pid=1183)[0;0m   warnings.warn(
2026-02-26T21:43:13.4059965Z [0;36m(Worker_DP0_TP5_EP5 pid=1495)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T21:43:13.4060996Z [0;36m(Worker_DP0_TP5_EP5 pid=1495)[0;0m   warnings.warn(
2026-02-26T21:43:13.4115339Z [0;36m(Worker_DP1_TP5_EP13 pid=1498)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T21:43:13.4116396Z [0;36m(Worker_DP1_TP5_EP13 pid=1498)[0;0m   warnings.warn(
2026-02-26T21:43:13.4132473Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T21:43:13.4133534Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m   warnings.warn(
2026-02-26T21:43:13.4149795Z [0;36m(Worker_DP1_TP6_EP14 pid=1602)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T21:43:13.4150816Z [0;36m(Worker_DP1_TP6_EP14 pid=1602)[0;0m   warnings.warn(
2026-02-26T21:43:13.4211672Z [0;36m(Worker_DP1_TP1_EP9 pid=1079)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T21:43:13.4212794Z [0;36m(Worker_DP1_TP1_EP9 pid=1079)[0;0m   warnings.warn(
2026-02-26T21:43:13.4296531Z [0;36m(Worker_DP1_TP4_EP12 pid=1394)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T21:43:13.4297590Z [0;36m(Worker_DP1_TP4_EP12 pid=1394)[0;0m   warnings.warn(
2026-02-26T21:43:13.4410307Z [0;36m(Worker_DP1_TP7_EP15 pid=1710)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T21:43:13.4411349Z [0;36m(Worker_DP1_TP7_EP15 pid=1710)[0;0m   warnings.warn(
2026-02-26T21:43:13.4644241Z [0;36m(Worker_DP0_TP4_EP4 pid=1391)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T21:43:13.4645578Z [0;36m(Worker_DP0_TP4_EP4 pid=1391)[0;0m   warnings.warn(
2026-02-26T21:43:13.5000429Z [0;36m(Worker_DP0_TP3_EP3 pid=1287)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T21:43:13.5001685Z [0;36m(Worker_DP0_TP3_EP3 pid=1287)[0;0m   warnings.warn(
2026-02-26T21:43:13.5405178Z [0;36m(Worker_DP1_TP3_EP11 pid=1290)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T21:43:13.5406282Z [0;36m(Worker_DP1_TP3_EP11 pid=1290)[0;0m   warnings.warn(
2026-02-26T21:43:13.5765271Z [0;36m(Worker_DP0_TP1_EP1 pid=1077)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-26T21:43:13.5766381Z [0;36m(Worker_DP0_TP1_EP1 pid=1077)[0;0m   warnings.warn(
2026-02-26T21:43:26.0333086Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m INFO 02-26 21:43:26 [backends.py:319] Compiling a graph for compile range (1, 8192) takes 12.74 s
2026-02-26T21:43:26.0333738Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m INFO 02-26 21:43:26 [monitor.py:34] torch.compile takes 17.30 s in total
2026-02-26T21:43:27.2791969Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m INFO 02-26 21:43:27 [backends.py:319] Compiling a graph for compile range (1, 8192) takes 13.87 s
2026-02-26T21:43:27.2792763Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m INFO 02-26 21:43:27 [monitor.py:34] torch.compile takes 18.84 s in total
2026-02-26T21:43:32.1515352Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m INFO 02-26 21:43:32 [worker.py:359] Available KV cache memory: 5.17 GiB
2026-02-26T21:43:32.2946894Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m INFO 02-26 21:43:32 [worker.py:359] Available KV cache memory: 5.15 GiB
2026-02-26T21:43:32.3156021Z [0;36m(EngineCore_DP0 pid=998)[0;0m INFO 02-26 21:43:32 [kv_cache_utils.py:1307] GPU KV cache size: 63,232 tokens
2026-02-26T21:43:32.3156719Z [0;36m(EngineCore_DP0 pid=998)[0;0m INFO 02-26 21:43:32 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 7.72x
2026-02-26T21:43:32.3285435Z [0;36m(EngineCore_DP1 pid=1017)[0;0m INFO 02-26 21:43:32 [kv_cache_utils.py:1307] GPU KV cache size: 63,488 tokens
2026-02-26T21:43:32.3286060Z [0;36m(EngineCore_DP1 pid=1017)[0;0m INFO 02-26 21:43:32 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 7.75x
2026-02-26T21:43:32.8863004Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m 
2026-02-26T21:43:47.4599001Z Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s][rank10]:[W226 21:43:47.385824245 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T21:43:47.4599971Z [rank8]:[W226 21:43:47.385826155 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T21:43:47.4600686Z [rank14]:[W226 21:43:47.385829825 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T21:43:47.4601381Z [rank11]:[W226 21:43:47.385836505 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T21:43:47.4602646Z [rank13]:[W226 21:43:47.385842105 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T21:43:47.4603393Z [rank2]:[W226 21:43:47.385849195 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T21:43:47.4604078Z [rank1]:[W226 21:43:47.385852445 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T21:43:47.4604752Z [rank9]:[W226 21:43:47.385852395 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T21:43:47.4605428Z [rank0]:[W226 21:43:47.385861155 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T21:43:47.4606119Z [rank15]:[W226 21:43:47.385864626 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T21:43:47.4606803Z [rank12]:[W226 21:43:47.385861785 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T21:43:47.4607497Z [rank4]:[W226 21:43:47.385857975 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T21:43:47.4608176Z [rank6]:[W226 21:43:47.385868106 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T21:43:47.4608899Z [rank3]:[W226 21:43:47.385872046 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T21:43:47.4609569Z [rank7]:[W226 21:43:47.385866306 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T21:43:47.4732564Z [rank5]:[W226 21:43:47.399595201 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-26T21:43:50.6717221Z [rank1]:[W226 21:43:50.597784279 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T21:43:50.6718048Z [rank4]:[W226 21:43:50.597945540 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T21:43:50.6720907Z [rank3]:[W226 21:43:50.598640896 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T21:43:50.6721632Z [rank5]:[W226 21:43:50.598692516 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T21:43:50.6722506Z [rank7]:[W226 21:43:50.598735057 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T21:43:50.6729039Z [rank2]:[W226 21:43:50.599383341 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T21:43:50.6731102Z [rank6]:[W226 21:43:50.599699444 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T21:43:50.6733958Z [rank0]:[W226 21:43:50.599936906 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T21:43:50.7425138Z [rank11]:[W226 21:43:50.668721304 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T21:43:50.7425882Z [rank9]:[W226 21:43:50.668913356 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T21:43:50.7428808Z [rank14]:[W226 21:43:50.669428090 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T21:43:50.7431473Z [rank13]:[W226 21:43:50.669714652 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T21:43:50.7432276Z [rank12]:[W226 21:43:50.669720922 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T21:43:50.7435234Z [rank8]:[W226 21:43:50.670098965 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T21:43:50.7437468Z [rank15]:[W226 21:43:50.670361707 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T21:43:50.7438190Z [rank10]:[W226 21:43:50.670452827 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-26T21:43:50.7540592Z 
2026-02-26T21:43:56.7105555Z Capturing CUDA graphs (decode, FULL):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:17<00:17, 17.87s/it]
2026-02-26T21:43:56.7106110Z Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:23<00:00, 10.86s/it]
2026-02-26T21:43:56.7106508Z Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:23<00:00, 11.91s/it]
2026-02-26T21:43:57.1418418Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m INFO 02-26 21:43:57 [gpu_model_runner.py:5051] Graph capturing finished in 25 secs, took 0.24 GiB
2026-02-26T21:43:57.1598108Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m INFO 02-26 21:43:57 [gpu_model_runner.py:5051] Graph capturing finished in 25 secs, took 0.25 GiB
2026-02-26T21:43:57.1851676Z [0;36m(EngineCore_DP0 pid=998)[0;0m INFO 02-26 21:43:57 [core.py:272] init engine (profile, create kv cache, warmup model) took 56.25 seconds
2026-02-26T21:43:57.2712406Z [0;36m(EngineCore_DP1 pid=1017)[0;0m INFO 02-26 21:43:57 [core.py:272] init engine (profile, create kv cache, warmup model) took 56.04 seconds
2026-02-26T21:43:58.1372119Z INFO 02-26 21:43:58 [coordinator.py:200] All engine subscriptions received by DP coordinator
2026-02-26T21:43:58.1372957Z [0;36m(EngineCore_DP0 pid=998)[0;0m INFO 02-26 21:43:58 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-26T21:43:58.1374978Z [0;36m(EngineCore_DP1 pid=1017)[0;0m INFO 02-26 21:43:58 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-26T21:43:58.1375383Z INFO 02-26 21:43:58 [utils.py:249] Waiting for API servers to complete ...
2026-02-26T21:43:58.1382980Z [0;36m(EngineCore_DP1 pid=1017)[0;0m INFO 02-26 21:43:58 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:43:58.1383444Z [0;36m(EngineCore_DP0 pid=998)[0;0m INFO 02-26 21:43:58 [ascend_config.py:412] Dynamic EPLB is False
2026-02-26T21:43:58.1383994Z [0;36m(EngineCore_DP0 pid=998)[0;0m INFO 02-26 21:43:58 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:43:58.1384532Z [0;36m(EngineCore_DP1 pid=1017)[0;0m INFO 02-26 21:43:58 [ascend_config.py:413] The number of redundant experts is 0
2026-02-26T21:43:58.1385572Z [0;36m(EngineCore_DP1 pid=1017)[0;0m INFO 02-26 21:43:58 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:43:58.1387178Z [0;36m(EngineCore_DP0 pid=998)[0;0m INFO 02-26 21:43:58 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-26T21:43:58.1388312Z [0;36m(EngineCore_DP0 pid=998)[0;0m INFO 02-26 21:43:58 [platform.py:323] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-26T21:43:58.1389539Z [0;36m(EngineCore_DP1 pid=1017)[0;0m INFO 02-26 21:43:58 [platform.py:323] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-26T21:43:58.1390151Z [0;36m(EngineCore_DP0 pid=998)[0;0m WARNING 02-26 21:43:58 [platform.py:340] [91m
2026-02-26T21:43:58.1390695Z [0;36m(EngineCore_DP0 pid=998)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             **********************************************************************************
2026-02-26T21:43:58.1391333Z [0;36m(EngineCore_DP0 pid=998)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             * WARNING: You have enabled the *full graph* feature.
2026-02-26T21:43:58.1392126Z [0;36m(EngineCore_DP0 pid=998)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             * This is an early experimental stage and may involve various unknown issues.
2026-02-26T21:43:58.1392891Z [0;36m(EngineCore_DP0 pid=998)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-26T21:43:58.1393609Z [0;36m(EngineCore_DP0 pid=998)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-26T21:43:58.1394352Z [0;36m(EngineCore_DP0 pid=998)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-26T21:43:58.1395001Z [0;36m(EngineCore_DP0 pid=998)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             * batch size for graph capture.
2026-02-26T21:43:58.1395559Z [0;36m(EngineCore_DP0 pid=998)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             * For more details, please refer to:
2026-02-26T21:43:58.1396362Z [0;36m(EngineCore_DP0 pid=998)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-26T21:43:58.1397244Z [0;36m(EngineCore_DP0 pid=998)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             **********************************************************************************[0m
2026-02-26T21:43:58.1397773Z [0;36m(EngineCore_DP0 pid=998)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             
2026-02-26T21:43:58.1398192Z [0;36m(EngineCore_DP1 pid=1017)[0;0m WARNING 02-26 21:43:58 [platform.py:340] [91m
2026-02-26T21:43:58.1398715Z [0;36m(EngineCore_DP1 pid=1017)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             **********************************************************************************
2026-02-26T21:43:58.1399335Z [0;36m(EngineCore_DP1 pid=1017)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             * WARNING: You have enabled the *full graph* feature.
2026-02-26T21:43:58.1400021Z [0;36m(EngineCore_DP1 pid=1017)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             * This is an early experimental stage and may involve various unknown issues.
2026-02-26T21:43:58.1400752Z [0;36m(EngineCore_DP1 pid=1017)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-26T21:43:58.1401467Z [0;36m(EngineCore_DP1 pid=1017)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-26T21:43:58.1402289Z [0;36m(EngineCore_DP1 pid=1017)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-26T21:43:58.1402957Z [0;36m(EngineCore_DP1 pid=1017)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             * batch size for graph capture.
2026-02-26T21:43:58.1403517Z [0;36m(EngineCore_DP1 pid=1017)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             * For more details, please refer to:
2026-02-26T21:43:58.1404323Z [0;36m(EngineCore_DP1 pid=1017)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-26T21:43:58.1405084Z [0;36m(EngineCore_DP1 pid=1017)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             **********************************************************************************[0m
2026-02-26T21:43:58.1405612Z [0;36m(EngineCore_DP1 pid=1017)[0;0m WARNING 02-26 21:43:58 [platform.py:340]             
2026-02-26T21:43:58.1406159Z ERROR 02-26 21:43:58 [utils.py:290] Exception occurred while running API servers: Process ApiServer_0 (PID: 1028) died with exit code 1
2026-02-26T21:43:58.1406628Z ERROR 02-26 21:43:58 [utils.py:290] Traceback (most recent call last):
2026-02-26T21:43:58.1407051Z ERROR 02-26 21:43:58 [utils.py:290]   File "/vllm-workspace/vllm/vllm/v1/utils.py", line 277, in wait_for_completion_or_failure
2026-02-26T21:43:58.1407453Z ERROR 02-26 21:43:58 [utils.py:290]     raise RuntimeError(
2026-02-26T21:43:58.1407812Z ERROR 02-26 21:43:58 [utils.py:290] RuntimeError: Process ApiServer_0 (PID: 1028) died with exit code 1
2026-02-26T21:43:58.1408178Z INFO 02-26 21:43:58 [utils.py:293] Terminating remaining processes ...
2026-02-26T21:43:58.1408660Z [0;36m(EngineCore_DP1 pid=1017)[0;0m INFO 02-26 21:43:58 [platform.py:448] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-26T21:43:58.1409240Z [0;36m(EngineCore_DP0 pid=998)[0;0m INFO 02-26 21:43:58 [platform.py:448] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-26T21:43:58.4191795Z [0;36m(Worker_DP0_TP2_EP2 pid=1186)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T21:43:58.4192624Z [0;36m(Worker_DP1_TP2_EP10 pid=1183)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T21:43:58.4193275Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T21:43:58.4194338Z [0;36m(Worker_DP0_TP3_EP3 pid=1287)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T21:43:58.4194976Z [0;36m(Worker_DP0_TP1_EP1 pid=1077)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T21:43:58.4195594Z [0;36m(Worker_DP0_TP4_EP4 pid=1391)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T21:43:58.4196234Z [0;36m(Worker_DP0_TP5_EP5 pid=1495)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T21:43:58.4196846Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T21:43:58.4197434Z [0;36m(Worker_DP0_TP6_EP6 pid=1599)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T21:43:58.4198000Z [0;36m(Worker_DP1_TP2_EP10 pid=1183)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T21:43:58.4198567Z [0;36m(Worker_DP1_TP7_EP15 pid=1710)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T21:43:58.4199173Z [0;36m(Worker_DP0_TP0_EP0 pid=1044)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T21:43:58.4199680Z [0;36m(Worker_DP0_TP2_EP2 pid=1186)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T21:43:58.4200189Z [0;36m(Worker_DP0_TP4_EP4 pid=1391)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T21:43:58.4200698Z [0;36m(Worker_DP0_TP1_EP1 pid=1077)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T21:43:58.4201202Z [0;36m(Worker_DP0_TP3_EP3 pid=1287)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T21:43:58.4201872Z [0;36m(Worker_DP1_TP1_EP9 pid=1079)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T21:43:58.4202598Z [0;36m(Worker_DP1_TP3_EP11 pid=1290)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T21:43:58.4203150Z [0;36m(Worker_DP0_TP5_EP5 pid=1495)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T21:43:58.4203711Z [0;36m(Worker_DP1_TP4_EP12 pid=1394)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T21:43:58.4204253Z [0;36m(Worker_DP1_TP0_EP8 pid=1043)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T21:43:58.4204813Z [0;36m(Worker_DP1_TP5_EP13 pid=1498)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T21:43:58.4205362Z [0;36m(Worker_DP0_TP6_EP6 pid=1599)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T21:43:58.4205913Z [0;36m(Worker_DP0_TP7_EP7 pid=1703)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T21:43:58.4206585Z [0;36m(Worker_DP1_TP6_EP14 pid=1602)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-26T21:43:58.4207136Z [0;36m(Worker_DP1_TP3_EP11 pid=1290)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T21:43:58.4207657Z [0;36m(Worker_DP1_TP7_EP15 pid=1710)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T21:43:58.4208169Z [0;36m(Worker_DP1_TP1_EP9 pid=1079)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T21:43:58.4208687Z [0;36m(Worker_DP1_TP4_EP12 pid=1394)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T21:43:58.4209198Z [0;36m(Worker_DP1_TP5_EP13 pid=1498)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T21:43:58.4209835Z [0;36m(Worker_DP1_TP6_EP14 pid=1602)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T21:43:58.4210332Z [0;36m(Worker_DP0_TP7_EP7 pid=1703)[0;0m INFO 02-26 21:43:58 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-26T21:44:03.4417669Z Traceback (most recent call last):
2026-02-26T21:44:03.4417995Z   File "/usr/local/python3.11.14/bin/vllm", line 7, in <module>
2026-02-26T21:44:03.4453557Z     sys.exit(main())
2026-02-26T21:44:03.4453761Z              ^^^^^^
2026-02-26T21:44:03.4454015Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/main.py", line 73, in main
2026-02-26T21:44:03.4486652Z     args.dispatch_function(args)
2026-02-26T21:44:03.4486931Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 108, in cmd
2026-02-26T21:44:03.4493184Z     run_multi_api_server(args)
2026-02-26T21:44:03.4494172Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 282, in run_multi_api_server
2026-02-26T21:44:03.4494547Z     wait_for_completion_or_failure(
2026-02-26T21:44:03.4494911Z   File "/vllm-workspace/vllm/vllm/v1/utils.py", line 277, in wait_for_completion_or_failure
2026-02-26T21:44:03.4495228Z     raise RuntimeError(
2026-02-26T21:44:03.4495990Z RuntimeError: Process ApiServer_0 (PID: 1028) died with exit code 1
2026-02-26T21:44:03.5523274Z [ERROR] 2026-02-26-21:44:03 (PID:982, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
2026-02-26T21:44:03.8541681Z sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-26T21:44:05.6125616Z /usr/local/python3.11.14/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked shared_memory objects to clean up at shutdown
2026-02-26T21:44:05.6126251Z   warnings.warn('resource_tracker: There appear to be %d '
2026-02-26T21:44:10.4625546Z FAILED
2026-02-26T21:44:10.4625791Z 
2026-02-26T21:44:10.4626313Z =================================== FAILURES ===================================
2026-02-26T21:44:10.4626676Z _______________ test_models[2-8-vllm-ascend/DeepSeek-V3.2-W8A8] ________________
2026-02-26T21:44:10.4626890Z 
2026-02-26T21:44:10.4627030Z model = 'vllm-ascend/DeepSeek-V3.2-W8A8', tp_size = 8, dp_size = 2
2026-02-26T21:44:10.4627222Z 
2026-02-26T21:44:10.4627301Z     @pytest.mark.asyncio
2026-02-26T21:44:10.4627507Z     @pytest.mark.parametrize("model", MODELS)
2026-02-26T21:44:10.4627759Z     @pytest.mark.parametrize("tp_size", TENSOR_PARALLELS)
2026-02-26T21:44:10.4628035Z     @pytest.mark.parametrize("dp_size", DATA_PARALLELS)
2026-02-26T21:44:10.4628388Z     async def test_models(model: str, tp_size: int, dp_size: int) -> None:
2026-02-26T21:44:10.4628656Z         port = get_open_port()
2026-02-26T21:44:10.4628832Z         env_dict = {
2026-02-26T21:44:10.4628999Z             "HCCL_OP_EXPANSION_MODE": "AIV",
2026-02-26T21:44:10.4629204Z             "OMP_PROC_BIND": "false",
2026-02-26T21:44:10.4629499Z             "OMP_NUM_THREADS": "1",
2026-02-26T21:44:10.4629690Z             "HCCL_BUFFSIZE": "1024",
2026-02-26T21:44:10.4629878Z             "VLLM_ASCEND_ENABLE_MLAPO": "1",
2026-02-26T21:44:10.4630115Z             "PYTORCH_NPU_ALLOC_CONF": "expandable_segments:True",
2026-02-26T21:44:10.4630368Z             "VLLM_ASCEND_ENABLE_FLASHCOMM1": "1",
2026-02-26T21:44:10.4630556Z         }
2026-02-26T21:44:10.4630685Z     
2026-02-26T21:44:10.4630805Z         server_args = [
2026-02-26T21:44:10.4631019Z             "--enable-expert-parallel", "--tensor-parallel-size",
2026-02-26T21:44:10.4631272Z             str(tp_size), "--data-parallel-size",
2026-02-26T21:44:10.4631478Z             str(dp_size), "--port",
2026-02-26T21:44:10.4631726Z             str(port), "--max-model-len", "8192", "--max-num-batched-tokens",
2026-02-26T21:44:10.4632159Z             "8192", "--max-num-seqs", "4", "--trust-remote-code", "--quantization",
2026-02-26T21:44:10.4632517Z             "ascend", "--gpu-memory-utilization", "0.98", "--compilation-config",
2026-02-26T21:44:10.4633035Z             '{"cudagraph_capture_sizes":[8, 16, 24, 32, 40, 48], "cudagraph_mode":"FULL_DECODE_ONLY"}',
2026-02-26T21:44:10.4633346Z             "--speculative-config",
2026-02-26T21:44:10.4633585Z             '{"num_speculative_tokens": 3, "method":"deepseek_mtp"}',
2026-02-26T21:44:10.4633824Z             "--additional-config",
2026-02-26T21:44:10.4634036Z             '{"layer_sharding": ["q_b_proj", "o_proj"]}',
2026-02-26T21:44:10.4634323Z             "--reasoning-parser", "deepseek_v3", "--tokenizer_mode", "deepseek_v32"
2026-02-26T21:44:10.4634583Z         ]
2026-02-26T21:44:10.4634737Z         request_keyword_args: dict[str, Any] = {
2026-02-26T21:44:10.4634936Z             **api_keyword_args,
2026-02-26T21:44:10.4635197Z         }
2026-02-26T21:44:10.4635356Z >       with RemoteOpenAIServer(model,
2026-02-26T21:44:10.4635558Z                                 server_args,
2026-02-26T21:44:10.4635749Z                                 server_port=port,
2026-02-26T21:44:10.4635959Z                                 env_dict=env_dict,
2026-02-26T21:44:10.4636172Z                                 auto_port=False) as server:
2026-02-26T21:44:10.4636327Z 
2026-02-26T21:44:10.4636465Z tests/e2e/nightly/single_node/models/test_deepseek_v3_2_w8a8.py:107: 
2026-02-26T21:44:10.4636766Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-26T21:44:10.4637000Z tests/e2e/conftest.py:306: in __init__
2026-02-26T21:44:10.4637201Z     self._wait_for_multiple_servers(
2026-02-26T21:44:10.4637414Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-26T21:44:10.4637581Z 
2026-02-26T21:44:10.4637728Z self = <tests.e2e.conftest.RemoteOpenAIServer object at 0xffff06e72910>
2026-02-26T21:44:10.4638087Z targets = [('0.0.0.0', 'http://0.0.0.0:39027/health')], timeout = 2800
2026-02-26T21:44:10.4638323Z log_interval = 30.0
2026-02-26T21:44:10.4638412Z 
2026-02-26T21:44:10.4638491Z     def _wait_for_multiple_servers(self,
2026-02-26T21:44:10.4638750Z                                    targets,
2026-02-26T21:44:10.4638949Z                                    timeout: float,
2026-02-26T21:44:10.4639162Z                                    log_interval: float = 30.0):
2026-02-26T21:44:10.4639421Z         """
2026-02-26T21:44:10.4639572Z         targets: List[(node_ip, url)]
2026-02-26T21:44:10.4639758Z         log_interval
2026-02-26T21:44:10.4639899Z         """
2026-02-26T21:44:10.4640035Z         start = time.time()
2026-02-26T21:44:10.4640201Z         client = requests
2026-02-26T21:44:10.4640351Z     
2026-02-26T21:44:10.4640521Z         ready = {node_ip: False for node_ip, _ in targets}
2026-02-26T21:44:10.4640718Z     
2026-02-26T21:44:10.4640847Z         last_log_time = 0.0
2026-02-26T21:44:10.4640992Z     
2026-02-26T21:44:10.4641115Z         while True:
2026-02-26T21:44:10.4641257Z             now = time.time()
2026-02-26T21:44:10.4641428Z             all_ready = True
2026-02-26T21:44:10.4641631Z             should_log = (now - last_log_time) >= log_interval
2026-02-26T21:44:10.4641843Z     
2026-02-26T21:44:10.4641980Z             for node_ip, url in targets:
2026-02-26T21:44:10.4642255Z                 if ready[node_ip]:
2026-02-26T21:44:10.4642430Z                     continue
2026-02-26T21:44:10.4642577Z     
2026-02-26T21:44:10.4642699Z                 try:
2026-02-26T21:44:10.4642852Z                     resp = client.get(url)
2026-02-26T21:44:10.4643056Z                     if resp.status_code == 200:
2026-02-26T21:44:10.4643257Z                         ready[node_ip] = True
2026-02-26T21:44:10.4643488Z                         logger.info(f"[READY] Node {node_ip} is ready.")
2026-02-26T21:44:10.4643727Z                 except RequestException:
2026-02-26T21:44:10.4643922Z                     all_ready = False
2026-02-26T21:44:10.4644101Z                     if should_log:
2026-02-26T21:44:10.4644312Z                         logger.debug(f"[WAIT] {url}: connection failed")
2026-02-26T21:44:10.4644533Z     
2026-02-26T21:44:10.4644664Z                     # check unexpected exit
2026-02-26T21:44:10.4644956Z                     result = self._poll()
2026-02-26T21:44:10.4645161Z                     if result is not None and result != 0:
2026-02-26T21:44:10.4645374Z >                       raise RuntimeError(
2026-02-26T21:44:10.4645591Z                             f"Server at {node_ip} exited unexpectedly."
2026-02-26T21:44:10.4645814Z                         ) from None
2026-02-26T21:44:10.4646041Z E                       RuntimeError: Server at 0.0.0.0 exited unexpectedly.
2026-02-26T21:44:10.4646221Z 
2026-02-26T21:44:10.4646298Z tests/e2e/conftest.py:399: RuntimeError
2026-02-26T21:44:10.4646571Z ------------------------------ Captured log call -------------------------------
2026-02-26T21:44:10.4648322Z INFO     tests.e2e.conftest:conftest.py:241 Starting server with command: vllm serve vllm-ascend/DeepSeek-V3.2-W8A8 --enable-expert-parallel --tensor-parallel-size 8 --data-parallel-size 2 --port 39027 --max-model-len 8192 --max-num-batched-tokens 8192 --max-num-seqs 4 --trust-remote-code --quantization ascend --gpu-memory-utilization 0.98 --compilation-config {"cudagraph_capture_sizes":[8, 16, 24, 32, 40, 48], "cudagraph_mode":"FULL_DECODE_ONLY"} --speculative-config {"num_speculative_tokens": 3, "method":"deepseek_mtp"} --additional-config {"layer_sharding": ["q_b_proj", "o_proj"]} --reasoning-parser deepseek_v3 --tokenizer_mode deepseek_v32
2026-02-26T21:44:10.4650111Z =============================== warnings summary ===============================
2026-02-26T21:44:10.4650352Z <frozen importlib._bootstrap>:241
2026-02-26T21:44:10.4650718Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute
2026-02-26T21:44:10.4651025Z 
2026-02-26T21:44:10.4651093Z <frozen importlib._bootstrap>:241
2026-02-26T21:44:10.4651438Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute
2026-02-26T21:44:10.4651817Z 
2026-02-26T21:44:10.4652161Z ../../usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647
2026-02-26T21:44:10.4653109Z   /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-26T21:44:10.4653858Z     warnings.warn(
2026-02-26T21:44:10.4653949Z 
2026-02-26T21:44:10.4654150Z ../../usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8
2026-02-26T21:44:10.4654931Z   /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
2026-02-26T21:44:10.4655542Z     import pkg_resources
2026-02-26T21:44:10.4655644Z 
2026-02-26T21:44:10.4655795Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2026-02-26T21:44:10.4656112Z =========================== short test summary info ============================
2026-02-26T21:44:10.4656734Z FAILED tests/e2e/nightly/single_node/models/test_deepseek_v3_2_w8a8.py::test_models[2-8-vllm-ascend/DeepSeek-V3.2-W8A8] - RuntimeError: Server at 0.0.0.0 exited unexpectedly.
2026-02-26T21:44:10.4657270Z ================== 1 failed, 4 warnings in 1571.69s (0:26:11) ==================
2026-02-26T21:44:12.2479748Z ##[error]Error: failed to run script step: Error: command terminated with non-zero exit code: command terminated with exit code 1
2026-02-26T21:44:12.2517830Z ##[error]Process completed with exit code 1.
2026-02-26T21:44:12.2614530Z ##[error]Executing the custom container implementation failed. Please contact your self hosted runner administrator.
2026-02-26T21:44:12.2656747Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-26T21:44:12.2657417Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-26T21:44:12.2657690Z ##[endgroup]
2026-02-26T21:44:12.6975447Z Cleaning up orphan processes
