# Run ID: 21719776452
# Commit: 81f3c09d6d46917052fc62759a93bd5a797968ef
# Job: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
# Date: 2026-02-05
============================================================

ï»¿2026-02-05T20:04:50.4915104Z Current runner version: '2.330.0'
2026-02-05T20:04:50.4919849Z Runner name: 'linux-aarch64-a3-0-wgt9d-runner-x975h'
2026-02-05T20:04:50.4920549Z Runner group name: 'Default'
2026-02-05T20:04:50.4921319Z Machine name: 'linux-aarch64-a3-0-wgt9d-runner-x975h'
2026-02-05T20:04:50.4924998Z ##[group]GITHUB_TOKEN Permissions
2026-02-05T20:04:50.4926997Z Actions: write
2026-02-05T20:04:50.4927473Z ArtifactMetadata: write
2026-02-05T20:04:50.4927878Z Attestations: write
2026-02-05T20:04:50.4928279Z Checks: write
2026-02-05T20:04:50.4928666Z Contents: write
2026-02-05T20:04:50.4929150Z Deployments: write
2026-02-05T20:04:50.4929538Z Discussions: write
2026-02-05T20:04:50.4929920Z Issues: write
2026-02-05T20:04:50.4930307Z Metadata: read
2026-02-05T20:04:50.4930677Z Models: read
2026-02-05T20:04:50.4931049Z Packages: write
2026-02-05T20:04:50.4931496Z Pages: write
2026-02-05T20:04:50.4931852Z PullRequests: write
2026-02-05T20:04:50.4932450Z RepositoryProjects: write
2026-02-05T20:04:50.4933014Z SecurityEvents: write
2026-02-05T20:04:50.4933519Z Statuses: write
2026-02-05T20:04:50.4933985Z ##[endgroup]
2026-02-05T20:04:50.4935836Z Secret source: Actions
2026-02-05T20:04:50.4936419Z Prepare workflow directory
2026-02-05T20:04:50.5498502Z Prepare all required actions
2026-02-05T20:04:50.5530320Z Getting action download info
2026-02-05T20:04:51.6166626Z Download action repository 'actions/checkout@v6' (SHA:de0fac2e4500dabe0009e67214ff5f5447ce83dd)
2026-02-05T20:04:57.4810276Z Download action repository 'actions/upload-artifact@v6' (SHA:b7c566a772e6b6bfb58ed0dc250532a479d7789f)
2026-02-05T20:05:06.5743068Z Uses: vllm-project/vllm-ascend/.github/workflows/_e2e_nightly_multi_node.yaml@refs/heads/main (81f3c09d6d46917052fc62759a93bd5a797968ef)
2026-02-05T20:05:06.5746241Z ##[group] Inputs
2026-02-05T20:05:06.5746515Z   soc_version: a3
2026-02-05T20:05:06.5746809Z   runner: linux-aarch64-a3-0
2026-02-05T20:05:06.5747327Z   image: swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3
2026-02-05T20:05:06.5747773Z   config_file_path: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-05T20:05:06.5748100Z   replicas: 1
2026-02-05T20:05:06.5748317Z   size: 2
2026-02-05T20:05:06.5748515Z   vllm_version: v0.15.0
2026-02-05T20:05:06.5748875Z   vllm_ascend_remote_url: https://github.com/vllm-project/vllm-ascend.git
2026-02-05T20:05:06.5749409Z   vllm_ascend_ref: main
2026-02-05T20:05:06.5749661Z ##[endgroup]
2026-02-05T20:05:06.5750174Z Complete job name: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-05T20:05:06.6277033Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-05T20:05:06.6279483Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-05T20:05:06.6280079Z ##[endgroup]
2026-02-05T20:05:22.2150959Z (node:70) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-05T20:05:22.2151775Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-05T20:05:47.9388006Z ##[group]Run # Decode and save kubeconfig
2026-02-05T20:05:47.9388546Z [36;1m# Decode and save kubeconfig[0m
2026-02-05T20:05:47.9420976Z [36;1mecho "***" | base64 -d > "$KUBECONFIG"[0m
2026-02-05T20:05:47.9421571Z shell: bash -el {0}
2026-02-05T20:05:47.9421800Z ##[endgroup]
2026-02-05T20:05:47.9581464Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-05T20:05:47.9582684Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-05T20:05:47.9583087Z ##[endgroup]
2026-02-05T20:05:48.3274356Z (node:5684) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-05T20:05:48.3275288Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-05T20:06:23.4760767Z ##[group]Run actions/checkout@v6
2026-02-05T20:06:23.4761136Z with:
2026-02-05T20:06:23.4761361Z   repository: vllm-project/vllm-ascend
2026-02-05T20:06:23.4762243Z   token: ***
2026-02-05T20:06:23.4762589Z   ssh-strict: true
2026-02-05T20:06:23.4762774Z   ssh-user: git
2026-02-05T20:06:23.4763047Z   persist-credentials: true
2026-02-05T20:06:23.4763302Z   clean: true
2026-02-05T20:06:23.4763500Z   sparse-checkout-cone-mode: true
2026-02-05T20:06:23.4763804Z   fetch-depth: 1
2026-02-05T20:06:23.4764033Z   fetch-tags: false
2026-02-05T20:06:23.4764223Z   show-progress: true
2026-02-05T20:06:23.4764481Z   lfs: false
2026-02-05T20:06:23.4764696Z   submodules: false
2026-02-05T20:06:23.4764893Z   set-safe-directory: true
2026-02-05T20:06:23.4765165Z ##[endgroup]
2026-02-05T20:06:23.4820763Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-05T20:06:23.4821647Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-05T20:06:23.4821942Z ##[endgroup]
2026-02-05T20:06:23.8346392Z (node:6111) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-05T20:06:23.8347417Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-05T20:06:41.9843798Z Syncing repository: vllm-project/vllm-ascend
2026-02-05T20:06:41.9844753Z ##[group]Getting Git version info
2026-02-05T20:06:41.9845178Z Working directory is '/__w/vllm-ascend/vllm-ascend'
2026-02-05T20:06:41.9845575Z [command]/usr/bin/git version
2026-02-05T20:06:42.0036916Z git version 2.34.1
2026-02-05T20:06:42.0040292Z ##[endgroup]
2026-02-05T20:06:42.0043993Z Copying '/root/.gitconfig' to '/__w/_temp/16581b0c-d1bd-4a1e-87d8-1f6fef13cdda/.gitconfig'
2026-02-05T20:06:42.0044575Z Temporarily overriding HOME='/__w/_temp/16581b0c-d1bd-4a1e-87d8-1f6fef13cdda' before making global git config changes
2026-02-05T20:06:42.0045336Z Adding repository directory to the temporary git global config as a safe directory
2026-02-05T20:06:42.0045794Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-05T20:06:42.0063943Z Deleting the contents of '/__w/vllm-ascend/vllm-ascend'
2026-02-05T20:06:42.0066494Z ##[group]Initializing the repository
2026-02-05T20:06:42.0069636Z [command]/usr/bin/git init /__w/vllm-ascend/vllm-ascend
2026-02-05T20:06:42.0195134Z hint: Using 'master' as the name for the initial branch. This default branch name
2026-02-05T20:06:42.0195547Z hint: is subject to change. To configure the initial branch name to use in all
2026-02-05T20:06:42.0195912Z hint: of your new repositories, which will suppress this warning, call:
2026-02-05T20:06:42.0196181Z hint: 
2026-02-05T20:06:42.0196411Z hint: 	git config --global init.defaultBranch <name>
2026-02-05T20:06:42.0196636Z hint: 
2026-02-05T20:06:42.0196854Z hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and
2026-02-05T20:06:42.0197250Z hint: 'development'. The just-created branch can be renamed via this command:
2026-02-05T20:06:42.0197530Z hint: 
2026-02-05T20:06:42.0197735Z hint: 	git branch -m <name>
2026-02-05T20:06:42.0206403Z Initialized empty Git repository in /__w/vllm-ascend/vllm-ascend/.git/
2026-02-05T20:06:42.0216759Z [command]/usr/bin/git remote add origin https://github.com/vllm-project/vllm-ascend
2026-02-05T20:06:42.0266975Z ##[endgroup]
2026-02-05T20:06:42.0267261Z ##[group]Disabling automatic garbage collection
2026-02-05T20:06:42.0270129Z [command]/usr/bin/git config --local gc.auto 0
2026-02-05T20:06:42.0294479Z ##[endgroup]
2026-02-05T20:06:42.0294779Z ##[group]Setting up auth
2026-02-05T20:06:42.0295878Z Removing SSH command configuration
2026-02-05T20:06:42.0300688Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-05T20:06:42.0327298Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-05T20:06:42.0784234Z Removing HTTP extra header
2026-02-05T20:06:42.0786765Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-05T20:06:42.0812746Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-05T20:06:42.0992394Z Removing includeIf entries pointing to credentials config files
2026-02-05T20:06:42.0995829Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-05T20:06:42.1020728Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-05T20:06:42.1199827Z [command]/usr/bin/git config --file /__w/_temp/git-credentials-f5578cd7-0628-4dca-aec5-0eea296b0b50.config http.https://github.com/.extraheader AUTHORIZATION: basic ***
2026-02-05T20:06:42.1235884Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-f5578cd7-0628-4dca-aec5-0eea296b0b50.config
2026-02-05T20:06:42.1260924Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-f5578cd7-0628-4dca-aec5-0eea296b0b50.config
2026-02-05T20:06:42.1285916Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-f5578cd7-0628-4dca-aec5-0eea296b0b50.config
2026-02-05T20:06:42.1314311Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-f5578cd7-0628-4dca-aec5-0eea296b0b50.config
2026-02-05T20:06:42.1337001Z ##[endgroup]
2026-02-05T20:06:42.1337281Z ##[group]Fetching the repository
2026-02-05T20:06:42.1344623Z [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --no-recurse-submodules --depth=1 origin +81f3c09d6d46917052fc62759a93bd5a797968ef:refs/remotes/origin/main
2026-02-05T20:06:43.8017813Z From https://gh-proxy.test.osinfra.cn/https://github.com/vllm-project/vllm-ascend
2026-02-05T20:06:43.8018295Z  * [new ref]         81f3c09d6d46917052fc62759a93bd5a797968ef -> origin/main
2026-02-05T20:06:43.8018949Z [command]/usr/bin/git branch --list --remote origin/main
2026-02-05T20:06:43.8021809Z   origin/main
2026-02-05T20:06:43.8028732Z [command]/usr/bin/git rev-parse refs/remotes/origin/main
2026-02-05T20:06:43.8047494Z 81f3c09d6d46917052fc62759a93bd5a797968ef
2026-02-05T20:06:43.8050407Z ##[endgroup]
2026-02-05T20:06:43.8050774Z ##[group]Determining the checkout info
2026-02-05T20:06:43.8054925Z ##[endgroup]
2026-02-05T20:06:43.8056722Z [command]/usr/bin/git sparse-checkout disable
2026-02-05T20:06:43.8101895Z [command]/usr/bin/git config --local --unset-all extensions.worktreeConfig
2026-02-05T20:06:43.8125726Z ##[group]Checking out the ref
2026-02-05T20:06:43.8129231Z [command]/usr/bin/git checkout --progress --force -B main refs/remotes/origin/main
2026-02-05T20:06:43.8978942Z Switched to a new branch 'main'
2026-02-05T20:06:43.8979252Z Branch 'main' set up to track remote branch 'main' from 'origin'.
2026-02-05T20:06:43.8988188Z ##[endgroup]
2026-02-05T20:06:43.9029499Z [command]/usr/bin/git log -1 --format=%H
2026-02-05T20:06:43.9051169Z 81f3c09d6d46917052fc62759a93bd5a797968ef
2026-02-05T20:07:02.4328954Z ##[group]Run # prepare for lws entrypoint scripts
2026-02-05T20:07:02.4329276Z [36;1m# prepare for lws entrypoint scripts[0m
2026-02-05T20:07:02.4329621Z [36;1minstall -D tests/e2e/nightly/multi_node/scripts/run.sh /root/.cache/tests/run.sh[0m
2026-02-05T20:07:02.4330045Z shell: bash -el {0}
2026-02-05T20:07:02.4330209Z ##[endgroup]
2026-02-05T20:07:02.4616854Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-05T20:07:02.4617548Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-05T20:07:02.4617772Z ##[endgroup]
2026-02-05T20:07:02.8524073Z (node:6699) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-05T20:07:02.8524769Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-05T20:07:38.5626259Z ##[group]Run set -euo pipefail
2026-02-05T20:07:38.5626617Z [36;1mset -euo pipefail[0m
2026-02-05T20:07:38.5626864Z [36;1m[0m
2026-02-05T20:07:38.5627116Z [36;1mCRD_NAME="${CRD_NAME:-vllm}"[0m
2026-02-05T20:07:38.5627379Z [36;1mTIMEOUT=${TIMEOUT:-120}[0m
2026-02-05T20:07:38.5627727Z [36;1mSLEEP_INTERVAL=2[0m
2026-02-05T20:07:38.5627976Z [36;1m[0m
2026-02-05T20:07:38.5628319Z [36;1mecho "Deleting leaderworkerset [$CRD_NAME] in namespace [$NAMESPACE]..."[0m
2026-02-05T20:07:38.5628798Z [36;1mkubectl delete leaderworkerset "$CRD_NAME" -n "$NAMESPACE" --ignore-not-found[0m
2026-02-05T20:07:38.5629194Z [36;1m[0m
2026-02-05T20:07:38.5629461Z [36;1mecho "Waiting for all pods starting with 'vllm' to be deleted..."[0m
2026-02-05T20:07:38.5629782Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-05T20:07:38.5629987Z [36;1m[0m
2026-02-05T20:07:38.5630129Z [36;1mwhile true; do[0m
2026-02-05T20:07:38.5630415Z [36;1m  NOW=$(date +%s)[0m
2026-02-05T20:07:38.5630661Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-05T20:07:38.5630853Z [36;1m[0m
2026-02-05T20:07:38.5631019Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-05T20:07:38.5631314Z [36;1m    echo "Timeout reached ($TIMEOUT seconds), some pods still exist:"[0m
2026-02-05T20:07:38.5631643Z [36;1m    kubectl get pods -n "$NAMESPACE" | grep '^vllm' || true[0m
2026-02-05T20:07:38.5631888Z [36;1m    exit 1[0m
2026-02-05T20:07:38.5632169Z [36;1m  fi[0m
2026-02-05T20:07:38.5632315Z [36;1m[0m
2026-02-05T20:07:38.5632699Z [36;1m  PODS_EXIST=$(kubectl get pods -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null | tr ' ' '\n' | grep '^vllm' || true)[0m
2026-02-05T20:07:38.5633114Z [36;1m[0m
2026-02-05T20:07:38.5633271Z [36;1m  if [[ -z "$PODS_EXIST" ]]; then[0m
2026-02-05T20:07:38.5633493Z [36;1m    echo "All vllm pods deleted."[0m
2026-02-05T20:07:38.5633685Z [36;1m    break[0m
2026-02-05T20:07:38.5633837Z [36;1m  else[0m
2026-02-05T20:07:38.5634050Z [36;1m    echo "Waiting for pods to be deleted: $PODS_EXIST"[0m
2026-02-05T20:07:38.5634296Z [36;1m    sleep $SLEEP_INTERVAL[0m
2026-02-05T20:07:38.5634478Z [36;1m  fi[0m
2026-02-05T20:07:38.5634611Z [36;1mdone[0m
2026-02-05T20:07:38.5634893Z shell: bash -el {0}
2026-02-05T20:07:38.5635048Z ##[endgroup]
2026-02-05T20:07:38.5702228Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-05T20:07:38.5702926Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-05T20:07:38.5703138Z ##[endgroup]
2026-02-05T20:07:38.9216191Z (node:7483) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-05T20:07:38.9217220Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-05T20:07:57.0940558Z Deleting leaderworkerset [vllm] in namespace [vllm-project]...
2026-02-05T20:07:57.4089169Z Waiting for all pods starting with 'vllm' to be deleted...
2026-02-05T20:07:57.4734317Z All vllm pods deleted.
2026-02-05T20:08:15.4887184Z ##[group]Run set -e
2026-02-05T20:08:15.4887399Z [36;1mset -e[0m
2026-02-05T20:08:15.4887548Z [36;1m[0m
2026-02-05T20:08:15.4887680Z [36;1msize="2"[0m
2026-02-05T20:08:15.4887832Z [36;1mreplicas="1"[0m
2026-02-05T20:08:15.4888157Z [36;1mimage="swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3"[0m
2026-02-05T20:08:15.4888574Z [36;1mconfig_file_path="DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-05T20:08:15.4888893Z [36;1mfail_tag=FAIL_TAG_"DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-05T20:08:15.4889180Z [36;1mecho "FAIL_TAG=${fail_tag}" >> "$GITHUB_ENV"[0m
2026-02-05T20:08:15.4889387Z [36;1m[0m
2026-02-05T20:08:15.4889601Z [36;1mrequired_params=("size" "replicas" "image" "config_file_path")[0m
2026-02-05T20:08:15.4889890Z [36;1mfor param in "${required_params[@]}"; do[0m
2026-02-05T20:08:15.4890135Z [36;1m  if [ -z "${!param}" ]; then[0m
2026-02-05T20:08:15.4890725Z [36;1m    echo "Error: Parameter '$param' is required but empty"[0m
2026-02-05T20:08:15.4890966Z [36;1m    exit 1[0m
2026-02-05T20:08:15.4891119Z [36;1m  fi[0m
2026-02-05T20:08:15.4891264Z [36;1mdone[0m
2026-02-05T20:08:15.4891394Z [36;1m[0m
2026-02-05T20:08:15.4891538Z [36;1mif [ "a3" = "a3" ]; then[0m
2026-02-05T20:08:15.4891721Z [36;1m  npu_per_node=16[0m
2026-02-05T20:08:15.4892102Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws.yaml.jinja2"[0m
2026-02-05T20:08:15.4892400Z [36;1melse[0m
2026-02-05T20:08:15.4892544Z [36;1m  npu_per_node=8[0m
2026-02-05T20:08:15.4892818Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws-a2.yaml.jinja2"[0m
2026-02-05T20:08:15.4893109Z [36;1mfi[0m
2026-02-05T20:08:15.4893241Z [36;1m[0m
2026-02-05T20:08:15.4893392Z [36;1mjinja2 $TEMPLATE_FILE \[0m
2026-02-05T20:08:15.4893585Z [36;1m  -D size="$size" \[0m
2026-02-05T20:08:15.4893771Z [36;1m  -D replicas="$replicas" \[0m
2026-02-05T20:08:15.4893975Z [36;1m  -D image="$image" \[0m
2026-02-05T20:08:15.4894194Z [36;1m  -D config_file_path="$config_file_path" \[0m
2026-02-05T20:08:15.4894436Z [36;1m  -D npu_per_node="$npu_per_node" \[0m
2026-02-05T20:08:15.4894645Z [36;1m  -D fail_tag="$fail_tag" \[0m
2026-02-05T20:08:15.4894832Z [36;1m  --outfile lws.yaml[0m
2026-02-05T20:08:15.4895005Z [36;1m[0m
2026-02-05T20:08:15.4895151Z [36;1mkubectl apply -f ./lws.yaml[0m
2026-02-05T20:08:15.4895468Z shell: bash -el {0}
2026-02-05T20:08:15.4895625Z ##[endgroup]
2026-02-05T20:08:15.4981003Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-05T20:08:15.4981759Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-05T20:08:15.4981972Z ##[endgroup]
2026-02-05T20:08:15.8856822Z (node:8451) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-05T20:08:15.8857527Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-05T20:08:34.5402829Z leaderworkerset.leaderworkerset.x-k8s.io/vllm created
2026-02-05T20:08:34.5639906Z service/vllm-leader created
2026-02-05T20:08:52.9662546Z ##[group]Run POD_PREFIX="${POD_PREFIX:-vllm-0}"
2026-02-05T20:08:52.9662864Z [36;1mPOD_PREFIX="${POD_PREFIX:-vllm-0}"[0m
2026-02-05T20:08:52.9663064Z [36;1mSIZE="2"[0m
2026-02-05T20:08:52.9663245Z [36;1mTIMEOUT=1200  # default timeout 20 minutes[0m
2026-02-05T20:08:52.9663459Z [36;1m[0m
2026-02-05T20:08:52.9663756Z [36;1mecho "Waiting for Pods in namespace [$NAMESPACE] to become Running and Ready (timeout ${TIMEOUT}s)..."[0m
2026-02-05T20:08:52.9664100Z [36;1m[0m
2026-02-05T20:08:52.9664245Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-05T20:08:52.9664413Z [36;1m[0m
2026-02-05T20:08:52.9664630Z [36;1mwhile true; do[0m
2026-02-05T20:08:52.9664794Z [36;1m  NOW=$(date +%s)[0m
2026-02-05T20:08:52.9664985Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-05T20:08:52.9665205Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-05T20:08:52.9665452Z [36;1m    echo "Timeout reached after ${ELAPSED}s"[0m
2026-02-05T20:08:52.9665728Z [36;1m    echo "Dumping pod status for debugging:"[0m
2026-02-05T20:08:52.9665958Z [36;1m    kubectl get pods -n "$NAMESPACE"[0m
2026-02-05T20:08:52.9666210Z [36;1m    kubectl describe pod "$LEADER_POD" -n "$NAMESPACE"[0m
2026-02-05T20:08:52.9666446Z [36;1m    exit 1[0m
2026-02-05T20:08:52.9666588Z [36;1m  fi[0m
2026-02-05T20:08:52.9666726Z [36;1m[0m
2026-02-05T20:08:52.9666869Z [36;1m  # 1) check follower pods[0m
2026-02-05T20:08:52.9667056Z [36;1m  ALL_FOLLOWERS_READY=true[0m
2026-02-05T20:08:52.9667248Z [36;1m  for ((i=1; i<SIZE; i++)); do[0m
2026-02-05T20:08:52.9667436Z [36;1m    POD="${POD_PREFIX}-${i}"[0m
2026-02-05T20:08:52.9667800Z [36;1m    PHASE=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-05T20:08:52.9668326Z [36;1m    READY=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-05T20:08:52.9668819Z [36;1m[0m
2026-02-05T20:08:52.9669007Z [36;1m    echo "Follower [$POD] phase=$PHASE ready=$READY"[0m
2026-02-05T20:08:52.9669236Z [36;1m[0m
2026-02-05T20:08:52.9669419Z [36;1m    if [[ "$PHASE" != "Running" || "$READY" != "true" ]]; then[0m
2026-02-05T20:08:52.9669699Z [36;1m      echo "Follower [$POD] not Ready yet..."[0m
2026-02-05T20:08:52.9669927Z [36;1m      ALL_FOLLOWERS_READY=false[0m
2026-02-05T20:08:52.9670112Z [36;1m      break[0m
2026-02-05T20:08:52.9670264Z [36;1m    fi[0m
2026-02-05T20:08:52.9670504Z [36;1m  done[0m
2026-02-05T20:08:52.9670639Z [36;1m[0m
2026-02-05T20:08:52.9670783Z [36;1m  # 2) check leader pod[0m
2026-02-05T20:08:52.9671169Z [36;1m  LEADER_PHASE=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-05T20:08:52.9671755Z [36;1m  LEADER_READY=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-05T20:08:52.9672248Z [36;1m[0m
2026-02-05T20:08:52.9672478Z [36;1m  echo "Leader [$LEADER_POD] phase=$LEADER_PHASE ready=$LEADER_READY"[0m
2026-02-05T20:08:52.9672742Z [36;1m[0m
2026-02-05T20:08:52.9672954Z [36;1m  if [[ "$LEADER_PHASE" != "Running" || "$LEADER_READY" != "true" ]]; then[0m
2026-02-05T20:08:52.9673246Z [36;1m    echo "Leader not Ready yet..."[0m
2026-02-05T20:08:52.9673451Z [36;1m    ALL_FOLLOWERS_READY=false[0m
2026-02-05T20:08:52.9673639Z [36;1m  fi[0m
2026-02-05T20:08:52.9673771Z [36;1m[0m
2026-02-05T20:08:52.9673940Z [36;1m  if [[ "$ALL_FOLLOWERS_READY" == "true" ]]; then[0m
2026-02-05T20:08:52.9674273Z [36;1m    echo "All follower pods and leader pod are Running and Ready â€” continuing."[0m
2026-02-05T20:08:52.9674560Z [36;1m    break[0m
2026-02-05T20:08:52.9674708Z [36;1m  fi[0m
2026-02-05T20:08:52.9674844Z [36;1m[0m
2026-02-05T20:08:52.9674971Z [36;1m  sleep 2[0m
2026-02-05T20:08:52.9675121Z [36;1mdone[0m
2026-02-05T20:08:52.9675413Z shell: bash -el {0}
2026-02-05T20:08:52.9675561Z env:
2026-02-05T20:08:52.9675883Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-05T20:08:52.9676166Z ##[endgroup]
2026-02-05T20:08:52.9759062Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-05T20:08:52.9759713Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-05T20:08:52.9759931Z ##[endgroup]
2026-02-05T20:08:53.3280932Z (node:9592) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-05T20:08:53.3281610Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-05T20:09:11.6419984Z Waiting for Pods in namespace [vllm-project] to become Running and Ready (timeout 1200s)...
2026-02-05T20:09:11.7665733Z Follower [vllm-0-1] phase=Running ready=true
2026-02-05T20:09:11.9067238Z Leader [vllm-0] phase=Running ready=true
2026-02-05T20:09:11.9067785Z All follower pods and leader pod are Running and Ready â€” continuing.
2026-02-05T20:09:30.0986967Z ##[group]Run set -euo pipefail
2026-02-05T20:09:30.0987391Z [36;1mset -euo pipefail[0m
2026-02-05T20:09:30.0987567Z [36;1m[0m
2026-02-05T20:09:30.0987714Z [36;1msize="2"[0m
2026-02-05T20:09:30.0987865Z [36;1mpids=()[0m
2026-02-05T20:09:30.0988004Z [36;1m[0m
2026-02-05T20:09:30.0988143Z [36;1mcleanup() {[0m
2026-02-05T20:09:30.0988346Z [36;1m  echo "Cleaning up background log streams..."[0m
2026-02-05T20:09:30.0988596Z [36;1m  for pid in "${pids[@]}"; do[0m
2026-02-05T20:09:30.0988807Z [36;1m    kill "$pid" 2>/dev/null || true[0m
2026-02-05T20:09:30.0988998Z [36;1m  done[0m
2026-02-05T20:09:30.0989143Z [36;1m}[0m
2026-02-05T20:09:30.0989290Z [36;1mtrap cleanup EXIT[0m
2026-02-05T20:09:30.0989452Z [36;1m[0m
2026-02-05T20:09:30.0989607Z [36;1mfor i in $(seq 1 $((size - 1))); do[0m
2026-02-05T20:09:30.0989818Z [36;1m  POD="vllm-0-${i}"[0m
2026-02-05T20:09:30.0989980Z [36;1m[0m
2026-02-05T20:09:30.0990413Z [36;1m  echo "==== Collecting logs from worker pod: $POD ===="[0m
2026-02-05T20:09:30.0990699Z [36;1m  kubectl logs -f "$POD" -n "$NAMESPACE" \[0m
2026-02-05T20:09:30.0990941Z [36;1m    > "/tmp/${POD}_logs.txt" 2>&1 &[0m
2026-02-05T20:09:30.0991133Z [36;1m[0m
2026-02-05T20:09:30.0991270Z [36;1m  pids+=($!)[0m
2026-02-05T20:09:30.0991430Z [36;1mdone[0m
2026-02-05T20:09:30.0991562Z [36;1m[0m
2026-02-05T20:09:30.0991763Z [36;1mecho "==== Streaming logs from leader pod: $LEADER_POD ===="[0m
2026-02-05T20:09:30.0992167Z [36;1mecho "Looking for logs containing: $FAIL_TAG"[0m
2026-02-05T20:09:30.0992387Z [36;1m[0m
2026-02-05T20:09:30.0992633Z [36;1mkubectl logs -f "$LEADER_POD" -n "$NAMESPACE" | while IFS= read -r line; do[0m
2026-02-05T20:09:30.0992941Z [36;1m  echo "$line"[0m
2026-02-05T20:09:30.0993137Z [36;1m  if echo "$line" | grep -q "$FAIL_TAG"; then[0m
2026-02-05T20:09:30.0993352Z [36;1m    exit 1[0m
2026-02-05T20:09:30.0993495Z [36;1m  fi[0m
2026-02-05T20:09:30.0993640Z [36;1mdone[0m
2026-02-05T20:09:30.0993967Z shell: bash -el {0}
2026-02-05T20:09:30.0994119Z env:
2026-02-05T20:09:30.0994319Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-05T20:09:30.0994552Z ##[endgroup]
2026-02-05T20:09:30.1068895Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-05T20:09:30.1069781Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-05T20:09:30.1070094Z ##[endgroup]
2026-02-05T20:09:30.4617628Z (node:10914) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-05T20:09:30.4618314Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-05T20:09:49.1048427Z ==== Collecting logs from worker pod: vllm-0-1 ====
2026-02-05T20:09:49.1048760Z ==== Streaming logs from leader pod: vllm-0 ====
2026-02-05T20:09:49.1049071Z Looking for logs containing: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-05T20:09:49.1788702Z ====> Check NPU info
2026-02-05T20:09:49.1803061Z +------------------------------------------------------------------------------------------------+
2026-02-05T20:09:49.1814464Z | npu-smi 25.5.0                   Version: 25.5.0                                               |
2026-02-05T20:09:49.1824605Z +---------------------------+---------------+----------------------------------------------------+
2026-02-05T20:09:49.1835262Z | NPU   Name                | Health        | Power(W)    Temp(C)           Hugepages-Usage(page)|
2026-02-05T20:09:49.1845874Z | Chip  Phy-ID              | Bus-Id        | AICore(%)   Memory-Usage(MB)  HBM-Usage(MB)        |
2026-02-05T20:09:49.1856133Z +===========================+===============+====================================================+
2026-02-05T20:09:49.1865886Z | 0     Ascend910           | OK            | 165.5       37                0    / 0             |
2026-02-05T20:09:49.1875282Z | 0     0                   | 0000:9D:00.0  | 0           0    / 0          3160 / 65536         |
2026-02-05T20:09:49.1886059Z +------------------------------------------------------------------------------------------------+
2026-02-05T20:09:49.1895157Z | 0     Ascend910           | OK            | -           36                0    / 0             |
2026-02-05T20:09:49.1905222Z | 1     1                   | 0000:9F:00.0  | 0           0    / 0          2883 / 65536         |
2026-02-05T20:09:49.1914768Z +===========================+===============+====================================================+
2026-02-05T20:09:49.1924841Z | 1     Ascend910           | OK            | 169.4       37                0    / 0             |
2026-02-05T20:09:49.1935008Z | 0     2                   | 0000:99:00.0  | 0           0    / 0          3155 / 65536         |
2026-02-05T20:09:49.1944827Z +------------------------------------------------------------------------------------------------+
2026-02-05T20:09:49.1954872Z | 1     Ascend910           | OK            | -           37                0    / 0             |
2026-02-05T20:09:49.1964885Z | 1     3                   | 0000:9B:00.0  | 0           0    / 0          2881 / 65536         |
2026-02-05T20:09:49.1975618Z +===========================+===============+====================================================+
2026-02-05T20:09:49.1985008Z | 2     Ascend910           | OK            | 168.8       37                0    / 0             |
2026-02-05T20:09:49.1996488Z | 0     4                   | 0000:95:00.0  | 0           0    / 0          3144 / 65536         |
2026-02-05T20:09:49.2008472Z +------------------------------------------------------------------------------------------------+
2026-02-05T20:09:49.2017479Z | 2     Ascend910           | OK            | -           36                0    / 0             |
2026-02-05T20:09:49.2027114Z | 1     5                   | 0000:97:00.0  | 0           0    / 0          2892 / 65536         |
2026-02-05T20:09:49.2037604Z +===========================+===============+====================================================+
2026-02-05T20:09:49.2047569Z | 3     Ascend910           | OK            | 168.7       36                0    / 0             |
2026-02-05T20:09:49.2057741Z | 0     6                   | 0000:91:00.0  | 0           0    / 0          3154 / 65536         |
2026-02-05T20:09:49.2067424Z +------------------------------------------------------------------------------------------------+
2026-02-05T20:09:49.2078143Z | 3     Ascend910           | OK            | -           36                0    / 0             |
2026-02-05T20:09:49.2087536Z | 1     7                   | 0000:93:00.0  | 0           0    / 0          2882 / 65536         |
2026-02-05T20:09:49.2099164Z +===========================+===============+====================================================+
2026-02-05T20:09:49.2111645Z | 4     Ascend910           | OK            | 169.8       36                0    / 0             |
2026-02-05T20:09:49.2122123Z | 0     8                   | 0000:8D:00.0  | 0           0    / 0          3157 / 65536         |
2026-02-05T20:09:49.2131901Z +------------------------------------------------------------------------------------------------+
2026-02-05T20:09:49.2142076Z | 4     Ascend910           | OK            | -           37                0    / 0             |
2026-02-05T20:09:49.2151775Z | 1     9                   | 0000:8F:00.0  | 0           0    / 0          2880 / 65536         |
2026-02-05T20:09:49.2161838Z +===========================+===============+====================================================+
2026-02-05T20:09:49.2171190Z | 5     Ascend910           | OK            | 163.3       35                0    / 0             |
2026-02-05T20:09:49.2180557Z | 0     10                  | 0000:89:00.0  | 0           0    / 0          3141 / 65536         |
2026-02-05T20:09:49.2190397Z +------------------------------------------------------------------------------------------------+
2026-02-05T20:09:49.2200638Z | 5     Ascend910           | OK            | -           35                0    / 0             |
2026-02-05T20:09:49.2210773Z | 1     11                  | 0000:8B:00.0  | 0           0    / 0          2892 / 65536         |
2026-02-05T20:09:49.2220322Z +===========================+===============+====================================================+
2026-02-05T20:09:49.2232355Z | 6     Ascend910           | OK            | 169.9       36                0    / 0             |
2026-02-05T20:09:49.2244926Z | 0     12                  | 0000:85:00.0  | 0           0    / 0          3156 / 65536         |
2026-02-05T20:09:49.2254918Z +------------------------------------------------------------------------------------------------+
2026-02-05T20:09:49.2264528Z | 6     Ascend910           | OK            | -           36                0    / 0             |
2026-02-05T20:09:49.2274286Z | 1     13                  | 0000:87:00.0  | 0           0    / 0          2879 / 65536         |
2026-02-05T20:09:49.2284711Z +===========================+===============+====================================================+
2026-02-05T20:09:49.2295124Z | 7     Ascend910           | OK            | 168.8       36                0    / 0             |
2026-02-05T20:09:49.2307917Z | 0     14                  | 0000:81:00.0  | 0           0    / 0          3155 / 65536         |
2026-02-05T20:09:49.2318401Z +------------------------------------------------------------------------------------------------+
2026-02-05T20:09:49.2328766Z | 7     Ascend910           | OK            | -           36                0    / 0             |
2026-02-05T20:09:49.2338542Z | 1     15                  | 0000:83:00.0  | 0           0    / 0          2883 / 65536         |
2026-02-05T20:09:49.2348323Z +===========================+===============+====================================================+
2026-02-05T20:09:49.2358737Z +---------------------------+---------------+----------------------------------------------------+
2026-02-05T20:09:49.2368334Z | NPU     Chip              | Process id    | Process name             | Process memory(MB)      |
2026-02-05T20:09:49.2377114Z +===========================+===============+====================================================+
2026-02-05T20:09:49.2386712Z | No running processes found in NPU 0                                                            |
2026-02-05T20:09:49.2396394Z +===========================+===============+====================================================+
2026-02-05T20:09:49.2406696Z | No running processes found in NPU 1                                                            |
2026-02-05T20:09:49.2415602Z +===========================+===============+====================================================+
2026-02-05T20:09:49.2426266Z | No running processes found in NPU 2                                                            |
2026-02-05T20:09:49.2434950Z +===========================+===============+====================================================+
2026-02-05T20:09:49.2445031Z | No running processes found in NPU 3                                                            |
2026-02-05T20:09:49.2454637Z +===========================+===============+====================================================+
2026-02-05T20:09:49.2463874Z | No running processes found in NPU 4                                                            |
2026-02-05T20:09:49.2473491Z +===========================+===============+====================================================+
2026-02-05T20:09:49.2484354Z | No running processes found in NPU 5                                                            |
2026-02-05T20:09:49.2493538Z +===========================+===============+====================================================+
2026-02-05T20:09:49.2503365Z | No running processes found in NPU 6                                                            |
2026-02-05T20:09:49.2513129Z +===========================+===============+====================================================+
2026-02-05T20:09:49.2523022Z | No running processes found in NPU 7                                                            |
2026-02-05T20:09:49.2531840Z +===========================+===============+====================================================+
2026-02-05T20:09:49.2563428Z package_name=Ascend-cann-toolkit
2026-02-05T20:09:49.2563641Z version=8.5.0
2026-02-05T20:09:49.2564104Z innerversion=V100R001C25SPC001B232
2026-02-05T20:09:49.2571031Z compatible_version=[V100R001C15],[V100R001C18],[V100R001C19],[V100R001C20],[V100R001C21],[V100R001C23]
2026-02-05T20:09:49.2579803Z arch=aarch64
2026-02-05T20:09:49.2588637Z os=linux
2026-02-05T20:09:49.2598474Z path=/usr/local/Ascend/cann-8.5.0
2026-02-05T20:09:49.2608032Z ====> Configure mirrors and git proxy
2026-02-05T20:09:49.2617025Z Writing to /root/.config/pip/pip.conf
2026-02-05T20:09:49.2626469Z Installed vLLM-related Python packages:
2026-02-05T20:09:49.2636951Z ais_bench_benchmark               3.0.20250930               /vllm-workspace/vllm-ascend/benchmark
2026-02-05T20:09:49.2646250Z vllm                              0.15.0+empty               /vllm-workspace/vllm
2026-02-05T20:09:49.2655650Z vllm_ascend                       0.14.0rc2.dev99+g2c1608265 /vllm-workspace/vllm-ascend
2026-02-05T20:09:49.2664628Z 
2026-02-05T20:09:49.2674575Z ============================
2026-02-05T20:09:49.2684238Z vLLM Git information
2026-02-05T20:09:49.2693649Z ============================
2026-02-05T20:09:49.2702738Z Branch:      HEAD
2026-02-05T20:09:49.2712686Z Commit hash: f176443446f659dbab5315e056e605d8984fd976
2026-02-05T20:09:49.2721695Z Author:      TJian <tunjian.tan@embeddedllm.com>
2026-02-05T20:09:49.2732692Z Date:        2026-01-29 14:45:42 +0800
2026-02-05T20:09:49.2740844Z Message:     [Release] [CI] Optim release pipeline (#33156)
2026-02-05T20:09:49.2749977Z Tags:        v0.15.0
2026-02-05T20:09:49.2761366Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm.git (fetch)
2026-02-05T20:09:49.2770075Z 
2026-02-05T20:09:49.2780123Z 
2026-02-05T20:09:49.2789144Z ============================
2026-02-05T20:09:49.2798521Z vLLM-Ascend Git information
2026-02-05T20:09:49.2807823Z ============================
2026-02-05T20:09:49.2817364Z Branch:      main
2026-02-05T20:09:49.2826964Z Commit hash: 2c1608265be02b499c0056f0454533c77d42366f
2026-02-05T20:09:49.2837725Z Author:      ChenCangtao <50493711+ChenCangtao@users.noreply.github.com>
2026-02-05T20:09:49.2846707Z Date:        2026-02-05 14:03:10 +0800
2026-02-05T20:09:49.2856492Z Message:     [CI][npugraph_ex]Fix npugraph ex e2e test (#6553)
2026-02-05T20:09:49.2866103Z Tags:        
2026-02-05T20:09:49.2875866Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm-ascend (fetch)
2026-02-05T20:09:49.2885950Z 
2026-02-05T20:09:49.2895056Z ====> Check triton ascend info
2026-02-05T20:09:49.2904680Z Ubuntu clang version 15.0.7
2026-02-05T20:09:49.2919704Z Target: aarch64-unknown-linux-gnu
2026-02-05T20:09:49.2934111Z Thread model: posix
2026-02-05T20:09:49.2943348Z InstalledDir: /usr/bin
2026-02-05T20:09:49.2953054Z Found candidate GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-05T20:09:49.2963164Z Selected GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-05T20:09:49.2972601Z Candidate multilib: .;@m64
2026-02-05T20:09:49.2981454Z Selected multilib: .;@m64
2026-02-05T20:09:49.2991148Z /usr/local/Ascend/ascend-toolkit/latest/bin/bishengir-compile
2026-02-05T20:09:49.3000573Z Name: triton-ascend
2026-02-05T20:09:49.3010141Z Version: 3.2.0
2026-02-05T20:09:49.3020042Z Summary: A language and compiler for custom Deep Learning operations on Ascend hardwares
2026-02-05T20:09:49.3029278Z Home-page: https://gitcode.com/Ascend/triton-ascend/
2026-02-05T20:09:49.3038507Z Author: 
2026-02-05T20:09:49.3048304Z Author-email: 
2026-02-05T20:09:49.3056712Z License: 
2026-02-05T20:09:49.3066366Z Location: /usr/local/python3.11.14/lib/python3.11/site-packages
2026-02-05T20:09:49.3076068Z Requires: 
2026-02-05T20:09:49.3085721Z Required-by: vllm_ascend
2026-02-05T20:09:49.3095207Z INFO 02-05 20:09:12 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:09:49.3105067Z INFO 02-05 20:09:12 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:09:49.3114341Z INFO 02-05 20:09:12 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:09:49.3123981Z INFO 02-05 20:09:12 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:09:49.3133244Z ============================= test session starts ==============================
2026-02-05T20:09:49.3143065Z platform linux -- Python 3.11.14, pytest-8.4.2, pluggy-1.6.0 -- /usr/local/python3.11.14/bin/python3
2026-02-05T20:09:49.3152571Z cachedir: .pytest_cache
2026-02-05T20:09:49.3161360Z rootdir: /vllm-workspace/vllm-ascend
2026-02-05T20:09:49.3171037Z configfile: pyproject.toml
2026-02-05T20:09:49.3180563Z plugins: cov-7.0.0, asyncio-1.3.0, mock-3.15.1, anyio-4.12.1
2026-02-05T20:09:49.3189867Z asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
2026-02-05T20:09:49.3198921Z collecting ... collected 1 item
2026-02-05T20:09:49.3207454Z 
2026-02-05T20:09:49.3217296Z [2026-02-05 20:09:18] INFO multi_node_config.py:294: Loading config yaml: tests/e2e/nightly/multi_node/config/DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-05T20:09:49.3226634Z [2026-02-05 20:09:18] INFO multi_node_config.py:351: Resolving cluster IPs via DNS...
2026-02-05T20:09:49.3238333Z [2026-02-05 20:09:18] INFO multi_node_config.py:212: Node 0 envs: {'HCCL_OP_EXPANSION_MODE': 'AIV', 'VLLM_USE_MODELSCOPE': 'True', 'HCCL_BUFFSIZE': '1024', 'SERVER_PORT': '8080', 'OMP_PROC_BIND': 'False', 'OMP_NUM_THREADS': '1', 'PYTORCH_NPU_ALLOC_CONF': 'expandable_segments:True', 'VLLM_ASCEND_ENABLE_FLASHCOMM1': '1', 'ASCEND_A3_EBA_ENABLE': '1', 'HCCL_IF_IP': '10.0.0.139', 'HCCL_SOCKET_IFNAME': 'eth0', 'GLOO_SOCKET_IFNAME': 'eth0', 'TP_SOCKET_IFNAME': 'eth0', 'LOCAL_IP': '10.0.0.139', 'NIC_NAME': 'eth0', 'MASTER_IP': '10.0.0.139'}
2026-02-05T20:09:49.3247377Z [2026-02-05 20:09:18] INFO multi_node_config.py:137: Not launching proxy on non-master node
2026-02-05T20:09:49.3260288Z [2026-02-05 20:09:18] INFO conftest.py:232: Starting server with command: vllm serve vllm-ascend/DeepSeek-V3.2-W8A8 --host 0.0.0.0 --port 8080 --data-parallel-size 4 --data-parallel-size-local 2 --data-parallel-address 10.0.0.139 --data-parallel-rpc-port 13399 --tensor-parallel-size 8 --quantization ascend --seed 1024 --enable-expert-parallel --max-num-seqs 16 --max-model-len 8192 --max-num-batched-tokens 4096 --no-enable-prefix-caching --gpu-memory-utilization 0.85 --trust-remote-code --speculative-config {"num_speculative_tokens": 2, "method":"deepseek_mtp"} --compilation-config {"cudagraph_capture_sizes": [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], "cudagraph_mode": "FULL_DECODE_ONLY"} --additional-config {"layer_sharding": ["q_b_proj", "o_proj"]} --tokenizer-mode deepseek_v32 --reasoning-parser deepseek_v3
2026-02-05T20:09:49.3271773Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node INFO 02-05 20:09:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:09:49.3277287Z INFO 02-05 20:09:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:09:49.3287456Z INFO 02-05 20:09:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:09:49.3296603Z INFO 02-05 20:09:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:09:49.3306669Z 2026-02-05 20:09:29,780 - 66 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:09:49.3318090Z INFO 02-05 20:09:29 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:09:49.3326758Z INFO 02-05 20:09:30 [serve.py:100] Defaulting api_server_count to data_parallel_size (4).
2026-02-05T20:09:49.3335867Z INFO 02-05 20:09:30 [utils.py:325] 
2026-02-05T20:09:49.3345683Z INFO 02-05 20:09:30 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
2026-02-05T20:09:49.3354444Z INFO 02-05 20:09:30 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
2026-02-05T20:09:49.3364671Z INFO 02-05 20:09:30 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   vllm-ascend/DeepSeek-V3.2-W8A8
2026-02-05T20:09:49.3373737Z INFO 02-05 20:09:30 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
2026-02-05T20:09:49.3383307Z INFO 02-05 20:09:30 [utils.py:325] 
2026-02-05T20:09:49.3401686Z INFO 02-05 20:09:30 [utils.py:261] non-default args: {'model_tag': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'api_server_count': 4, 'host': '0.0.0.0', 'port': 8080, 'model': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'tokenizer_mode': 'deepseek_v32', 'trust_remote_code': True, 'seed': 1024, 'max_model_len': 8192, 'quantization': 'ascend', 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 8, 'data_parallel_size': 4, 'data_parallel_size_local': 2, 'data_parallel_address': '10.0.0.139', 'data_parallel_rpc_port': 13399, 'enable_expert_parallel': True, 'gpu_memory_utilization': 0.85, 'enable_prefix_caching': False, 'max_num_batched_tokens': 4096, 'max_num_seqs': 16, 'speculative_config': {'num_speculative_tokens': 2, 'method': 'deepseek_mtp'}, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}, 'additional_config': {'layer_sharding': ['q_b_proj', 'o_proj']}}
2026-02-05T20:09:49.3407686Z 2026-02-05 20:09:30,076 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-05T20:09:49.3417441Z INFO 02-05 20:09:30 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-05T20:09:49.3427227Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:09:49.3435628Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:09:49.3445968Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:09:49.3455734Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-05T20:09:49.3464616Z INFO 02-05 20:09:30 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-05T20:09:49.3473933Z INFO 02-05 20:09:30 [model.py:1561] Using max model len 8192
2026-02-05T20:09:49.3484652Z WARNING 02-05 20:09:30 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-05T20:09:49.3494100Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:09:49.3504139Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:09:49.3513491Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-05T20:09:49.3523419Z INFO 02-05 20:09:30 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-05T20:09:49.3533076Z INFO 02-05 20:09:30 [model.py:1561] Using max model len 163840
2026-02-05T20:09:49.3542232Z WARNING 02-05 20:09:30 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-05T20:09:49.3551168Z INFO 02-05 20:09:30 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-05T20:09:49.3561322Z INFO 02-05 20:09:30 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-05T20:09:49.3570582Z INFO 02-05 20:09:30 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-05T20:09:49.3583370Z WARNING 02-05 20:09:30 [platform.py:707] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-05T20:09:49.3590308Z WARNING 02-05 20:09:30 [platform.py:748] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-05T20:09:49.3600595Z INFO 02-05 20:09:30 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:09:49.3610110Z WARNING 02-05 20:09:30 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-05T20:09:49.3619249Z INFO 02-05 20:09:30 [platform.py:310] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-05T20:09:49.3628574Z WARNING 02-05 20:09:30 [platform.py:327] [91m
2026-02-05T20:09:49.3638460Z WARNING 02-05 20:09:30 [platform.py:327]             **********************************************************************************
2026-02-05T20:09:49.3648061Z WARNING 02-05 20:09:30 [platform.py:327]             * WARNING: You have enabled the *full graph* feature.
2026-02-05T20:09:49.3657505Z WARNING 02-05 20:09:30 [platform.py:327]             * This is an early experimental stage and may involve various unknown issues.
2026-02-05T20:09:49.3666885Z WARNING 02-05 20:09:30 [platform.py:327]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-05T20:09:49.3676523Z WARNING 02-05 20:09:30 [platform.py:327]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-05T20:09:49.3686472Z WARNING 02-05 20:09:30 [platform.py:327]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-05T20:09:49.3695769Z WARNING 02-05 20:09:30 [platform.py:327]             * batch size for graph capture.
2026-02-05T20:09:49.3705068Z WARNING 02-05 20:09:30 [platform.py:327]             * For more details, please refer to:
2026-02-05T20:09:49.3714990Z WARNING 02-05 20:09:30 [platform.py:327]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-05T20:09:49.3724975Z WARNING 02-05 20:09:30 [platform.py:327]             **********************************************************************************[0m
2026-02-05T20:09:49.3734054Z WARNING 02-05 20:09:30 [platform.py:327]             
2026-02-05T20:09:49.3743499Z INFO 02-05 20:09:30 [platform.py:420] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-05T20:09:49.3753115Z INFO 02-05 20:09:30 [utils.py:851] Started DP Coordinator process (PID: 79)
2026-02-05T20:09:49.3768133Z INFO 02-05 20:09:35 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:09:49.3781889Z INFO 02-05 20:09:35 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:09:49.3791741Z INFO 02-05 20:09:35 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:09:49.3801636Z INFO 02-05 20:09:35 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:09:49.3812197Z INFO 02-05 20:09:35 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:09:49.3821616Z INFO 02-05 20:09:35 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:09:49.3830976Z INFO 02-05 20:09:35 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:09:49.3841129Z INFO 02-05 20:09:35 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:09:49.3850883Z INFO 02-05 20:09:45 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:09:49.3860461Z INFO 02-05 20:09:45 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:09:49.3870141Z INFO 02-05 20:09:45 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:09:49.3879680Z INFO 02-05 20:09:45 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:09:50.7103492Z INFO 02-05 20:09:50 [utils.py:218] Started 4 API server processes
2026-02-05T20:09:50.9013594Z [0;36m(EngineCore_DP1 pid=101)[0;0m 2026-02-05 20:09:50,899 - 101 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:09:50.9037540Z [0;36m(EngineCore_DP0 pid=82)[0;0m 2026-02-05 20:09:50,899 - 82 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:09:50.9062334Z [0;36m(EngineCore_DP1 pid=101)[0;0m INFO 02-05 20:09:50 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:09:50.9072526Z [0;36m(EngineCore_DP0 pid=82)[0;0m INFO 02-05 20:09:50 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:09:50.9099631Z [0;36m(EngineCore_DP0 pid=82)[0;0m INFO 02-05 20:09:50 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', speculative_config=SpeculativeConfig(method='mtp', model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', num_spec_tokens=2), tokenizer='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', skip_tokenizer_init=False, tokenizer_mode=deepseek_v32, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=4, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=1024, served_model_name=/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [24, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 48, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
2026-02-05T20:09:55.7212287Z INFO 02-05 20:09:55 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:09:55.7222749Z INFO 02-05 20:09:55 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:09:55.7234756Z INFO 02-05 20:09:55 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:09:55.7245198Z INFO 02-05 20:09:55 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:09:55.7255219Z INFO 02-05 20:09:55 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:09:55.7265687Z INFO 02-05 20:09:55 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:09:55.7283496Z INFO 02-05 20:09:55 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:09:55.7302118Z INFO 02-05 20:09:55 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:09:55.8529479Z INFO 02-05 20:09:55 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:09:55.8543397Z INFO 02-05 20:09:55 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:09:55.8547534Z INFO 02-05 20:09:55 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:09:55.8610349Z INFO 02-05 20:09:55 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:09:56.1488740Z INFO 02-05 20:09:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:09:56.1499344Z INFO 02-05 20:09:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:09:56.1510502Z INFO 02-05 20:09:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:09:56.1519273Z INFO 02-05 20:09:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:09:56.1530682Z INFO 02-05 20:09:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:09:56.1544774Z INFO 02-05 20:09:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:09:56.1560655Z INFO 02-05 20:09:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:09:56.1573332Z INFO 02-05 20:09:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:09:56.1583612Z INFO 02-05 20:09:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:09:56.1595447Z INFO 02-05 20:09:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:09:56.1606476Z INFO 02-05 20:09:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:09:56.1650318Z INFO 02-05 20:09:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:10:00.9822526Z 2026-02-05 20:10:00,980 - 129 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:10:00.9854396Z INFO 02-05 20:10:00 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:10:01.0287277Z 2026-02-05 20:10:01,026 - 130 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:10:01.0323008Z INFO 02-05 20:10:01 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:10:02.3950527Z [0;36m(ApiServer_2 pid=114)[0;0m 2026-02-05 20:10:02,393 - 114 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:10:02.4109782Z [0;36m(ApiServer_1 pid=113)[0;0m 2026-02-05 20:10:02,393 - 113 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:10:02.4119324Z [0;36m(ApiServer_3 pid=115)[0;0m 2026-02-05 20:10:02,393 - 115 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:10:02.4427100Z [0;36m(ApiServer_1 pid=113)[0;0m INFO 02-05 20:10:02 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:10:02.4435123Z [0;36m(ApiServer_2 pid=114)[0;0m INFO 02-05 20:10:02 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:10:02.4446160Z [0;36m(ApiServer_3 pid=115)[0;0m INFO 02-05 20:10:02 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:10:02.4881061Z [0;36m(ApiServer_2 pid=114)[0;0m 2026-02-05 20:10:02,486 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-05T20:10:02.4890870Z [0;36m(ApiServer_2 pid=114)[0;0m INFO 02-05 20:10:02 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-05T20:10:02.5296104Z [0;36m(ApiServer_3 pid=115)[0;0m 2026-02-05 20:10:02,527 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-05T20:10:02.5307682Z [0;36m(ApiServer_3 pid=115)[0;0m INFO 02-05 20:10:02 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-05T20:10:02.5846145Z [0;36m(ApiServer_1 pid=113)[0;0m 2026-02-05 20:10:02,583 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-05T20:10:02.5857891Z [0;36m(ApiServer_1 pid=113)[0;0m INFO 02-05 20:10:02 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-05T20:10:02.6801206Z [0;36m(ApiServer_2 pid=114)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:10:02.6810453Z [0;36m(ApiServer_3 pid=115)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:10:02.6846527Z [0;36m(ApiServer_3 pid=115)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:10:02.6856845Z [0;36m(ApiServer_2 pid=114)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:10:02.6876613Z [0;36m(ApiServer_1 pid=113)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:10:02.6886478Z [0;36m(ApiServer_3 pid=115)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:10:02.6894814Z [0;36m(ApiServer_2 pid=114)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:10:02.6904886Z [0;36m(ApiServer_1 pid=113)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:10:02.6915881Z [0;36m(ApiServer_3 pid=115)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-05T20:10:02.6926675Z [0;36m(ApiServer_2 pid=114)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-05T20:10:02.6935609Z [0;36m(ApiServer_1 pid=113)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:10:02.6946501Z [0;36m(ApiServer_1 pid=113)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-05T20:10:02.7063157Z [0;36m(ApiServer_2 pid=114)[0;0m INFO 02-05 20:10:02 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-05T20:10:02.7072219Z [0;36m(ApiServer_1 pid=113)[0;0m INFO 02-05 20:10:02 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-05T20:10:02.7081100Z [0;36m(ApiServer_3 pid=115)[0;0m INFO 02-05 20:10:02 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-05T20:10:02.7090382Z [0;36m(ApiServer_2 pid=114)[0;0m INFO 02-05 20:10:02 [model.py:1561] Using max model len 8192
2026-02-05T20:10:02.7099381Z [0;36m(ApiServer_3 pid=115)[0;0m INFO 02-05 20:10:02 [model.py:1561] Using max model len 8192
2026-02-05T20:10:02.7108954Z [0;36m(ApiServer_1 pid=113)[0;0m INFO 02-05 20:10:02 [model.py:1561] Using max model len 8192
2026-02-05T20:10:02.9838181Z [0;36m(ApiServer_0 pid=112)[0;0m 2026-02-05 20:10:02,982 - 112 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:10:03.0014724Z [0;36m(ApiServer_0 pid=112)[0;0m INFO 02-05 20:10:02 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:10:03.0158602Z [0;36m(ApiServer_0 pid=112)[0;0m 2026-02-05 20:10:03,014 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-05T20:10:03.0170622Z [0;36m(ApiServer_0 pid=112)[0;0m INFO 02-05 20:10:03 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-05T20:10:03.1204038Z [0;36m(ApiServer_0 pid=112)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:10:03.1225073Z [0;36m(ApiServer_0 pid=112)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:10:03.1234675Z [0;36m(ApiServer_0 pid=112)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:10:03.1250809Z [0;36m(ApiServer_0 pid=112)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-05T20:10:03.1300377Z [0;36m(ApiServer_0 pid=112)[0;0m INFO 02-05 20:10:03 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-05T20:10:03.1321515Z [0;36m(ApiServer_0 pid=112)[0;0m INFO 02-05 20:10:03 [model.py:1561] Using max model len 8192
2026-02-05T20:10:03.2135180Z [0;36m(ApiServer_1 pid=113)[0;0m WARNING 02-05 20:10:03 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-05T20:10:03.2178736Z [0;36m(ApiServer_2 pid=114)[0;0m WARNING 02-05 20:10:03 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-05T20:10:03.2188596Z [0;36m(ApiServer_1 pid=113)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:10:03.2198453Z [0;36m(ApiServer_2 pid=114)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:10:03.2209469Z [0;36m(ApiServer_1 pid=113)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:10:03.2219869Z [0;36m(ApiServer_2 pid=114)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:10:03.2230218Z [0;36m(ApiServer_1 pid=113)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-05T20:10:03.2240826Z [0;36m(ApiServer_2 pid=114)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-05T20:10:03.2250409Z [0;36m(ApiServer_2 pid=114)[0;0m INFO 02-05 20:10:03 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-05T20:10:03.2260145Z [0;36m(ApiServer_1 pid=113)[0;0m INFO 02-05 20:10:03 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-05T20:10:03.2270360Z [0;36m(ApiServer_1 pid=113)[0;0m INFO 02-05 20:10:03 [model.py:1561] Using max model len 163840
2026-02-05T20:10:03.2280524Z [0;36m(ApiServer_2 pid=114)[0;0m INFO 02-05 20:10:03 [model.py:1561] Using max model len 163840
2026-02-05T20:10:03.2292225Z [0;36m(ApiServer_2 pid=114)[0;0m WARNING 02-05 20:10:03 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-05T20:10:03.2302154Z [0;36m(ApiServer_1 pid=113)[0;0m WARNING 02-05 20:10:03 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-05T20:10:03.2312359Z [0;36m(ApiServer_1 pid=113)[0;0m INFO 02-05 20:10:03 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-05T20:10:03.2322462Z [0;36m(ApiServer_2 pid=114)[0;0m INFO 02-05 20:10:03 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-05T20:10:03.2974712Z [0;36m(ApiServer_3 pid=115)[0;0m WARNING 02-05 20:10:03 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-05T20:10:03.2997830Z [0;36m(ApiServer_3 pid=115)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:10:03.3008216Z [0;36m(ApiServer_3 pid=115)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:10:03.3019760Z [0;36m(ApiServer_3 pid=115)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-05T20:10:03.3054812Z [0;36m(ApiServer_3 pid=115)[0;0m INFO 02-05 20:10:03 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-05T20:10:03.3074749Z [0;36m(ApiServer_3 pid=115)[0;0m INFO 02-05 20:10:03 [model.py:1561] Using max model len 163840
2026-02-05T20:10:03.3085619Z [0;36m(ApiServer_3 pid=115)[0;0m WARNING 02-05 20:10:03 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-05T20:10:03.3094736Z [0;36m(ApiServer_3 pid=115)[0;0m INFO 02-05 20:10:03 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-05T20:10:03.4154032Z [0;36m(ApiServer_2 pid=114)[0;0m INFO 02-05 20:10:03 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-05T20:10:03.4161808Z [0;36m(ApiServer_2 pid=114)[0;0m INFO 02-05 20:10:03 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-05T20:10:03.4171462Z [0;36m(ApiServer_2 pid=114)[0;0m WARNING 02-05 20:10:03 [platform.py:707] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-05T20:10:03.4180897Z [0;36m(ApiServer_2 pid=114)[0;0m WARNING 02-05 20:10:03 [platform.py:748] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-05T20:10:03.4190794Z [0;36m(ApiServer_2 pid=114)[0;0m INFO 02-05 20:10:03 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:10:03.4199860Z [0;36m(ApiServer_2 pid=114)[0;0m WARNING 02-05 20:10:03 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-05T20:10:03.4210229Z [0;36m(ApiServer_2 pid=114)[0;0m INFO 02-05 20:10:03 [platform.py:310] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-05T20:10:03.4218826Z [0;36m(ApiServer_2 pid=114)[0;0m WARNING 02-05 20:10:03 [platform.py:327] [91m
2026-02-05T20:10:03.4228797Z [0;36m(ApiServer_2 pid=114)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             **********************************************************************************
2026-02-05T20:10:03.4238648Z [0;36m(ApiServer_2 pid=114)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * WARNING: You have enabled the *full graph* feature.
2026-02-05T20:10:03.4248835Z [0;36m(ApiServer_2 pid=114)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * This is an early experimental stage and may involve various unknown issues.
2026-02-05T20:10:03.4257699Z [0;36m(ApiServer_2 pid=114)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-05T20:10:03.4267430Z [0;36m(ApiServer_2 pid=114)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-05T20:10:03.4277396Z [0;36m(ApiServer_2 pid=114)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-05T20:10:03.4287286Z [0;36m(ApiServer_2 pid=114)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * batch size for graph capture.
2026-02-05T20:10:03.4296228Z [0;36m(ApiServer_2 pid=114)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * For more details, please refer to:
2026-02-05T20:10:03.4308169Z [0;36m(ApiServer_2 pid=114)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-05T20:10:03.4316940Z [0;36m(ApiServer_2 pid=114)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             **********************************************************************************[0m
2026-02-05T20:10:03.4326844Z [0;36m(ApiServer_2 pid=114)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             
2026-02-05T20:10:03.4335929Z [0;36m(ApiServer_2 pid=114)[0;0m INFO 02-05 20:10:03 [platform.py:420] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-05T20:10:03.4345010Z [0;36m(ApiServer_3 pid=115)[0;0m INFO 02-05 20:10:03 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-05T20:10:03.4354758Z [0;36m(ApiServer_3 pid=115)[0;0m INFO 02-05 20:10:03 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-05T20:10:03.4365974Z [0;36m(ApiServer_3 pid=115)[0;0m WARNING 02-05 20:10:03 [platform.py:707] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-05T20:10:03.4377085Z [0;36m(ApiServer_3 pid=115)[0;0m WARNING 02-05 20:10:03 [platform.py:748] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-05T20:10:03.4387986Z [0;36m(ApiServer_3 pid=115)[0;0m INFO 02-05 20:10:03 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:10:03.4396303Z [0;36m(ApiServer_3 pid=115)[0;0m WARNING 02-05 20:10:03 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-05T20:10:03.4405993Z [0;36m(ApiServer_3 pid=115)[0;0m INFO 02-05 20:10:03 [platform.py:310] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-05T20:10:03.4415664Z [0;36m(ApiServer_3 pid=115)[0;0m WARNING 02-05 20:10:03 [platform.py:327] [91m
2026-02-05T20:10:03.4424525Z [0;36m(ApiServer_3 pid=115)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             **********************************************************************************
2026-02-05T20:10:03.4435016Z [0;36m(ApiServer_3 pid=115)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * WARNING: You have enabled the *full graph* feature.
2026-02-05T20:10:03.4444631Z [0;36m(ApiServer_3 pid=115)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * This is an early experimental stage and may involve various unknown issues.
2026-02-05T20:10:03.4454546Z [0;36m(ApiServer_3 pid=115)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-05T20:10:03.4464533Z [0;36m(ApiServer_3 pid=115)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-05T20:10:03.4474110Z [0;36m(ApiServer_3 pid=115)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-05T20:10:03.4484095Z [0;36m(ApiServer_3 pid=115)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * batch size for graph capture.
2026-02-05T20:10:03.4493735Z [0;36m(ApiServer_3 pid=115)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * For more details, please refer to:
2026-02-05T20:10:03.4503168Z [0;36m(ApiServer_3 pid=115)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-05T20:10:03.4513048Z [0;36m(ApiServer_3 pid=115)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             **********************************************************************************[0m
2026-02-05T20:10:03.4525365Z [0;36m(ApiServer_3 pid=115)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             
2026-02-05T20:10:03.4535076Z [0;36m(ApiServer_3 pid=115)[0;0m INFO 02-05 20:10:03 [platform.py:420] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-05T20:10:03.4544499Z [0;36m(ApiServer_1 pid=113)[0;0m INFO 02-05 20:10:03 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-05T20:10:03.4556689Z [0;36m(ApiServer_1 pid=113)[0;0m INFO 02-05 20:10:03 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-05T20:10:03.4567068Z [0;36m(ApiServer_1 pid=113)[0;0m WARNING 02-05 20:10:03 [platform.py:707] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-05T20:10:03.4577269Z [0;36m(ApiServer_1 pid=113)[0;0m WARNING 02-05 20:10:03 [platform.py:748] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-05T20:10:03.4587740Z [0;36m(ApiServer_1 pid=113)[0;0m INFO 02-05 20:10:03 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:10:03.4598170Z [0;36m(ApiServer_1 pid=113)[0;0m WARNING 02-05 20:10:03 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-05T20:10:03.4607874Z [0;36m(ApiServer_1 pid=113)[0;0m INFO 02-05 20:10:03 [platform.py:310] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-05T20:10:03.4617242Z [0;36m(ApiServer_1 pid=113)[0;0m WARNING 02-05 20:10:03 [platform.py:327] [91m
2026-02-05T20:10:03.4628001Z [0;36m(ApiServer_1 pid=113)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             **********************************************************************************
2026-02-05T20:10:03.4640092Z [0;36m(ApiServer_1 pid=113)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * WARNING: You have enabled the *full graph* feature.
2026-02-05T20:10:03.4653559Z [0;36m(ApiServer_1 pid=113)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * This is an early experimental stage and may involve various unknown issues.
2026-02-05T20:10:03.4664479Z [0;36m(ApiServer_1 pid=113)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-05T20:10:03.4674321Z [0;36m(ApiServer_1 pid=113)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-05T20:10:03.4684238Z [0;36m(ApiServer_1 pid=113)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-05T20:10:03.4696401Z [0;36m(ApiServer_1 pid=113)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * batch size for graph capture.
2026-02-05T20:10:03.4706980Z [0;36m(ApiServer_1 pid=113)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * For more details, please refer to:
2026-02-05T20:10:03.4717062Z [0;36m(ApiServer_1 pid=113)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-05T20:10:03.4727456Z [0;36m(ApiServer_1 pid=113)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             **********************************************************************************[0m
2026-02-05T20:10:03.4737372Z [0;36m(ApiServer_1 pid=113)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             
2026-02-05T20:10:03.4747534Z [0;36m(ApiServer_1 pid=113)[0;0m INFO 02-05 20:10:03 [platform.py:420] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-05T20:10:03.5550698Z [0;36m(ApiServer_0 pid=112)[0;0m WARNING 02-05 20:10:03 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-05T20:10:03.5572340Z [0;36m(ApiServer_0 pid=112)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:10:03.5581169Z [0;36m(ApiServer_0 pid=112)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-05T20:10:03.5590838Z [0;36m(ApiServer_0 pid=112)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-05T20:10:03.5630950Z [0;36m(ApiServer_0 pid=112)[0;0m INFO 02-05 20:10:03 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-05T20:10:03.5650428Z [0;36m(ApiServer_0 pid=112)[0;0m INFO 02-05 20:10:03 [model.py:1561] Using max model len 163840
2026-02-05T20:10:03.5661093Z [0;36m(ApiServer_0 pid=112)[0;0m WARNING 02-05 20:10:03 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-05T20:10:03.5669880Z [0;36m(ApiServer_0 pid=112)[0;0m INFO 02-05 20:10:03 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-05T20:10:03.6886049Z [0;36m(ApiServer_0 pid=112)[0;0m INFO 02-05 20:10:03 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-05T20:10:03.6904296Z [0;36m(ApiServer_0 pid=112)[0;0m INFO 02-05 20:10:03 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-05T20:10:03.6913998Z [0;36m(ApiServer_0 pid=112)[0;0m WARNING 02-05 20:10:03 [platform.py:707] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-05T20:10:03.6924575Z [0;36m(ApiServer_0 pid=112)[0;0m WARNING 02-05 20:10:03 [platform.py:748] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-05T20:10:03.6934513Z [0;36m(ApiServer_0 pid=112)[0;0m INFO 02-05 20:10:03 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:10:03.6943784Z [0;36m(ApiServer_0 pid=112)[0;0m WARNING 02-05 20:10:03 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-05T20:10:03.6953188Z [0;36m(ApiServer_0 pid=112)[0;0m INFO 02-05 20:10:03 [platform.py:310] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-05T20:10:03.6962622Z [0;36m(ApiServer_0 pid=112)[0;0m WARNING 02-05 20:10:03 [platform.py:327] [91m
2026-02-05T20:10:03.6972288Z [0;36m(ApiServer_0 pid=112)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             **********************************************************************************
2026-02-05T20:10:03.6981727Z [0;36m(ApiServer_0 pid=112)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * WARNING: You have enabled the *full graph* feature.
2026-02-05T20:10:03.6991274Z [0;36m(ApiServer_0 pid=112)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * This is an early experimental stage and may involve various unknown issues.
2026-02-05T20:10:03.7000830Z [0;36m(ApiServer_0 pid=112)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-05T20:10:03.7011070Z [0;36m(ApiServer_0 pid=112)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-05T20:10:03.7020607Z [0;36m(ApiServer_0 pid=112)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-05T20:10:03.7029299Z [0;36m(ApiServer_0 pid=112)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * batch size for graph capture.
2026-02-05T20:10:03.7039035Z [0;36m(ApiServer_0 pid=112)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * For more details, please refer to:
2026-02-05T20:10:03.7048943Z [0;36m(ApiServer_0 pid=112)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-05T20:10:03.7058653Z [0;36m(ApiServer_0 pid=112)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             **********************************************************************************[0m
2026-02-05T20:10:03.7067253Z [0;36m(ApiServer_0 pid=112)[0;0m WARNING 02-05 20:10:03 [platform.py:327]             
2026-02-05T20:10:03.7076565Z [0;36m(ApiServer_0 pid=112)[0;0m INFO 02-05 20:10:03 [platform.py:420] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-05T20:10:06.8481480Z INFO 02-05 20:10:06 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:10:06.8489099Z INFO 02-05 20:10:06 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:10:06.8928757Z INFO 02-05 20:10:06 [parallel_state.py:1212] world_size=32 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.139:45133 backend=hccl
2026-02-05T20:10:06.8949988Z INFO 02-05 20:10:06 [parallel_state.py:1212] world_size=32 rank=8 local_rank=0 distributed_init_method=tcp://10.0.0.139:45133 backend=hccl
2026-02-05T20:10:08.5216103Z INFO 02-05 20:10:08 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:10:08.5223006Z INFO 02-05 20:10:08 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:10:08.5232132Z INFO 02-05 20:10:08 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:10:08.5241125Z INFO 02-05 20:10:08 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:10:08.5250941Z INFO 02-05 20:10:08 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:10:08.5260440Z INFO 02-05 20:10:08 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:10:08.5342888Z INFO 02-05 20:10:08 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:10:08.5361702Z INFO 02-05 20:10:08 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:10:16.0943426Z 2026-02-05 20:10:16,092 - 179 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:10:16.0973409Z INFO 02-05 20:10:16 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:10:16.0990331Z 2026-02-05 20:10:16,097 - 180 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:10:16.1030901Z INFO 02-05 20:10:16 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:10:19.4140067Z INFO 02-05 20:10:19 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:10:19.4184398Z INFO 02-05 20:10:19 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:10:19.8562719Z INFO 02-05 20:10:19 [parallel_state.py:1212] world_size=32 rank=9 local_rank=1 distributed_init_method=tcp://10.0.0.139:45133 backend=hccl
2026-02-05T20:10:19.8819778Z INFO 02-05 20:10:19 [parallel_state.py:1212] world_size=32 rank=1 local_rank=1 distributed_init_method=tcp://10.0.0.139:45133 backend=hccl
2026-02-05T20:10:21.0066171Z INFO 02-05 20:10:21 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:10:21.0073603Z INFO 02-05 20:10:21 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:10:21.0083973Z INFO 02-05 20:10:21 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:10:21.0093406Z INFO 02-05 20:10:21 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:10:21.0103579Z INFO 02-05 20:10:21 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:10:21.0113294Z INFO 02-05 20:10:21 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:10:21.0138511Z INFO 02-05 20:10:21 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:10:21.0148457Z INFO 02-05 20:10:21 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:10:26.1274537Z 2026-02-05 20:10:26,125 - 298 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:10:26.1284325Z 2026-02-05 20:10:26,125 - 297 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:10:26.1306018Z INFO 02-05 20:10:26 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:10:26.1315963Z INFO 02-05 20:10:26 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:10:29.1637496Z INFO 02-05 20:10:29 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:10:29.2190004Z INFO 02-05 20:10:29 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:10:29.5961323Z INFO 02-05 20:10:29 [parallel_state.py:1212] world_size=32 rank=2 local_rank=2 distributed_init_method=tcp://10.0.0.139:45133 backend=hccl
2026-02-05T20:10:29.6546531Z INFO 02-05 20:10:29 [parallel_state.py:1212] world_size=32 rank=10 local_rank=2 distributed_init_method=tcp://10.0.0.139:45133 backend=hccl
2026-02-05T20:10:30.7209215Z INFO 02-05 20:10:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:10:30.7220165Z INFO 02-05 20:10:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:10:30.7231188Z INFO 02-05 20:10:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:10:30.7285939Z INFO 02-05 20:10:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:10:30.7664409Z INFO 02-05 20:10:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:10:30.7674105Z INFO 02-05 20:10:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:10:30.7685969Z INFO 02-05 20:10:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:10:30.7739313Z INFO 02-05 20:10:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:10:35.9412163Z 2026-02-05 20:10:35,938 - 402 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:10:35.9439899Z INFO 02-05 20:10:35 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:10:35.9565699Z 2026-02-05 20:10:35,955 - 401 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:10:35.9605564Z INFO 02-05 20:10:35 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:10:39.0256841Z INFO 02-05 20:10:39 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:10:39.0284238Z INFO 02-05 20:10:39 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:10:39.4660067Z INFO 02-05 20:10:39 [parallel_state.py:1212] world_size=32 rank=11 local_rank=3 distributed_init_method=tcp://10.0.0.139:45133 backend=hccl
2026-02-05T20:10:39.4682292Z INFO 02-05 20:10:39 [parallel_state.py:1212] world_size=32 rank=3 local_rank=3 distributed_init_method=tcp://10.0.0.139:45133 backend=hccl
2026-02-05T20:10:40.5666602Z INFO 02-05 20:10:40 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:10:40.5673375Z INFO 02-05 20:10:40 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:10:40.5683473Z INFO 02-05 20:10:40 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:10:40.5739879Z INFO 02-05 20:10:40 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:10:40.7265484Z INFO 02-05 20:10:40 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:10:40.7274551Z INFO 02-05 20:10:40 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:10:40.7284202Z INFO 02-05 20:10:40 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:10:40.7350248Z INFO 02-05 20:10:40 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:10:45.7784616Z 2026-02-05 20:10:45,776 - 505 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:10:45.7816768Z INFO 02-05 20:10:45 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:10:45.8702284Z 2026-02-05 20:10:45,868 - 506 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:10:45.8737164Z INFO 02-05 20:10:45 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:10:48.8300948Z INFO 02-05 20:10:48 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:10:49.0316179Z INFO 02-05 20:10:49 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:10:49.2633377Z INFO 02-05 20:10:49 [parallel_state.py:1212] world_size=32 rank=12 local_rank=4 distributed_init_method=tcp://10.0.0.139:45133 backend=hccl
2026-02-05T20:10:49.4665606Z INFO 02-05 20:10:49 [parallel_state.py:1212] world_size=32 rank=4 local_rank=4 distributed_init_method=tcp://10.0.0.139:45133 backend=hccl
2026-02-05T20:10:50.4976250Z INFO 02-05 20:10:50 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:10:50.4983623Z INFO 02-05 20:10:50 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:10:50.4994079Z INFO 02-05 20:10:50 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:10:50.5051573Z INFO 02-05 20:10:50 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:10:50.5205401Z INFO 02-05 20:10:50 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:10:50.5213954Z INFO 02-05 20:10:50 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:10:50.5223968Z INFO 02-05 20:10:50 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:10:50.5280988Z INFO 02-05 20:10:50 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:10:55.6876250Z 2026-02-05 20:10:55,685 - 610 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:10:55.6908497Z INFO 02-05 20:10:55 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:10:55.8014151Z 2026-02-05 20:10:55,799 - 609 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:10:55.8050174Z INFO 02-05 20:10:55 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:10:58.7386950Z INFO 02-05 20:10:58 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:10:59.1834207Z INFO 02-05 20:10:59 [parallel_state.py:1212] world_size=32 rank=5 local_rank=5 distributed_init_method=tcp://10.0.0.139:45133 backend=hccl
2026-02-05T20:10:59.4187472Z INFO 02-05 20:10:59 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:10:59.8631667Z INFO 02-05 20:10:59 [parallel_state.py:1212] world_size=32 rank=13 local_rank=5 distributed_init_method=tcp://10.0.0.139:45133 backend=hccl
2026-02-05T20:11:00.3234817Z INFO 02-05 20:11:00 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:11:00.3243758Z INFO 02-05 20:11:00 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:11:00.3255395Z INFO 02-05 20:11:00 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:11:00.3307635Z INFO 02-05 20:11:00 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:11:00.5143815Z INFO 02-05 20:11:00 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:11:00.5155322Z INFO 02-05 20:11:00 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:11:00.5165000Z INFO 02-05 20:11:00 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:11:00.5221185Z INFO 02-05 20:11:00 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:11:05.3349961Z 2026-02-05 20:11:05,332 - 713 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:11:05.3383525Z INFO 02-05 20:11:05 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:11:05.5623210Z 2026-02-05 20:11:05,560 - 715 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:11:05.5657632Z INFO 02-05 20:11:05 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:11:08.3987960Z INFO 02-05 20:11:08 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:11:08.6618104Z INFO 02-05 20:11:08 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:11:08.8590071Z INFO 02-05 20:11:08 [parallel_state.py:1212] world_size=32 rank=6 local_rank=6 distributed_init_method=tcp://10.0.0.139:45133 backend=hccl
2026-02-05T20:11:09.0879614Z INFO 02-05 20:11:09 [parallel_state.py:1212] world_size=32 rank=14 local_rank=6 distributed_init_method=tcp://10.0.0.139:45133 backend=hccl
2026-02-05T20:11:09.9298751Z INFO 02-05 20:11:09 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:11:09.9305869Z INFO 02-05 20:11:09 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:11:09.9315600Z INFO 02-05 20:11:09 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:11:09.9371324Z INFO 02-05 20:11:09 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:11:10.1032540Z INFO 02-05 20:11:10 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-05T20:11:10.1039331Z INFO 02-05 20:11:10 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-05T20:11:10.1049820Z INFO 02-05 20:11:10 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-05T20:11:10.1109258Z INFO 02-05 20:11:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-05T20:11:15.1525950Z 2026-02-05 20:11:15,150 - 817 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:11:15.1555135Z INFO 02-05 20:11:15 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:11:15.3105859Z 2026-02-05 20:11:15,308 - 820 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-05T20:11:15.3158779Z INFO 02-05 20:11:15 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-05T20:11:18.3163911Z INFO 02-05 20:11:18 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:11:18.3543896Z INFO 02-05 20:11:18 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-05T20:11:18.7866130Z INFO 02-05 20:11:18 [parallel_state.py:1212] world_size=32 rank=7 local_rank=7 distributed_init_method=tcp://10.0.0.139:45133 backend=hccl
2026-02-05T20:11:18.7926111Z INFO 02-05 20:11:18 [parallel_state.py:1212] world_size=32 rank=15 local_rank=7 distributed_init_method=tcp://10.0.0.139:45133 backend=hccl
2026-02-05T20:11:18.8289803Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:18.8315250Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:18.8325223Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:18.8335866Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:18.8344594Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:18.8353927Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:18.8810946Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:18.8834265Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:18.8853339Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:18.8863427Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:18.9083038Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:18.9092379Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:18.9101218Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:18.9110495Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:18.9133092Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:18.9154265Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.0095142Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-05T20:11:19.0116966Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-05T20:11:19.0126482Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-05T20:11:19.0135382Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-05T20:11:19.0144142Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-05T20:11:19.0153112Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-05T20:11:19.0162363Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-05T20:11:19.0171946Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-05T20:11:19.0181653Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-05T20:11:19.0190783Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-05T20:11:19.0200565Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-05T20:11:19.0210297Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-05T20:11:19.0220114Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-05T20:11:19.0232923Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-05T20:11:19.0243936Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-05T20:11:19.0253467Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-05T20:11:19.0262931Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0273309Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0283514Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0293840Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0302266Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0312320Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0323367Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0332352Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0341384Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0351060Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0360769Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0370785Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0380026Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0389329Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0399361Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0408387Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0417945Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0427184Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0436946Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0446218Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0455509Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0465170Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0474995Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0485268Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0835261Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0851757Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0860833Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0870749Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0880165Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0890137Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0899055Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0908090Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0917573Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0927917Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0936501Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0946259Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0956886Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0966146Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0975804Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0985507Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.0995824Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.1005330Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.1014382Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.1024224Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.1033650Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.1044105Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.1053059Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.1062886Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.1072168Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.1082290Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-05T20:11:19.1091252Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.1100873Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.1110378Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.1120321Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.1129873Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.1139199Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.1148157Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.1157813Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.1167946Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.1176761Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.1186028Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.1195068Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.1204983Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.1213859Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.1350690Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.1368660Z INFO 02-05 20:11:19 [parallel_state.py:1423] rank 0 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
2026-02-05T20:11:19.2081562Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.2100649Z INFO 02-05 20:11:19 [parallel_state.py:1423] rank 1 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
2026-02-05T20:11:19.2119181Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.2138818Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.2148594Z INFO 02-05 20:11:19 [parallel_state.py:1423] rank 2 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
2026-02-05T20:11:19.2157803Z INFO 02-05 20:11:19 [parallel_state.py:1423] rank 4 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 4, EP rank 4
2026-02-05T20:11:19.2167316Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.2176132Z INFO 02-05 20:11:19 [parallel_state.py:1423] rank 10 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 2, EP rank 10
2026-02-05T20:11:19.2185838Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.2195066Z INFO 02-05 20:11:19 [parallel_state.py:1423] rank 14 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 6, EP rank 14
2026-02-05T20:11:19.2664169Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.2684215Z INFO 02-05 20:11:19 [parallel_state.py:1423] rank 7 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 7, EP rank 7
2026-02-05T20:11:19.2702882Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.2712222Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.2721645Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.2731200Z INFO 02-05 20:11:19 [parallel_state.py:1423] rank 6 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 6, EP rank 6
2026-02-05T20:11:19.2740925Z INFO 02-05 20:11:19 [parallel_state.py:1423] rank 5 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 5, EP rank 5
2026-02-05T20:11:19.2749962Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.2758645Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.2767956Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.2777745Z INFO 02-05 20:11:19 [parallel_state.py:1423] rank 3 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
2026-02-05T20:11:19.2787214Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.2796271Z INFO 02-05 20:11:19 [parallel_state.py:1423] rank 12 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 4, EP rank 12
2026-02-05T20:11:19.2806519Z INFO 02-05 20:11:19 [parallel_state.py:1423] rank 9 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 1, EP rank 9
2026-02-05T20:11:19.2816173Z INFO 02-05 20:11:19 [parallel_state.py:1423] rank 13 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 5, EP rank 13
2026-02-05T20:11:19.2825478Z INFO 02-05 20:11:19 [parallel_state.py:1423] rank 15 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 7, EP rank 15
2026-02-05T20:11:19.2834802Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.2845263Z INFO 02-05 20:11:19 [parallel_state.py:1423] rank 11 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 3, EP rank 11
2026-02-05T20:11:19.2854647Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.2864091Z INFO 02-05 20:11:19 [parallel_state.py:1423] rank 8 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 0, EP rank 8
2026-02-05T20:11:19.3113802Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.3147106Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.3157245Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.3166910Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.3176552Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.3187537Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.3194461Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.3204248Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.3213621Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.3223204Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.3232293Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.3241799Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.3252161Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.3261347Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.3270320Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.3280488Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-05T20:11:19.3290195Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.3300240Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.3309203Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.3318562Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.3328175Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.3337701Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.3346860Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.3356716Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.3366411Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.3376205Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.3386047Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.3395714Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.3405458Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.3415019Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.3424251Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.3434094Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-05T20:11:19.4794663Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.4821927Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.4831395Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.4844006Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.4853534Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.4862975Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.4873067Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.4883175Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.4892333Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.4901806Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.4911137Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.4920956Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.4930616Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.4940512Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.4950175Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.4959774Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.5346478Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.5355581Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.5364736Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.5374009Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.5384211Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.5393403Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.5402729Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.5413564Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m INFO 02-05 20:11:19 [model_runner_v1.py:2300] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-05T20:11:19.5422617Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m INFO 02-05 20:11:19 [model_runner_v1.py:2300] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-05T20:11:19.5433303Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m INFO 02-05 20:11:19 [model_runner_v1.py:2300] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-05T20:11:19.5443156Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.5497139Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m INFO 02-05 20:11:19 [model_runner_v1.py:2300] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-05T20:11:19.5506139Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.5516102Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.5525838Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m INFO 02-05 20:11:19 [model_runner_v1.py:2300] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-05T20:11:19.5535754Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.5545659Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m INFO 02-05 20:11:19 [model_runner_v1.py:2300] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-05T20:11:19.5554723Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m INFO 02-05 20:11:19 [model_runner_v1.py:2300] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-05T20:11:19.5565043Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.5575802Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m INFO 02-05 20:11:19 [model_runner_v1.py:2300] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-05T20:11:19.5584434Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.5593972Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m INFO 02-05 20:11:19 [model_runner_v1.py:2300] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-05T20:11:19.5604061Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.5617099Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.5627776Z WARNING 02-05 20:11:19 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-05T20:11:19.5637896Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m INFO 02-05 20:11:19 [model_runner_v1.py:2300] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-05T20:11:19.5647682Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m INFO 02-05 20:11:19 [model_runner_v1.py:2300] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-05T20:11:19.5661421Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m INFO 02-05 20:11:19 [model_runner_v1.py:2300] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-05T20:11:19.5671499Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m INFO 02-05 20:11:19 [model_runner_v1.py:2300] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-05T20:11:19.5681863Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m INFO 02-05 20:11:19 [model_runner_v1.py:2300] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-05T20:11:19.5692436Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m INFO 02-05 20:11:19 [model_runner_v1.py:2300] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-05T20:11:19.6254476Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m INFO 02-05 20:11:19 [model_runner_v1.py:2300] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-05T20:11:19.9004864Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m [2026-02-05 20:11:19] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-05T20:11:19.9558190Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m [2026-02-05 20:11:19] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-05T20:11:19.9560391Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m [2026-02-05 20:11:19] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-05T20:11:19.9593401Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m [2026-02-05 20:11:19] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-05T20:11:19.9625402Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m [2026-02-05 20:11:19] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-05T20:11:20.0677087Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m [2026-02-05 20:11:20] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-05T20:11:20.0955077Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m INFO 02-05 20:11:20 [layer.py:475] [EP Rank 11/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-05T20:11:20.0964382Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m INFO 02-05 20:11:20 [layer.py:475] [EP Rank 2/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-05T20:11:20.0974058Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m INFO 02-05 20:11:20 [layer.py:475] [EP Rank 5/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-05T20:11:20.0983641Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m INFO 02-05 20:11:20 [layer.py:475] [EP Rank 7/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-05T20:11:20.0994147Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m INFO 02-05 20:11:20 [layer.py:475] [EP Rank 3/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-05T20:11:20.1809383Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m INFO 02-05 20:11:20 [layer.py:475] [EP Rank 13/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-05T20:11:20.2383814Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m [2026-02-05 20:11:20] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-05T20:11:20.2518023Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m [2026-02-05 20:11:20] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-05T20:11:20.3154285Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m [2026-02-05 20:11:20] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-05T20:11:20.3191618Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m [2026-02-05 20:11:20] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-05T20:11:20.3490135Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m INFO 02-05 20:11:20 [layer.py:475] [EP Rank 15/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-05T20:11:20.3563087Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m [2026-02-05 20:11:20] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-05T20:11:20.3619118Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m INFO 02-05 20:11:20 [layer.py:475] [EP Rank 14/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-05T20:11:20.3628538Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m [2026-02-05 20:11:20] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-05T20:11:20.3692680Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m [2026-02-05 20:11:20] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-05T20:11:20.4028139Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m [2026-02-05 20:11:20] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-05T20:11:20.4161945Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m [2026-02-05 20:11:20] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-05T20:11:20.4294313Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m INFO 02-05 20:11:20 [layer.py:475] [EP Rank 4/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-05T20:11:20.4375775Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m INFO 02-05 20:11:20 [layer.py:475] [EP Rank 10/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-05T20:11:20.4697202Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m [2026-02-05 20:11:20] INFO modelslim_config.py:297: Using the vLLM Ascend modelslim Quantization now!
2026-02-05T20:11:20.4720688Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m INFO 02-05 20:11:20 [layer.py:475] [EP Rank 9/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-05T20:11:20.4894371Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m INFO 02-05 20:11:20 [layer.py:475] [EP Rank 1/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-05T20:11:20.4901879Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m INFO 02-05 20:11:20 [layer.py:475] [EP Rank 6/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-05T20:11:20.5199059Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m INFO 02-05 20:11:20 [layer.py:475] [EP Rank 12/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-05T20:11:20.5884698Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m INFO 02-05 20:11:20 [layer.py:475] [EP Rank 0/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-05T20:11:20.6386153Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m INFO 02-05 20:11:20 [layer.py:475] [EP Rank 8/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-05T20:11:21.5322555Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-05T20:11:21.5327773Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m   return func(*args, **kwargs)
2026-02-05T20:11:21.6066589Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m INFO 02-05 20:11:21 [fused_moe.py:211] [EP Rank 0/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-05T20:11:21.7500050Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-05T20:11:21.7520275Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m   return func(*args, **kwargs)
2026-02-05T20:11:21.7703874Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-05T20:11:21.7711659Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m   return func(*args, **kwargs)
2026-02-05T20:11:21.7878824Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-05T20:11:21.7955580Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m   return func(*args, **kwargs)
2026-02-05T20:11:21.7965350Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-05T20:11:21.7973291Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m   return func(*args, **kwargs)
2026-02-05T20:11:21.7990004Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-05T20:11:21.7997059Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m   return func(*args, **kwargs)
2026-02-05T20:11:21.8008694Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-05T20:11:21.8017460Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m   return func(*args, **kwargs)
2026-02-05T20:11:21.8029218Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-05T20:11:21.8038859Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m   return func(*args, **kwargs)
2026-02-05T20:11:21.8051376Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-05T20:11:21.8060392Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m   return func(*args, **kwargs)
2026-02-05T20:11:21.8202767Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-05T20:11:21.8211135Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m   return func(*args, **kwargs)
2026-02-05T20:11:21.8221554Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m INFO 02-05 20:11:21 [fused_moe.py:211] [EP Rank 4/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-05T20:11:21.8258747Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-05T20:11:21.8267252Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m   return func(*args, **kwargs)
2026-02-05T20:11:21.8430015Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-05T20:11:21.8439480Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m   return func(*args, **kwargs)
2026-02-05T20:11:21.8494731Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-05T20:11:21.8497565Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m   return func(*args, **kwargs)
2026-02-05T20:11:21.8508080Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m INFO 02-05 20:11:21 [fused_moe.py:211] [EP Rank 6/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-05T20:11:21.8565811Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m INFO 02-05 20:11:21 [fused_moe.py:211] [EP Rank 15/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-05T20:11:21.8585734Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-05T20:11:21.8595605Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m   return func(*args, **kwargs)
2026-02-05T20:11:21.8605122Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-05T20:11:21.8613930Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m   return func(*args, **kwargs)
2026-02-05T20:11:21.8681847Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m INFO 02-05 20:11:21 [fused_moe.py:211] [EP Rank 3/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-05T20:11:21.8683275Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m INFO 02-05 20:11:21 [fused_moe.py:211] [EP Rank 8/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-05T20:11:21.8688936Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m INFO 02-05 20:11:21 [fused_moe.py:211] [EP Rank 9/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-05T20:11:21.8700128Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m INFO 02-05 20:11:21 [fused_moe.py:211] [EP Rank 12/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-05T20:11:21.8709629Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m INFO 02-05 20:11:21 [fused_moe.py:211] [EP Rank 1/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-05T20:11:21.8742288Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-05T20:11:21.8750921Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m   return func(*args, **kwargs)
2026-02-05T20:11:21.8796223Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m INFO 02-05 20:11:21 [fused_moe.py:211] [EP Rank 5/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-05T20:11:21.8846221Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m INFO 02-05 20:11:21 [fused_moe.py:211] [EP Rank 11/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-05T20:11:21.9024903Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m INFO 02-05 20:11:21 [fused_moe.py:211] [EP Rank 7/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-05T20:11:21.9092210Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m INFO 02-05 20:11:21 [fused_moe.py:211] [EP Rank 14/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-05T20:11:21.9187619Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m INFO 02-05 20:11:21 [fused_moe.py:211] [EP Rank 13/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-05T20:11:21.9215771Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m INFO 02-05 20:11:21 [fused_moe.py:211] [EP Rank 2/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-05T20:11:21.9342233Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m INFO 02-05 20:11:21 [fused_moe.py:211] [EP Rank 10/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-05T20:13:22.3400049Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-05T20:13:22.3401673Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-05T20:13:22.3402853Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-05T20:13:22.3403739Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-05T20:13:22.3404437Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.3405282Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-05T20:13:22.3406033Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-05T20:13:22.3407178Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-05T20:13:22.3407985Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-05T20:13:22.3445869Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2303, in load_model
2026-02-05T20:13:22.3447109Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-05T20:13:22.3448120Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.3449397Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-05T20:13:22.3450738Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return loader.load_model(
2026-02-05T20:13:22.3461819Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.3471447Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-05T20:13:22.3483105Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     model = initialize_model(
2026-02-05T20:13:22.3493139Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.3504115Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-05T20:13:22.3514362Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-05T20:13:22.3525351Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.3535752Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-05T20:13:22.3545024Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-05T20:13:22.3555106Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-05T20:13:22.3566583Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-05T20:13:22.3576588Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-05T20:13:22.3587983Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-05T20:13:22.3600209Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-05T20:13:22.3609471Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-05T20:13:22.3620263Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-05T20:13:22.3630725Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     + [
2026-02-05T20:13:22.3643239Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]       ^
2026-02-05T20:13:22.3652412Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-05T20:13:22.3662541Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-05T20:13:22.3673437Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.3684322Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-05T20:13:22.3694651Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-05T20:13:22.3704614Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.3715894Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-05T20:13:22.3726304Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-05T20:13:22.3735380Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-05T20:13:22.3746518Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-05T20:13:22.3757279Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-05T20:13:22.3768779Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-05T20:13:22.3779787Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-05T20:13:22.3790234Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-05T20:13:22.3805152Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-05T20:13:22.3815455Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-05T20:13:22.3826485Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-05T20:13:22.3837605Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-05T20:13:22.3850503Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.3860238Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-05T20:13:22.3870417Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-05T20:13:22.3880688Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.3891403Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-05T20:13:22.3901282Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-05T20:13:22.3911815Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.3922629Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-05T20:13:22.3933190Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-05T20:13:22.3942818Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.3954489Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-05T20:13:22.3964614Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [ERROR] 2026-02-05-20:13:21 (PID:817, Device:7, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-05T20:13:22.3975607Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [PID: 817] 2026-02-05-20:13:21.610.781 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-05T20:13:22.3987977Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-05T20:13:22.3997172Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-05T20:13:22.4008260Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-05T20:13:22.4016812Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] 
2026-02-05T20:13:22.4028484Z [0;36m(Worker_DP0_TP7_EP7 pid=817)[0;0m INFO 02-05 20:13:22 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-05T20:13:22.4537715Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m INFO 02-05 20:13:22 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-05T20:13:22.4574194Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-05T20:13:22.4583311Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-05T20:13:22.4593154Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-05T20:13:22.4603694Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-05T20:13:22.4614480Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.4624158Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-05T20:13:22.4633951Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-05T20:13:22.4644647Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-05T20:13:22.4654191Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-05T20:13:22.4664034Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2303, in load_model
2026-02-05T20:13:22.4674409Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-05T20:13:22.4684036Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.4694312Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-05T20:13:22.4703891Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return loader.load_model(
2026-02-05T20:13:22.4713635Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.4726792Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-05T20:13:22.4735033Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     model = initialize_model(
2026-02-05T20:13:22.4743298Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.4752692Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-05T20:13:22.4763300Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-05T20:13:22.4773090Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.4783037Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-05T20:13:22.4793127Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-05T20:13:22.4803648Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-05T20:13:22.4813763Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-05T20:13:22.4822503Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-05T20:13:22.4831975Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-05T20:13:22.4841675Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-05T20:13:22.4851234Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-05T20:13:22.4862614Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-05T20:13:22.4872732Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     + [
2026-02-05T20:13:22.4883351Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]       ^
2026-02-05T20:13:22.4893687Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-05T20:13:22.4904419Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-05T20:13:22.4918130Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.4928593Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-05T20:13:22.4938230Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-05T20:13:22.4948351Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.4958458Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-05T20:13:22.4968227Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-05T20:13:22.4978246Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-05T20:13:22.4989339Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-05T20:13:22.4999138Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-05T20:13:22.5009048Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-05T20:13:22.5019440Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-05T20:13:22.5028921Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-05T20:13:22.5038971Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-05T20:13:22.5049286Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-05T20:13:22.5059770Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-05T20:13:22.5069620Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-05T20:13:22.5079231Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.5089571Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-05T20:13:22.5099005Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-05T20:13:22.5110377Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.5120168Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-05T20:13:22.5130214Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-05T20:13:22.5139871Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.5150511Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-05T20:13:22.5160556Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-05T20:13:22.5170424Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.5180659Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-05T20:13:22.5189728Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [ERROR] 2026-02-05-20:13:21 (PID:401, Device:3, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-05T20:13:22.5201345Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [PID: 401] 2026-02-05-20:13:21.610.449 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-05T20:13:22.5212569Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-05T20:13:22.5221449Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-05T20:13:22.5232633Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-05T20:13:22.5246703Z [0;36m(Worker_DP0_TP3_EP3 pid=401)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] 
2026-02-05T20:13:22.5257222Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-05T20:13:22.5267584Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-05T20:13:22.5278352Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-05T20:13:22.5288178Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-05T20:13:22.5298104Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.5307922Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-05T20:13:22.5317559Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-05T20:13:22.5328417Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-05T20:13:22.5337843Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-05T20:13:22.5347801Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2303, in load_model
2026-02-05T20:13:22.5358172Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-05T20:13:22.5368469Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.5378675Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-05T20:13:22.5387682Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return loader.load_model(
2026-02-05T20:13:22.5398739Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.5409189Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-05T20:13:22.5418913Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     model = initialize_model(
2026-02-05T20:13:22.5428777Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.5439483Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-05T20:13:22.5448673Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-05T20:13:22.5458646Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.5467894Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-05T20:13:22.5477960Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-05T20:13:22.5487671Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-05T20:13:22.5497858Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-05T20:13:22.5507172Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-05T20:13:22.5518111Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-05T20:13:22.5527298Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-05T20:13:22.5537493Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-05T20:13:22.5547065Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-05T20:13:22.5555694Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     + [
2026-02-05T20:13:22.5565863Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]       ^
2026-02-05T20:13:22.5576031Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-05T20:13:22.5586343Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-05T20:13:22.5596413Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.5606152Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-05T20:13:22.5616060Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-05T20:13:22.5626208Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.5636878Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-05T20:13:22.5646291Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-05T20:13:22.5656165Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-05T20:13:22.5666404Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-05T20:13:22.5675374Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-05T20:13:22.5685151Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-05T20:13:22.5695197Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-05T20:13:22.5705080Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-05T20:13:22.5714909Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-05T20:13:22.5724660Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-05T20:13:22.5735612Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-05T20:13:22.5744856Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-05T20:13:22.5754265Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.5765214Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-05T20:13:22.5775603Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-05T20:13:22.5786208Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.5796111Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-05T20:13:22.5806777Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-05T20:13:22.5816314Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.5826851Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-05T20:13:22.5837028Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-05T20:13:22.5846271Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.5857532Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-05T20:13:22.5867130Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [ERROR] 2026-02-05-20:13:21 (PID:820, Device:7, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-05T20:13:22.5878389Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [PID: 820] 2026-02-05-20:13:21.610.173 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-05T20:13:22.5889501Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-05T20:13:22.5897754Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-05T20:13:22.5908424Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-05T20:13:22.5917296Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] 
2026-02-05T20:13:22.5926765Z [0;36m(Worker_DP1_TP7_EP15 pid=820)[0;0m INFO 02-05 20:13:22 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-05T20:13:22.5936453Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m INFO 02-05 20:13:22 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-05T20:13:22.5946213Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m INFO 02-05 20:13:22 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-05T20:13:22.5956452Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m INFO 02-05 20:13:22 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-05T20:13:22.5966397Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-05T20:13:22.5976219Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-05T20:13:22.5986859Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-05T20:13:22.5995689Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-05T20:13:22.6005920Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.6016065Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-05T20:13:22.6025713Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-05T20:13:22.6035777Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-05T20:13:22.6046284Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-05T20:13:22.6056146Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2303, in load_model
2026-02-05T20:13:22.6065847Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-05T20:13:22.6075743Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.6086115Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-05T20:13:22.6095635Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return loader.load_model(
2026-02-05T20:13:22.6105394Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.6114862Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-05T20:13:22.6124201Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     model = initialize_model(
2026-02-05T20:13:22.6134202Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.6145457Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-05T20:13:22.6154687Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-05T20:13:22.6164489Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.6174209Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-05T20:13:22.6183601Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-05T20:13:22.6193740Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-05T20:13:22.6204395Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-05T20:13:22.6214125Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-05T20:13:22.6224106Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-05T20:13:22.6234611Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-05T20:13:22.6245669Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-05T20:13:22.6256057Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-05T20:13:22.6265756Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     + [
2026-02-05T20:13:22.6275904Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]       ^
2026-02-05T20:13:22.6286760Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-05T20:13:22.6296449Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-05T20:13:22.6305775Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.6316049Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-05T20:13:22.6326105Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-05T20:13:22.6335924Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.6346112Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-05T20:13:22.6355807Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-05T20:13:22.6367257Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-05T20:13:22.6376875Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-05T20:13:22.6386350Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-05T20:13:22.6395717Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-05T20:13:22.6405954Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-05T20:13:22.6415624Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-05T20:13:22.6425707Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-05T20:13:22.6435302Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-05T20:13:22.6445699Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-05T20:13:22.6455576Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-05T20:13:22.6466462Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.6475362Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-05T20:13:22.6485918Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-05T20:13:22.6495575Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.6505713Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-05T20:13:22.6514817Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-05T20:13:22.6525829Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.6536598Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-05T20:13:22.6547095Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-05T20:13:22.6557672Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.6568648Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-05T20:13:22.6578156Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [ERROR] 2026-02-05-20:13:21 (PID:402, Device:3, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-05T20:13:22.6588625Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [PID: 402] 2026-02-05-20:13:21.610.519 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-05T20:13:22.6599949Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-05T20:13:22.6609293Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-05T20:13:22.6620022Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-05T20:13:22.6629113Z [0;36m(Worker_DP1_TP3_EP11 pid=402)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] 
2026-02-05T20:13:22.6640520Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-05T20:13:22.6649568Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-05T20:13:22.6659826Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-05T20:13:22.6669233Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-05T20:13:22.6680763Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.6690598Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-05T20:13:22.6699100Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-05T20:13:22.6709166Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-05T20:13:22.6719088Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-05T20:13:22.6729094Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2303, in load_model
2026-02-05T20:13:22.6738758Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-05T20:13:22.6748799Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.6758391Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-05T20:13:22.6768869Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return loader.load_model(
2026-02-05T20:13:22.6778777Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.6788550Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-05T20:13:22.6799054Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     model = initialize_model(
2026-02-05T20:13:22.6809497Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.6818951Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-05T20:13:22.6828615Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-05T20:13:22.6839666Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.6849477Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-05T20:13:22.6873596Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-05T20:13:22.6877430Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-05T20:13:22.6892570Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-05T20:13:22.6914843Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-05T20:13:22.6945977Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-05T20:13:22.6946888Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-05T20:13:22.6952743Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-05T20:13:22.6963793Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-05T20:13:22.6973217Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     + [
2026-02-05T20:13:22.6983449Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]       ^
2026-02-05T20:13:22.6993556Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-05T20:13:22.7002329Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-05T20:13:22.7011415Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.7021689Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-05T20:13:22.7031931Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-05T20:13:22.7043429Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.7052445Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-05T20:13:22.7061466Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-05T20:13:22.7072139Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-05T20:13:22.7081781Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-05T20:13:22.7090659Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-05T20:13:22.7100516Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-05T20:13:22.7110147Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-05T20:13:22.7120841Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-05T20:13:22.7130409Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-05T20:13:22.7139489Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-05T20:13:22.7150088Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-05T20:13:22.7160141Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-05T20:13:22.7171091Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.7180296Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-05T20:13:22.7189617Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-05T20:13:22.7200341Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.7209951Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-05T20:13:22.7218879Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-05T20:13:22.7228755Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.7239190Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-05T20:13:22.7249134Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-05T20:13:22.7259034Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.7269944Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-05T20:13:22.7280148Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [ERROR] 2026-02-05-20:13:21 (PID:713, Device:6, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-05T20:13:22.7290755Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [PID: 713] 2026-02-05-20:13:21.610.639 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-05T20:13:22.7301935Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-05T20:13:22.7311089Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-05T20:13:22.7322166Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-05T20:13:22.7331592Z [0;36m(Worker_DP0_TP6_EP6 pid=713)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] 
2026-02-05T20:13:22.7341779Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m INFO 02-05 20:13:22 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-05T20:13:22.7351460Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-05T20:13:22.7361526Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-05T20:13:22.7371517Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-05T20:13:22.7381534Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-05T20:13:22.7391693Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.7401716Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-05T20:13:22.7411578Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-05T20:13:22.7422123Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-05T20:13:22.7431554Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-05T20:13:22.7441798Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2303, in load_model
2026-02-05T20:13:22.7451155Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-05T20:13:22.7461391Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.7471395Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-05T20:13:22.7481832Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return loader.load_model(
2026-02-05T20:13:22.7491717Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.7501897Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-05T20:13:22.7511484Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     model = initialize_model(
2026-02-05T20:13:22.7521323Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.7531622Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-05T20:13:22.7541497Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-05T20:13:22.7551550Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.7561588Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-05T20:13:22.7584701Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-05T20:13:22.7585320Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-05T20:13:22.7590451Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-05T20:13:22.7600296Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-05T20:13:22.7611424Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-05T20:13:22.7619800Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-05T20:13:22.7629378Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-05T20:13:22.7640067Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-05T20:13:22.7649457Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     + [
2026-02-05T20:13:22.7658764Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]       ^
2026-02-05T20:13:22.7668436Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-05T20:13:22.7678528Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-05T20:13:22.7688684Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.7698077Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-05T20:13:22.7707296Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-05T20:13:22.7716664Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.7728026Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-05T20:13:22.7738103Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-05T20:13:22.7747655Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-05T20:13:22.7758328Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-05T20:13:22.7768986Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-05T20:13:22.7778902Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-05T20:13:22.7789847Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-05T20:13:22.7799356Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-05T20:13:22.7809778Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-05T20:13:22.7819207Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-05T20:13:22.7830384Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-05T20:13:22.7839920Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-05T20:13:22.7850346Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.7859653Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-05T20:13:22.7869716Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-05T20:13:22.7880346Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.7890864Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-05T20:13:22.7900642Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-05T20:13:22.7910353Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.7920866Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-05T20:13:22.7931236Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-05T20:13:22.7940576Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.7950583Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-05T20:13:22.7960149Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [ERROR] 2026-02-05-20:13:21 (PID:180, Device:1, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-05T20:13:22.7970942Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [PID: 180] 2026-02-05-20:13:21.610.704 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-05T20:13:22.7981748Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-05T20:13:22.7990082Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-05T20:13:22.8000505Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-05T20:13:22.8009297Z [0;36m(Worker_DP0_TP1_EP1 pid=180)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] 
2026-02-05T20:13:22.8018444Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m INFO 02-05 20:13:22 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-05T20:13:22.8027554Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m INFO 02-05 20:13:22 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-05T20:13:22.8038037Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-05T20:13:22.8048001Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-05T20:13:22.8057501Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-05T20:13:22.8066664Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-05T20:13:22.8075914Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.8086990Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-05T20:13:22.8095840Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-05T20:13:22.8105410Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-05T20:13:22.8114980Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-05T20:13:22.8125287Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2303, in load_model
2026-02-05T20:13:22.8134778Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-05T20:13:22.8144707Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.8153994Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-05T20:13:22.8164182Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return loader.load_model(
2026-02-05T20:13:22.8174203Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.8183750Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-05T20:13:22.8193468Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     model = initialize_model(
2026-02-05T20:13:22.8203597Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.8213708Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-05T20:13:22.8223611Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-05T20:13:22.8233617Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.8243380Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-05T20:13:22.8253263Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-05T20:13:22.8263201Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-05T20:13:22.8272258Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-05T20:13:22.8282241Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-05T20:13:22.8292532Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-05T20:13:22.8302131Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-05T20:13:22.8314585Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-05T20:13:22.8325549Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-05T20:13:22.8335493Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     + [
2026-02-05T20:13:22.8344704Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]       ^
2026-02-05T20:13:22.8354875Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-05T20:13:22.8364857Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-05T20:13:22.8375112Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.8385324Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-05T20:13:22.8395656Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-05T20:13:22.8406702Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.8416709Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-05T20:13:22.8425251Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-05T20:13:22.8435129Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-05T20:13:22.8445758Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-05T20:13:22.8455713Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-05T20:13:22.8465093Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-05T20:13:22.8474012Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-05T20:13:22.8484148Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-05T20:13:22.8494474Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-05T20:13:22.8504291Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-05T20:13:22.8513795Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-05T20:13:22.8523744Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-05T20:13:22.8533777Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.8543722Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-05T20:13:22.8553482Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-05T20:13:22.8563875Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.8574188Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-05T20:13:22.8583843Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-05T20:13:22.8593785Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.8604501Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-05T20:13:22.8614210Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-05T20:13:22.8624113Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.8634835Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-05T20:13:22.8645675Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [ERROR] 2026-02-05-20:13:21 (PID:298, Device:2, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-05T20:13:22.8656049Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [PID: 298] 2026-02-05-20:13:21.610.625 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-05T20:13:22.8667049Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-05T20:13:22.8676338Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-05T20:13:22.8710922Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-05T20:13:22.8712275Z [0;36m(Worker_DP0_TP2_EP2 pid=298)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] 
2026-02-05T20:13:22.8712921Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m INFO 02-05 20:13:22 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-05T20:13:22.8715509Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m INFO 02-05 20:13:22 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-05T20:13:22.8725645Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-05T20:13:22.8735681Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-05T20:13:22.8745721Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-05T20:13:22.8755283Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-05T20:13:22.8766239Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.8776046Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-05T20:13:22.8786239Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-05T20:13:22.8795604Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-05T20:13:22.8806216Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-05T20:13:22.8816643Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2303, in load_model
2026-02-05T20:13:22.8826413Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-05T20:13:22.8837065Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.8847592Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-05T20:13:22.8856775Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return loader.load_model(
2026-02-05T20:13:22.8866502Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.8877170Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-05T20:13:22.8887117Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     model = initialize_model(
2026-02-05T20:13:22.8897279Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.8907887Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-05T20:13:22.8917977Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-05T20:13:22.8927603Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.8937786Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-05T20:13:22.8946721Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-05T20:13:22.8956584Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-05T20:13:22.8968362Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-05T20:13:22.8978792Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-05T20:13:22.8987948Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-05T20:13:22.8998286Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-05T20:13:22.9007916Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-05T20:13:22.9017734Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-05T20:13:22.9026939Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     + [
2026-02-05T20:13:22.9036303Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]       ^
2026-02-05T20:13:22.9047260Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-05T20:13:22.9057112Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-05T20:13:22.9066915Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.9076949Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-05T20:13:22.9086972Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-05T20:13:22.9096967Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.9107156Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-05T20:13:22.9116981Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-05T20:13:22.9127409Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-05T20:13:22.9137817Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-05T20:13:22.9146996Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-05T20:13:22.9156861Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-05T20:13:22.9167244Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-05T20:13:22.9176582Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-05T20:13:22.9186601Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-05T20:13:22.9196323Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-05T20:13:22.9207117Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-05T20:13:22.9216920Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-05T20:13:22.9227032Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.9237181Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-05T20:13:22.9247309Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-05T20:13:22.9256783Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.9266993Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-05T20:13:22.9276831Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-05T20:13:22.9288252Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.9299677Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-05T20:13:22.9313420Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-05T20:13:22.9323807Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.9334510Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-05T20:13:22.9344262Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [ERROR] 2026-02-05-20:13:21 (PID:610, Device:5, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-05T20:13:22.9354929Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [PID: 610] 2026-02-05-20:13:21.610.296 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-05T20:13:22.9366661Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-05T20:13:22.9375884Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-05T20:13:22.9386592Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-05T20:13:22.9395538Z [0;36m(Worker_DP0_TP5_EP5 pid=610)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] 
2026-02-05T20:13:22.9405735Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m INFO 02-05 20:13:22 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-05T20:13:22.9415855Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-05T20:13:22.9426005Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-05T20:13:22.9435908Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-05T20:13:22.9446319Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-05T20:13:22.9455889Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.9466099Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-05T20:13:22.9475685Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-05T20:13:22.9487571Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-05T20:13:22.9497370Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-05T20:13:22.9507775Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2303, in load_model
2026-02-05T20:13:22.9517765Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-05T20:13:22.9528319Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.9538686Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-05T20:13:22.9549224Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return loader.load_model(
2026-02-05T20:13:22.9559003Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.9569801Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-05T20:13:22.9579979Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     model = initialize_model(
2026-02-05T20:13:22.9590469Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.9601736Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-05T20:13:22.9612541Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-05T20:13:22.9622479Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.9633539Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-05T20:13:22.9644267Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-05T20:13:22.9654526Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-05T20:13:22.9665149Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-05T20:13:22.9674973Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-05T20:13:22.9686114Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-05T20:13:22.9696006Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-05T20:13:22.9705592Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-05T20:13:22.9715744Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-05T20:13:22.9726439Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     + [
2026-02-05T20:13:22.9736158Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]       ^
2026-02-05T20:13:22.9746267Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-05T20:13:22.9756150Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-05T20:13:22.9793008Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.9810624Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-05T20:13:22.9905041Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-05T20:13:22.9916367Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:22.9930798Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-05T20:13:22.9935662Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-05T20:13:22.9945279Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-05T20:13:22.9955555Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-05T20:13:22.9966080Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-05T20:13:22.9976555Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-05T20:13:22.9990970Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-05T20:13:23.0000837Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-05T20:13:23.0012734Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-05T20:13:23.0023850Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-05T20:13:23.0035066Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-05T20:13:23.0046686Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-05T20:13:23.0056452Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.0066769Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-05T20:13:23.0076781Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-05T20:13:23.0088451Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.0098738Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-05T20:13:23.0107801Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-05T20:13:23.0118007Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.0128141Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-05T20:13:23.0138254Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-05T20:13:23.0147663Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.0158017Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-05T20:13:23.0168312Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [ERROR] 2026-02-05-20:13:21 (PID:609, Device:5, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-05T20:13:23.0179136Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [PID: 609] 2026-02-05-20:13:21.611.099 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-05T20:13:23.0190384Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-05T20:13:23.0199797Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-05T20:13:23.0209310Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-05T20:13:23.0218852Z [0;36m(Worker_DP1_TP5_EP13 pid=609)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] 
2026-02-05T20:13:23.0228881Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m INFO 02-05 20:13:22 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-05T20:13:23.0238435Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-05T20:13:23.0248611Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-05T20:13:23.0258784Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-05T20:13:23.0268622Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-05T20:13:23.0279110Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.0288635Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-05T20:13:23.0297974Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-05T20:13:23.0308213Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-05T20:13:23.0318481Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-05T20:13:23.0328344Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2303, in load_model
2026-02-05T20:13:23.0338017Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-05T20:13:23.0348108Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.0358958Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-05T20:13:23.0367901Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return loader.load_model(
2026-02-05T20:13:23.0377700Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.0387975Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-05T20:13:23.0399227Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     model = initialize_model(
2026-02-05T20:13:23.0408936Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.0419237Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-05T20:13:23.0429047Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-05T20:13:23.0438766Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.0448379Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-05T20:13:23.0457797Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-05T20:13:23.0468561Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-05T20:13:23.0479482Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-05T20:13:23.0489304Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-05T20:13:23.0499495Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-05T20:13:23.0508818Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-05T20:13:23.0520919Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-05T20:13:23.0556946Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-05T20:13:23.0557657Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     + [
2026-02-05T20:13:23.0558250Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]       ^
2026-02-05T20:13:23.0563318Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-05T20:13:23.0572889Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-05T20:13:23.0582438Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.0593378Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-05T20:13:23.0602507Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-05T20:13:23.0612161Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.0621968Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-05T20:13:23.0631802Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-05T20:13:23.0642612Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-05T20:13:23.0653212Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-05T20:13:23.0663696Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-05T20:13:23.0673799Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-05T20:13:23.0684466Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-05T20:13:23.0694375Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-05T20:13:23.0704169Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-05T20:13:23.0713961Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-05T20:13:23.0724351Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-05T20:13:23.0734403Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-05T20:13:23.0744208Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.0754899Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-05T20:13:23.0765127Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-05T20:13:23.0775212Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.0785526Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-05T20:13:23.0795578Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-05T20:13:23.0806097Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.0816203Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-05T20:13:23.0826346Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-05T20:13:23.0835516Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.0847425Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-05T20:13:23.0857145Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [ERROR] 2026-02-05-20:13:21 (PID:505, Device:4, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-05T20:13:23.0868547Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [PID: 505] 2026-02-05-20:13:21.611.174 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-05T20:13:23.0880389Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-05T20:13:23.0889513Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-05T20:13:23.0899849Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-05T20:13:23.0909428Z [0;36m(Worker_DP1_TP4_EP12 pid=505)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] 
2026-02-05T20:13:23.0919115Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m INFO 02-05 20:13:22 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-05T20:13:23.0929010Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-05T20:13:23.0938805Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-05T20:13:23.0948955Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-05T20:13:23.0958710Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-05T20:13:23.0968535Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.0978177Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-05T20:13:23.0987972Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-05T20:13:23.0998393Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-05T20:13:23.1007600Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-05T20:13:23.1018329Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2303, in load_model
2026-02-05T20:13:23.1027974Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-05T20:13:23.1038895Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.1049284Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-05T20:13:23.1058965Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return loader.load_model(
2026-02-05T20:13:23.1068137Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.1078674Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-05T20:13:23.1088013Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     model = initialize_model(
2026-02-05T20:13:23.1097516Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.1107738Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-05T20:13:23.1116459Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-05T20:13:23.1126471Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.1136479Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-05T20:13:23.1145499Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-05T20:13:23.1154887Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-05T20:13:23.1165801Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-05T20:13:23.1174886Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-05T20:13:23.1184525Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-05T20:13:23.1195246Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-05T20:13:23.1204704Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-05T20:13:23.1215027Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-05T20:13:23.1224930Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     + [
2026-02-05T20:13:23.1234658Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]       ^
2026-02-05T20:13:23.1245135Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-05T20:13:23.1253967Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-05T20:13:23.1263969Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.1274304Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-05T20:13:23.1284644Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-05T20:13:23.1294219Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.1304667Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-05T20:13:23.1314595Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-05T20:13:23.1324886Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-05T20:13:23.1334916Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-05T20:13:23.1345651Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-05T20:13:23.1354833Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-05T20:13:23.1365680Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-05T20:13:23.1375407Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-05T20:13:23.1385087Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-05T20:13:23.1394726Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-05T20:13:23.1405808Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-05T20:13:23.1414844Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-05T20:13:23.1424852Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.1435161Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-05T20:13:23.1445595Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-05T20:13:23.1455077Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.1465669Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-05T20:13:23.1475325Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-05T20:13:23.1485477Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.1495974Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-05T20:13:23.1505821Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-05T20:13:23.1515590Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.1526962Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-05T20:13:23.1536847Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [ERROR] 2026-02-05-20:13:21 (PID:129, Device:0, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-05T20:13:23.1548000Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] [PID: 129] 2026-02-05-20:13:21.610.596 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-05T20:13:23.1559396Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-05T20:13:23.1567506Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-05T20:13:23.1601136Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-05T20:13:23.1602127Z [0;36m(Worker_DP1_TP0_EP8 pid=129)[0;0m ERROR 02-05 20:13:22 [multiproc_executor.py:772] 
2026-02-05T20:13:23.1602723Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m INFO 02-05 20:13:23 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-05T20:13:23.1608099Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-05T20:13:23.1617452Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-05T20:13:23.1627557Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-05T20:13:23.1637180Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-05T20:13:23.1646650Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.1656481Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-05T20:13:23.1710132Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-05T20:13:23.1710909Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-05T20:13:23.1711740Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-05T20:13:23.1712703Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2303, in load_model
2026-02-05T20:13:23.1713653Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-05T20:13:23.1715016Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.1725683Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-05T20:13:23.1735632Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     return loader.load_model(
2026-02-05T20:13:23.1746375Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.1755954Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-05T20:13:23.1766611Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     model = initialize_model(
2026-02-05T20:13:23.1776924Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.1787120Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-05T20:13:23.1795779Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-05T20:13:23.1806365Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.1816943Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-05T20:13:23.1827396Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-05T20:13:23.1838597Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-05T20:13:23.1849514Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-05T20:13:23.1859301Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-05T20:13:23.1869882Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-05T20:13:23.1879687Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-05T20:13:23.1889349Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-05T20:13:23.1900386Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-05T20:13:23.1909533Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     + [
2026-02-05T20:13:23.1919399Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]       ^
2026-02-05T20:13:23.1930376Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-05T20:13:23.1941100Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-05T20:13:23.1950792Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.1961584Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-05T20:13:23.1974835Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-05T20:13:23.1984500Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.1994902Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-05T20:13:23.2005338Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-05T20:13:23.2015202Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-05T20:13:23.2024725Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-05T20:13:23.2034816Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-05T20:13:23.2045477Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-05T20:13:23.2056152Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-05T20:13:23.2066091Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-05T20:13:23.2076902Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-05T20:13:23.2087277Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-05T20:13:23.2097328Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-05T20:13:23.2107618Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-05T20:13:23.2118564Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.2129096Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-05T20:13:23.2139339Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-05T20:13:23.2149038Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.2160391Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-05T20:13:23.2169700Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-05T20:13:23.2179627Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.2190516Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-05T20:13:23.2203699Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-05T20:13:23.2214144Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.2225427Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-05T20:13:23.2235333Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] [ERROR] 2026-02-05-20:13:21 (PID:179, Device:1, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-05T20:13:23.2247133Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] [PID: 179] 2026-02-05-20:13:21.610.687 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-05T20:13:23.2259345Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-05T20:13:23.2268946Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-05T20:13:23.2280184Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-05T20:13:23.2289479Z [0;36m(Worker_DP1_TP1_EP9 pid=179)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] 
2026-02-05T20:13:23.2300315Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m INFO 02-05 20:13:23 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-05T20:13:23.2310453Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-05T20:13:23.2320954Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-05T20:13:23.2333074Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-05T20:13:23.2342571Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-05T20:13:23.2352585Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.2363590Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-05T20:13:23.2373312Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-05T20:13:23.2386126Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-05T20:13:23.2394696Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-05T20:13:23.2405684Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2303, in load_model
2026-02-05T20:13:23.2415619Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-05T20:13:23.2425528Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.2436291Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-05T20:13:23.2448526Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     return loader.load_model(
2026-02-05T20:13:23.2459188Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.2470094Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-05T20:13:23.2479889Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     model = initialize_model(
2026-02-05T20:13:23.2489576Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.2499713Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-05T20:13:23.2510252Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-05T20:13:23.2520782Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.2531565Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-05T20:13:23.2541741Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-05T20:13:23.2552311Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-05T20:13:23.2562994Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-05T20:13:23.2572432Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-05T20:13:23.2582519Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-05T20:13:23.2593386Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-05T20:13:23.2603673Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-05T20:13:23.2614076Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-05T20:13:23.2624261Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     + [
2026-02-05T20:13:23.2634720Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]       ^
2026-02-05T20:13:23.2645934Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-05T20:13:23.2655804Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-05T20:13:23.2666887Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.2677749Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-05T20:13:23.2687797Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-05T20:13:23.2698034Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.2708667Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-05T20:13:23.2719219Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-05T20:13:23.2728816Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-05T20:13:23.2739485Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-05T20:13:23.2749580Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-05T20:13:23.2760435Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-05T20:13:23.2771031Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-05T20:13:23.2780727Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-05T20:13:23.2790775Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-05T20:13:23.2801251Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-05T20:13:23.2811463Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-05T20:13:23.2821805Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-05T20:13:23.2831974Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.2843392Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-05T20:13:23.2853701Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-05T20:13:23.2863485Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.2874482Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-05T20:13:23.2884531Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-05T20:13:23.2894589Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.2904635Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-05T20:13:23.2914250Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-05T20:13:23.2925072Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.2935724Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-05T20:13:23.2946018Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] [ERROR] 2026-02-05-20:13:21 (PID:715, Device:6, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-05T20:13:23.2957065Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] [PID: 715] 2026-02-05-20:13:21.610.380 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-05T20:13:23.2968795Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-05T20:13:23.2977767Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-05T20:13:23.2988294Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-05T20:13:23.2999783Z [0;36m(Worker_DP1_TP6_EP14 pid=715)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] 
2026-02-05T20:13:23.3009374Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-05T20:13:23.3018413Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-05T20:13:23.3028714Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-05T20:13:23.3038718Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-05T20:13:23.3048670Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.3059058Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-05T20:13:23.3071615Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-05T20:13:23.3084839Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-05T20:13:23.3095451Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-05T20:13:23.3106351Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2303, in load_model
2026-02-05T20:13:23.3117563Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-05T20:13:23.3127291Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.3137185Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-05T20:13:23.3146264Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     return loader.load_model(
2026-02-05T20:13:23.3155756Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.3166594Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-05T20:13:23.3176626Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     model = initialize_model(
2026-02-05T20:13:23.3186504Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.3196768Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-05T20:13:23.3208813Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-05T20:13:23.3217644Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.3227826Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-05T20:13:23.3238130Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-05T20:13:23.3248131Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-05T20:13:23.3257884Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-05T20:13:23.3267858Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-05T20:13:23.3279831Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-05T20:13:23.3288727Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-05T20:13:23.3299415Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-05T20:13:23.3309431Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-05T20:13:23.3322492Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     + [
2026-02-05T20:13:23.3332344Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]       ^
2026-02-05T20:13:23.3342744Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-05T20:13:23.3352608Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-05T20:13:23.3363673Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.3373349Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-05T20:13:23.3383019Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-05T20:13:23.3392353Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.3403958Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-05T20:13:23.3414429Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-05T20:13:23.3424835Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-05T20:13:23.3434349Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-05T20:13:23.3444005Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-05T20:13:23.3453008Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-05T20:13:23.3463662Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-05T20:13:23.3473440Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-05T20:13:23.3483052Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-05T20:13:23.3494278Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-05T20:13:23.3503517Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-05T20:13:23.3513143Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-05T20:13:23.3523744Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.3532733Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-05T20:13:23.3542703Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-05T20:13:23.3553129Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.3564409Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-05T20:13:23.3574630Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-05T20:13:23.3584102Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.3594794Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-05T20:13:23.3604696Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-05T20:13:23.3615250Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.3629171Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-05T20:13:23.3640006Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] [ERROR] 2026-02-05-20:13:21 (PID:297, Device:2, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-05T20:13:23.3651566Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] [PID: 297] 2026-02-05-20:13:21.610.640 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-05T20:13:23.3664399Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-05T20:13:23.3673689Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-05T20:13:23.3685386Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-05T20:13:23.3697675Z [0;36m(Worker_DP1_TP2_EP10 pid=297)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] 
2026-02-05T20:13:23.3707475Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-05T20:13:23.3718449Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-05T20:13:23.3728888Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-05T20:13:23.3739191Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-05T20:13:23.3748421Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.3759050Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-05T20:13:23.3768555Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-05T20:13:23.3778887Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-05T20:13:23.3788207Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-05T20:13:23.3799192Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2303, in load_model
2026-02-05T20:13:23.3808652Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-05T20:13:23.3818032Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.3828629Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-05T20:13:23.3837912Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     return loader.load_model(
2026-02-05T20:13:23.3847462Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.3857973Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-05T20:13:23.3867950Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     model = initialize_model(
2026-02-05T20:13:23.3878062Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.3888233Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-05T20:13:23.3898536Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-05T20:13:23.3908365Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.3918393Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-05T20:13:23.3963287Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-05T20:13:23.3963950Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-05T20:13:23.3964715Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-05T20:13:23.3965441Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-05T20:13:23.3968293Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-05T20:13:23.3978408Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-05T20:13:23.3988499Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-05T20:13:23.3999201Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-05T20:13:23.4008919Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     + [
2026-02-05T20:13:23.4018611Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]       ^
2026-02-05T20:13:23.4029289Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-05T20:13:23.4039293Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-05T20:13:23.4048904Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.4059372Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-05T20:13:23.4069375Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-05T20:13:23.4078923Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.4088740Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-05T20:13:23.4097602Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-05T20:13:23.4107626Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-05T20:13:23.4116972Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-05T20:13:23.4127047Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-05T20:13:23.4137096Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-05T20:13:23.4147240Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-05T20:13:23.4156982Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-05T20:13:23.4166604Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-05T20:13:23.4176532Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-05T20:13:23.4186607Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-05T20:13:23.4195478Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-05T20:13:23.4206966Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.4217179Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-05T20:13:23.4226921Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-05T20:13:23.4237306Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.4247007Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-05T20:13:23.4256715Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-05T20:13:23.4266435Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.4276797Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-05T20:13:23.4286988Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-05T20:13:23.4297166Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.4308043Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-05T20:13:23.4318758Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] [ERROR] 2026-02-05-20:13:21 (PID:130, Device:0, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-05T20:13:23.4329041Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] [PID: 130] 2026-02-05-20:13:21.610.790 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-05T20:13:23.4340359Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-05T20:13:23.4349355Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-05T20:13:23.4360259Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-05T20:13:23.4370257Z [0;36m(Worker_DP0_TP0_EP0 pid=130)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] 
2026-02-05T20:13:23.4380703Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] WorkerProc failed to start.
2026-02-05T20:13:23.4423643Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] Traceback (most recent call last):
2026-02-05T20:13:23.4424490Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 743, in worker_main
2026-02-05T20:13:23.4425279Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     worker = WorkerProc(*args, **kwargs)
2026-02-05T20:13:23.4425947Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.4429812Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 578, in __init__
2026-02-05T20:13:23.4440034Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.worker.load_model()
2026-02-05T20:13:23.4450420Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-02-05T20:13:23.4460396Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.model_runner.load_model()
2026-02-05T20:13:23.4470348Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2303, in load_model
2026-02-05T20:13:23.4480460Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.model = get_model(vllm_config=self.vllm_config)
2026-02-05T20:13:23.4490478Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.4501049Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 135, in get_model
2026-02-05T20:13:23.4510733Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     return loader.load_model(
2026-02-05T20:13:23.4520097Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.4531023Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-02-05T20:13:23.4540466Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     model = initialize_model(
2026-02-05T20:13:23.4550448Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]             ^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.4561112Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-02-05T20:13:23.4571081Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-02-05T20:13:23.4580651Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.4590160Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1210, in __init__
2026-02-05T20:13:23.4600319Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.model = self.model_cls(
2026-02-05T20:13:23.4610041Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                  ^^^^^^^^^^^^^^^
2026-02-05T20:13:23.4620196Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 306, in __init__
2026-02-05T20:13:23.4629232Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     old_init(self, **kwargs)
2026-02-05T20:13:23.4639757Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1067, in __init__
2026-02-05T20:13:23.4649911Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-02-05T20:13:23.4660052Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                                                     ^^^^^^^^^^^^
2026-02-05T20:13:23.4669854Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 706, in make_layers
2026-02-05T20:13:23.4679979Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     + [
2026-02-05T20:13:23.4690400Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]       ^
2026-02-05T20:13:23.4701028Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 707, in <listcomp>
2026-02-05T20:13:23.4711059Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-02-05T20:13:23.4721444Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.4732626Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1069, in <lambda>
2026-02-05T20:13:23.4741773Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     lambda prefix: DeepseekV2DecoderLayer(
2026-02-05T20:13:23.4751636Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.4762693Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 963, in __init__
2026-02-05T20:13:23.4772282Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.mlp = DeepseekV2MoE(
2026-02-05T20:13:23.4782513Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                ^^^^^^^^^^^^^^
2026-02-05T20:13:23.4793209Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 298, in __init__
2026-02-05T20:13:23.4803517Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.experts = SharedFusedMoE(
2026-02-05T20:13:23.4813306Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                    ^^^^^^^^^^^^^^^
2026-02-05T20:13:23.4823767Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-02-05T20:13:23.4833703Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     AscendFusedMoE.__init__(self, **kwargs)
2026-02-05T20:13:23.4844439Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-02-05T20:13:23.4854734Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     setup_moe_comm_method(self.moe_config)
2026-02-05T20:13:23.4864740Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-02-05T20:13:23.4875014Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-02-05T20:13:23.4885285Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.4895480Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-02-05T20:13:23.4905460Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.token_dispatcher = self._get_token_dispatcher()
2026-02-05T20:13:23.4915936Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.4927400Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-02-05T20:13:23.4937653Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     return TokenDispatcherWithAll2AllV(
2026-02-05T20:13:23.4947560Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.4957543Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-02-05T20:13:23.4967121Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-02-05T20:13:23.4976987Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:23.4987965Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-02-05T20:13:23.4997686Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] [ERROR] 2026-02-05-20:13:21 (PID:506, Device:4, RankID:-1) ERR02200 DIST call hccl api failed.
2026-02-05T20:13:23.5008607Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] [PID: 506] 2026-02-05-20:13:21.611.193 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-02-05T20:13:23.5020310Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-02-05T20:13:23.5028416Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]         TraceBack (most recent call last):
2026-02-05T20:13:23.5038759Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-02-05T20:13:23.5048344Z [0;36m(Worker_DP0_TP4_EP4 pid=506)[0;0m ERROR 02-05 20:13:23 [multiproc_executor.py:772] 
2026-02-05T20:13:27.2511954Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946] EngineCore failed to start.
2026-02-05T20:13:27.2524714Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946] Traceback (most recent call last):
2026-02-05T20:13:27.2535336Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 929, in run_engine_core
2026-02-05T20:13:27.2545752Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946]     engine_core = DPEngineCoreProc(*args, **kwargs)
2026-02-05T20:13:27.2555948Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:27.2567252Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 1279, in __init__
2026-02-05T20:13:27.2588205Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946]     super().__init__(
2026-02-05T20:13:27.2588887Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 691, in __init__
2026-02-05T20:13:27.2596482Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946]     super().__init__(
2026-02-05T20:13:27.2607050Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 105, in __init__
2026-02-05T20:13:27.2617508Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946]     self.model_executor = executor_class(vllm_config)
2026-02-05T20:13:27.2626491Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:27.2636906Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
2026-02-05T20:13:27.2646849Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946]     super().__init__(vllm_config)
2026-02-05T20:13:27.2656693Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
2026-02-05T20:13:27.2666161Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946]     self._init_executor()
2026-02-05T20:13:27.2676847Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
2026-02-05T20:13:27.2687282Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
2026-02-05T20:13:27.2697966Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:27.2705826Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
2026-02-05T20:13:27.2716154Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946]     raise e from None
2026-02-05T20:13:27.2727202Z [0;36m(EngineCore_DP0 pid=82)[0;0m ERROR 02-05 20:13:27 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
2026-02-05T20:13:27.2736164Z [0;36m(EngineCore_DP0 pid=82)[0;0m Process EngineCore_DP0:
2026-02-05T20:13:27.2746648Z [0;36m(EngineCore_DP0 pid=82)[0;0m Traceback (most recent call last):
2026-02-05T20:13:27.2755887Z [0;36m(EngineCore_DP0 pid=82)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-05T20:13:27.2766138Z [0;36m(EngineCore_DP0 pid=82)[0;0m     self.run()
2026-02-05T20:13:27.2776324Z [0;36m(EngineCore_DP0 pid=82)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-05T20:13:27.2785823Z [0;36m(EngineCore_DP0 pid=82)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-05T20:13:27.2795837Z [0;36m(EngineCore_DP0 pid=82)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 950, in run_engine_core
2026-02-05T20:13:27.2805580Z [0;36m(EngineCore_DP0 pid=82)[0;0m     raise e
2026-02-05T20:13:27.2815102Z [0;36m(EngineCore_DP0 pid=82)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 929, in run_engine_core
2026-02-05T20:13:27.2825158Z [0;36m(EngineCore_DP0 pid=82)[0;0m     engine_core = DPEngineCoreProc(*args, **kwargs)
2026-02-05T20:13:27.2834704Z [0;36m(EngineCore_DP0 pid=82)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:27.2845200Z [0;36m(EngineCore_DP0 pid=82)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 1279, in __init__
2026-02-05T20:13:27.2854821Z [0;36m(EngineCore_DP0 pid=82)[0;0m     super().__init__(
2026-02-05T20:13:27.2864800Z [0;36m(EngineCore_DP0 pid=82)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 691, in __init__
2026-02-05T20:13:27.2873839Z [0;36m(EngineCore_DP0 pid=82)[0;0m     super().__init__(
2026-02-05T20:13:27.2884601Z [0;36m(EngineCore_DP0 pid=82)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 105, in __init__
2026-02-05T20:13:27.2894381Z [0;36m(EngineCore_DP0 pid=82)[0;0m     self.model_executor = executor_class(vllm_config)
2026-02-05T20:13:27.2903715Z [0;36m(EngineCore_DP0 pid=82)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:27.2913817Z [0;36m(EngineCore_DP0 pid=82)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
2026-02-05T20:13:27.2923985Z [0;36m(EngineCore_DP0 pid=82)[0;0m     super().__init__(vllm_config)
2026-02-05T20:13:27.2934525Z [0;36m(EngineCore_DP0 pid=82)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
2026-02-05T20:13:27.2944747Z [0;36m(EngineCore_DP0 pid=82)[0;0m     self._init_executor()
2026-02-05T20:13:27.2954100Z [0;36m(EngineCore_DP0 pid=82)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
2026-02-05T20:13:27.2964483Z [0;36m(EngineCore_DP0 pid=82)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
2026-02-05T20:13:27.2973716Z [0;36m(EngineCore_DP0 pid=82)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:27.2984526Z [0;36m(EngineCore_DP0 pid=82)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
2026-02-05T20:13:27.2992758Z [0;36m(EngineCore_DP0 pid=82)[0;0m     raise e from None
2026-02-05T20:13:27.3003780Z [0;36m(EngineCore_DP0 pid=82)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
2026-02-05T20:13:27.4868019Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946] EngineCore failed to start.
2026-02-05T20:13:27.4877508Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946] Traceback (most recent call last):
2026-02-05T20:13:27.4888197Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 929, in run_engine_core
2026-02-05T20:13:27.4897197Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946]     engine_core = DPEngineCoreProc(*args, **kwargs)
2026-02-05T20:13:27.4905488Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:27.4915518Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 1279, in __init__
2026-02-05T20:13:27.4925745Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946]     super().__init__(
2026-02-05T20:13:27.4935234Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 691, in __init__
2026-02-05T20:13:27.4944576Z [0;36m(EngineCore_DP1 pid=101)[0;0m Process EngineCore_DP1:
2026-02-05T20:13:27.4954743Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946]     super().__init__(
2026-02-05T20:13:27.4966343Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 105, in __init__
2026-02-05T20:13:27.4976460Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946]     self.model_executor = executor_class(vllm_config)
2026-02-05T20:13:27.4985985Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:27.4994937Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
2026-02-05T20:13:27.5005026Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946]     super().__init__(vllm_config)
2026-02-05T20:13:27.5014845Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
2026-02-05T20:13:27.5023656Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946]     self._init_executor()
2026-02-05T20:13:27.5033782Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
2026-02-05T20:13:27.5043676Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946]     self.workers = WorkerProc.wait_for_ready(unready_workers)
2026-02-05T20:13:27.5052984Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:27.5062472Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
2026-02-05T20:13:27.5071525Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946]     raise e from None
2026-02-05T20:13:27.5082183Z [0;36m(EngineCore_DP1 pid=101)[0;0m ERROR 02-05 20:13:27 [core.py:946] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
2026-02-05T20:13:27.5092419Z [0;36m(EngineCore_DP1 pid=101)[0;0m Traceback (most recent call last):
2026-02-05T20:13:27.5101828Z [0;36m(EngineCore_DP1 pid=101)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-05T20:13:27.5111104Z [0;36m(EngineCore_DP1 pid=101)[0;0m     self.run()
2026-02-05T20:13:27.5120864Z [0;36m(EngineCore_DP1 pid=101)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-05T20:13:27.5130054Z [0;36m(EngineCore_DP1 pid=101)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-05T20:13:27.5139264Z [0;36m(EngineCore_DP1 pid=101)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 950, in run_engine_core
2026-02-05T20:13:27.5148016Z [0;36m(EngineCore_DP1 pid=101)[0;0m     raise e
2026-02-05T20:13:27.5157611Z [0;36m(EngineCore_DP1 pid=101)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 929, in run_engine_core
2026-02-05T20:13:27.5167202Z [0;36m(EngineCore_DP1 pid=101)[0;0m     engine_core = DPEngineCoreProc(*args, **kwargs)
2026-02-05T20:13:27.5176668Z [0;36m(EngineCore_DP1 pid=101)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:27.5186511Z [0;36m(EngineCore_DP1 pid=101)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 1279, in __init__
2026-02-05T20:13:27.5196145Z [0;36m(EngineCore_DP1 pid=101)[0;0m     super().__init__(
2026-02-05T20:13:27.5206474Z [0;36m(EngineCore_DP1 pid=101)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 691, in __init__
2026-02-05T20:13:27.5215426Z [0;36m(EngineCore_DP1 pid=101)[0;0m     super().__init__(
2026-02-05T20:13:27.5225163Z [0;36m(EngineCore_DP1 pid=101)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 105, in __init__
2026-02-05T20:13:27.5234042Z [0;36m(EngineCore_DP1 pid=101)[0;0m     self.model_executor = executor_class(vllm_config)
2026-02-05T20:13:27.5243825Z [0;36m(EngineCore_DP1 pid=101)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:27.5253757Z [0;36m(EngineCore_DP1 pid=101)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
2026-02-05T20:13:27.5262897Z [0;36m(EngineCore_DP1 pid=101)[0;0m     super().__init__(vllm_config)
2026-02-05T20:13:27.5271899Z [0;36m(EngineCore_DP1 pid=101)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
2026-02-05T20:13:27.5281608Z [0;36m(EngineCore_DP1 pid=101)[0;0m     self._init_executor()
2026-02-05T20:13:27.5291647Z [0;36m(EngineCore_DP1 pid=101)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
2026-02-05T20:13:27.5300802Z [0;36m(EngineCore_DP1 pid=101)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
2026-02-05T20:13:27.5309923Z [0;36m(EngineCore_DP1 pid=101)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-05T20:13:27.5320488Z [0;36m(EngineCore_DP1 pid=101)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 678, in wait_for_ready
2026-02-05T20:13:27.5329736Z [0;36m(EngineCore_DP1 pid=101)[0;0m     raise e from None
2026-02-05T20:13:27.5339552Z [0;36m(EngineCore_DP1 pid=101)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
2026-02-05T20:13:29.3594666Z Traceback (most recent call last):
2026-02-05T20:13:29.3602587Z   File "/usr/local/python3.11.14/bin/vllm", line 7, in <module>
2026-02-05T20:13:29.3625756Z     sys.exit(main())
2026-02-05T20:13:29.3626095Z              ^^^^^^
2026-02-05T20:13:29.3631521Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/main.py", line 73, in main
2026-02-05T20:13:29.3640832Z     args.dispatch_function(args)
2026-02-05T20:13:29.3650403Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 108, in cmd
2026-02-05T20:13:29.3660867Z     run_multi_api_server(args)
2026-02-05T20:13:29.3669537Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 248, in run_multi_api_server
2026-02-05T20:13:29.3679180Z     with launch_core_engines(
2026-02-05T20:13:29.3688582Z   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 144, in __exit__
2026-02-05T20:13:29.3698018Z     next(self.gen)
2026-02-05T20:13:29.3706981Z   File "/vllm-workspace/vllm/vllm/v1/engine/utils.py", line 933, in launch_core_engines
2026-02-05T20:13:29.3717325Z     wait_for_engine_startup(
2026-02-05T20:13:29.3726319Z   File "/vllm-workspace/vllm/vllm/v1/engine/utils.py", line 992, in wait_for_engine_startup
2026-02-05T20:13:29.3736685Z     raise RuntimeError(
2026-02-05T20:13:29.3745926Z RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
2026-02-05T20:13:30.1861736Z [ERROR] 2026-02-05-20:13:29 (PID:66, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
2026-02-05T20:13:30.4714689Z sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-05T20:13:32.0077888Z /usr/local/python3.11.14/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 18 leaked shared_memory objects to clean up at shutdown
2026-02-05T20:13:32.0091572Z   warnings.warn('resource_tracker: There appear to be %d '
2026-02-05T20:13:35.4396095Z FAILED
2026-02-05T20:13:35.4408240Z 
2026-02-05T20:13:35.4418761Z =================================== FAILURES ===================================
2026-02-05T20:13:35.4429770Z _______________________________ test_multi_node ________________________________
2026-02-05T20:13:35.4442424Z 
2026-02-05T20:13:35.4454142Z     @pytest.mark.asyncio
2026-02-05T20:13:35.4463069Z     async def test_multi_node() -> None:
2026-02-05T20:13:35.4471567Z         config = MultiNodeConfigLoader.from_yaml()
2026-02-05T20:13:35.4481589Z     
2026-02-05T20:13:35.4492163Z         with ProxyLauncher(
2026-02-05T20:13:35.4502931Z                 nodes=config.nodes,
2026-02-05T20:13:35.4519553Z                 disagg_cfg=config.disagg_cfg,
2026-02-05T20:13:35.4530080Z                 envs=config.envs,
2026-02-05T20:13:35.4540371Z                 proxy_port=config.proxy_port,
2026-02-05T20:13:35.4561721Z                 cur_index=config.cur_index,
2026-02-05T20:13:35.4562454Z         ) as proxy:
2026-02-05T20:13:35.4573972Z     
2026-02-05T20:13:35.4583464Z >           with RemoteOpenAIServer(
2026-02-05T20:13:35.4594529Z                     model=config.model,
2026-02-05T20:13:35.4604220Z                     vllm_serve_args=config.server_cmd,
2026-02-05T20:13:35.4614322Z                     server_port=config.server_port,
2026-02-05T20:13:35.4626142Z                     server_host=config.master_ip,
2026-02-05T20:13:35.4635449Z                     env_dict=config.envs,
2026-02-05T20:13:35.4645493Z                     auto_port=False,
2026-02-05T20:13:35.4654914Z                     proxy_port=proxy.proxy_port,
2026-02-05T20:13:35.4664698Z                     disaggregated_prefill=config.disagg_cfg,
2026-02-05T20:13:35.4675403Z                     nodes_info=config.nodes,
2026-02-05T20:13:35.4685250Z                     max_wait_seconds=2800,
2026-02-05T20:13:35.4694799Z             ) as server:
2026-02-05T20:13:35.4703312Z 
2026-02-05T20:13:35.4713420Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py:21: 
2026-02-05T20:13:35.4723645Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-05T20:13:35.4733294Z tests/e2e/conftest.py:297: in __init__
2026-02-05T20:13:35.4742957Z     self._wait_for_multiple_servers(
2026-02-05T20:13:35.4752977Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-05T20:13:35.4762672Z 
2026-02-05T20:13:35.4773697Z self = <tests.e2e.conftest.RemoteOpenAIServer object at 0xffff1b3db150>
2026-02-05T20:13:35.4783051Z targets = [('10.0.0.139', 'http://10.0.0.139:8080/health')], timeout = 2800
2026-02-05T20:13:35.4792804Z log_interval = 30.0
2026-02-05T20:13:35.4803638Z 
2026-02-05T20:13:35.4813538Z     def _wait_for_multiple_servers(self,
2026-02-05T20:13:35.4823097Z                                    targets,
2026-02-05T20:13:35.4832731Z                                    timeout: float,
2026-02-05T20:13:35.4843188Z                                    log_interval: float = 30.0):
2026-02-05T20:13:35.4852926Z         """
2026-02-05T20:13:35.4861557Z         targets: List[(node_ip, url)]
2026-02-05T20:13:35.4871192Z         log_interval
2026-02-05T20:13:35.4881234Z         """
2026-02-05T20:13:35.4890867Z         start = time.time()
2026-02-05T20:13:35.4900866Z         client = requests
2026-02-05T20:13:35.4909995Z     
2026-02-05T20:13:35.4920003Z         ready = {node_ip: False for node_ip, _ in targets}
2026-02-05T20:13:35.4930309Z     
2026-02-05T20:13:35.4940085Z         last_log_time = 0.0
2026-02-05T20:13:35.4948952Z     
2026-02-05T20:13:35.4958683Z         while True:
2026-02-05T20:13:35.4968211Z             now = time.time()
2026-02-05T20:13:35.4978153Z             all_ready = True
2026-02-05T20:13:35.4988537Z             should_log = (now - last_log_time) >= log_interval
2026-02-05T20:13:35.4998222Z     
2026-02-05T20:13:35.5007475Z             for node_ip, url in targets:
2026-02-05T20:13:35.5017271Z                 if ready[node_ip]:
2026-02-05T20:13:35.5027011Z                     continue
2026-02-05T20:13:35.5037225Z     
2026-02-05T20:13:35.5046399Z                 try:
2026-02-05T20:13:35.5055776Z                     resp = client.get(url)
2026-02-05T20:13:35.5065564Z                     if resp.status_code == 200:
2026-02-05T20:13:35.5075440Z                         ready[node_ip] = True
2026-02-05T20:13:35.5086451Z                         logger.info(f"[READY] Node {node_ip} is ready.")
2026-02-05T20:13:35.5096094Z                 except RequestException:
2026-02-05T20:13:35.5106109Z                     all_ready = False
2026-02-05T20:13:35.5168442Z                     if should_log:
2026-02-05T20:13:35.5168775Z                         logger.debug(f"[WAIT] {url}: connection failed")
2026-02-05T20:13:35.5169081Z     
2026-02-05T20:13:35.5169268Z                     # check unexpected exit
2026-02-05T20:13:35.5169545Z                     result = self._poll()
2026-02-05T20:13:35.5169845Z                     if result is not None and result != 0:
2026-02-05T20:13:35.5173902Z >                       raise RuntimeError(
2026-02-05T20:13:35.5183533Z                             f"Server at {node_ip} exited unexpectedly."
2026-02-05T20:13:35.5192932Z                         ) from None
2026-02-05T20:13:35.5203385Z E                       RuntimeError: Server at 10.0.0.139 exited unexpectedly.
2026-02-05T20:13:35.5212133Z 
2026-02-05T20:13:35.5222263Z tests/e2e/conftest.py:390: RuntimeError
2026-02-05T20:13:35.5231876Z =============================== warnings summary ===============================
2026-02-05T20:13:35.5241383Z <frozen importlib._bootstrap>:241
2026-02-05T20:13:35.5251467Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute
2026-02-05T20:13:35.5260516Z 
2026-02-05T20:13:35.5269817Z <frozen importlib._bootstrap>:241
2026-02-05T20:13:35.5287499Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute
2026-02-05T20:13:35.5287853Z 
2026-02-05T20:13:35.5298589Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2026-02-05T20:13:35.5307516Z =========================== short test summary info ============================
2026-02-05T20:13:35.5317127Z FAILED tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node
2026-02-05T20:13:35.5327077Z ================== 1 failed, 2 warnings in 256.12s (0:04:16) ===================
2026-02-05T20:13:37.1290041Z [0;31mFAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml âœ— ERROR: Some tests failed, please check the error stack above for details. If this is insufficient to pinpoint the error, please download and review the logs of all other nodes from the job's summary.[0m
2026-02-05T20:13:37.3597614Z Cleaning up background log streams...
2026-02-05T20:13:37.4292909Z ##[error]Error: failed to run script step: Error: command terminated with non-zero exit code: command terminated with exit code 1
2026-02-05T20:13:37.4333250Z ##[error]Process completed with exit code 1.
2026-02-05T20:13:37.4420350Z ##[error]Executing the custom container implementation failed. Please contact your self hosted runner administrator.
2026-02-05T20:13:37.4814807Z ##[group]Run actions/upload-artifact@v6
2026-02-05T20:13:37.4815098Z with:
2026-02-05T20:13:37.4815364Z   name: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs
2026-02-05T20:13:37.4815672Z   path: /tmp/vllm*_logs.txt
2026-02-05T20:13:37.4815933Z   retention-days: 7
2026-02-05T20:13:37.4816123Z   if-no-files-found: warn
2026-02-05T20:13:37.4816395Z   compression-level: 6
2026-02-05T20:13:37.4816627Z   overwrite: false
2026-02-05T20:13:37.4816898Z   include-hidden-files: false
2026-02-05T20:13:37.4817265Z env:
2026-02-05T20:13:37.4817531Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-05T20:13:37.4817794Z ##[endgroup]
2026-02-05T20:13:37.4844610Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-05T20:13:37.4845399Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-05T20:13:37.4845719Z ##[endgroup]
2026-02-05T20:13:37.8414019Z (node:11712) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-05T20:13:37.8414787Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-05T20:13:56.8203586Z With the provided path, there will be 1 file uploaded
2026-02-05T20:13:56.8207853Z Artifact name is valid!
2026-02-05T20:13:56.8208248Z Root directory input is valid!
2026-02-05T20:13:57.8325757Z Beginning upload of artifact content to blob storage
2026-02-05T20:13:59.6367656Z Uploaded bytes 15753
2026-02-05T20:14:00.0111087Z Finished uploading artifact content to blob storage!
2026-02-05T20:14:00.0111788Z SHA256 digest of uploaded artifact zip is f05fd6a88e3eb6ab664df2d4fe8a487c240a5f623b90d09a7d8d1d3a0466a55e
2026-02-05T20:14:00.0112343Z Finalizing artifact upload
2026-02-05T20:14:00.8973913Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs.zip successfully finalized. Artifact ID 5396543613
2026-02-05T20:14:00.8974831Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs has been successfully uploaded! Final size is 15753 bytes. Artifact ID is 5396543613
2026-02-05T20:14:00.8978521Z Artifact download URL: https://github.com/vllm-project/vllm-ascend/actions/runs/21719776452/artifacts/5396543613
2026-02-05T20:14:19.7093047Z ##[group]Run kubectl get pods -n "$NAMESPACE" --ignore-not-found=true
2026-02-05T20:14:19.7093449Z [36;1mkubectl get pods -n "$NAMESPACE" --ignore-not-found=true[0m
2026-02-05T20:14:19.7093784Z [36;1mkubectl delete -f ./lws.yaml --ignore-not-found=true || true[0m
2026-02-05T20:14:19.7094113Z shell: bash -el {0}
2026-02-05T20:14:19.7094264Z env:
2026-02-05T20:14:19.7094459Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-05T20:14:19.7094698Z ##[endgroup]
2026-02-05T20:14:19.7161861Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-05T20:14:19.7162660Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-05T20:14:19.7162881Z ##[endgroup]
2026-02-05T20:14:20.0722265Z (node:13369) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-05T20:14:20.0723042Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-05T20:14:38.9249668Z NAME                                             READY   STATUS    RESTARTS      AGE
2026-02-05T20:14:38.9250169Z linux-aarch64-a3-0-wgt9d-runner-x975h            1/1     Running   0             10m
2026-02-05T20:14:38.9250609Z linux-aarch64-a3-0-wgt9d-runner-x975h-workflow   1/1     Running   0             9m31s
2026-02-05T20:14:38.9250970Z vllm-0                                           1/1     Running   1 (61s ago)   6m4s
2026-02-05T20:14:38.9251258Z vllm-0-1                                         1/1     Running   0             6m4s
2026-02-05T20:14:38.9909429Z leaderworkerset.leaderworkerset.x-k8s.io "vllm" deleted from vllm-project namespace
2026-02-05T20:14:39.0184071Z service "vllm-leader" deleted from vllm-project namespace
2026-02-05T20:14:57.9624175Z Post job cleanup.
2026-02-05T20:14:57.9647274Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-05T20:14:57.9647996Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-05T20:14:57.9648273Z ##[endgroup]
2026-02-05T20:14:58.3153082Z (node:15205) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-05T20:14:58.3153791Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-05T20:15:17.7072081Z [command]/usr/bin/git version
2026-02-05T20:15:17.7341061Z git version 2.34.1
2026-02-05T20:15:17.7367120Z Copying '/root/.gitconfig' to '/__w/_temp/1206df0f-6cb2-4c59-8067-e3f8f825e44a/.gitconfig'
2026-02-05T20:15:17.7374521Z Temporarily overriding HOME='/__w/_temp/1206df0f-6cb2-4c59-8067-e3f8f825e44a' before making global git config changes
2026-02-05T20:15:17.7375065Z Adding repository directory to the temporary git global config as a safe directory
2026-02-05T20:15:17.7378324Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-05T20:15:17.7417900Z Removing SSH command configuration
2026-02-05T20:15:17.7422305Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-05T20:15:17.7481081Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-05T20:15:17.7974333Z Removing HTTP extra header
2026-02-05T20:15:17.7976842Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-05T20:15:17.8002052Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-05T20:15:17.8229991Z Removing includeIf entries pointing to credentials config files
2026-02-05T20:15:17.8230364Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-05T20:15:17.8230711Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-05T20:15:17.8231725Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-05T20:15:17.8232143Z includeif.gitdir:/github/workspace/.git.path
2026-02-05T20:15:17.8232418Z includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-05T20:15:17.8233080Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-05T20:15:17.8233533Z /__w/_temp/git-credentials-f5578cd7-0628-4dca-aec5-0eea296b0b50.config
2026-02-05T20:15:17.8235676Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-f5578cd7-0628-4dca-aec5-0eea296b0b50.config
2026-02-05T20:15:17.8267274Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-05T20:15:17.8286184Z /__w/_temp/git-credentials-f5578cd7-0628-4dca-aec5-0eea296b0b50.config
2026-02-05T20:15:17.8291474Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-f5578cd7-0628-4dca-aec5-0eea296b0b50.config
2026-02-05T20:15:17.8316759Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git.path
2026-02-05T20:15:17.8333896Z /github/runner_temp/git-credentials-f5578cd7-0628-4dca-aec5-0eea296b0b50.config
2026-02-05T20:15:17.8340471Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-f5578cd7-0628-4dca-aec5-0eea296b0b50.config
2026-02-05T20:15:17.8369723Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-05T20:15:17.8387063Z /github/runner_temp/git-credentials-f5578cd7-0628-4dca-aec5-0eea296b0b50.config
2026-02-05T20:15:17.8392809Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-f5578cd7-0628-4dca-aec5-0eea296b0b50.config
2026-02-05T20:15:17.8420101Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-05T20:15:17.8592226Z Removing credentials config '/__w/_temp/git-credentials-f5578cd7-0628-4dca-aec5-0eea296b0b50.config'
2026-02-05T20:15:36.4420498Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-05T20:15:36.4421214Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-05T20:15:36.4421443Z ##[endgroup]
2026-02-05T20:15:36.8826207Z Cleaning up orphan processes
