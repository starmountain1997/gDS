# Run ID: 22260012431
# Commit: f0caeeadcb37261beebd4a6e32934fa9f460db98
# Job: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
# Date: 2026-02-21
============================================================

ï»¿2026-02-21T18:43:26.4766241Z Current runner version: '2.330.0'
2026-02-21T18:43:26.4771061Z Runner name: 'linux-aarch64-a3-0-n4cwm-runner-nngvl'
2026-02-21T18:43:26.4771793Z Runner group name: 'Default'
2026-02-21T18:43:26.4772833Z Machine name: 'linux-aarch64-a3-0-n4cwm-runner-nngvl'
2026-02-21T18:43:26.4776235Z ##[group]GITHUB_TOKEN Permissions
2026-02-21T18:43:26.4778288Z Actions: write
2026-02-21T18:43:26.4778757Z ArtifactMetadata: write
2026-02-21T18:43:26.4779141Z Attestations: write
2026-02-21T18:43:26.4779540Z Checks: write
2026-02-21T18:43:26.4779932Z Contents: write
2026-02-21T18:43:26.4780335Z Deployments: write
2026-02-21T18:43:26.4780744Z Discussions: write
2026-02-21T18:43:26.4781135Z Issues: write
2026-02-21T18:43:26.4781557Z Metadata: read
2026-02-21T18:43:26.4781938Z Models: read
2026-02-21T18:43:26.4782445Z Packages: write
2026-02-21T18:43:26.4782848Z Pages: write
2026-02-21T18:43:26.4783231Z PullRequests: write
2026-02-21T18:43:26.4783599Z RepositoryProjects: write
2026-02-21T18:43:26.4784172Z SecurityEvents: write
2026-02-21T18:43:26.4784618Z Statuses: write
2026-02-21T18:43:26.4785001Z ##[endgroup]
2026-02-21T18:43:26.4786661Z Secret source: Actions
2026-02-21T18:43:26.4787377Z Prepare workflow directory
2026-02-21T18:43:26.5345007Z Prepare all required actions
2026-02-21T18:43:26.5376430Z Getting action download info
2026-02-21T18:43:27.9154481Z Download action repository 'actions/checkout@v6' (SHA:de0fac2e4500dabe0009e67214ff5f5447ce83dd)
2026-02-21T18:43:32.2517447Z Download action repository 'actions/upload-artifact@v6' (SHA:b7c566a772e6b6bfb58ed0dc250532a479d7789f)
2026-02-21T18:43:40.0377901Z Uses: vllm-project/vllm-ascend/.github/workflows/_e2e_nightly_multi_node.yaml@refs/heads/main (f0caeeadcb37261beebd4a6e32934fa9f460db98)
2026-02-21T18:43:40.0381410Z ##[group] Inputs
2026-02-21T18:43:40.0381801Z   soc_version: a3
2026-02-21T18:43:40.0382203Z   runner: linux-aarch64-a3-0
2026-02-21T18:43:40.0382593Z   image: swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3
2026-02-21T18:43:40.0383071Z   config_file_path: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-21T18:43:40.0383373Z   replicas: 1
2026-02-21T18:43:40.0383542Z   size: 2
2026-02-21T18:43:40.0383765Z   vllm_version: v0.15.0
2026-02-21T18:43:40.0384110Z   vllm_ascend_remote_url: https://github.com/vllm-project/vllm-ascend.git
2026-02-21T18:43:40.0384408Z   vllm_ascend_ref: main
2026-02-21T18:43:40.0384659Z ##[endgroup]
2026-02-21T18:43:40.0385147Z Complete job name: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-21T18:43:40.0878808Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-21T18:43:40.0881310Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-21T18:43:40.0881922Z ##[endgroup]
2026-02-21T18:43:55.6009452Z (node:70) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-21T18:43:55.6010241Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-21T18:43:57.4309025Z ##[group]Run # Decode and save kubeconfig
2026-02-21T18:43:57.4309508Z [36;1m# Decode and save kubeconfig[0m
2026-02-21T18:43:57.4341757Z [36;1mecho "***" | base64 -d > "$KUBECONFIG"[0m
2026-02-21T18:43:57.4342407Z shell: bash -el {0}
2026-02-21T18:43:57.4342633Z ##[endgroup]
2026-02-21T18:43:57.4456989Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-21T18:43:57.4458049Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-21T18:43:57.4458330Z ##[endgroup]
2026-02-21T18:43:57.7968346Z (node:401) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-21T18:43:57.7969224Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-21T18:43:58.6701704Z ##[group]Run actions/checkout@v6
2026-02-21T18:43:58.6702247Z with:
2026-02-21T18:43:58.6702561Z   repository: vllm-project/vllm-ascend
2026-02-21T18:43:58.6703277Z   token: ***
2026-02-21T18:43:58.6703483Z   ssh-strict: true
2026-02-21T18:43:58.6703741Z   ssh-user: git
2026-02-21T18:43:58.6703979Z   persist-credentials: true
2026-02-21T18:43:58.6704191Z   clean: true
2026-02-21T18:43:58.6704526Z   sparse-checkout-cone-mode: true
2026-02-21T18:43:58.6704797Z   fetch-depth: 1
2026-02-21T18:43:58.6704976Z   fetch-tags: false
2026-02-21T18:43:58.6705219Z   show-progress: true
2026-02-21T18:43:58.6705442Z   lfs: false
2026-02-21T18:43:58.6705708Z   submodules: false
2026-02-21T18:43:58.6705965Z   set-safe-directory: true
2026-02-21T18:43:58.6706184Z ##[endgroup]
2026-02-21T18:43:58.6747653Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-21T18:43:58.6748588Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-21T18:43:58.6748858Z ##[endgroup]
2026-02-21T18:43:59.0299806Z (node:432) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-21T18:43:59.0300617Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-21T18:43:59.6072227Z Syncing repository: vllm-project/vllm-ascend
2026-02-21T18:43:59.6073446Z ##[group]Getting Git version info
2026-02-21T18:43:59.6073829Z Working directory is '/__w/vllm-ascend/vllm-ascend'
2026-02-21T18:43:59.6074300Z [command]/usr/bin/git version
2026-02-21T18:43:59.6074514Z git version 2.34.1
2026-02-21T18:43:59.6076071Z ##[endgroup]
2026-02-21T18:43:59.6079029Z Copying '/root/.gitconfig' to '/__w/_temp/888d3bb5-3b96-40c4-ae7a-6ae2a711b786/.gitconfig'
2026-02-21T18:43:59.6085446Z Temporarily overriding HOME='/__w/_temp/888d3bb5-3b96-40c4-ae7a-6ae2a711b786' before making global git config changes
2026-02-21T18:43:59.6086250Z Adding repository directory to the temporary git global config as a safe directory
2026-02-21T18:43:59.6088601Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-21T18:43:59.6119259Z Deleting the contents of '/__w/vllm-ascend/vllm-ascend'
2026-02-21T18:43:59.6121768Z ##[group]Initializing the repository
2026-02-21T18:43:59.6126965Z [command]/usr/bin/git init /__w/vllm-ascend/vllm-ascend
2026-02-21T18:43:59.6256418Z hint: Using 'master' as the name for the initial branch. This default branch name
2026-02-21T18:43:59.6256833Z hint: is subject to change. To configure the initial branch name to use in all
2026-02-21T18:43:59.6257249Z hint: of your new repositories, which will suppress this warning, call:
2026-02-21T18:43:59.6257567Z hint: 
2026-02-21T18:43:59.6257828Z hint: 	git config --global init.defaultBranch <name>
2026-02-21T18:43:59.6258103Z hint: 
2026-02-21T18:43:59.6258385Z hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and
2026-02-21T18:43:59.6258780Z hint: 'development'. The just-created branch can be renamed via this command:
2026-02-21T18:43:59.6259114Z hint: 
2026-02-21T18:43:59.6259302Z hint: 	git branch -m <name>
2026-02-21T18:43:59.6265648Z Initialized empty Git repository in /__w/vllm-ascend/vllm-ascend/.git/
2026-02-21T18:43:59.6273703Z [command]/usr/bin/git remote add origin https://github.com/vllm-project/vllm-ascend
2026-02-21T18:43:59.6319177Z ##[endgroup]
2026-02-21T18:43:59.6319533Z ##[group]Disabling automatic garbage collection
2026-02-21T18:43:59.6322591Z [command]/usr/bin/git config --local gc.auto 0
2026-02-21T18:43:59.6346258Z ##[endgroup]
2026-02-21T18:43:59.6346575Z ##[group]Setting up auth
2026-02-21T18:43:59.6349315Z Removing SSH command configuration
2026-02-21T18:43:59.6353530Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-21T18:43:59.6380817Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-21T18:43:59.6564353Z Removing HTTP extra header
2026-02-21T18:43:59.6567742Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-21T18:43:59.6593049Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-21T18:43:59.6767135Z Removing includeIf entries pointing to credentials config files
2026-02-21T18:43:59.6770956Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-21T18:43:59.6797533Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-21T18:43:59.6980383Z [command]/usr/bin/git config --file /__w/_temp/git-credentials-3840c8d1-6969-4afe-9435-6d2577271682.config http.https://github.com/.extraheader AUTHORIZATION: basic ***
2026-02-21T18:43:59.7012121Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-3840c8d1-6969-4afe-9435-6d2577271682.config
2026-02-21T18:43:59.7042278Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-3840c8d1-6969-4afe-9435-6d2577271682.config
2026-02-21T18:43:59.7067737Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-3840c8d1-6969-4afe-9435-6d2577271682.config
2026-02-21T18:43:59.7099365Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-3840c8d1-6969-4afe-9435-6d2577271682.config
2026-02-21T18:43:59.7121899Z ##[endgroup]
2026-02-21T18:43:59.7122456Z ##[group]Fetching the repository
2026-02-21T18:43:59.7129781Z [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --no-recurse-submodules --depth=1 origin +f0caeeadcb37261beebd4a6e32934fa9f460db98:refs/remotes/origin/main
2026-02-21T18:44:01.5398354Z From https://gh-proxy.test.osinfra.cn/https://github.com/vllm-project/vllm-ascend
2026-02-21T18:44:01.5399106Z  * [new ref]         f0caeeadcb37261beebd4a6e32934fa9f460db98 -> origin/main
2026-02-21T18:44:01.5419038Z [command]/usr/bin/git branch --list --remote origin/main
2026-02-21T18:44:01.5440923Z   origin/main
2026-02-21T18:44:01.5448070Z [command]/usr/bin/git rev-parse refs/remotes/origin/main
2026-02-21T18:44:01.5467836Z f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-21T18:44:01.5471716Z ##[endgroup]
2026-02-21T18:44:01.5472172Z ##[group]Determining the checkout info
2026-02-21T18:44:01.5474103Z ##[endgroup]
2026-02-21T18:44:01.5477632Z [command]/usr/bin/git sparse-checkout disable
2026-02-21T18:44:01.5511575Z [command]/usr/bin/git config --local --unset-all extensions.worktreeConfig
2026-02-21T18:44:01.5537516Z ##[group]Checking out the ref
2026-02-21T18:44:01.5538603Z [command]/usr/bin/git checkout --progress --force -B main refs/remotes/origin/main
2026-02-21T18:44:01.6388043Z Switched to a new branch 'main'
2026-02-21T18:44:01.6388464Z Branch 'main' set up to track remote branch 'main' from 'origin'.
2026-02-21T18:44:01.6398339Z ##[endgroup]
2026-02-21T18:44:01.6429423Z [command]/usr/bin/git log -1 --format=%H
2026-02-21T18:44:01.6449622Z f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-21T18:44:03.2318904Z ##[group]Run # prepare for lws entrypoint scripts
2026-02-21T18:44:03.2319313Z [36;1m# prepare for lws entrypoint scripts[0m
2026-02-21T18:44:03.2319689Z [36;1minstall -D tests/e2e/nightly/multi_node/scripts/run.sh /root/.cache/tests/run.sh[0m
2026-02-21T18:44:03.2320238Z shell: bash -el {0}
2026-02-21T18:44:03.2320471Z ##[endgroup]
2026-02-21T18:44:03.2428275Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-21T18:44:03.2429133Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-21T18:44:03.2429433Z ##[endgroup]
2026-02-21T18:44:03.5928302Z (node:488) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-21T18:44:03.9553448Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-21T18:44:04.5982278Z ##[group]Run set -euo pipefail
2026-02-21T18:44:04.5982623Z [36;1mset -euo pipefail[0m
2026-02-21T18:44:04.5983066Z [36;1m[0m
2026-02-21T18:44:04.5983299Z [36;1mCRD_NAME="${CRD_NAME:-vllm}"[0m
2026-02-21T18:44:04.5983601Z [36;1mTIMEOUT=${TIMEOUT:-120}[0m
2026-02-21T18:44:04.5983835Z [36;1mSLEEP_INTERVAL=2[0m
2026-02-21T18:44:04.5984075Z [36;1m[0m
2026-02-21T18:44:04.5984427Z [36;1mecho "Deleting leaderworkerset [$CRD_NAME] in namespace [$NAMESPACE]..."[0m
2026-02-21T18:44:04.5984867Z [36;1mkubectl delete leaderworkerset "$CRD_NAME" -n "$NAMESPACE" --ignore-not-found[0m
2026-02-21T18:44:04.5985354Z [36;1m[0m
2026-02-21T18:44:04.5985643Z [36;1mecho "Waiting for all pods starting with 'vllm' to be deleted..."[0m
2026-02-21T18:44:04.5986069Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-21T18:44:04.5986327Z [36;1m[0m
2026-02-21T18:44:04.5986539Z [36;1mwhile true; do[0m
2026-02-21T18:44:04.5986777Z [36;1m  NOW=$(date +%s)[0m
2026-02-21T18:44:04.5987137Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-21T18:44:04.5987444Z [36;1m[0m
2026-02-21T18:44:04.5987694Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-21T18:44:04.5988048Z [36;1m    echo "Timeout reached ($TIMEOUT seconds), some pods still exist:"[0m
2026-02-21T18:44:04.5988431Z [36;1m    kubectl get pods -n "$NAMESPACE" | grep '^vllm' || true[0m
2026-02-21T18:44:04.5988837Z [36;1m    exit 1[0m
2026-02-21T18:44:04.5989090Z [36;1m  fi[0m
2026-02-21T18:44:04.5989253Z [36;1m[0m
2026-02-21T18:44:04.5989750Z [36;1m  PODS_EXIST=$(kubectl get pods -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null | tr ' ' '\n' | grep '^vllm' || true)[0m
2026-02-21T18:44:04.5990214Z [36;1m[0m
2026-02-21T18:44:04.5990414Z [36;1m  if [[ -z "$PODS_EXIST" ]]; then[0m
2026-02-21T18:44:04.5990719Z [36;1m    echo "All vllm pods deleted."[0m
2026-02-21T18:44:04.5990949Z [36;1m    break[0m
2026-02-21T18:44:04.5991172Z [36;1m  else[0m
2026-02-21T18:44:04.5991478Z [36;1m    echo "Waiting for pods to be deleted: $PODS_EXIST"[0m
2026-02-21T18:44:04.5991759Z [36;1m    sleep $SLEEP_INTERVAL[0m
2026-02-21T18:44:04.5992129Z [36;1m  fi[0m
2026-02-21T18:44:04.5992422Z [36;1mdone[0m
2026-02-21T18:44:04.5992753Z shell: bash -el {0}
2026-02-21T18:44:04.5992963Z ##[endgroup]
2026-02-21T18:44:04.6123468Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-21T18:44:04.6124331Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-21T18:44:04.6124597Z ##[endgroup]
2026-02-21T18:44:04.9678439Z (node:542) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-21T18:44:04.9679614Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-21T18:44:05.4898272Z Deleting leaderworkerset [vllm] in namespace [vllm-project]...
2026-02-21T18:44:05.5685424Z Waiting for all pods starting with 'vllm' to be deleted...
2026-02-21T18:44:05.6364491Z All vllm pods deleted.
2026-02-21T18:44:06.0343347Z ##[group]Run set -e
2026-02-21T18:44:06.0343661Z [36;1mset -e[0m
2026-02-21T18:44:06.0343952Z [36;1m[0m
2026-02-21T18:44:06.0344159Z [36;1msize="2"[0m
2026-02-21T18:44:06.0344359Z [36;1mreplicas="1"[0m
2026-02-21T18:44:06.0344731Z [36;1mimage="swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3"[0m
2026-02-21T18:44:06.0345212Z [36;1mconfig_file_path="DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-21T18:44:06.0345572Z [36;1mfail_tag=FAIL_TAG_"DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-21T18:44:06.0345908Z [36;1mecho "FAIL_TAG=${fail_tag}" >> "$GITHUB_ENV"[0m
2026-02-21T18:44:06.0346175Z [36;1m[0m
2026-02-21T18:44:06.0346430Z [36;1mrequired_params=("size" "replicas" "image" "config_file_path")[0m
2026-02-21T18:44:06.0346771Z [36;1mfor param in "${required_params[@]}"; do[0m
2026-02-21T18:44:06.0347059Z [36;1m  if [ -z "${!param}" ]; then[0m
2026-02-21T18:44:06.0347700Z [36;1m    echo "Error: Parameter '$param' is required but empty"[0m
2026-02-21T18:44:06.0348018Z [36;1m    exit 1[0m
2026-02-21T18:44:06.0348199Z [36;1m  fi[0m
2026-02-21T18:44:06.0348405Z [36;1mdone[0m
2026-02-21T18:44:06.0348613Z [36;1m[0m
2026-02-21T18:44:06.0348787Z [36;1mif [ "a3" = "a3" ]; then[0m
2026-02-21T18:44:06.0349037Z [36;1m  npu_per_node=16[0m
2026-02-21T18:44:06.0349368Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws.yaml.jinja2"[0m
2026-02-21T18:44:06.0349671Z [36;1melse[0m
2026-02-21T18:44:06.0349877Z [36;1m  npu_per_node=8[0m
2026-02-21T18:44:06.0350214Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws-a2.yaml.jinja2"[0m
2026-02-21T18:44:06.0350529Z [36;1mfi[0m
2026-02-21T18:44:06.0350799Z [36;1m[0m
2026-02-21T18:44:06.0351017Z [36;1mjinja2 $TEMPLATE_FILE \[0m
2026-02-21T18:44:06.0351253Z [36;1m  -D size="$size" \[0m
2026-02-21T18:44:06.0351492Z [36;1m  -D replicas="$replicas" \[0m
2026-02-21T18:44:06.0351758Z [36;1m  -D image="$image" \[0m
2026-02-21T18:44:06.0352122Z [36;1m  -D config_file_path="$config_file_path" \[0m
2026-02-21T18:44:06.0352436Z [36;1m  -D npu_per_node="$npu_per_node" \[0m
2026-02-21T18:44:06.0352714Z [36;1m  -D fail_tag="$fail_tag" \[0m
2026-02-21T18:44:06.0352945Z [36;1m  --outfile lws.yaml[0m
2026-02-21T18:44:06.0353167Z [36;1m[0m
2026-02-21T18:44:06.0353359Z [36;1mkubectl apply -f ./lws.yaml[0m
2026-02-21T18:44:06.0353779Z shell: bash -el {0}
2026-02-21T18:44:06.0354114Z ##[endgroup]
2026-02-21T18:44:06.0430477Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-21T18:44:06.0431415Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-21T18:44:06.0431691Z ##[endgroup]
2026-02-21T18:44:06.4000630Z (node:608) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-21T18:44:06.4001638Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-21T18:44:07.3538340Z leaderworkerset.leaderworkerset.x-k8s.io/vllm created
2026-02-21T18:44:07.3742053Z service/vllm-leader created
2026-02-21T18:44:07.7950497Z ##[group]Run POD_PREFIX="${POD_PREFIX:-vllm-0}"
2026-02-21T18:44:07.7950934Z [36;1mPOD_PREFIX="${POD_PREFIX:-vllm-0}"[0m
2026-02-21T18:44:07.7951180Z [36;1mSIZE="2"[0m
2026-02-21T18:44:07.7951446Z [36;1mTIMEOUT=1200  # default timeout 20 minutes[0m
2026-02-21T18:44:07.7951729Z [36;1m[0m
2026-02-21T18:44:07.7952263Z [36;1mecho "Waiting for Pods in namespace [$NAMESPACE] to become Running and Ready (timeout ${TIMEOUT}s)..."[0m
2026-02-21T18:44:07.7952702Z [36;1m[0m
2026-02-21T18:44:07.7952918Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-21T18:44:07.7953128Z [36;1m[0m
2026-02-21T18:44:07.7953423Z [36;1mwhile true; do[0m
2026-02-21T18:44:07.7953655Z [36;1m  NOW=$(date +%s)[0m
2026-02-21T18:44:07.7953883Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-21T18:44:07.7954182Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-21T18:44:07.7954517Z [36;1m    echo "Timeout reached after ${ELAPSED}s"[0m
2026-02-21T18:44:07.7954895Z [36;1m    echo "Dumping pod status for debugging:"[0m
2026-02-21T18:44:07.7955197Z [36;1m    kubectl get pods -n "$NAMESPACE"[0m
2026-02-21T18:44:07.7955496Z [36;1m    kubectl describe pod "$LEADER_POD" -n "$NAMESPACE"[0m
2026-02-21T18:44:07.7955802Z [36;1m    exit 1[0m
2026-02-21T18:44:07.7956029Z [36;1m  fi[0m
2026-02-21T18:44:07.7956210Z [36;1m[0m
2026-02-21T18:44:07.7956493Z [36;1m  # 1) check follower pods[0m
2026-02-21T18:44:07.7956772Z [36;1m  ALL_FOLLOWERS_READY=true[0m
2026-02-21T18:44:07.7957003Z [36;1m  for ((i=1; i<SIZE; i++)); do[0m
2026-02-21T18:44:07.7957269Z [36;1m    POD="${POD_PREFIX}-${i}"[0m
2026-02-21T18:44:07.7957704Z [36;1m    PHASE=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-21T18:44:07.7958368Z [36;1m    READY=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-21T18:44:07.7958899Z [36;1m[0m
2026-02-21T18:44:07.7959166Z [36;1m    echo "Follower [$POD] phase=$PHASE ready=$READY"[0m
2026-02-21T18:44:07.7959488Z [36;1m[0m
2026-02-21T18:44:07.7959728Z [36;1m    if [[ "$PHASE" != "Running" || "$READY" != "true" ]]; then[0m
2026-02-21T18:44:07.7960081Z [36;1m      echo "Follower [$POD] not Ready yet..."[0m
2026-02-21T18:44:07.7960337Z [36;1m      ALL_FOLLOWERS_READY=false[0m
2026-02-21T18:44:07.7960650Z [36;1m      break[0m
2026-02-21T18:44:07.7960844Z [36;1m    fi[0m
2026-02-21T18:44:07.7961053Z [36;1m  done[0m
2026-02-21T18:44:07.7961244Z [36;1m[0m
2026-02-21T18:44:07.7961439Z [36;1m  # 2) check leader pod[0m
2026-02-21T18:44:07.7961882Z [36;1m  LEADER_PHASE=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-21T18:44:07.7962668Z [36;1m  LEADER_READY=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-21T18:44:07.7963135Z [36;1m[0m
2026-02-21T18:44:07.7963392Z [36;1m  echo "Leader [$LEADER_POD] phase=$LEADER_PHASE ready=$LEADER_READY"[0m
2026-02-21T18:44:07.7963717Z [36;1m[0m
2026-02-21T18:44:07.7963985Z [36;1m  if [[ "$LEADER_PHASE" != "Running" || "$LEADER_READY" != "true" ]]; then[0m
2026-02-21T18:44:07.7964406Z [36;1m    echo "Leader not Ready yet..."[0m
2026-02-21T18:44:07.7964671Z [36;1m    ALL_FOLLOWERS_READY=false[0m
2026-02-21T18:44:07.7964909Z [36;1m  fi[0m
2026-02-21T18:44:07.7965111Z [36;1m[0m
2026-02-21T18:44:07.7965333Z [36;1m  if [[ "$ALL_FOLLOWERS_READY" == "true" ]]; then[0m
2026-02-21T18:44:07.7965711Z [36;1m    echo "All follower pods and leader pod are Running and Ready â€” continuing."[0m
2026-02-21T18:44:07.7966068Z [36;1m    break[0m
2026-02-21T18:44:07.7966279Z [36;1m  fi[0m
2026-02-21T18:44:07.7966459Z [36;1m[0m
2026-02-21T18:44:07.7966653Z [36;1m  sleep 2[0m
2026-02-21T18:44:07.7966864Z [36;1mdone[0m
2026-02-21T18:44:07.7967183Z shell: bash -el {0}
2026-02-21T18:44:07.7967470Z env:
2026-02-21T18:44:07.7967825Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-21T18:44:07.7968206Z ##[endgroup]
2026-02-21T18:44:07.8040194Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-21T18:44:07.8041022Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-21T18:44:07.8041288Z ##[endgroup]
2026-02-21T18:44:08.1550557Z (node:686) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-21T18:44:08.1551418Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-21T18:44:08.6839111Z Waiting for Pods in namespace [vllm-project] to become Running and Ready (timeout 1200s)...
2026-02-21T18:44:08.8054287Z Follower [vllm-0-1] phase=Pending ready=
2026-02-21T18:44:08.8054627Z Follower [vllm-0-1] not Ready yet...
2026-02-21T18:44:08.9207539Z Leader [vllm-0] phase=Pending ready=
2026-02-21T18:44:08.9207828Z Leader not Ready yet...
2026-02-21T18:44:11.0346409Z Follower [vllm-0-1] phase=Pending ready=
2026-02-21T18:44:11.0346836Z Follower [vllm-0-1] not Ready yet...
2026-02-21T18:44:11.1429821Z Leader [vllm-0] phase=Pending ready=
2026-02-21T18:44:11.1430151Z Leader not Ready yet...
2026-02-21T18:44:13.2618470Z Follower [vllm-0-1] phase=Pending ready=
2026-02-21T18:44:13.2618811Z Follower [vllm-0-1] not Ready yet...
2026-02-21T18:44:13.3743266Z Leader [vllm-0] phase=Pending ready=
2026-02-21T18:44:13.3743599Z Leader not Ready yet...
2026-02-21T18:44:15.4915990Z Follower [vllm-0-1] phase=Pending ready=
2026-02-21T18:44:15.4916357Z Follower [vllm-0-1] not Ready yet...
2026-02-21T18:44:15.6116459Z Leader [vllm-0] phase=Pending ready=
2026-02-21T18:44:15.6116816Z Leader not Ready yet...
2026-02-21T18:44:17.7290161Z Follower [vllm-0-1] phase=Pending ready=
2026-02-21T18:44:17.7290493Z Follower [vllm-0-1] not Ready yet...
2026-02-21T18:44:17.8431871Z Leader [vllm-0] phase=Pending ready=
2026-02-21T18:44:17.8432543Z Leader not Ready yet...
2026-02-21T18:44:19.9642222Z Follower [vllm-0-1] phase=Pending ready=
2026-02-21T18:44:19.9642757Z Follower [vllm-0-1] not Ready yet...
2026-02-21T18:44:20.0850036Z Leader [vllm-0] phase=Pending ready=
2026-02-21T18:44:20.0850358Z Leader not Ready yet...
2026-02-21T18:44:22.2105698Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-21T18:44:22.2106031Z Follower [vllm-0-1] not Ready yet...
2026-02-21T18:44:22.3307105Z Leader [vllm-0] phase=Pending ready=
2026-02-21T18:44:22.3307411Z Leader not Ready yet...
2026-02-21T18:44:24.4524508Z Follower [vllm-0-1] phase=Running ready=true
2026-02-21T18:44:24.5731495Z Leader [vllm-0] phase=Pending ready=false
2026-02-21T18:44:24.5731963Z Leader not Ready yet...
2026-02-21T18:44:26.7015076Z Follower [vllm-0-1] phase=Running ready=true
2026-02-21T18:44:26.8226838Z Leader [vllm-0] phase=Running ready=true
2026-02-21T18:44:26.8227468Z All follower pods and leader pod are Running and Ready â€” continuing.
2026-02-21T18:44:27.2593253Z ##[group]Run set -euo pipefail
2026-02-21T18:44:27.2593722Z [36;1mset -euo pipefail[0m
2026-02-21T18:44:27.2593979Z [36;1m[0m
2026-02-21T18:44:27.2594155Z [36;1msize="2"[0m
2026-02-21T18:44:27.2594389Z [36;1mpids=()[0m
2026-02-21T18:44:27.2594701Z [36;1m[0m
2026-02-21T18:44:27.2594880Z [36;1mcleanup() {[0m
2026-02-21T18:44:27.2595165Z [36;1m  echo "Cleaning up background log streams..."[0m
2026-02-21T18:44:27.2595485Z [36;1m  for pid in "${pids[@]}"; do[0m
2026-02-21T18:44:27.2595740Z [36;1m    kill "$pid" 2>/dev/null || true[0m
2026-02-21T18:44:27.2596003Z [36;1m  done[0m
2026-02-21T18:44:27.2596209Z [36;1m}[0m
2026-02-21T18:44:27.2596461Z [36;1mtrap cleanup EXIT[0m
2026-02-21T18:44:27.2596699Z [36;1m[0m
2026-02-21T18:44:27.2596881Z [36;1mfor i in $(seq 1 $((size - 1))); do[0m
2026-02-21T18:44:27.2597165Z [36;1m  POD="vllm-0-${i}"[0m
2026-02-21T18:44:27.2597451Z [36;1m[0m
2026-02-21T18:44:27.2597776Z [36;1m  echo "==== Collecting logs from worker pod: $POD ===="[0m
2026-02-21T18:44:27.2598106Z [36;1m  kubectl logs -f "$POD" -n "$NAMESPACE" \[0m
2026-02-21T18:44:27.2598427Z [36;1m    > "/tmp/${POD}_logs.txt" 2>&1 &[0m
2026-02-21T18:44:27.2598669Z [36;1m[0m
2026-02-21T18:44:27.2598868Z [36;1m  pids+=($!)[0m
2026-02-21T18:44:27.2599045Z [36;1mdone[0m
2026-02-21T18:44:27.2599273Z [36;1m[0m
2026-02-21T18:44:27.2599591Z [36;1mecho "==== Streaming logs from leader pod: $LEADER_POD ===="[0m
2026-02-21T18:44:27.2599893Z [36;1mecho "Looking for logs containing: $FAIL_TAG"[0m
2026-02-21T18:44:27.2600192Z [36;1m[0m
2026-02-21T18:44:27.2600493Z [36;1mkubectl logs -f "$LEADER_POD" -n "$NAMESPACE" | while IFS= read -r line; do[0m
2026-02-21T18:44:27.2600817Z [36;1m  echo "$line"[0m
2026-02-21T18:44:27.2601112Z [36;1m  if echo "$line" | grep -q "$FAIL_TAG"; then[0m
2026-02-21T18:44:27.2601389Z [36;1m    exit 1[0m
2026-02-21T18:44:27.2601559Z [36;1m  fi[0m
2026-02-21T18:44:27.2601786Z [36;1mdone[0m
2026-02-21T18:44:27.2602290Z shell: bash -el {0}
2026-02-21T18:44:27.2602478Z env:
2026-02-21T18:44:27.2602828Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-21T18:44:27.2603093Z ##[endgroup]
2026-02-21T18:44:27.2708770Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-21T18:44:27.2709601Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-21T18:44:27.2709949Z ##[endgroup]
2026-02-21T18:44:27.6250365Z (node:780) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-21T18:44:27.6251142Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-21T18:44:28.1599220Z ==== Collecting logs from worker pod: vllm-0-1 ====
2026-02-21T18:44:28.1600122Z ==== Streaming logs from leader pod: vllm-0 ====
2026-02-21T18:44:28.1600547Z Looking for logs containing: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-21T18:44:28.2413074Z /usr/local/Ascend/ascend-toolkit/set_env.sh: line 31: CMAKE_PREFIX_PATH: unbound variable
2026-02-21T18:44:28.2423452Z ====> Check NPU info
2026-02-21T18:44:28.2434007Z +------------------------------------------------------------------------------------------------+
2026-02-21T18:44:28.2446706Z | npu-smi 25.5.0                   Version: 25.5.0                                               |
2026-02-21T18:44:28.2456243Z +---------------------------+---------------+----------------------------------------------------+
2026-02-21T18:44:28.2465671Z | NPU   Name                | Health        | Power(W)    Temp(C)           Hugepages-Usage(page)|
2026-02-21T18:44:28.2475779Z | Chip  Phy-ID              | Bus-Id        | AICore(%)   Memory-Usage(MB)  HBM-Usage(MB)        |
2026-02-21T18:44:28.2486538Z +===========================+===============+====================================================+
2026-02-21T18:44:28.2494657Z | 0     Ascend910           | OK            | 167.7       36                0    / 0             |
2026-02-21T18:44:28.2504756Z | 0     0                   | 0000:9D:00.0  | 0           0    / 0          3169 / 65536         |
2026-02-21T18:44:28.2514299Z +------------------------------------------------------------------------------------------------+
2026-02-21T18:44:28.2525621Z | 0     Ascend910           | OK            | -           36                0    / 0             |
2026-02-21T18:44:28.2534695Z | 1     1                   | 0000:9F:00.0  | 0           0    / 0          2888 / 65536         |
2026-02-21T18:44:28.2544736Z +===========================+===============+====================================================+
2026-02-21T18:44:28.2554591Z | 1     Ascend910           | OK            | 167.0       35                0    / 0             |
2026-02-21T18:44:28.2564662Z | 0     2                   | 0000:99:00.0  | 0           0    / 0          3156 / 65536         |
2026-02-21T18:44:28.2573920Z +------------------------------------------------------------------------------------------------+
2026-02-21T18:44:28.2583942Z | 1     Ascend910           | OK            | -           36                0    / 0             |
2026-02-21T18:44:28.2593657Z | 1     3                   | 0000:9B:00.0  | 0           0    / 0          2892 / 65536         |
2026-02-21T18:44:28.2603175Z +===========================+===============+====================================================+
2026-02-21T18:44:28.2613644Z | 2     Ascend910           | OK            | 167.6       37                0    / 0             |
2026-02-21T18:44:28.2622558Z | 0     4                   | 0000:95:00.0  | 0           0    / 0          3161 / 65536         |
2026-02-21T18:44:28.2632304Z +------------------------------------------------------------------------------------------------+
2026-02-21T18:44:28.2642528Z | 2     Ascend910           | OK            | -           35                0    / 0             |
2026-02-21T18:44:28.2652400Z | 1     5                   | 0000:97:00.0  | 0           0    / 0          2886 / 65536         |
2026-02-21T18:44:28.2661604Z +===========================+===============+====================================================+
2026-02-21T18:44:28.2671809Z | 3     Ascend910           | OK            | 155.2       35                0    / 0             |
2026-02-21T18:44:28.2680965Z | 0     6                   | 0000:91:00.0  | 0           0    / 0          3161 / 65536         |
2026-02-21T18:44:28.2690806Z +------------------------------------------------------------------------------------------------+
2026-02-21T18:44:28.2700355Z | 3     Ascend910           | OK            | -           34                0    / 0             |
2026-02-21T18:44:28.2710047Z | 1     7                   | 0000:93:00.0  | 0           0    / 0          2887 / 65536         |
2026-02-21T18:44:28.2720578Z +===========================+===============+====================================================+
2026-02-21T18:44:28.2730339Z | 4     Ascend910           | OK            | 164.3       36                0    / 0             |
2026-02-21T18:44:28.2739968Z | 0     8                   | 0000:8D:00.0  | 0           0    / 0          3167 / 65536         |
2026-02-21T18:44:28.2749184Z +------------------------------------------------------------------------------------------------+
2026-02-21T18:44:28.2760287Z | 4     Ascend910           | OK            | -           35                0    / 0             |
2026-02-21T18:44:28.2770682Z | 1     9                   | 0000:8F:00.0  | 0           0    / 0          2886 / 65536         |
2026-02-21T18:44:28.2780513Z +===========================+===============+====================================================+
2026-02-21T18:44:28.2790272Z | 5     Ascend910           | OK            | 169.5       36                0    / 0             |
2026-02-21T18:44:28.2800722Z | 0     10                  | 0000:89:00.0  | 0           0    / 0          3151 / 65536         |
2026-02-21T18:44:28.2810120Z +------------------------------------------------------------------------------------------------+
2026-02-21T18:44:28.2819426Z | 5     Ascend910           | OK            | -           37                0    / 0             |
2026-02-21T18:44:28.2829488Z | 1     11                  | 0000:8B:00.0  | 0           0    / 0          2897 / 65536         |
2026-02-21T18:44:28.2839656Z +===========================+===============+====================================================+
2026-02-21T18:44:28.2849272Z | 6     Ascend910           | OK            | 163.8       36                0    / 0             |
2026-02-21T18:44:28.2858998Z | 0     12                  | 0000:85:00.0  | 0           0    / 0          3164 / 65536         |
2026-02-21T18:44:28.2868349Z +------------------------------------------------------------------------------------------------+
2026-02-21T18:44:28.2878457Z | 6     Ascend910           | OK            | -           35                0    / 0             |
2026-02-21T18:44:28.2888303Z | 1     13                  | 0000:87:00.0  | 0           0    / 0          2889 / 65536         |
2026-02-21T18:44:28.2898439Z +===========================+===============+====================================================+
2026-02-21T18:44:28.2908555Z | 7     Ascend910           | OK            | 163.9       34                0    / 0             |
2026-02-21T18:44:28.2918677Z | 0     14                  | 0000:81:00.0  | 0           0    / 0          3149 / 65536         |
2026-02-21T18:44:28.2928538Z +------------------------------------------------------------------------------------------------+
2026-02-21T18:44:28.2938418Z | 7     Ascend910           | OK            | -           36                0    / 0             |
2026-02-21T18:44:28.2948364Z | 1     15                  | 0000:83:00.0  | 0           0    / 0          2898 / 65536         |
2026-02-21T18:44:28.2958419Z +===========================+===============+====================================================+
2026-02-21T18:44:28.2968731Z +---------------------------+---------------+----------------------------------------------------+
2026-02-21T18:44:28.2978239Z | NPU     Chip              | Process id    | Process name             | Process memory(MB)      |
2026-02-21T18:44:28.2988183Z +===========================+===============+====================================================+
2026-02-21T18:44:28.2998806Z | No running processes found in NPU 0                                                            |
2026-02-21T18:44:28.3009744Z +===========================+===============+====================================================+
2026-02-21T18:44:28.3019293Z | No running processes found in NPU 1                                                            |
2026-02-21T18:44:28.3028581Z +===========================+===============+====================================================+
2026-02-21T18:44:28.3038379Z | No running processes found in NPU 2                                                            |
2026-02-21T18:44:28.3048061Z +===========================+===============+====================================================+
2026-02-21T18:44:28.3058117Z | No running processes found in NPU 3                                                            |
2026-02-21T18:44:28.3067492Z +===========================+===============+====================================================+
2026-02-21T18:44:28.3078753Z | No running processes found in NPU 4                                                            |
2026-02-21T18:44:28.3088566Z +===========================+===============+====================================================+
2026-02-21T18:44:28.3098163Z | No running processes found in NPU 5                                                            |
2026-02-21T18:44:28.3107555Z +===========================+===============+====================================================+
2026-02-21T18:44:28.3117437Z | No running processes found in NPU 6                                                            |
2026-02-21T18:44:28.3127966Z +===========================+===============+====================================================+
2026-02-21T18:44:28.3137337Z | No running processes found in NPU 7                                                            |
2026-02-21T18:44:28.3146221Z +===========================+===============+====================================================+
2026-02-21T18:44:28.3155225Z package_name=Ascend-cann-toolkit
2026-02-21T18:44:28.3165754Z version=8.5.0
2026-02-21T18:44:28.3175229Z innerversion=V100R001C25SPC001B232
2026-02-21T18:44:28.3184224Z compatible_version=[V100R001C15],[V100R001C18],[V100R001C19],[V100R001C20],[V100R001C21],[V100R001C23]
2026-02-21T18:44:28.3193136Z arch=aarch64
2026-02-21T18:44:28.3202387Z os=linux
2026-02-21T18:44:28.3211892Z path=/usr/local/Ascend/cann-8.5.0
2026-02-21T18:44:28.3222053Z ====> Configure mirrors and git proxy
2026-02-21T18:44:28.3230784Z Writing to /root/.config/pip/pip.conf
2026-02-21T18:44:28.3240009Z Installed vLLM-related Python packages:
2026-02-21T18:44:28.3251101Z ais_bench_benchmark               3.0.20250930                /vllm-workspace/vllm-ascend/benchmark
2026-02-21T18:44:28.3261014Z vllm                              0.15.0+empty                /vllm-workspace/vllm
2026-02-21T18:44:28.3271014Z vllm_ascend                       0.14.0rc2.dev171+gf0caeeadc /vllm-workspace/vllm-ascend
2026-02-21T18:44:28.3280174Z 
2026-02-21T18:44:28.3290169Z ============================
2026-02-21T18:44:28.3299896Z vLLM Git information
2026-02-21T18:44:28.3308895Z ============================
2026-02-21T18:44:28.3318491Z Branch:      HEAD
2026-02-21T18:44:28.3328650Z Commit hash: f176443446f659dbab5315e056e605d8984fd976
2026-02-21T18:44:28.3338140Z Author:      TJian <tunjian.tan@embeddedllm.com>
2026-02-21T18:44:28.3347465Z Date:        2026-01-29 14:45:42 +0800
2026-02-21T18:44:28.3356920Z Message:     [Release] [CI] Optim release pipeline (#33156)
2026-02-21T18:44:28.3367343Z Tags:        v0.15.0
2026-02-21T18:44:28.3377001Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm.git (fetch)
2026-02-21T18:44:28.3387131Z 
2026-02-21T18:44:28.3395793Z 
2026-02-21T18:44:28.3407334Z ============================
2026-02-21T18:44:28.3415269Z vLLM-Ascend Git information
2026-02-21T18:44:28.3424816Z ============================
2026-02-21T18:44:28.3434211Z Branch:      main
2026-02-21T18:44:28.3444278Z Commit hash: f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-21T18:44:28.3454288Z Author:      Nengjun Ma <nengjunma@outlook.com>
2026-02-21T18:44:28.3463378Z Date:        2026-02-14 18:54:04 +0800
2026-02-21T18:44:28.3473676Z Message:     [CI] unlock when load model (#6771)
2026-02-21T18:44:28.3483483Z Tags:        
2026-02-21T18:44:28.3493434Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm-ascend (fetch)
2026-02-21T18:44:28.3502962Z 
2026-02-21T18:44:28.3512487Z ====> Check triton ascend info
2026-02-21T18:44:28.3522253Z Ubuntu clang version 15.0.7
2026-02-21T18:44:28.3532122Z Target: aarch64-unknown-linux-gnu
2026-02-21T18:44:28.3541582Z Thread model: posix
2026-02-21T18:44:28.3551137Z InstalledDir: /usr/bin
2026-02-21T18:44:28.3561695Z Found candidate GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-21T18:44:28.3571214Z Selected GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-21T18:44:28.3580333Z Candidate multilib: .;@m64
2026-02-21T18:44:28.3590231Z Selected multilib: .;@m64
2026-02-21T18:44:28.3600264Z /usr/local/Ascend/cann-8.5.0/tools/bishengir/bin/bishengir-compile
2026-02-21T18:44:28.3610669Z Name: triton-ascend
2026-02-21T18:44:28.3619951Z Version: 3.2.0
2026-02-21T18:44:28.3631216Z Summary: A language and compiler for custom Deep Learning operations on Ascend hardwares
2026-02-21T18:44:28.3639804Z Home-page: https://gitcode.com/Ascend/triton-ascend/
2026-02-21T18:44:28.3649672Z Author: 
2026-02-21T18:44:28.3659633Z Author-email: 
2026-02-21T18:44:28.3668945Z License: 
2026-02-21T18:44:28.3678844Z Location: /usr/local/python3.11.14/lib/python3.11/site-packages
2026-02-21T18:44:28.3688415Z Requires: 
2026-02-21T18:44:28.3698001Z Required-by: vllm_ascend
2026-02-21T18:44:39.2651720Z INFO 02-21 18:44:39 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:44:39.2659256Z INFO 02-21 18:44:39 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:44:39.2670223Z INFO 02-21 18:44:39 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:44:39.3076873Z INFO 02-21 18:44:39 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:44:43.6275291Z ============================= test session starts ==============================
2026-02-21T18:44:43.6284222Z platform linux -- Python 3.11.14, pytest-8.4.2, pluggy-1.6.0 -- /usr/local/python3.11.14/bin/python3
2026-02-21T18:44:43.6294497Z cachedir: .pytest_cache
2026-02-21T18:44:43.6304221Z rootdir: /vllm-workspace/vllm-ascend
2026-02-21T18:44:43.6314489Z configfile: pyproject.toml
2026-02-21T18:44:43.6325387Z plugins: asyncio-1.3.0, cov-7.0.0, mock-3.15.1, anyio-4.12.1
2026-02-21T18:44:43.6335497Z asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
2026-02-21T18:44:43.9447937Z collecting ... collected 1 item
2026-02-21T18:44:43.9455620Z 
2026-02-21T18:44:43.9470313Z [2026-02-21 18:44:43] INFO multi_node_config.py:294: Loading config yaml: tests/e2e/nightly/multi_node/config/DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-21T18:44:43.9507241Z [2026-02-21 18:44:43] INFO multi_node_config.py:351: Resolving cluster IPs via DNS...
2026-02-21T18:44:43.9574841Z [2026-02-21 18:44:43] INFO multi_node_config.py:212: Node 0 envs: {'HCCL_OP_EXPANSION_MODE': 'AIV', 'VLLM_USE_MODELSCOPE': 'True', 'HCCL_BUFFSIZE': '1024', 'SERVER_PORT': '8080', 'OMP_PROC_BIND': 'False', 'OMP_NUM_THREADS': '1', 'PYTORCH_NPU_ALLOC_CONF': 'expandable_segments:True', 'VLLM_ASCEND_ENABLE_FLASHCOMM1': '1', 'ASCEND_A3_EBA_ENABLE': '1', 'HCCL_IF_IP': '10.0.0.62', 'HCCL_SOCKET_IFNAME': 'eth0', 'GLOO_SOCKET_IFNAME': 'eth0', 'TP_SOCKET_IFNAME': 'eth0', 'LOCAL_IP': '10.0.0.62', 'NIC_NAME': 'eth0', 'MASTER_IP': '10.0.0.62'}
2026-02-21T18:44:43.9595928Z [2026-02-21 18:44:43] INFO multi_node_config.py:137: Not launching proxy on non-master node
2026-02-21T18:44:43.9609582Z [2026-02-21 18:44:43] INFO conftest.py:241: Starting server with command: vllm serve vllm-ascend/DeepSeek-V3.2-W8A8 --host 0.0.0.0 --port 8080 --data-parallel-size 4 --data-parallel-size-local 2 --data-parallel-address 10.0.0.62 --data-parallel-rpc-port 13399 --tensor-parallel-size 8 --quantization ascend --seed 1024 --enable-expert-parallel --max-num-seqs 16 --max-model-len 8192 --max-num-batched-tokens 4096 --no-enable-prefix-caching --gpu-memory-utilization 0.85 --trust-remote-code --speculative-config {"num_speculative_tokens": 2, "method":"deepseek_mtp"} --compilation-config {"cudagraph_capture_sizes": [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], "cudagraph_mode": "FULL_DECODE_ONLY"} --additional-config {"layer_sharding": ["q_b_proj", "o_proj"]} --tokenizer-mode deepseek_v32 --reasoning-parser deepseek_v3
2026-02-21T18:44:48.5659641Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node INFO 02-21 18:44:48 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:44:48.5670318Z INFO 02-21 18:44:48 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:44:48.5681582Z INFO 02-21 18:44:48 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:44:48.5725808Z INFO 02-21 18:44:48 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:44:54.8176582Z 2026-02-21 18:44:54,815 - 138 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:44:54.8486198Z INFO 02-21 18:44:54 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:44:54.9898253Z INFO 02-21 18:44:54 [serve.py:100] Defaulting api_server_count to data_parallel_size (4).
2026-02-21T18:44:54.9909413Z INFO 02-21 18:44:54 [utils.py:325] 
2026-02-21T18:44:54.9919973Z INFO 02-21 18:44:54 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
2026-02-21T18:44:54.9930028Z INFO 02-21 18:44:54 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
2026-02-21T18:44:54.9939653Z INFO 02-21 18:44:54 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   vllm-ascend/DeepSeek-V3.2-W8A8
2026-02-21T18:44:54.9950335Z INFO 02-21 18:44:54 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
2026-02-21T18:44:54.9959875Z INFO 02-21 18:44:54 [utils.py:325] 
2026-02-21T18:44:54.9982827Z INFO 02-21 18:44:54 [utils.py:261] non-default args: {'model_tag': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'api_server_count': 4, 'host': '0.0.0.0', 'port': 8080, 'model': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'tokenizer_mode': 'deepseek_v32', 'trust_remote_code': True, 'seed': 1024, 'max_model_len': 8192, 'quantization': 'ascend', 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 8, 'data_parallel_size': 4, 'data_parallel_size_local': 2, 'data_parallel_address': '10.0.0.62', 'data_parallel_rpc_port': 13399, 'enable_expert_parallel': True, 'gpu_memory_utilization': 0.85, 'enable_prefix_caching': False, 'max_num_batched_tokens': 4096, 'max_num_seqs': 16, 'speculative_config': {'num_speculative_tokens': 2, 'method': 'deepseek_mtp'}, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}, 'additional_config': {'layer_sharding': ['q_b_proj', 'o_proj']}}
2026-02-21T18:44:55.0510532Z 2026-02-21 18:44:55,049 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-21T18:44:55.0570479Z INFO 02-21 18:44:55 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-21T18:44:55.0588718Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:44:55.0637400Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:44:55.0656187Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:44:55.0666406Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-21T18:44:55.0888405Z INFO 02-21 18:44:55 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-21T18:44:55.0897559Z INFO 02-21 18:44:55 [model.py:1561] Using max model len 8192
2026-02-21T18:44:55.2939656Z WARNING 02-21 18:44:55 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-21T18:44:55.2959542Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:44:55.2969761Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:44:55.2979157Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-21T18:44:55.3060860Z INFO 02-21 18:44:55 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-21T18:44:55.3079315Z INFO 02-21 18:44:55 [model.py:1561] Using max model len 163840
2026-02-21T18:44:55.3092380Z WARNING 02-21 18:44:55 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-21T18:44:55.3101525Z INFO 02-21 18:44:55 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-21T18:44:55.6600886Z INFO 02-21 18:44:55 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-21T18:44:55.6613098Z INFO 02-21 18:44:55 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-21T18:44:55.6621808Z WARNING 02-21 18:44:55 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-21T18:44:55.6631930Z WARNING 02-21 18:44:55 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-21T18:44:55.6641250Z INFO 02-21 18:44:55 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:44:55.6651279Z INFO 02-21 18:44:55 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:44:55.6662164Z INFO 02-21 18:44:55 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:44:55.6672363Z WARNING 02-21 18:44:55 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-21T18:44:55.6682076Z INFO 02-21 18:44:55 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-21T18:44:55.6690838Z WARNING 02-21 18:44:55 [platform.py:335] [91m
2026-02-21T18:44:55.6700268Z WARNING 02-21 18:44:55 [platform.py:335]             **********************************************************************************
2026-02-21T18:44:55.6709643Z WARNING 02-21 18:44:55 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-21T18:44:55.6719622Z WARNING 02-21 18:44:55 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-21T18:44:55.6729106Z WARNING 02-21 18:44:55 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-21T18:44:55.6739567Z WARNING 02-21 18:44:55 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-21T18:44:55.6749124Z WARNING 02-21 18:44:55 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-21T18:44:55.6759350Z WARNING 02-21 18:44:55 [platform.py:335]             * batch size for graph capture.
2026-02-21T18:44:55.6769596Z WARNING 02-21 18:44:55 [platform.py:335]             * For more details, please refer to:
2026-02-21T18:44:55.6780553Z WARNING 02-21 18:44:55 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-21T18:44:55.6791804Z WARNING 02-21 18:44:55 [platform.py:335]             **********************************************************************************[0m
2026-02-21T18:44:55.6801779Z WARNING 02-21 18:44:55 [platform.py:335]             
2026-02-21T18:44:55.6811600Z INFO 02-21 18:44:55 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-21T18:44:55.6820759Z INFO 02-21 18:44:55 [utils.py:851] Started DP Coordinator process (PID: 151)
2026-02-21T18:45:00.0120043Z INFO 02-21 18:45:00 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:45:00.0128446Z INFO 02-21 18:45:00 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:45:00.0139158Z INFO 02-21 18:45:00 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:45:00.0188796Z INFO 02-21 18:45:00 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:45:00.0627922Z INFO 02-21 18:45:00 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:45:00.0636997Z INFO 02-21 18:45:00 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:45:00.0647258Z INFO 02-21 18:45:00 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:45:00.0702205Z INFO 02-21 18:45:00 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:45:14.8335166Z INFO 02-21 18:45:14 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:45:14.8462391Z INFO 02-21 18:45:14 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:45:14.8473125Z INFO 02-21 18:45:14 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:45:14.8482413Z INFO 02-21 18:45:14 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:45:21.2068912Z INFO 02-21 18:45:21 [utils.py:218] Started 4 API server processes
2026-02-21T18:45:26.1089606Z INFO 02-21 18:45:26 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:45:26.1099960Z INFO 02-21 18:45:26 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:45:26.1109960Z INFO 02-21 18:45:26 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:45:26.1173401Z INFO 02-21 18:45:26 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:45:26.2653669Z INFO 02-21 18:45:26 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:45:26.2661845Z INFO 02-21 18:45:26 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:45:26.2672106Z INFO 02-21 18:45:26 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:45:26.2729816Z INFO 02-21 18:45:26 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:45:26.3162678Z INFO 02-21 18:45:26 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:45:26.3171469Z INFO 02-21 18:45:26 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:45:26.3180364Z INFO 02-21 18:45:26 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:45:26.3241527Z INFO 02-21 18:45:26 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:45:26.3815436Z INFO 02-21 18:45:26 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:45:26.3824610Z INFO 02-21 18:45:26 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:45:26.3835188Z INFO 02-21 18:45:26 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:45:26.3979796Z INFO 02-21 18:45:26 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:45:30.9792121Z [0;36m(EngineCore_DP0 pid=154)[0;0m 2026-02-21 18:45:30,976 - 154 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:45:30.9813134Z [0;36m(EngineCore_DP1 pid=173)[0;0m 2026-02-21 18:45:30,977 - 173 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:45:30.9932639Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-21 18:45:30 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:45:30.9943300Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-21 18:45:30 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:45:30.9969089Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-21 18:45:30 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', speculative_config=SpeculativeConfig(method='mtp', model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', num_spec_tokens=2), tokenizer='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', skip_tokenizer_init=False, tokenizer_mode=deepseek_v32, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=4, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=bfloat16, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=1024, served_model_name=/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [24, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 48, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
2026-02-21T18:45:32.1735010Z [0;36m(ApiServer_1 pid=185)[0;0m 2026-02-21 18:45:32,171 - 185 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:45:32.1753881Z [0;36m(ApiServer_0 pid=184)[0;0m 2026-02-21 18:45:32,173 - 184 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:45:32.1889415Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-21 18:45:32 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:45:32.1911856Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-21 18:45:32 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:45:32.2388013Z [0;36m(ApiServer_1 pid=185)[0;0m 2026-02-21 18:45:32,237 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-21T18:45:32.2445640Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-21 18:45:32 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-21T18:45:32.2706902Z [0;36m(ApiServer_0 pid=184)[0;0m 2026-02-21 18:45:32,269 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-21T18:45:32.2768848Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-21 18:45:32 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-21T18:45:32.3478199Z [0;36m(ApiServer_1 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:45:32.3532286Z [0;36m(ApiServer_1 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:45:32.3553704Z [0;36m(ApiServer_1 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:45:32.3575392Z [0;36m(ApiServer_1 pid=185)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-21T18:45:32.3793480Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-21 18:45:32 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-21T18:45:32.3811865Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-21 18:45:32 [model.py:1561] Using max model len 8192
2026-02-21T18:45:32.3834443Z [0;36m(ApiServer_0 pid=184)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:45:32.3843460Z [0;36m(ApiServer_0 pid=184)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:45:32.3885465Z [0;36m(ApiServer_0 pid=184)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:45:32.3893725Z [0;36m(ApiServer_0 pid=184)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-21T18:45:32.3945718Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-21 18:45:32 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-21T18:45:32.3955603Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-21 18:45:32 [model.py:1561] Using max model len 8192
2026-02-21T18:45:32.4073507Z [0;36m(ApiServer_2 pid=186)[0;0m 2026-02-21 18:45:32,405 - 186 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:45:32.4234837Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-21 18:45:32 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:45:32.4405766Z [0;36m(ApiServer_2 pid=186)[0;0m 2026-02-21 18:45:32,438 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-21T18:45:32.4460230Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-21 18:45:32 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-21T18:45:32.5424485Z [0;36m(ApiServer_3 pid=187)[0;0m 2026-02-21 18:45:32,540 - 187 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:45:32.5447405Z [0;36m(ApiServer_2 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:45:32.5471806Z [0;36m(ApiServer_2 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:45:32.5482484Z [0;36m(ApiServer_2 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:45:32.5491935Z [0;36m(ApiServer_2 pid=186)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-21T18:45:32.5546607Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-21 18:45:32 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-21T18:45:32.5568184Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-21 18:45:32 [model.py:1561] Using max model len 8192
2026-02-21T18:45:32.5589241Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-21 18:45:32 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:45:32.5794425Z [0;36m(ApiServer_3 pid=187)[0;0m 2026-02-21 18:45:32,577 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-21T18:45:32.5856432Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-21 18:45:32 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-21T18:45:32.5971170Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-21 18:45:32 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-21T18:45:32.5994585Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-21 18:45:32 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-21T18:45:32.6004433Z [0;36m(ApiServer_0 pid=184)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:45:32.6014412Z [0;36m(ApiServer_1 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:45:32.6023953Z [0;36m(ApiServer_0 pid=184)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:45:32.6034609Z [0;36m(ApiServer_1 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:45:32.6045362Z [0;36m(ApiServer_0 pid=184)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-21T18:45:32.6055302Z [0;36m(ApiServer_1 pid=185)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-21T18:45:32.6087187Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-21 18:45:32 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-21T18:45:32.6096678Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-21 18:45:32 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-21T18:45:32.6109628Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-21 18:45:32 [model.py:1561] Using max model len 163840
2026-02-21T18:45:32.6120009Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-21 18:45:32 [model.py:1561] Using max model len 163840
2026-02-21T18:45:32.6129701Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-21 18:45:32 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-21T18:45:32.6140556Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-21 18:45:32 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-21T18:45:32.6149886Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-21 18:45:32 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-21T18:45:32.6159725Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-21 18:45:32 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-21T18:45:32.6657117Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-21 18:45:32 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-21T18:45:32.6679935Z [0;36m(ApiServer_2 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:45:32.6688969Z [0;36m(ApiServer_2 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:45:32.6698664Z [0;36m(ApiServer_2 pid=186)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-21T18:45:32.6742529Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-21 18:45:32 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-21T18:45:32.6764386Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-21 18:45:32 [model.py:1561] Using max model len 163840
2026-02-21T18:45:32.6775154Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-21 18:45:32 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-21T18:45:32.6784472Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-21 18:45:32 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-21T18:45:32.6846376Z [0;36m(ApiServer_3 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:45:32.6866420Z [0;36m(ApiServer_3 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:45:32.6886631Z [0;36m(ApiServer_3 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:45:32.6897649Z [0;36m(ApiServer_3 pid=187)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-21T18:45:32.6961571Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-21 18:45:32 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-21T18:45:32.6981709Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-21 18:45:32 [model.py:1561] Using max model len 8192
2026-02-21T18:45:32.7689146Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-21 18:45:32 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-21T18:45:32.7698294Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-21 18:45:32 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-21T18:45:32.7767329Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-21 18:45:32 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-21T18:45:32.7777391Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-21 18:45:32 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-21T18:45:32.7786625Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-21 18:45:32 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:45:32.7797099Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-21 18:45:32 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:45:32.7807329Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-21 18:45:32 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:45:32.7816956Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-21 18:45:32 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-21T18:45:32.7826054Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-21 18:45:32 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-21T18:45:32.7834799Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-21 18:45:32 [platform.py:335] [91m
2026-02-21T18:45:32.7844805Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             **********************************************************************************
2026-02-21T18:45:32.7853694Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-21T18:45:32.7862999Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-21T18:45:32.7873789Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-21T18:45:32.7883297Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-21T18:45:32.7892681Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-21T18:45:32.7902346Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * batch size for graph capture.
2026-02-21T18:45:32.7911210Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * For more details, please refer to:
2026-02-21T18:45:32.7921459Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-21T18:45:32.7931468Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             **********************************************************************************[0m
2026-02-21T18:45:32.7941183Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             
2026-02-21T18:45:32.7951121Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-21 18:45:32 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-21T18:45:32.7960074Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-21 18:45:32 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-21T18:45:32.7969383Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-21 18:45:32 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-21T18:45:32.7979393Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-21 18:45:32 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-21T18:45:32.7988534Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-21 18:45:32 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-21T18:45:32.7998642Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-21 18:45:32 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:45:32.8008418Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-21 18:45:32 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:45:32.8018799Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-21 18:45:32 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:45:32.8028173Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-21 18:45:32 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-21T18:45:32.8038191Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-21 18:45:32 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-21T18:45:32.8046949Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-21 18:45:32 [platform.py:335] [91m
2026-02-21T18:45:32.8056602Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             **********************************************************************************
2026-02-21T18:45:32.8066073Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-21T18:45:32.8075805Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-21T18:45:32.8086242Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-21T18:45:32.8095784Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-21T18:45:32.8105732Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-21T18:45:32.8115496Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * batch size for graph capture.
2026-02-21T18:45:32.8126334Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * For more details, please refer to:
2026-02-21T18:45:32.8135701Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-21T18:45:32.8145225Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             **********************************************************************************[0m
2026-02-21T18:45:32.8154854Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             
2026-02-21T18:45:32.8165043Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-21 18:45:32 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-21T18:45:32.8174684Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-21 18:45:32 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-21T18:45:32.8184661Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-21 18:45:32 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-21T18:45:32.8194697Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-21 18:45:32 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-21T18:45:32.8205757Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-21 18:45:32 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-21T18:45:32.8215290Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-21 18:45:32 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:45:32.8223719Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-21 18:45:32 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:45:32.8235334Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-21 18:45:32 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:45:32.8246232Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-21 18:45:32 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-21T18:45:32.8256058Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-21 18:45:32 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-21T18:45:32.8264875Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-21 18:45:32 [platform.py:335] [91m
2026-02-21T18:45:32.8274804Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             **********************************************************************************
2026-02-21T18:45:32.8285215Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-21T18:45:32.8294971Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-21T18:45:32.8304529Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-21T18:45:32.8315804Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-21T18:45:32.8325754Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-21T18:45:32.8334150Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * batch size for graph capture.
2026-02-21T18:45:32.8343260Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * For more details, please refer to:
2026-02-21T18:45:32.8353395Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-21T18:45:32.8363291Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             **********************************************************************************[0m
2026-02-21T18:45:32.8373662Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             
2026-02-21T18:45:32.8383332Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-21 18:45:32 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-21T18:45:32.8392635Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-21 18:45:32 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-21T18:45:32.8402490Z [0;36m(ApiServer_3 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:45:32.8412352Z [0;36m(ApiServer_3 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-21T18:45:32.8422570Z [0;36m(ApiServer_3 pid=187)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-21T18:45:32.8431770Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-21 18:45:32 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-21T18:45:32.8441637Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-21 18:45:32 [model.py:1561] Using max model len 163840
2026-02-21T18:45:32.8451809Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-21 18:45:32 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-21T18:45:32.8461539Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-21 18:45:32 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-21T18:45:32.9328603Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-21 18:45:32 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-21T18:45:32.9343883Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-21 18:45:32 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-21T18:45:32.9355587Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-21 18:45:32 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-21T18:45:32.9366194Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-21 18:45:32 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-21T18:45:32.9376603Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-21 18:45:32 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:45:32.9386938Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-21 18:45:32 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:45:32.9398289Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-21 18:45:32 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:45:32.9416093Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-21 18:45:32 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-21T18:45:32.9427589Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-21 18:45:32 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-21T18:45:32.9438753Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-21 18:45:32 [platform.py:335] [91m
2026-02-21T18:45:32.9454374Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             **********************************************************************************
2026-02-21T18:45:32.9465126Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-21T18:45:32.9475891Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-21T18:45:32.9486548Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-21T18:45:32.9496644Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-21T18:45:32.9507037Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-21T18:45:32.9517684Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * batch size for graph capture.
2026-02-21T18:45:32.9527814Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * For more details, please refer to:
2026-02-21T18:45:32.9537494Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-21T18:45:32.9547411Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             **********************************************************************************[0m
2026-02-21T18:45:32.9557676Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-21 18:45:32 [platform.py:335]             
2026-02-21T18:45:32.9567631Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-21 18:45:32 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-21T18:45:33.3996656Z [0;36m(ApiServer_3 pid=187)[0;0m Process ApiServer_3:
2026-02-21T18:45:33.4064581Z [0;36m(ApiServer_3 pid=187)[0;0m Traceback (most recent call last):
2026-02-21T18:45:33.4092307Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-21T18:45:33.4101603Z [0;36m(ApiServer_3 pid=187)[0;0m     self.run()
2026-02-21T18:45:33.4110891Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-21T18:45:33.4120390Z [0;36m(ApiServer_3 pid=187)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-21T18:45:33.4130561Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-21T18:45:33.4138568Z [0;36m(ApiServer_3 pid=187)[0;0m     uvloop.run(
2026-02-21T18:45:33.4148200Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-21T18:45:33.4157609Z [0;36m(ApiServer_3 pid=187)[0;0m     return runner.run(wrapper())
2026-02-21T18:45:33.4167351Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:45:33.4177499Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-21T18:45:33.4186712Z [0;36m(ApiServer_3 pid=187)[0;0m     return self._loop.run_until_complete(task)
2026-02-21T18:45:33.4196763Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:45:33.4207070Z [0;36m(ApiServer_3 pid=187)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-21T18:45:33.4216547Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-21T18:45:33.4225229Z [0;36m(ApiServer_3 pid=187)[0;0m     return await main
2026-02-21T18:45:33.4234483Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^
2026-02-21T18:45:33.4245447Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-21T18:45:33.4254970Z [0;36m(ApiServer_3 pid=187)[0;0m     async with build_async_engine_client(
2026-02-21T18:45:33.4264361Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-21T18:45:33.4274319Z [0;36m(ApiServer_3 pid=187)[0;0m     return await anext(self.gen)
2026-02-21T18:45:33.4285083Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:45:33.4297831Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-21T18:45:33.4307814Z [0;36m(ApiServer_3 pid=187)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-21T18:45:33.4316740Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-21T18:45:33.4326337Z [0;36m(ApiServer_3 pid=187)[0;0m     return await anext(self.gen)
2026-02-21T18:45:33.4335279Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:45:33.4345328Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-21T18:45:33.4354463Z [0;36m(ApiServer_3 pid=187)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-21T18:45:33.4365192Z [0;36m(ApiServer_3 pid=187)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:45:33.4375434Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-21T18:45:33.4384403Z [0;36m(ApiServer_3 pid=187)[0;0m     return cls(
2026-02-21T18:45:33.4394125Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^
2026-02-21T18:45:33.4404631Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-21T18:45:33.4413637Z [0;36m(ApiServer_3 pid=187)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-21T18:45:33.4422466Z [0;36m(ApiServer_3 pid=187)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:45:33.4431923Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-21T18:45:33.4441870Z [0;36m(ApiServer_3 pid=187)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-21T18:45:33.4451968Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:45:33.4461628Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-21T18:45:33.4471386Z [0;36m(ApiServer_3 pid=187)[0;0m     super().__init__(
2026-02-21T18:45:33.4481538Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-21T18:45:33.4489950Z [0;36m(ApiServer_3 pid=187)[0;0m     super().__init__(
2026-02-21T18:45:33.4499845Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-21T18:45:33.4509117Z [0;36m(ApiServer_3 pid=187)[0;0m     super().__init__(
2026-02-21T18:45:33.4520700Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 496, in __init__
2026-02-21T18:45:33.4529223Z [0;36m(ApiServer_3 pid=187)[0;0m     self.input_socket = self.resources.input_socket = make_zmq_socket(
2026-02-21T18:45:33.4538564Z [0;36m(ApiServer_3 pid=187)[0;0m                                                       ^^^^^^^^^^^^^^^^
2026-02-21T18:45:33.4547832Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/utils/network_utils.py", line 309, in make_zmq_socket
2026-02-21T18:45:33.4557368Z [0;36m(ApiServer_3 pid=187)[0;0m     socket.bind(path)
2026-02-21T18:45:33.4566840Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/zmq/sugar/socket.py", line 320, in bind
2026-02-21T18:45:33.4575990Z [0;36m(ApiServer_3 pid=187)[0;0m     super().bind(addr)
2026-02-21T18:45:33.4586090Z [0;36m(ApiServer_3 pid=187)[0;0m   File "zmq/backend/cython/_zmq.py", line 1009, in zmq.backend.cython._zmq.Socket.bind
2026-02-21T18:45:33.4595971Z [0;36m(ApiServer_3 pid=187)[0;0m   File "zmq/backend/cython/_zmq.py", line 190, in zmq.backend.cython._zmq._check_rc
2026-02-21T18:45:33.4606353Z [0;36m(ApiServer_3 pid=187)[0;0m zmq.error.ZMQError: Address already in use (addr='tcp://10.0.0.62:43203')
2026-02-21T18:45:33.7936086Z [0;36m(ApiServer_3 pid=187)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-21T18:45:35.6595859Z INFO 02-21 18:45:35 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:45:35.6605149Z INFO 02-21 18:45:35 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:45:35.6615312Z INFO 02-21 18:45:35 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:45:35.6666093Z INFO 02-21 18:45:35 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:45:35.7055875Z INFO 02-21 18:45:35 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:45:35.7066027Z INFO 02-21 18:45:35 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:45:35.7077541Z INFO 02-21 18:45:35 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:45:35.7129579Z INFO 02-21 18:45:35 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:45:40.7068758Z 2026-02-21 18:45:40,704 - 234 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:45:40.7123345Z INFO 02-21 18:45:40 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:45:40.7152489Z 2026-02-21 18:45:40,712 - 233 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:45:40.7178715Z INFO 02-21 18:45:40 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:45:42.7013298Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-21T18:45:42.7020047Z   warnings.warn(
2026-02-21T18:45:42.7030589Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-21T18:45:42.7039431Z   warnings.warn(
2026-02-21T18:45:45.3241479Z INFO 02-21 18:45:45 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:45:45.3251912Z INFO 02-21 18:45:45 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:45:45.3263009Z INFO 02-21 18:45:45 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:45:45.3331376Z INFO 02-21 18:45:45 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:45:45.4994352Z INFO 02-21 18:45:45 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:45:45.5004047Z INFO 02-21 18:45:45 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:45:45.5012927Z INFO 02-21 18:45:45 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:45:45.5085397Z INFO 02-21 18:45:45 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:45:46.7167969Z INFO 02-21 18:45:46 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:45:46.7175724Z INFO 02-21 18:45:46 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:45:46.7185211Z INFO 02-21 18:45:46 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:45:46.7195086Z INFO 02-21 18:45:46 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:45:46.7205849Z INFO 02-21 18:45:46 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:45:46.7215418Z INFO 02-21 18:45:46 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:45:46.7564085Z INFO 02-21 18:45:46 [parallel_state.py:1212] world_size=32 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.62:36027 backend=hccl
2026-02-21T18:45:46.7583340Z INFO 02-21 18:45:46 [parallel_state.py:1212] world_size=32 rank=8 local_rank=0 distributed_init_method=tcp://10.0.0.62:36027 backend=hccl
2026-02-21T18:45:50.4678290Z 2026-02-21 18:45:50,465 - 275 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:45:50.4736450Z INFO 02-21 18:45:50 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:45:50.5041766Z 2026-02-21 18:45:50,502 - 276 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:45:50.5079905Z INFO 02-21 18:45:50 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:45:51.8533484Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-21T18:45:51.8541547Z   warnings.warn(
2026-02-21T18:45:51.8652637Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-21T18:45:51.8660377Z   warnings.warn(
2026-02-21T18:45:53.9597906Z INFO 02-21 18:45:53 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:45:53.9607718Z INFO 02-21 18:45:53 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:45:53.9618964Z INFO 02-21 18:45:53 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:45:53.9790423Z INFO 02-21 18:45:53 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:45:53.9805817Z INFO 02-21 18:45:53 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:45:53.9818098Z INFO 02-21 18:45:53 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:45:54.3890796Z INFO 02-21 18:45:54 [parallel_state.py:1212] world_size=32 rank=1 local_rank=1 distributed_init_method=tcp://10.0.0.62:36027 backend=hccl
2026-02-21T18:45:54.4071885Z INFO 02-21 18:45:54 [parallel_state.py:1212] world_size=32 rank=9 local_rank=1 distributed_init_method=tcp://10.0.0.62:36027 backend=hccl
2026-02-21T18:45:54.9373909Z INFO 02-21 18:45:54 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:45:54.9384576Z INFO 02-21 18:45:54 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:45:54.9395144Z INFO 02-21 18:45:54 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:45:54.9446418Z INFO 02-21 18:45:54 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:45:55.0506139Z INFO 02-21 18:45:55 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:45:55.0518218Z INFO 02-21 18:45:55 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:45:55.0529643Z INFO 02-21 18:45:55 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:45:55.0572396Z INFO 02-21 18:45:55 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:45:59.9567317Z 2026-02-21 18:45:59,948 - 371 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:45:59.9576552Z INFO 02-21 18:45:59 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:46:00.1842617Z 2026-02-21 18:46:00,182 - 369 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:46:00.1900008Z INFO 02-21 18:46:00 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:46:01.2719182Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-21T18:46:01.2726274Z   warnings.warn(
2026-02-21T18:46:01.4808008Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-21T18:46:01.4820512Z   warnings.warn(
2026-02-21T18:46:03.2545788Z INFO 02-21 18:46:03 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:46:03.2556840Z INFO 02-21 18:46:03 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:46:03.2569725Z INFO 02-21 18:46:03 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:46:03.6577214Z INFO 02-21 18:46:03 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:46:03.6584406Z INFO 02-21 18:46:03 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:46:03.6595992Z INFO 02-21 18:46:03 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:46:03.6895669Z INFO 02-21 18:46:03 [parallel_state.py:1212] world_size=32 rank=2 local_rank=2 distributed_init_method=tcp://10.0.0.62:36027 backend=hccl
2026-02-21T18:46:04.0831508Z INFO 02-21 18:46:04 [parallel_state.py:1212] world_size=32 rank=10 local_rank=2 distributed_init_method=tcp://10.0.0.62:36027 backend=hccl
2026-02-21T18:46:04.3211617Z INFO 02-21 18:46:04 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:46:04.3218985Z INFO 02-21 18:46:04 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:46:04.3228763Z INFO 02-21 18:46:04 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:46:04.3282868Z INFO 02-21 18:46:04 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:46:04.7089552Z INFO 02-21 18:46:04 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:46:04.7100152Z INFO 02-21 18:46:04 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:46:04.7109733Z INFO 02-21 18:46:04 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:46:04.7162338Z INFO 02-21 18:46:04 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:46:09.1933381Z 2026-02-21 18:46:09,190 - 473 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:46:09.1995751Z INFO 02-21 18:46:09 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:46:09.7010342Z 2026-02-21 18:46:09,698 - 476 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:46:09.7069745Z INFO 02-21 18:46:09 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:46:10.7478932Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-21T18:46:10.7485455Z   warnings.warn(
2026-02-21T18:46:10.9679943Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-21T18:46:10.9688365Z   warnings.warn(
2026-02-21T18:46:12.8240550Z INFO 02-21 18:46:12 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:46:12.8247875Z INFO 02-21 18:46:12 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:46:12.8259086Z INFO 02-21 18:46:12 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:46:13.0425156Z INFO 02-21 18:46:13 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:46:13.0433523Z INFO 02-21 18:46:13 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:46:13.0445259Z INFO 02-21 18:46:13 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:46:13.2522361Z INFO 02-21 18:46:13 [parallel_state.py:1212] world_size=32 rank=3 local_rank=3 distributed_init_method=tcp://10.0.0.62:36027 backend=hccl
2026-02-21T18:46:13.4771400Z INFO 02-21 18:46:13 [parallel_state.py:1212] world_size=32 rank=11 local_rank=3 distributed_init_method=tcp://10.0.0.62:36027 backend=hccl
2026-02-21T18:46:13.7720259Z INFO 02-21 18:46:13 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:46:13.7727395Z INFO 02-21 18:46:13 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:46:13.7736896Z INFO 02-21 18:46:13 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:46:13.7794747Z INFO 02-21 18:46:13 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:46:14.3744461Z INFO 02-21 18:46:14 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:46:14.3752919Z INFO 02-21 18:46:14 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:46:14.3764283Z INFO 02-21 18:46:14 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:46:14.3822977Z INFO 02-21 18:46:14 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:46:18.8561974Z 2026-02-21 18:46:18,853 - 577 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:46:18.8620894Z INFO 02-21 18:46:18 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:46:19.4252130Z 2026-02-21 18:46:19,422 - 580 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:46:19.4285001Z INFO 02-21 18:46:19 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:46:20.1658568Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-21T18:46:20.1666501Z   warnings.warn(
2026-02-21T18:46:20.7191129Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-21T18:46:20.7197648Z   warnings.warn(
2026-02-21T18:46:22.2741276Z INFO 02-21 18:46:22 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:46:22.2748865Z INFO 02-21 18:46:22 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:46:22.2760240Z INFO 02-21 18:46:22 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:46:22.7144939Z INFO 02-21 18:46:22 [parallel_state.py:1212] world_size=32 rank=4 local_rank=4 distributed_init_method=tcp://10.0.0.62:36027 backend=hccl
2026-02-21T18:46:22.8396210Z INFO 02-21 18:46:22 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:46:22.8405631Z INFO 02-21 18:46:22 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:46:22.8419640Z INFO 02-21 18:46:22 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:46:23.2744967Z INFO 02-21 18:46:23 [parallel_state.py:1212] world_size=32 rank=12 local_rank=4 distributed_init_method=tcp://10.0.0.62:36027 backend=hccl
2026-02-21T18:46:23.3923244Z INFO 02-21 18:46:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:46:23.3932600Z INFO 02-21 18:46:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:46:23.3943896Z INFO 02-21 18:46:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:46:23.4025120Z INFO 02-21 18:46:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:46:23.9574697Z INFO 02-21 18:46:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:46:23.9582431Z INFO 02-21 18:46:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:46:23.9592543Z INFO 02-21 18:46:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:46:23.9655482Z INFO 02-21 18:46:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:46:28.3565030Z 2026-02-21 18:46:28,354 - 681 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:46:28.3598500Z INFO 02-21 18:46:28 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:46:28.8887315Z 2026-02-21 18:46:28,886 - 684 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:46:28.8942378Z INFO 02-21 18:46:28 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:46:29.6578192Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-21T18:46:29.6584527Z   warnings.warn(
2026-02-21T18:46:30.1965494Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-21T18:46:30.1972432Z   warnings.warn(
2026-02-21T18:46:32.1090265Z INFO 02-21 18:46:32 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:46:32.1098989Z INFO 02-21 18:46:32 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:46:32.1110854Z INFO 02-21 18:46:32 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:46:32.5481445Z INFO 02-21 18:46:32 [parallel_state.py:1212] world_size=32 rank=5 local_rank=5 distributed_init_method=tcp://10.0.0.62:36027 backend=hccl
2026-02-21T18:46:32.6291270Z INFO 02-21 18:46:32 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:46:32.6299178Z INFO 02-21 18:46:32 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:46:32.6309372Z INFO 02-21 18:46:32 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:46:32.9030782Z INFO 02-21 18:46:32 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:46:32.9039375Z INFO 02-21 18:46:32 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:46:32.9050157Z INFO 02-21 18:46:32 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:46:32.9121278Z INFO 02-21 18:46:32 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:46:33.0482242Z INFO 02-21 18:46:33 [parallel_state.py:1212] world_size=32 rank=13 local_rank=5 distributed_init_method=tcp://10.0.0.62:36027 backend=hccl
2026-02-21T18:46:33.4392817Z INFO 02-21 18:46:33 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:46:33.4402486Z INFO 02-21 18:46:33 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:46:33.4415256Z INFO 02-21 18:46:33 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:46:33.4466426Z INFO 02-21 18:46:33 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:46:37.9671303Z 2026-02-21 18:46:37,964 - 785 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:46:37.9725325Z INFO 02-21 18:46:37 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:46:38.4142474Z 2026-02-21 18:46:38,412 - 788 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:46:38.4176997Z INFO 02-21 18:46:38 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:46:39.4076298Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-21T18:46:39.4084719Z   warnings.warn(
2026-02-21T18:46:40.0476927Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-21T18:46:40.0484245Z   warnings.warn(
2026-02-21T18:46:41.5105440Z INFO 02-21 18:46:41 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:46:41.5112902Z INFO 02-21 18:46:41 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:46:41.5123699Z INFO 02-21 18:46:41 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:46:41.9522566Z INFO 02-21 18:46:41 [parallel_state.py:1212] world_size=32 rank=6 local_rank=6 distributed_init_method=tcp://10.0.0.62:36027 backend=hccl
2026-02-21T18:46:42.0734219Z INFO 02-21 18:46:42 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:46:42.0744159Z INFO 02-21 18:46:42 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:46:42.0755722Z INFO 02-21 18:46:42 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:46:42.5026513Z INFO 02-21 18:46:42 [parallel_state.py:1212] world_size=32 rank=14 local_rank=6 distributed_init_method=tcp://10.0.0.62:36027 backend=hccl
2026-02-21T18:46:42.5756347Z INFO 02-21 18:46:42 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:46:42.5758615Z INFO 02-21 18:46:42 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:46:42.5759287Z INFO 02-21 18:46:42 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:46:42.5813286Z INFO 02-21 18:46:42 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:46:43.0422454Z INFO 02-21 18:46:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:46:43.0429705Z INFO 02-21 18:46:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:46:43.0440153Z INFO 02-21 18:46:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:46:43.0492691Z INFO 02-21 18:46:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:46:47.8386077Z 2026-02-21 18:46:47,836 - 889 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:46:47.8440926Z INFO 02-21 18:46:47 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:46:47.9485972Z 2026-02-21 18:46:47,946 - 892 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-21T18:46:47.9543995Z INFO 02-21 18:46:47 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-21T18:46:49.1229591Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-21T18:46:49.1241511Z   warnings.warn(
2026-02-21T18:46:49.2141573Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-21T18:46:49.2153762Z   warnings.warn(
2026-02-21T18:46:51.1545083Z INFO 02-21 18:46:51 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:46:51.1554902Z INFO 02-21 18:46:51 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:46:51.1567658Z INFO 02-21 18:46:51 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:46:51.3892596Z INFO 02-21 18:46:51 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:46:51.3900971Z INFO 02-21 18:46:51 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:46:51.3919359Z INFO 02-21 18:46:51 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:46:51.5855002Z INFO 02-21 18:46:51 [parallel_state.py:1212] world_size=32 rank=7 local_rank=7 distributed_init_method=tcp://10.0.0.62:36027 backend=hccl
2026-02-21T18:46:51.8161023Z INFO 02-21 18:46:51 [parallel_state.py:1212] world_size=32 rank=15 local_rank=7 distributed_init_method=tcp://10.0.0.62:36027 backend=hccl
2026-02-21T18:46:54.0749693Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.0751373Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.0764664Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.1312345Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.1336475Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.1348447Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.1359549Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.1368874Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.1377760Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.1387921Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.1398709Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.1471530Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.1509057Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.1530071Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.1539396Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.1548458Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.1870658Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-21T18:46:54.1891843Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-21T18:46:54.1901374Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-21T18:46:54.1911008Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-21T18:46:54.1921664Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-21T18:46:54.1931654Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-21T18:46:54.1941593Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-21T18:46:54.1951498Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-21T18:46:54.1961600Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-21T18:46:54.1971624Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-21T18:46:54.1982822Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-21T18:46:54.1992838Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-21T18:46:54.2004141Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-21T18:46:54.2014145Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-21T18:46:54.2023652Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-21T18:46:54.2034138Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2044483Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2054631Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2065414Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2075227Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2085312Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2095326Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2105819Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2115507Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2125134Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2135200Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2144712Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2154146Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2164413Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2173533Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2183658Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2192827Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2202286Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2212134Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2221520Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2231278Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2240915Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2251370Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2259794Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2269747Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-21T18:46:54.2312649Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2325809Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2335486Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2346207Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2356968Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2366246Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2376456Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2386813Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2396261Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2406062Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2430748Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2431310Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2436415Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2447065Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2456748Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2467167Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2476831Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2486987Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2497849Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2512223Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2526393Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2536562Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2547563Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2559493Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-21T18:46:54.2573168Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.2583281Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.2593391Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.2604695Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.2614015Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.2624601Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.2634307Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.2644471Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.2700294Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.2700739Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.2701216Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.2701660Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.2702182Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.2704387Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.2714179Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.2725464Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.2816590Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.2835926Z INFO 02-21 18:46:54 [parallel_state.py:1423] rank 0 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
2026-02-21T18:46:54.3750252Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.3770777Z INFO 02-21 18:46:54 [parallel_state.py:1423] rank 1 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
2026-02-21T18:46:54.3782348Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.3793238Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.3804596Z INFO 02-21 18:46:54 [parallel_state.py:1423] rank 3 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
2026-02-21T18:46:54.3814924Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.3824567Z INFO 02-21 18:46:54 [parallel_state.py:1423] rank 4 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 4, EP rank 4
2026-02-21T18:46:54.3835508Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.3846261Z INFO 02-21 18:46:54 [parallel_state.py:1423] rank 5 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 5, EP rank 5
2026-02-21T18:46:54.3856156Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.3865753Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.3875850Z INFO 02-21 18:46:54 [parallel_state.py:1423] rank 8 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 0, EP rank 8
2026-02-21T18:46:54.3886045Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.3895992Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.3906492Z INFO 02-21 18:46:54 [parallel_state.py:1423] rank 7 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 7, EP rank 7
2026-02-21T18:46:54.3915748Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.3926236Z INFO 02-21 18:46:54 [parallel_state.py:1423] rank 9 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 1, EP rank 9
2026-02-21T18:46:54.3936354Z INFO 02-21 18:46:54 [parallel_state.py:1423] rank 10 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 2, EP rank 10
2026-02-21T18:46:54.3945487Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.3955392Z INFO 02-21 18:46:54 [parallel_state.py:1423] rank 12 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 4, EP rank 12
2026-02-21T18:46:54.3965367Z INFO 02-21 18:46:54 [parallel_state.py:1423] rank 11 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 3, EP rank 11
2026-02-21T18:46:54.3976227Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.3986566Z INFO 02-21 18:46:54 [parallel_state.py:1423] rank 13 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 5, EP rank 13
2026-02-21T18:46:54.3996333Z INFO 02-21 18:46:54 [parallel_state.py:1423] rank 14 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 6, EP rank 14
2026-02-21T18:46:54.4006292Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.4017660Z INFO 02-21 18:46:54 [parallel_state.py:1423] rank 2 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
2026-02-21T18:46:54.4028240Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.4039563Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.4050383Z INFO 02-21 18:46:54 [parallel_state.py:1423] rank 6 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 6, EP rank 6
2026-02-21T18:46:54.4060526Z INFO 02-21 18:46:54 [parallel_state.py:1423] rank 15 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 7, EP rank 15
2026-02-21T18:46:54.4187817Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.4210450Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.4219361Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.4229077Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.4238958Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.4248781Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.4258330Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.4267278Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.4277109Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.4286804Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.4296406Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.4305463Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.4314649Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.4324596Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.4334420Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.4343457Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-21T18:46:54.4356352Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.4384226Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.4394395Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.4405061Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.4441208Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.4441778Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.4442506Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.4445738Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.4454591Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.4466513Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.4474502Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.4485977Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.4497341Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.4507702Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.4518318Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.4528630Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-21T18:46:54.5425905Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.5452506Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.5464217Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.5473237Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.5482551Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.5492467Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.5501232Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.5511737Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.5520367Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.5530058Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.5539838Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.5549137Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.5558821Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.5568093Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.5577494Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.5591321Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.5961831Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.5971342Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.5980631Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.5989279Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.5999436Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.6008705Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.6017848Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.6027225Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.6037545Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.6047035Z [0;36m(Worker_DP0_TP2_EP2 pid=371)[0;0m INFO 02-21 18:46:54 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-21T18:46:54.6056418Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-21 18:46:54 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-21T18:46:54.6067174Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-21 18:46:54 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-21T18:46:54.6075650Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-21 18:46:54 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-21T18:46:54.6086081Z [0;36m(Worker_DP1_TP3_EP11 pid=476)[0;0m INFO 02-21 18:46:54 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-21T18:46:54.6096325Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-21 18:46:54 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-21T18:46:54.6105362Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.6115344Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.6125938Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-21 18:46:54 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-21T18:46:54.6134978Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.6144206Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.6153946Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.6164343Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-21 18:46:54 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-21T18:46:54.6173994Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.6184285Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m INFO 02-21 18:46:54 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-21T18:46:54.6194098Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-21 18:46:54 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-21T18:46:54.6205007Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-21 18:46:54 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-21T18:46:54.6213837Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-21 18:46:54 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-21T18:46:54.6223663Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-21 18:46:54 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-21T18:46:54.6233151Z WARNING 02-21 18:46:54 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-21T18:46:54.6243747Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-21 18:46:54 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-21T18:46:54.6277888Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-21 18:46:54 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-21T18:46:54.6968249Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m INFO 02-21 18:46:54 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-21T18:46:54.9927382Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m [2026-02-21 18:46:54] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-21T18:46:55.0228359Z [0;36m(Worker_DP0_TP2_EP2 pid=371)[0;0m [2026-02-21 18:46:55] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-21T18:46:55.0413188Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m [2026-02-21 18:46:55] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-21T18:46:55.0442830Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m [2026-02-21 18:46:55] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-21T18:46:55.0471191Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m [2026-02-21 18:46:55] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-21T18:46:55.0504093Z [0;36m(Worker_DP1_TP3_EP11 pid=476)[0;0m [2026-02-21 18:46:55] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-21T18:46:55.1820475Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-21 18:46:55 [layer.py:475] [EP Rank 7/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-21T18:46:55.1830706Z [0;36m(Worker_DP0_TP2_EP2 pid=371)[0;0m INFO 02-21 18:46:55 [layer.py:475] [EP Rank 2/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-21T18:46:55.1839236Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-21 18:46:55 [layer.py:475] [EP Rank 6/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-21T18:46:55.1849280Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-21 18:46:55 [layer.py:475] [EP Rank 10/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-21T18:46:55.1859040Z [0;36m(Worker_DP1_TP3_EP11 pid=476)[0;0m INFO 02-21 18:46:55 [layer.py:475] [EP Rank 11/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-21T18:46:55.1868989Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-21 18:46:55 [layer.py:475] [EP Rank 1/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-21T18:46:55.3129247Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m [2026-02-21 18:46:55] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-21T18:46:55.3322856Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m [2026-02-21 18:46:55] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-21T18:46:55.3865227Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m [2026-02-21 18:46:55] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-21T18:46:55.4303138Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m [2026-02-21 18:46:55] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-21T18:46:55.4409227Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-21 18:46:55 [layer.py:475] [EP Rank 5/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-21T18:46:55.4718083Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m INFO 02-21 18:46:55 [layer.py:475] [EP Rank 8/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-21T18:46:55.4881970Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m [2026-02-21 18:46:55] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-21T18:46:55.4932717Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-21 18:46:55 [layer.py:475] [EP Rank 9/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-21T18:46:55.5078415Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m [2026-02-21 18:46:55] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-21T18:46:55.5459169Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-21 18:46:55 [layer.py:475] [EP Rank 13/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-21T18:46:55.5786791Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m [2026-02-21 18:46:55] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-21T18:46:55.6039892Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-21 18:46:55 [layer.py:475] [EP Rank 12/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-21T18:46:55.6206907Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-21 18:46:55 [layer.py:475] [EP Rank 3/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-21T18:46:55.6879879Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-21 18:46:55 [layer.py:475] [EP Rank 4/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-21T18:46:55.6942324Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m [2026-02-21 18:46:55] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-21T18:46:55.8664617Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m INFO 02-21 18:46:55 [layer.py:475] [EP Rank 0/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-21T18:46:56.2958846Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m [2026-02-21 18:46:56] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-21T18:46:56.3884471Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m [2026-02-21 18:46:56] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-21T18:46:56.3984834Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-21 18:46:56 [layer.py:475] [EP Rank 15/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-21T18:46:56.5170696Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-21 18:46:56 [layer.py:475] [EP Rank 14/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-21T18:46:57.0235029Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-21T18:46:57.0241578Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m   return func(*args, **kwargs)
2026-02-21T18:46:57.1039261Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m INFO 02-21 18:46:57 [fused_moe.py:207] [EP Rank 0/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-21T18:46:57.1041171Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-21T18:46:57.1042457Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m   return func(*args, **kwargs)
2026-02-21T18:46:57.1636643Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-21 18:46:57 [fused_moe.py:207] [EP Rank 15/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-21T18:46:57.1963768Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-21T18:46:57.1971534Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m   return func(*args, **kwargs)
2026-02-21T18:46:57.2169452Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-21T18:46:57.2178009Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m   return func(*args, **kwargs)
2026-02-21T18:46:57.2229084Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-21T18:46:57.2246975Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m   return func(*args, **kwargs)
2026-02-21T18:46:57.2399424Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-21T18:46:57.2408952Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m   return func(*args, **kwargs)
2026-02-21T18:46:57.2538219Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-21T18:46:57.2547210Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m   return func(*args, **kwargs)
2026-02-21T18:46:57.2558527Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m INFO 02-21 18:46:57 [fused_moe.py:207] [EP Rank 8/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-21T18:46:57.2667678Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-21T18:46:57.2676582Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m   return func(*args, **kwargs)
2026-02-21T18:46:57.2804987Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-21T18:46:57.2813657Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m   return func(*args, **kwargs)
2026-02-21T18:46:57.2859329Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-21T18:46:57.2868457Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m   return func(*args, **kwargs)
2026-02-21T18:46:57.2879915Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-21 18:46:57 [fused_moe.py:207] [EP Rank 12/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-21T18:46:57.2901657Z [0;36m(Worker_DP1_TP3_EP11 pid=476)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-21T18:46:57.2910005Z [0;36m(Worker_DP1_TP3_EP11 pid=476)[0;0m   return func(*args, **kwargs)
2026-02-21T18:46:57.3040824Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-21 18:46:57 [fused_moe.py:207] [EP Rank 1/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-21T18:46:57.3188283Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-21T18:46:57.3196543Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m   return func(*args, **kwargs)
2026-02-21T18:46:57.3213684Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-21T18:46:57.3215950Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m   return func(*args, **kwargs)
2026-02-21T18:46:57.3227488Z [0;36m(Worker_DP0_TP2_EP2 pid=371)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-21T18:46:57.3235778Z [0;36m(Worker_DP0_TP2_EP2 pid=371)[0;0m   return func(*args, **kwargs)
2026-02-21T18:46:57.3247054Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-21 18:46:57 [fused_moe.py:207] [EP Rank 3/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-21T18:46:57.3259765Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-21 18:46:57 [fused_moe.py:207] [EP Rank 14/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-21T18:46:57.3269907Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-21 18:46:57 [fused_moe.py:207] [EP Rank 7/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-21T18:46:57.3388393Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-21T18:46:57.3397237Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m   return func(*args, **kwargs)
2026-02-21T18:46:57.3424960Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-21T18:46:57.3433043Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m   return func(*args, **kwargs)
2026-02-21T18:46:57.3443242Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-21 18:46:57 [fused_moe.py:207] [EP Rank 4/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-21T18:46:57.3492790Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-21 18:46:57 [fused_moe.py:207] [EP Rank 13/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-21T18:46:57.3512601Z [0;36m(Worker_DP1_TP3_EP11 pid=476)[0;0m INFO 02-21 18:46:57 [fused_moe.py:207] [EP Rank 11/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-21T18:46:57.3780372Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-21 18:46:57 [fused_moe.py:207] [EP Rank 6/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-21T18:46:57.3798434Z [0;36m(Worker_DP0_TP2_EP2 pid=371)[0;0m INFO 02-21 18:46:57 [fused_moe.py:207] [EP Rank 2/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-21T18:46:57.3867681Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-21 18:46:57 [fused_moe.py:207] [EP Rank 9/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-21T18:46:57.3998975Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-21 18:46:57 [fused_moe.py:207] [EP Rank 5/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-21T18:46:57.4076291Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-21 18:46:57 [fused_moe.py:207] [EP Rank 10/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-21T18:46:57.5629042Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m INFO 02-21 18:46:57 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-21T18:46:57.5886971Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-21 18:46:57 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-21T18:46:57.6059438Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-21 18:46:57 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-21T18:46:57.6291456Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-21 18:46:57 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-21T18:46:57.6300884Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-21 18:46:57 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-21T18:46:57.6312466Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-21 18:46:57 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-21T18:46:57.6480218Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-21 18:46:57 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-21T18:46:57.6524814Z [0;36m(Worker_DP1_TP3_EP11 pid=476)[0;0m INFO 02-21 18:46:57 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-21T18:46:57.6622609Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-21 18:46:57 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-21T18:46:57.6668583Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-21 18:46:57 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-21T18:46:57.7015910Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-21 18:46:57 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-21T18:46:57.7152359Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-21 18:46:57 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-21T18:46:57.7262380Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-21 18:46:57 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-21T18:46:57.7330691Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-21 18:46:57 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-21T18:46:57.7340190Z [0;36m(Worker_DP0_TP2_EP2 pid=371)[0;0m INFO 02-21 18:46:57 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-21T18:46:57.7484066Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m INFO 02-21 18:46:57 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-21T18:47:00.9660321Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-21 18:47:00 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-21T18:47:01.0308908Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-21 18:47:01 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-21T18:47:01.0457559Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-21 18:47:01 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-21T18:47:01.0515955Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-21 18:47:01 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-21T18:47:01.0654790Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-21 18:47:01 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-21T18:47:01.0696242Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-21 18:47:01 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-21T18:47:01.0722533Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-21 18:47:01 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-21T18:47:01.0905951Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-21 18:47:01 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-21T18:47:01.0975020Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-21 18:47:01 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-21T18:47:01.1061235Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m INFO 02-21 18:47:01 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-21T18:47:01.1072440Z [0;36m(Worker_DP1_TP3_EP11 pid=476)[0;0m INFO 02-21 18:47:01 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-21T18:47:01.1771296Z [0;36m(Worker_DP0_TP2_EP2 pid=371)[0;0m INFO 02-21 18:47:01 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-21T18:47:01.1932935Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-21 18:47:01 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-21T18:47:01.1984784Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-21 18:47:01 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-21T18:47:01.2797894Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-21 18:47:01 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-21T18:47:01.2899320Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m INFO 02-21 18:47:01 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-21T18:47:01.3516000Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:01.3516414Z Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
2026-02-21T18:47:02.5690390Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:02.5690972Z Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:01<03:17,  1.22s/it]
2026-02-21T18:47:06.5812861Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:06.5813323Z Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:05<07:40,  2.86s/it]
2026-02-21T18:47:07.5693349Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:07.5693880Z Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:06<05:20,  2.01s/it]
2026-02-21T18:47:09.2736904Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:09.2737513Z Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:07<05:00,  1.89s/it]
2026-02-21T18:47:12.4753008Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:12.4753428Z Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:11<06:13,  2.36s/it]
2026-02-21T18:47:16.3235115Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:16.3235532Z Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:14<07:30,  2.87s/it]
2026-02-21T18:47:19.7411303Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:19.7411776Z Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:18<07:55,  3.05s/it]
2026-02-21T18:47:22.7742848Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:22.7743364Z Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:21<07:51,  3.04s/it]
2026-02-21T18:47:24.3661317Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:24.3661747Z Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:23<06:38,  2.59s/it]
2026-02-21T18:47:25.8848587Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:25.8849048Z Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:24<05:45,  2.26s/it]
2026-02-21T18:47:27.5125554Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:27.5126060Z Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:26<05:13,  2.07s/it]
2026-02-21T18:47:29.0858718Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:29.0859166Z Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:27<04:49,  1.92s/it]
2026-02-21T18:47:30.6434105Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:30.6434685Z Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:29<04:31,  1.81s/it]
2026-02-21T18:47:34.8854669Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:34.8855224Z Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:33<06:18,  2.54s/it]
2026-02-21T18:47:39.8405591Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:39.8406489Z Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:38<08:03,  3.27s/it]
2026-02-21T18:47:42.7459276Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:42.7459755Z Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:41<07:44,  3.16s/it]
2026-02-21T18:47:45.2382253Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:45.2382761Z Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:43<07:12,  2.96s/it]
2026-02-21T18:47:47.3634321Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:47.3634842Z Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:46<06:32,  2.71s/it]
2026-02-21T18:47:48.2118444Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:48.2118916Z Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:46<05:09,  2.15s/it]
2026-02-21T18:47:50.0151936Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:50.0152539Z Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:48<04:52,  2.05s/it]
2026-02-21T18:47:53.8785059Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:53.8785494Z Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:52<06:07,  2.59s/it]
2026-02-21T18:47:55.2166400Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:55.2166880Z Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:53<05:12,  2.22s/it]
2026-02-21T18:47:56.8904649Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:56.8905076Z Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:55<04:47,  2.05s/it]
2026-02-21T18:47:58.5287322Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:47:58.5287796Z Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:57<04:28,  1.93s/it]
2026-02-21T18:48:03.1129920Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:03.1130526Z Loading safetensors checkpoint shards:  15% Completed | 25/163 [01:01<06:16,  2.72s/it]
2026-02-21T18:48:03.6770167Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:03.6770610Z Loading safetensors checkpoint shards:  16% Completed | 26/163 [01:02<04:44,  2.08s/it]
2026-02-21T18:48:05.1407214Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:05.1407626Z Loading safetensors checkpoint shards:  17% Completed | 27/163 [01:03<04:17,  1.89s/it]
2026-02-21T18:48:08.8381283Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:08.8381694Z Loading safetensors checkpoint shards:  17% Completed | 28/163 [01:07<05:28,  2.43s/it]
2026-02-21T18:48:10.5174623Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:10.5175049Z Loading safetensors checkpoint shards:  18% Completed | 29/163 [01:09<04:55,  2.21s/it]
2026-02-21T18:48:12.0190054Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:12.0190529Z Loading safetensors checkpoint shards:  18% Completed | 30/163 [01:10<04:25,  2.00s/it]
2026-02-21T18:48:13.4844485Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:13.4845008Z Loading safetensors checkpoint shards:  19% Completed | 31/163 [01:12<04:02,  1.84s/it]
2026-02-21T18:48:15.3706383Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:15.3706854Z Loading safetensors checkpoint shards:  20% Completed | 32/163 [01:14<04:02,  1.85s/it]
2026-02-21T18:48:17.4755005Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:17.4755449Z Loading safetensors checkpoint shards:  20% Completed | 33/163 [01:16<04:10,  1.93s/it]
2026-02-21T18:48:19.6348002Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:19.6348460Z Loading safetensors checkpoint shards:  21% Completed | 34/163 [01:18<04:17,  2.00s/it]
2026-02-21T18:48:21.4906138Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:21.4906546Z Loading safetensors checkpoint shards:  21% Completed | 35/163 [01:20<04:10,  1.95s/it]
2026-02-21T18:48:28.0365663Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:28.0366196Z Loading safetensors checkpoint shards:  22% Completed | 36/163 [01:26<07:03,  3.33s/it]
2026-02-21T18:48:29.8429900Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:29.8430474Z Loading safetensors checkpoint shards:  23% Completed | 37/163 [01:28<06:02,  2.87s/it]
2026-02-21T18:48:31.9150430Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:31.9150885Z Loading safetensors checkpoint shards:  23% Completed | 38/163 [01:30<05:29,  2.63s/it]
2026-02-21T18:48:34.1613142Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:34.1613615Z Loading safetensors checkpoint shards:  24% Completed | 39/163 [01:32<05:12,  2.52s/it]
2026-02-21T18:48:40.0161310Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:40.0161838Z Loading safetensors checkpoint shards:  25% Completed | 40/163 [01:38<07:12,  3.52s/it]
2026-02-21T18:48:40.9989696Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:40.9990147Z Loading safetensors checkpoint shards:  25% Completed | 41/163 [01:39<05:36,  2.76s/it]
2026-02-21T18:48:42.0340538Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:42.0341101Z Loading safetensors checkpoint shards:  26% Completed | 42/163 [01:40<04:31,  2.24s/it]
2026-02-21T18:48:46.5705670Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:46.5706146Z Loading safetensors checkpoint shards:  26% Completed | 43/163 [01:45<05:51,  2.93s/it]
2026-02-21T18:48:47.9434673Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:47.9435267Z Loading safetensors checkpoint shards:  27% Completed | 44/163 [01:46<04:53,  2.46s/it]
2026-02-21T18:48:49.4465132Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:49.4465666Z Loading safetensors checkpoint shards:  28% Completed | 45/163 [01:48<04:16,  2.18s/it]
2026-02-21T18:48:50.9660760Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:48:50.9661225Z Loading safetensors checkpoint shards:  28% Completed | 46/163 [01:49<03:51,  1.98s/it]
2026-02-21T18:49:01.4701741Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:01.4702355Z Loading safetensors checkpoint shards:  29% Completed | 47/163 [02:00<08:46,  4.54s/it]
2026-02-21T18:49:04.8479357Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:04.8479783Z Loading safetensors checkpoint shards:  29% Completed | 48/163 [02:03<08:01,  4.19s/it]
2026-02-21T18:49:05.0379906Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:05.0380327Z Loading safetensors checkpoint shards:  30% Completed | 49/163 [02:03<05:40,  2.99s/it]
2026-02-21T18:49:07.2985650Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:07.2986057Z Loading safetensors checkpoint shards:  31% Completed | 50/163 [02:05<05:13,  2.77s/it]
2026-02-21T18:49:10.7610110Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:10.7610643Z Loading safetensors checkpoint shards:  31% Completed | 51/163 [02:09<05:33,  2.98s/it]
2026-02-21T18:49:11.6552693Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:11.6553190Z Loading safetensors checkpoint shards:  32% Completed | 52/163 [02:10<04:21,  2.35s/it]
2026-02-21T18:49:16.1205192Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:16.1206088Z Loading safetensors checkpoint shards:  33% Completed | 53/163 [02:14<05:28,  2.99s/it]
2026-02-21T18:49:17.3657225Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:17.3657708Z Loading safetensors checkpoint shards:  33% Completed | 54/163 [02:16<04:28,  2.46s/it]
2026-02-21T18:49:18.5907293Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:18.5907833Z Loading safetensors checkpoint shards:  34% Completed | 55/163 [02:17<03:45,  2.09s/it]
2026-02-21T18:49:20.0153545Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:20.0153973Z Loading safetensors checkpoint shards:  34% Completed | 56/163 [02:18<03:22,  1.89s/it]
2026-02-21T18:49:21.7198175Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:21.7198642Z Loading safetensors checkpoint shards:  35% Completed | 57/163 [02:20<03:14,  1.84s/it]
2026-02-21T18:49:24.1792994Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:24.1793778Z Loading safetensors checkpoint shards:  36% Completed | 58/163 [02:22<03:32,  2.02s/it]
2026-02-21T18:49:31.0977602Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:31.0978039Z Loading safetensors checkpoint shards:  36% Completed | 59/163 [02:29<06:03,  3.49s/it]
2026-02-21T18:49:32.3229731Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:32.3230254Z Loading safetensors checkpoint shards:  37% Completed | 60/163 [02:30<04:49,  2.81s/it]
2026-02-21T18:49:33.2806841Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:33.2807423Z Loading safetensors checkpoint shards:  37% Completed | 61/163 [02:31<03:50,  2.26s/it]
2026-02-21T18:49:37.1398199Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:37.1398661Z Loading safetensors checkpoint shards:  38% Completed | 62/163 [02:35<04:36,  2.74s/it]
2026-02-21T18:49:38.4712311Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:38.4712854Z Loading safetensors checkpoint shards:  39% Completed | 63/163 [02:37<03:51,  2.32s/it]
2026-02-21T18:49:39.9475593Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:39.9476152Z Loading safetensors checkpoint shards:  39% Completed | 64/163 [02:38<03:24,  2.06s/it]
2026-02-21T18:49:45.8198512Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:45.8198951Z Loading safetensors checkpoint shards:  40% Completed | 65/163 [02:44<05:14,  3.21s/it]
2026-02-21T18:49:49.0014458Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:49.0014892Z Loading safetensors checkpoint shards:  40% Completed | 66/163 [02:47<05:10,  3.20s/it]
2026-02-21T18:49:50.4874300Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:50.4874730Z Loading safetensors checkpoint shards:  41% Completed | 67/163 [02:49<04:17,  2.68s/it]
2026-02-21T18:49:54.7971401Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:54.7971876Z Loading safetensors checkpoint shards:  42% Completed | 68/163 [02:53<05:01,  3.17s/it]
2026-02-21T18:49:55.8822311Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:55.8822836Z Loading safetensors checkpoint shards:  42% Completed | 69/163 [02:54<03:59,  2.55s/it]
2026-02-21T18:49:57.0016105Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:57.0016554Z Loading safetensors checkpoint shards:  43% Completed | 70/163 [02:55<03:16,  2.12s/it]
2026-02-21T18:49:59.2215977Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:49:59.2216415Z Loading safetensors checkpoint shards:  44% Completed | 71/163 [02:57<03:17,  2.15s/it]
2026-02-21T18:50:02.7783804Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:02.7784415Z Loading safetensors checkpoint shards:  44% Completed | 72/163 [03:01<03:53,  2.57s/it]
2026-02-21T18:50:03.6076559Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:03.6077092Z Loading safetensors checkpoint shards:  45% Completed | 73/163 [03:02<03:04,  2.05s/it]
2026-02-21T18:50:05.0507156Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:05.0507612Z Loading safetensors checkpoint shards:  45% Completed | 74/163 [03:03<02:46,  1.87s/it]
2026-02-21T18:50:06.6716567Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:06.6717065Z Loading safetensors checkpoint shards:  46% Completed | 75/163 [03:05<02:37,  1.79s/it]
2026-02-21T18:50:07.9654994Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:07.9655599Z Loading safetensors checkpoint shards:  47% Completed | 76/163 [03:06<02:22,  1.64s/it]
2026-02-21T18:50:12.1140374Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:12.1140849Z Loading safetensors checkpoint shards:  47% Completed | 77/163 [03:10<03:25,  2.39s/it]
2026-02-21T18:50:13.0054687Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:13.0055130Z Loading safetensors checkpoint shards:  48% Completed | 78/163 [03:11<02:45,  1.94s/it]
2026-02-21T18:50:14.0863120Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:14.0863568Z Loading safetensors checkpoint shards:  48% Completed | 79/163 [03:12<02:21,  1.69s/it]
2026-02-21T18:50:17.9766008Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:17.9766833Z Loading safetensors checkpoint shards:  49% Completed | 80/163 [03:16<03:14,  2.35s/it]
2026-02-21T18:50:21.7940965Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:21.7941470Z Loading safetensors checkpoint shards:  50% Completed | 81/163 [03:20<03:48,  2.79s/it]
2026-02-21T18:50:22.5645957Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:22.5646481Z Loading safetensors checkpoint shards:  50% Completed | 82/163 [03:21<02:56,  2.18s/it]
2026-02-21T18:50:25.4727763Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:25.4728267Z Loading safetensors checkpoint shards:  51% Completed | 83/163 [03:24<03:12,  2.40s/it]
2026-02-21T18:50:27.0415110Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:27.0415648Z Loading safetensors checkpoint shards:  52% Completed | 84/163 [03:25<02:49,  2.15s/it]
2026-02-21T18:50:30.4385145Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:30.4385633Z Loading safetensors checkpoint shards:  52% Completed | 85/163 [03:29<03:16,  2.52s/it]
2026-02-21T18:50:31.7484211Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:31.7484635Z Loading safetensors checkpoint shards:  53% Completed | 86/163 [03:30<02:46,  2.16s/it]
2026-02-21T18:50:33.6556916Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:33.6557376Z Loading safetensors checkpoint shards:  53% Completed | 87/163 [03:32<02:38,  2.08s/it]
2026-02-21T18:50:38.5756583Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:38.5757063Z Loading safetensors checkpoint shards:  54% Completed | 88/163 [03:37<03:40,  2.94s/it]
2026-02-21T18:50:40.1916078Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:40.1916503Z Loading safetensors checkpoint shards:  55% Completed | 89/163 [03:38<03:07,  2.54s/it]
2026-02-21T18:50:44.3835254Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:44.3835778Z Loading safetensors checkpoint shards:  55% Completed | 90/163 [03:43<03:41,  3.04s/it]
2026-02-21T18:50:45.4870598Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:45.4871073Z Loading safetensors checkpoint shards:  56% Completed | 91/163 [03:44<02:56,  2.46s/it]
2026-02-21T18:50:50.5772567Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:50.5773111Z Loading safetensors checkpoint shards:  56% Completed | 92/163 [03:49<03:50,  3.25s/it]
2026-02-21T18:50:51.5619115Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:51.5619706Z Loading safetensors checkpoint shards:  57% Completed | 93/163 [03:50<02:59,  2.57s/it]
2026-02-21T18:50:52.4890559Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:52.4891008Z Loading safetensors checkpoint shards:  58% Completed | 94/163 [03:51<02:23,  2.08s/it]
2026-02-21T18:50:54.2878591Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:54.2879054Z Loading safetensors checkpoint shards:  58% Completed | 95/163 [03:52<02:15,  1.99s/it]
2026-02-21T18:50:58.2246322Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:58.2246850Z Loading safetensors checkpoint shards:  59% Completed | 96/163 [03:56<02:52,  2.58s/it]
2026-02-21T18:50:59.5861256Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:50:59.5861699Z Loading safetensors checkpoint shards:  60% Completed | 97/163 [03:58<02:25,  2.21s/it]
2026-02-21T18:51:01.0660650Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:01.0661038Z Loading safetensors checkpoint shards:  60% Completed | 98/163 [03:59<02:09,  1.99s/it]
2026-02-21T18:51:02.7177625Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:02.7178001Z Loading safetensors checkpoint shards:  61% Completed | 99/163 [04:01<02:00,  1.89s/it]
2026-02-21T18:51:04.3366864Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:04.3367230Z Loading safetensors checkpoint shards:  61% Completed | 100/163 [04:02<01:53,  1.81s/it]
2026-02-21T18:51:05.7326757Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:05.7327132Z Loading safetensors checkpoint shards:  62% Completed | 101/163 [04:04<01:44,  1.68s/it]
2026-02-21T18:51:09.8077641Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:09.8078043Z Loading safetensors checkpoint shards:  63% Completed | 102/163 [04:08<02:26,  2.40s/it]
2026-02-21T18:51:12.5260168Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:12.5260534Z Loading safetensors checkpoint shards:  63% Completed | 103/163 [04:11<02:29,  2.50s/it]
2026-02-21T18:51:15.8053050Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:15.8053417Z Loading safetensors checkpoint shards:  64% Completed | 104/163 [04:14<02:41,  2.73s/it]
2026-02-21T18:51:16.7664676Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:16.7665041Z Loading safetensors checkpoint shards:  64% Completed | 105/163 [04:15<02:07,  2.20s/it]
2026-02-21T18:51:20.5138769Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:20.5139128Z Loading safetensors checkpoint shards:  65% Completed | 106/163 [04:19<02:31,  2.66s/it]
2026-02-21T18:51:21.6264704Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:21.6265094Z Loading safetensors checkpoint shards:  66% Completed | 107/163 [04:20<02:03,  2.20s/it]
2026-02-21T18:51:24.8891407Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:24.8891770Z Loading safetensors checkpoint shards:  66% Completed | 108/163 [04:23<02:18,  2.52s/it]
2026-02-21T18:51:25.8403003Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:25.8403364Z Loading safetensors checkpoint shards:  67% Completed | 109/163 [04:24<01:50,  2.05s/it]
2026-02-21T18:51:29.2424270Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:29.2424665Z Loading safetensors checkpoint shards:  67% Completed | 110/163 [04:27<02:10,  2.45s/it]
2026-02-21T18:51:32.9620956Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:32.9621312Z Loading safetensors checkpoint shards:  68% Completed | 111/163 [04:31<02:27,  2.83s/it]
2026-02-21T18:51:35.3172837Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:35.3173261Z Loading safetensors checkpoint shards:  69% Completed | 112/163 [04:33<02:17,  2.69s/it]
2026-02-21T18:51:38.4802457Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:38.4802872Z Loading safetensors checkpoint shards:  69% Completed | 113/163 [04:37<02:21,  2.83s/it]
2026-02-21T18:51:39.7091100Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:39.7091468Z Loading safetensors checkpoint shards:  70% Completed | 114/163 [04:38<01:55,  2.35s/it]
2026-02-21T18:51:41.2321451Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:41.2321849Z Loading safetensors checkpoint shards:  71% Completed | 115/163 [04:39<01:40,  2.10s/it]
2026-02-21T18:51:45.7121327Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:45.7121689Z Loading safetensors checkpoint shards:  71% Completed | 116/163 [04:44<02:12,  2.82s/it]
2026-02-21T18:51:46.7501749Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:46.7502208Z Loading safetensors checkpoint shards:  72% Completed | 117/163 [04:45<01:45,  2.28s/it]
2026-02-21T18:51:50.7786229Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:50.7786922Z Loading safetensors checkpoint shards:  72% Completed | 118/163 [04:49<02:06,  2.81s/it]
2026-02-21T18:51:51.5156538Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:51.5156887Z Loading safetensors checkpoint shards:  73% Completed | 119/163 [04:50<01:36,  2.19s/it]
2026-02-21T18:51:53.1356847Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:53.1357218Z Loading safetensors checkpoint shards:  74% Completed | 120/163 [04:51<01:26,  2.02s/it]
2026-02-21T18:51:54.8911526Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:54.8911899Z Loading safetensors checkpoint shards:  74% Completed | 121/163 [04:53<01:21,  1.94s/it]
2026-02-21T18:51:56.3670627Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:51:56.3670980Z Loading safetensors checkpoint shards:  75% Completed | 122/163 [04:55<01:13,  1.80s/it]
2026-02-21T18:52:00.7560744Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:00.7561457Z Loading safetensors checkpoint shards:  75% Completed | 123/163 [04:59<01:43,  2.58s/it]
2026-02-21T18:52:02.1541701Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:02.1542162Z Loading safetensors checkpoint shards:  76% Completed | 124/163 [05:00<01:26,  2.22s/it]
2026-02-21T18:52:05.8600122Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:05.8600497Z Loading safetensors checkpoint shards:  77% Completed | 125/163 [05:04<01:41,  2.67s/it]
2026-02-21T18:52:07.3011523Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:07.3011882Z Loading safetensors checkpoint shards:  77% Completed | 126/163 [05:05<01:25,  2.30s/it]
2026-02-21T18:52:08.8600062Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:08.8600430Z Loading safetensors checkpoint shards:  78% Completed | 127/163 [05:07<01:14,  2.08s/it]
2026-02-21T18:52:12.9924466Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:12.9924834Z Loading safetensors checkpoint shards:  79% Completed | 128/163 [05:11<01:34,  2.69s/it]
2026-02-21T18:52:16.8043927Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:16.8044314Z Loading safetensors checkpoint shards:  79% Completed | 129/163 [05:15<01:42,  3.02s/it]
2026-02-21T18:52:17.6474175Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:17.6474546Z Loading safetensors checkpoint shards:  80% Completed | 130/163 [05:16<01:18,  2.38s/it]
2026-02-21T18:52:19.1848841Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:19.1849224Z Loading safetensors checkpoint shards:  80% Completed | 131/163 [05:17<01:07,  2.12s/it]
2026-02-21T18:52:20.8887437Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:20.8887800Z Loading safetensors checkpoint shards:  81% Completed | 132/163 [05:19<01:01,  2.00s/it]
2026-02-21T18:52:24.8064670Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:24.8065035Z Loading safetensors checkpoint shards:  82% Completed | 133/163 [05:23<01:17,  2.57s/it]
2026-02-21T18:52:26.5649794Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:26.5650202Z Loading safetensors checkpoint shards:  82% Completed | 134/163 [05:25<01:07,  2.33s/it]
2026-02-21T18:52:31.1450818Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:31.1451187Z Loading safetensors checkpoint shards:  83% Completed | 135/163 [05:29<01:24,  3.01s/it]
2026-02-21T18:52:31.8782448Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:31.8782812Z Loading safetensors checkpoint shards:  83% Completed | 136/163 [05:30<01:02,  2.32s/it]
2026-02-21T18:52:33.5359781Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:33.5360165Z Loading safetensors checkpoint shards:  84% Completed | 137/163 [05:32<00:55,  2.12s/it]
2026-02-21T18:52:38.4666124Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:38.4666539Z Loading safetensors checkpoint shards:  85% Completed | 138/163 [05:37<01:14,  2.97s/it]
2026-02-21T18:52:41.7465155Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:41.7465544Z Loading safetensors checkpoint shards:  85% Completed | 139/163 [05:40<01:13,  3.06s/it]
2026-02-21T18:52:42.4209271Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:42.4209637Z Loading safetensors checkpoint shards:  86% Completed | 140/163 [05:41<00:53,  2.34s/it]
2026-02-21T18:52:43.8891029Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:43.8891396Z Loading safetensors checkpoint shards:  87% Completed | 141/163 [05:42<00:45,  2.08s/it]
2026-02-21T18:52:45.0796632Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:45.0796994Z Loading safetensors checkpoint shards:  87% Completed | 142/163 [05:43<00:38,  1.81s/it]
2026-02-21T18:52:49.1115729Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:49.1116110Z Loading safetensors checkpoint shards:  88% Completed | 143/163 [05:47<00:49,  2.48s/it]
2026-02-21T18:52:52.4341757Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:52.4342212Z Loading safetensors checkpoint shards:  88% Completed | 144/163 [05:51<00:51,  2.73s/it]
2026-02-21T18:52:55.3131043Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:55.3131451Z Loading safetensors checkpoint shards:  89% Completed | 145/163 [05:53<00:49,  2.78s/it]
2026-02-21T18:52:57.5195087Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:57.5195451Z Loading safetensors checkpoint shards:  90% Completed | 146/163 [05:56<00:44,  2.61s/it]
2026-02-21T18:52:58.9330674Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:52:58.9331032Z Loading safetensors checkpoint shards:  90% Completed | 147/163 [05:57<00:35,  2.25s/it]
2026-02-21T18:53:00.3397066Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:53:00.3397431Z Loading safetensors checkpoint shards:  91% Completed | 148/163 [05:58<00:29,  2.00s/it]
2026-02-21T18:53:01.6206534Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:53:01.6206880Z Loading safetensors checkpoint shards:  91% Completed | 149/163 [06:00<00:24,  1.78s/it]
2026-02-21T18:53:05.2779598Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:53:05.2779998Z Loading safetensors checkpoint shards:  93% Completed | 151/163 [06:03<00:21,  1.80s/it]
2026-02-21T18:53:07.0178746Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:53:07.0179117Z Loading safetensors checkpoint shards:  93% Completed | 152/163 [06:05<00:19,  1.79s/it]
2026-02-21T18:53:08.7660584Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:53:08.7660965Z Loading safetensors checkpoint shards:  94% Completed | 153/163 [06:07<00:17,  1.78s/it]
2026-02-21T18:53:11.2224359Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:53:11.2224711Z Loading safetensors checkpoint shards:  94% Completed | 154/163 [06:09<00:17,  1.96s/it]
2026-02-21T18:53:12.4523534Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:53:12.4523895Z Loading safetensors checkpoint shards:  95% Completed | 155/163 [06:11<00:14,  1.76s/it]
2026-02-21T18:53:14.0461306Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:53:14.0461658Z Loading safetensors checkpoint shards:  96% Completed | 156/163 [06:12<00:11,  1.71s/it]
2026-02-21T18:53:15.7289825Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:53:15.7290190Z Loading safetensors checkpoint shards:  96% Completed | 157/163 [06:14<00:10,  1.70s/it]
2026-02-21T18:53:20.7755395Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:53:20.7755763Z Loading safetensors checkpoint shards:  97% Completed | 158/163 [06:19<00:13,  2.68s/it]
2026-02-21T18:53:21.9714092Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:53:21.9714539Z Loading safetensors checkpoint shards:  98% Completed | 159/163 [06:20<00:08,  2.24s/it]
2026-02-21T18:53:23.8039425Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:53:23.8039788Z Loading safetensors checkpoint shards:  98% Completed | 160/163 [06:22<00:06,  2.12s/it]
2026-02-21T18:53:25.1794120Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:53:25.1794495Z Loading safetensors checkpoint shards:  99% Completed | 161/163 [06:23<00:03,  1.90s/it]
2026-02-21T18:53:28.8600112Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:53:28.8600776Z Loading safetensors checkpoint shards:  99% Completed | 162/163 [06:27<00:02,  2.43s/it]
2026-02-21T18:53:30.5135294Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:53:30.5135657Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [06:29<00:00,  2.20s/it]
2026-02-21T18:53:30.5147652Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:53:30.5147943Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [06:29<00:00,  2.39s/it]
2026-02-21T18:53:30.5157311Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:53:30.5285296Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m INFO 02-21 18:53:30 [default_loader.py:291] Loading weights took 389.18 seconds
2026-02-21T18:53:31.4065192Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m INFO 02-21 18:53:31 [default_loader.py:291] Loading weights took 390.24 seconds
2026-02-21T18:53:43.7506190Z INFO 02-21 18:53:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:53:43.7506926Z INFO 02-21 18:53:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:53:43.7507355Z INFO 02-21 18:53:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:53:43.7560712Z INFO 02-21 18:53:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:53:43.7569861Z INFO 02-21 18:53:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:53:43.7581586Z INFO 02-21 18:53:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:53:43.7599655Z INFO 02-21 18:53:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:53:43.7643526Z INFO 02-21 18:53:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:53:43.7716635Z INFO 02-21 18:53:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:53:43.7716974Z INFO 02-21 18:53:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:53:43.7717508Z INFO 02-21 18:53:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:53:43.7718095Z INFO 02-21 18:53:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:53:43.7725769Z INFO 02-21 18:53:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:53:43.7731833Z INFO 02-21 18:53:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:53:43.7743534Z INFO 02-21 18:53:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:53:43.7752816Z INFO 02-21 18:53:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:53:43.7764467Z INFO 02-21 18:53:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:53:43.7775461Z INFO 02-21 18:53:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:53:43.7787165Z INFO 02-21 18:53:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:53:43.7798124Z INFO 02-21 18:53:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:53:43.7808912Z INFO 02-21 18:53:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:53:43.7818879Z INFO 02-21 18:53:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:53:43.7828850Z INFO 02-21 18:53:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:53:43.7840359Z INFO 02-21 18:53:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:53:43.7850736Z INFO 02-21 18:53:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:53:43.7861179Z INFO 02-21 18:53:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:53:43.7871734Z INFO 02-21 18:53:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:53:43.7881852Z INFO 02-21 18:53:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:53:43.7892669Z INFO 02-21 18:53:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:53:43.7909871Z INFO 02-21 18:53:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:53:43.7966012Z INFO 02-21 18:53:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:53:43.7966366Z INFO 02-21 18:53:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:53:43.7966742Z INFO 02-21 18:53:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:53:43.7967250Z INFO 02-21 18:53:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:53:43.7971762Z INFO 02-21 18:53:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:53:43.7988248Z INFO 02-21 18:53:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:53:43.8001137Z INFO 02-21 18:53:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:53:43.8017013Z INFO 02-21 18:53:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:53:43.8031143Z INFO 02-21 18:53:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:53:43.8042116Z INFO 02-21 18:53:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:53:43.8053066Z INFO 02-21 18:53:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:53:43.8064161Z INFO 02-21 18:53:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:53:43.8074528Z INFO 02-21 18:53:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:53:43.8085552Z INFO 02-21 18:53:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:53:43.8095823Z INFO 02-21 18:53:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:53:43.8106322Z INFO 02-21 18:53:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:53:43.8117027Z INFO 02-21 18:53:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:53:43.8128064Z INFO 02-21 18:53:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:53:43.8137579Z INFO 02-21 18:53:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:53:43.8147848Z INFO 02-21 18:53:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:53:43.8159168Z INFO 02-21 18:53:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:53:43.8169562Z INFO 02-21 18:53:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:53:43.8178898Z INFO 02-21 18:53:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:53:43.8189500Z INFO 02-21 18:53:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:53:43.8199805Z INFO 02-21 18:53:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:53:43.8210552Z INFO 02-21 18:53:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:53:43.8220070Z INFO 02-21 18:53:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:53:43.8230093Z INFO 02-21 18:53:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:53:43.8241159Z INFO 02-21 18:53:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:53:43.8250491Z INFO 02-21 18:53:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:53:43.9115263Z INFO 02-21 18:53:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-21T18:53:43.9125081Z INFO 02-21 18:53:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-21T18:53:43.9140514Z INFO 02-21 18:53:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-21T18:53:43.9197832Z INFO 02-21 18:53:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-21T18:54:09.3526189Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m WARNING 02-21 18:54:09 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-21T18:54:09.3549965Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m WARNING 02-21 18:54:09 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-21T18:54:09.3575773Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m WARNING 02-21 18:54:09 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-21T18:54:09.3620259Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m WARNING 02-21 18:54:09 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-21T18:54:09.3689673Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m WARNING 02-21 18:54:09 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-21T18:54:09.4019452Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m WARNING 02-21 18:54:09 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-21T18:54:09.4028550Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m WARNING 02-21 18:54:09 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-21T18:54:09.4038532Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m WARNING 02-21 18:54:09 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-21T18:54:09.4048914Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m WARNING 02-21 18:54:09 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-21T18:54:09.4059564Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m WARNING 02-21 18:54:09 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-21T18:54:09.4069525Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m WARNING 02-21 18:54:09 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-21T18:54:09.4080383Z [0;36m(Worker_DP1_TP3_EP11 pid=476)[0;0m WARNING 02-21 18:54:09 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-21T18:54:09.4090978Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m WARNING 02-21 18:54:09 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-21T18:54:09.4146498Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m WARNING 02-21 18:54:09 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-21T18:54:09.4202449Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-21 18:54:09 [model_runner_v1.py:2315] Loading drafter model...
2026-02-21T18:54:09.4244360Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m INFO 02-21 18:54:09 [model_runner_v1.py:2315] Loading drafter model...
2026-02-21T18:54:09.4254094Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-21 18:54:09 [model_runner_v1.py:2315] Loading drafter model...
2026-02-21T18:54:09.4266061Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-21 18:54:09 [model_runner_v1.py:2315] Loading drafter model...
2026-02-21T18:54:09.4328433Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-21 18:54:09 [model_runner_v1.py:2315] Loading drafter model...
2026-02-21T18:54:09.4339246Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m WARNING 02-21 18:54:09 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-21T18:54:09.4349307Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-21 18:54:09 [model_runner_v1.py:2315] Loading drafter model...
2026-02-21T18:54:09.4359536Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-21 18:54:09 [model_runner_v1.py:2315] Loading drafter model...
2026-02-21T18:54:09.4370530Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m INFO 02-21 18:54:09 [model_runner_v1.py:2315] Loading drafter model...
2026-02-21T18:54:09.4380822Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-21 18:54:09 [model_runner_v1.py:2315] Loading drafter model...
2026-02-21T18:54:09.4497578Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-21 18:54:09 [model_runner_v1.py:2315] Loading drafter model...
2026-02-21T18:54:09.4570917Z [0;36m(Worker_DP0_TP2_EP2 pid=371)[0;0m WARNING 02-21 18:54:09 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-21T18:54:09.4601371Z [0;36m(Worker_DP1_TP3_EP11 pid=476)[0;0m INFO 02-21 18:54:09 [model_runner_v1.py:2315] Loading drafter model...
2026-02-21T18:54:09.4662528Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-21 18:54:09 [model_runner_v1.py:2315] Loading drafter model...
2026-02-21T18:54:09.4670724Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-21 18:54:09 [model_runner_v1.py:2315] Loading drafter model...
2026-02-21T18:54:09.4680972Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-21 18:54:09 [model_runner_v1.py:2315] Loading drafter model...
2026-02-21T18:54:09.5203789Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:09.5205167Z Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
2026-02-21T18:54:09.5433749Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-21 18:54:09 [model_runner_v1.py:2315] Loading drafter model...
2026-02-21T18:54:09.5630314Z [0;36m(Worker_DP0_TP2_EP2 pid=371)[0;0m INFO 02-21 18:54:09 [model_runner_v1.py:2315] Loading drafter model...
2026-02-21T18:54:10.0547640Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:10.0547977Z Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<01:26,  1.87it/s]
2026-02-21T18:54:10.5565367Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:10.5565721Z Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:01<01:22,  1.94it/s]
2026-02-21T18:54:11.3009742Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:11.3010101Z Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:01<01:39,  1.61it/s]
2026-02-21T18:54:12.5174395Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:12.5174746Z Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:02<02:15,  1.17it/s]
2026-02-21T18:54:13.7006161Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:13.7006527Z Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:04<02:33,  1.03it/s]
2026-02-21T18:54:14.9134813Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:14.9135169Z Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:05<02:45,  1.06s/it]
2026-02-21T18:54:16.0999627Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:16.1000020Z Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:06<02:51,  1.10s/it]
2026-02-21T18:54:17.1664521Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:17.1664874Z Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:07<02:48,  1.09s/it]
2026-02-21T18:54:18.2848939Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:18.2849296Z Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:08<02:49,  1.10s/it]
2026-02-21T18:54:19.4240621Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:19.4240970Z Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:09<02:49,  1.11s/it]
2026-02-21T18:54:20.5795966Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:20.5796317Z Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:11<02:50,  1.12s/it]
2026-02-21T18:54:21.7404099Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:21.7404551Z Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:12<02:51,  1.14s/it]
2026-02-21T18:54:22.9315699Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:22.9316046Z Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:13<02:52,  1.15s/it]
2026-02-21T18:54:24.1296109Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:24.1296492Z Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:14<02:53,  1.17s/it]
2026-02-21T18:54:25.1888936Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:25.1889300Z Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:15<02:47,  1.13s/it]
2026-02-21T18:54:26.1565158Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:26.1565556Z Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:16<02:39,  1.08s/it]
2026-02-21T18:54:27.1352544Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:27.1352911Z Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:17<02:33,  1.05s/it]
2026-02-21T18:54:28.0658901Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:28.0659267Z Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:18<02:27,  1.02s/it]
2026-02-21T18:54:29.1002228Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:29.1002588Z Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:19<02:27,  1.02s/it]
2026-02-21T18:54:30.2274241Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:30.2274590Z Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:20<02:30,  1.05s/it]
2026-02-21T18:54:31.2648260Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:31.2648620Z Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:21<02:28,  1.05s/it]
2026-02-21T18:54:32.3443161Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:32.3443565Z Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:22<02:29,  1.06s/it]
2026-02-21T18:54:33.4399605Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:33.4400021Z Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:23<02:29,  1.07s/it]
2026-02-21T18:54:34.4852982Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:34.4853342Z Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:24<02:27,  1.06s/it]
2026-02-21T18:54:35.5146540Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:35.5146894Z Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:25<02:25,  1.05s/it]
2026-02-21T18:54:37.0018986Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:37.0019370Z Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:27<02:42,  1.18s/it]
2026-02-21T18:54:38.2511079Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:38.2511437Z Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:28<02:43,  1.20s/it]
2026-02-21T18:54:39.3156709Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:39.3157086Z Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:29<02:36,  1.16s/it]
2026-02-21T18:54:40.3623631Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:40.3624178Z Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:30<02:31,  1.13s/it]
2026-02-21T18:54:41.4674982Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:41.4675352Z Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:31<02:29,  1.12s/it]
2026-02-21T18:54:42.6550958Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:42.6551314Z Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:33<02:30,  1.14s/it]
2026-02-21T18:54:43.7431731Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:43.7432202Z Loading safetensors checkpoint shards:  20% Completed | 32/163 [00:34<02:27,  1.12s/it]
2026-02-21T18:54:44.8186670Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:44.8187024Z Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:35<02:24,  1.11s/it]
2026-02-21T18:54:45.9109468Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:45.9109866Z Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:36<02:22,  1.10s/it]
2026-02-21T18:54:46.9754480Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:46.9754841Z Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:37<02:19,  1.09s/it]
2026-02-21T18:54:47.9977872Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:47.9978224Z Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:38<02:15,  1.07s/it]
2026-02-21T18:54:49.0568482Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:49.0568839Z Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:39<02:14,  1.07s/it]
2026-02-21T18:54:50.1284368Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:50.1284725Z Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:40<02:13,  1.07s/it]
2026-02-21T18:54:51.3299483Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:51.3300165Z Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:41<02:17,  1.11s/it]
2026-02-21T18:54:52.3706504Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:52.3706874Z Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:42<02:13,  1.09s/it]
2026-02-21T18:54:53.5297926Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:53.5298279Z Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:44<02:15,  1.11s/it]
2026-02-21T18:54:54.6861065Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:54.6861424Z Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:45<02:15,  1.12s/it]
2026-02-21T18:54:55.7352849Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:55.7353214Z Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:46<02:12,  1.10s/it]
2026-02-21T18:54:56.7927051Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:56.7927402Z Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:47<02:09,  1.09s/it]
2026-02-21T18:54:57.9387480Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:57.9387911Z Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:48<02:10,  1.11s/it]
2026-02-21T18:54:59.0976610Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:59.0976960Z Loading safetensors checkpoint shards:  28% Completed | 46/163 [00:49<02:11,  1.12s/it]
2026-02-21T18:54:59.8199736Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:54:59.8200088Z Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:50<01:56,  1.00s/it]
2026-02-21T18:55:00.9913294Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:00.9913653Z Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:51<01:32,  1.23it/s]
2026-02-21T18:55:02.1874160Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:02.1874557Z Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:52<01:42,  1.10it/s]
2026-02-21T18:55:03.3069846Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:03.3070242Z Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:53<01:47,  1.04it/s]
2026-02-21T18:55:04.4587398Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:04.4587755Z Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:54<01:52,  1.01s/it]
2026-02-21T18:55:05.5163242Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:05.5163597Z Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:55<01:52,  1.03s/it]
2026-02-21T18:55:06.6989292Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:06.6989657Z Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:57<01:56,  1.07s/it]
2026-02-21T18:55:07.8339054Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:07.8339429Z Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:58<01:57,  1.09s/it]
2026-02-21T18:55:09.0427481Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:09.0427841Z Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:59<02:00,  1.12s/it]
2026-02-21T18:55:10.2389042Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:10.2389752Z Loading safetensors checkpoint shards:  35% Completed | 57/163 [01:00<02:01,  1.15s/it]
2026-02-21T18:55:11.4194684Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:11.4195060Z Loading safetensors checkpoint shards:  36% Completed | 58/163 [01:01<02:01,  1.16s/it]
2026-02-21T18:55:12.5813706Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:12.5814060Z Loading safetensors checkpoint shards:  36% Completed | 59/163 [01:03<02:00,  1.16s/it]
2026-02-21T18:55:13.8009962Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:13.8010346Z Loading safetensors checkpoint shards:  37% Completed | 60/163 [01:04<02:01,  1.18s/it]
2026-02-21T18:55:15.0533015Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:15.0533384Z Loading safetensors checkpoint shards:  37% Completed | 61/163 [01:05<02:02,  1.20s/it]
2026-02-21T18:55:16.2384603Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:16.2385250Z Loading safetensors checkpoint shards:  38% Completed | 62/163 [01:06<02:00,  1.19s/it]
2026-02-21T18:55:17.4219710Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:17.4220075Z Loading safetensors checkpoint shards:  39% Completed | 63/163 [01:07<01:59,  1.19s/it]
2026-02-21T18:55:18.5548482Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:18.5548845Z Loading safetensors checkpoint shards:  39% Completed | 64/163 [01:09<01:56,  1.17s/it]
2026-02-21T18:55:19.5341391Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:19.5341743Z Loading safetensors checkpoint shards:  40% Completed | 65/163 [01:10<01:49,  1.12s/it]
2026-02-21T18:55:20.5122765Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:20.5123117Z Loading safetensors checkpoint shards:  40% Completed | 66/163 [01:10<01:44,  1.07s/it]
2026-02-21T18:55:21.5741403Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:21.5741760Z Loading safetensors checkpoint shards:  41% Completed | 67/163 [01:12<01:42,  1.07s/it]
2026-02-21T18:55:22.6912356Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:22.6912700Z Loading safetensors checkpoint shards:  42% Completed | 68/163 [01:13<01:43,  1.08s/it]
2026-02-21T18:55:23.8776844Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:23.8777201Z Loading safetensors checkpoint shards:  42% Completed | 69/163 [01:14<01:44,  1.12s/it]
2026-02-21T18:55:25.0609353Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:25.0609695Z Loading safetensors checkpoint shards:  43% Completed | 70/163 [01:15<01:45,  1.13s/it]
2026-02-21T18:55:26.2129053Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:26.2129422Z Loading safetensors checkpoint shards:  44% Completed | 71/163 [01:16<01:45,  1.14s/it]
2026-02-21T18:55:27.4104172Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:27.4104532Z Loading safetensors checkpoint shards:  44% Completed | 72/163 [01:17<01:45,  1.16s/it]
2026-02-21T18:55:28.4464876Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:28.4465290Z Loading safetensors checkpoint shards:  45% Completed | 73/163 [01:18<01:40,  1.12s/it]
2026-02-21T18:55:29.6974532Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:29.6974895Z Loading safetensors checkpoint shards:  45% Completed | 74/163 [01:20<01:43,  1.16s/it]
2026-02-21T18:55:31.0501823Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:31.0502313Z Loading safetensors checkpoint shards:  46% Completed | 75/163 [01:21<01:47,  1.22s/it]
2026-02-21T18:55:32.3398531Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:32.3398884Z Loading safetensors checkpoint shards:  47% Completed | 76/163 [01:22<01:47,  1.24s/it]
2026-02-21T18:55:33.4445374Z [0;36m(ApiServer_1 pid=185)[0;0m Process ApiServer_1:
2026-02-21T18:55:33.4494669Z [0;36m(ApiServer_0 pid=184)[0;0m Process ApiServer_0:
2026-02-21T18:55:33.4518210Z [0;36m(ApiServer_2 pid=186)[0;0m Process ApiServer_2:
2026-02-21T18:55:33.4590867Z [0;36m(ApiServer_1 pid=185)[0;0m Traceback (most recent call last):
2026-02-21T18:55:33.4600994Z [0;36m(ApiServer_2 pid=186)[0;0m Traceback (most recent call last):
2026-02-21T18:55:33.4611862Z [0;36m(ApiServer_0 pid=184)[0;0m Traceback (most recent call last):
2026-02-21T18:55:33.4621268Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-21T18:55:33.4631099Z [0;36m(ApiServer_1 pid=185)[0;0m     self.run()
2026-02-21T18:55:33.4641032Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-21T18:55:33.4650264Z [0;36m(ApiServer_2 pid=186)[0;0m     self.run()
2026-02-21T18:55:33.4662987Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-21T18:55:33.4670063Z [0;36m(ApiServer_1 pid=185)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-21T18:55:33.4680326Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-21T18:55:33.4690209Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-21T18:55:33.4700227Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-21T18:55:33.4709965Z [0;36m(ApiServer_1 pid=185)[0;0m     uvloop.run(
2026-02-21T18:55:33.4719981Z [0;36m(ApiServer_0 pid=184)[0;0m     self.run()
2026-02-21T18:55:33.4729528Z [0;36m(ApiServer_2 pid=186)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-21T18:55:33.4738974Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-21T18:55:33.4748906Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-21T18:55:33.4759284Z [0;36m(ApiServer_1 pid=185)[0;0m     return runner.run(wrapper())
2026-02-21T18:55:33.4769648Z [0;36m(ApiServer_2 pid=186)[0;0m     uvloop.run(
2026-02-21T18:55:33.4779980Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-21T18:55:33.4789826Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.4800993Z [0;36m(ApiServer_0 pid=184)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-21T18:55:33.4811723Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-21T18:55:33.4821465Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-21T18:55:33.4831887Z [0;36m(ApiServer_2 pid=186)[0;0m     return runner.run(wrapper())
2026-02-21T18:55:33.4843620Z [0;36m(ApiServer_1 pid=185)[0;0m     return self._loop.run_until_complete(task)
2026-02-21T18:55:33.4854175Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.4865717Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-21T18:55:33.4875431Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.4885280Z [0;36m(ApiServer_0 pid=184)[0;0m     uvloop.run(
2026-02-21T18:55:33.4895557Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-21T18:55:33.4905191Z [0;36m(ApiServer_1 pid=185)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-21T18:55:33.4914788Z [0;36m(ApiServer_2 pid=186)[0;0m     return self._loop.run_until_complete(task)
2026-02-21T18:55:33.4926463Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-21T18:55:33.4935173Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.4944995Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-21T18:55:33.4954196Z [0;36m(ApiServer_0 pid=184)[0;0m     return runner.run(wrapper())
2026-02-21T18:55:33.4964979Z [0;36m(ApiServer_1 pid=185)[0;0m     return await main
2026-02-21T18:55:33.4974388Z [0;36m(ApiServer_2 pid=186)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-21T18:55:33.4983924Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.4993103Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^
2026-02-21T18:55:33.5004154Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-21T18:55:33.5013711Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-21T18:55:33.5023013Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-21T18:55:33.5032972Z [0;36m(ApiServer_2 pid=186)[0;0m     return await main
2026-02-21T18:55:33.5043085Z [0;36m(ApiServer_0 pid=184)[0;0m     return self._loop.run_until_complete(task)
2026-02-21T18:55:33.5052586Z [0;36m(ApiServer_1 pid=185)[0;0m     async with build_async_engine_client(
2026-02-21T18:55:33.5061821Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^
2026-02-21T18:55:33.5070916Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.5081210Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-21T18:55:33.5092117Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-21T18:55:33.5100991Z [0;36m(ApiServer_0 pid=184)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-21T18:55:33.5109871Z [0;36m(ApiServer_1 pid=185)[0;0m     return await anext(self.gen)
2026-02-21T18:55:33.5120512Z [0;36m(ApiServer_2 pid=186)[0;0m     async with build_async_engine_client(
2026-02-21T18:55:33.5129952Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.5140426Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-21T18:55:33.5149982Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-21T18:55:33.5160123Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-21T18:55:33.5169951Z [0;36m(ApiServer_0 pid=184)[0;0m     return await main
2026-02-21T18:55:33.5178975Z [0;36m(ApiServer_2 pid=186)[0;0m     return await anext(self.gen)
2026-02-21T18:55:33.5188981Z [0;36m(ApiServer_1 pid=185)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-21T18:55:33.5198386Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^
2026-02-21T18:55:33.5207655Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.5217555Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-21T18:55:33.5227383Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-21T18:55:33.5237828Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-21T18:55:33.5247306Z [0;36m(ApiServer_1 pid=185)[0;0m     return await anext(self.gen)
2026-02-21T18:55:33.5257269Z [0;36m(ApiServer_2 pid=186)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-21T18:55:33.5266438Z [0;36m(ApiServer_0 pid=184)[0;0m     async with build_async_engine_client(
2026-02-21T18:55:33.5275848Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.5286191Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-21T18:55:33.5295859Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-21T18:55:33.5305150Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-21T18:55:33.5314544Z [0;36m(ApiServer_2 pid=186)[0;0m     return await anext(self.gen)
2026-02-21T18:55:33.5323932Z [0;36m(ApiServer_0 pid=184)[0;0m     return await anext(self.gen)
2026-02-21T18:55:33.5333683Z [0;36m(ApiServer_1 pid=185)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-21T18:55:33.5344767Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.5353377Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.5362290Z [0;36m(ApiServer_1 pid=185)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.5372590Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-21T18:55:33.5382233Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-21T18:55:33.5391745Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-21T18:55:33.5401607Z [0;36m(ApiServer_2 pid=186)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-21T18:55:33.5411100Z [0;36m(ApiServer_0 pid=184)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-21T18:55:33.5420290Z [0;36m(ApiServer_1 pid=185)[0;0m     return cls(
2026-02-21T18:55:33.5429699Z [0;36m(ApiServer_2 pid=186)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.5440246Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^
2026-02-21T18:55:33.5450547Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-21T18:55:33.5460618Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-21T18:55:33.5471179Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-21T18:55:33.5484645Z [0;36m(ApiServer_0 pid=184)[0;0m     return await anext(self.gen)
2026-02-21T18:55:33.5494336Z [0;36m(ApiServer_2 pid=186)[0;0m     return cls(
2026-02-21T18:55:33.5504027Z [0;36m(ApiServer_1 pid=185)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-21T18:55:33.5513974Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.5524272Z [0;36m(ApiServer_1 pid=185)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.5533572Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^
2026-02-21T18:55:33.5544216Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-21T18:55:33.5558055Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-21T18:55:33.5568176Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-21T18:55:33.5578012Z [0;36m(ApiServer_0 pid=184)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-21T18:55:33.5589396Z [0;36m(ApiServer_1 pid=185)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-21T18:55:33.5599681Z [0;36m(ApiServer_2 pid=186)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-21T18:55:33.5608756Z [0;36m(ApiServer_0 pid=184)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.5618539Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.5628847Z [0;36m(ApiServer_2 pid=186)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.5637831Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-21T18:55:33.5646854Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-21T18:55:33.5657258Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-21T18:55:33.5665094Z [0;36m(ApiServer_1 pid=185)[0;0m     super().__init__(
2026-02-21T18:55:33.5673912Z [0;36m(ApiServer_0 pid=184)[0;0m     return cls(
2026-02-21T18:55:33.5684228Z [0;36m(ApiServer_2 pid=186)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-21T18:55:33.5693468Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^
2026-02-21T18:55:33.5742723Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.5743186Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-21T18:55:33.5743588Z [0;36m(ApiServer_1 pid=185)[0;0m     super().__init__(
2026-02-21T18:55:33.5744071Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-21T18:55:33.5744599Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-21T18:55:33.5749991Z [0;36m(ApiServer_2 pid=186)[0;0m     super().__init__(
2026-02-21T18:55:33.5760568Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-21T18:55:33.5771096Z [0;36m(ApiServer_0 pid=184)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-21T18:55:33.5781872Z [0;36m(ApiServer_1 pid=185)[0;0m     super().__init__(
2026-02-21T18:55:33.5792273Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-21T18:55:33.5802157Z [0;36m(ApiServer_0 pid=184)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.5811658Z [0;36m(ApiServer_2 pid=186)[0;0m     super().__init__(
2026-02-21T18:55:33.5821363Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-21T18:55:33.5831618Z [0;36m(ApiServer_1 pid=185)[0;0m     raise TimeoutError(
2026-02-21T18:55:33.5842078Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-21T18:55:33.5852526Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-21T18:55:33.5861851Z [0;36m(ApiServer_0 pid=184)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-21T18:55:33.5871722Z [0;36m(ApiServer_2 pid=186)[0;0m     super().__init__(
2026-02-21T18:55:33.5881971Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-21T18:55:33.5891949Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-21T18:55:33.5902184Z [0;36m(ApiServer_1 pid=185)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-21T18:55:33.5911291Z [0;36m(ApiServer_0 pid=184)[0;0m     super().__init__(
2026-02-21T18:55:33.5921252Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-21T18:55:33.5931038Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-21T18:55:33.5940453Z [0;36m(ApiServer_2 pid=186)[0;0m     raise TimeoutError(
2026-02-21T18:55:33.5949705Z [0;36m(ApiServer_0 pid=184)[0;0m     super().__init__(
2026-02-21T18:55:33.5958582Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-21T18:55:33.5968157Z [0;36m(ApiServer_0 pid=184)[0;0m     super().__init__(
2026-02-21T18:55:33.5977786Z [0;36m(ApiServer_2 pid=186)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-21T18:55:33.5987271Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-21T18:55:33.5996821Z [0;36m(ApiServer_0 pid=184)[0;0m     raise TimeoutError(
2026-02-21T18:55:33.6006759Z [0;36m(ApiServer_0 pid=184)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-21T18:55:33.6075595Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:33.6075903Z Loading safetensors checkpoint shards:  47% Completed | 77/163 [01:24<01:47,  1.25s/it]
2026-02-21T18:55:33.8793661Z [0;36m(ApiServer_1 pid=185)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-21T18:55:33.8821226Z [0;36m(ApiServer_0 pid=184)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-21T18:55:33.9145936Z [0;36m(ApiServer_2 pid=186)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-21T18:55:34.8632376Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:34.8632732Z Loading safetensors checkpoint shards:  48% Completed | 78/163 [01:25<01:46,  1.25s/it]
2026-02-21T18:55:36.1428094Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:36.1428457Z Loading safetensors checkpoint shards:  48% Completed | 79/163 [01:26<01:45,  1.26s/it]
2026-02-21T18:55:37.3076694Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:37.3077058Z Loading safetensors checkpoint shards:  49% Completed | 80/163 [01:27<01:42,  1.23s/it]
2026-02-21T18:55:38.5245273Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:38.5245639Z Loading safetensors checkpoint shards:  50% Completed | 81/163 [01:29<01:40,  1.23s/it]
2026-02-21T18:55:39.7779989Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:39.7780356Z Loading safetensors checkpoint shards:  50% Completed | 82/163 [01:30<01:40,  1.23s/it]
2026-02-21T18:55:40.9851452Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:40.9851810Z Loading safetensors checkpoint shards:  51% Completed | 83/163 [01:31<01:38,  1.23s/it]
2026-02-21T18:55:41.7360155Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:41.7360556Z Loading safetensors checkpoint shards:  52% Completed | 84/163 [01:32<01:25,  1.08s/it]
2026-02-21T18:55:42.4738283Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:42.4738642Z Loading safetensors checkpoint shards:  52% Completed | 85/163 [01:32<01:16,  1.02it/s]
2026-02-21T18:55:43.2586281Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:43.2586655Z Loading safetensors checkpoint shards:  53% Completed | 86/163 [01:33<01:10,  1.09it/s]
2026-02-21T18:55:44.1069621Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:44.1069981Z Loading safetensors checkpoint shards:  53% Completed | 87/163 [01:34<01:08,  1.11it/s]
2026-02-21T18:55:44.8930808Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:44.8931166Z Loading safetensors checkpoint shards:  54% Completed | 88/163 [01:35<01:04,  1.16it/s]
2026-02-21T18:55:45.6987175Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:45.6987531Z Loading safetensors checkpoint shards:  55% Completed | 89/163 [01:36<01:02,  1.18it/s]
2026-02-21T18:55:46.5465185Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:46.5465832Z Loading safetensors checkpoint shards:  55% Completed | 90/163 [01:37<01:01,  1.18it/s]
2026-02-21T18:55:47.3587602Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:47.3587972Z Loading safetensors checkpoint shards:  56% Completed | 91/163 [01:37<01:00,  1.19it/s]
2026-02-21T18:55:48.1338362Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:48.1338727Z Loading safetensors checkpoint shards:  56% Completed | 92/163 [01:38<00:58,  1.22it/s]
2026-02-21T18:55:48.9177528Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:48.9177937Z Loading safetensors checkpoint shards:  57% Completed | 93/163 [01:39<00:56,  1.24it/s]
2026-02-21T18:55:49.7757678Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:49.7758101Z Loading safetensors checkpoint shards:  58% Completed | 94/163 [01:40<00:56,  1.21it/s]
2026-02-21T18:55:50.8840137Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:50.8840795Z Loading safetensors checkpoint shards:  58% Completed | 95/163 [01:41<01:01,  1.10it/s]
2026-02-21T18:55:51.6460006Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:51.6460353Z Loading safetensors checkpoint shards:  59% Completed | 96/163 [01:42<00:57,  1.16it/s]
2026-02-21T18:55:52.5042854Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:52.5043209Z Loading safetensors checkpoint shards:  60% Completed | 97/163 [01:42<00:56,  1.16it/s]
2026-02-21T18:55:53.5924293Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:53.5924669Z Loading safetensors checkpoint shards:  60% Completed | 98/163 [01:44<01:00,  1.07it/s]
2026-02-21T18:55:54.3264327Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:54.3264716Z Loading safetensors checkpoint shards:  61% Completed | 99/163 [01:44<00:55,  1.15it/s]
2026-02-21T18:55:55.2972900Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:55.2973286Z Loading safetensors checkpoint shards:  61% Completed | 100/163 [01:45<00:56,  1.11it/s]
2026-02-21T18:55:56.1724065Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:56.1724435Z Loading safetensors checkpoint shards:  62% Completed | 101/163 [01:46<00:55,  1.12it/s]
2026-02-21T18:55:56.9589377Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:56.9589737Z Loading safetensors checkpoint shards:  63% Completed | 102/163 [01:47<00:52,  1.16it/s]
2026-02-21T18:55:57.7082115Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:57.7082470Z Loading safetensors checkpoint shards:  63% Completed | 103/163 [01:48<00:49,  1.21it/s]
2026-02-21T18:55:58.2964095Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:58.2964465Z Loading safetensors checkpoint shards:  64% Completed | 104/163 [01:48<00:44,  1.32it/s]
2026-02-21T18:55:58.8719342Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:58.8719700Z Loading safetensors checkpoint shards:  64% Completed | 105/163 [01:49<00:40,  1.43it/s]
2026-02-21T18:55:59.3401357Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:55:59.3401694Z Loading safetensors checkpoint shards:  65% Completed | 106/163 [01:49<00:36,  1.58it/s]
2026-02-21T18:56:01.8930472Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:01.8930841Z Loading safetensors checkpoint shards:  66% Completed | 107/163 [01:52<01:07,  1.21s/it]
2026-02-21T18:56:02.3123522Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:02.3123887Z Loading safetensors checkpoint shards:  66% Completed | 108/163 [01:52<00:53,  1.03it/s]
2026-02-21T18:56:03.1342984Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:03.1343381Z Loading safetensors checkpoint shards:  67% Completed | 109/163 [01:53<00:50,  1.08it/s]
2026-02-21T18:56:04.3031705Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:04.3032232Z Loading safetensors checkpoint shards:  67% Completed | 110/163 [01:54<00:52,  1.00it/s]
2026-02-21T18:56:05.6167604Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:05.6167998Z Loading safetensors checkpoint shards:  68% Completed | 111/163 [01:56<00:56,  1.09s/it]
2026-02-21T18:56:06.5723128Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:06.5723513Z Loading safetensors checkpoint shards:  69% Completed | 112/163 [01:57<00:53,  1.05s/it]
2026-02-21T18:56:07.4972998Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:07.4973366Z Loading safetensors checkpoint shards:  69% Completed | 113/163 [01:57<00:50,  1.01s/it]
2026-02-21T18:56:08.5026195Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:08.5026573Z Loading safetensors checkpoint shards:  70% Completed | 114/163 [01:58<00:49,  1.01s/it]
2026-02-21T18:56:09.5667462Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:09.5667840Z Loading safetensors checkpoint shards:  71% Completed | 115/163 [02:00<00:49,  1.03s/it]
2026-02-21T18:56:10.4816602Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:10.4816978Z Loading safetensors checkpoint shards:  71% Completed | 116/163 [02:00<00:46,  1.01it/s]
2026-02-21T18:56:11.4961775Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:11.4962272Z Loading safetensors checkpoint shards:  72% Completed | 117/163 [02:01<00:45,  1.00it/s]
2026-02-21T18:56:12.4288723Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:12.4289088Z Loading safetensors checkpoint shards:  72% Completed | 118/163 [02:02<00:44,  1.02it/s]
2026-02-21T18:56:13.5558217Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:13.5558578Z Loading safetensors checkpoint shards:  73% Completed | 119/163 [02:04<00:45,  1.02s/it]
2026-02-21T18:56:14.6689921Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:14.6690290Z Loading safetensors checkpoint shards:  74% Completed | 120/163 [02:05<00:45,  1.05s/it]
2026-02-21T18:56:15.7959369Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:15.7959720Z Loading safetensors checkpoint shards:  74% Completed | 121/163 [02:06<00:45,  1.07s/it]
2026-02-21T18:56:16.9223866Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:16.9224255Z Loading safetensors checkpoint shards:  75% Completed | 122/163 [02:07<00:44,  1.09s/it]
2026-02-21T18:56:18.0319551Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:18.0319914Z Loading safetensors checkpoint shards:  75% Completed | 123/163 [02:08<00:43,  1.10s/it]
2026-02-21T18:56:19.1981667Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:19.1982106Z Loading safetensors checkpoint shards:  76% Completed | 124/163 [02:09<00:43,  1.12s/it]
2026-02-21T18:56:20.2409094Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:20.2409443Z Loading safetensors checkpoint shards:  77% Completed | 125/163 [02:10<00:41,  1.09s/it]
2026-02-21T18:56:21.2810387Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:21.2810738Z Loading safetensors checkpoint shards:  77% Completed | 126/163 [02:11<00:39,  1.08s/it]
2026-02-21T18:56:22.3641311Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:22.3641716Z Loading safetensors checkpoint shards:  78% Completed | 127/163 [02:12<00:38,  1.08s/it]
2026-02-21T18:56:23.4770155Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:23.4770607Z Loading safetensors checkpoint shards:  79% Completed | 128/163 [02:13<00:38,  1.09s/it]
2026-02-21T18:56:24.5860135Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:24.5860494Z Loading safetensors checkpoint shards:  80% Completed | 130/163 [02:15<00:27,  1.19it/s]
2026-02-21T18:56:25.6971608Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:25.6971971Z Loading safetensors checkpoint shards:  80% Completed | 131/163 [02:16<00:29,  1.10it/s]
2026-02-21T18:56:26.8644605Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:26.8644967Z Loading safetensors checkpoint shards:  81% Completed | 132/163 [02:17<00:30,  1.02it/s]
2026-02-21T18:56:27.9340581Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:27.9340929Z Loading safetensors checkpoint shards:  82% Completed | 133/163 [02:18<00:30,  1.00s/it]
2026-02-21T18:56:29.0243682Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:29.0244581Z Loading safetensors checkpoint shards:  82% Completed | 134/163 [02:19<00:29,  1.03s/it]
2026-02-21T18:56:30.1051745Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:30.1052214Z Loading safetensors checkpoint shards:  83% Completed | 135/163 [02:20<00:29,  1.04s/it]
2026-02-21T18:56:31.2117397Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:31.2117780Z Loading safetensors checkpoint shards:  83% Completed | 136/163 [02:21<00:28,  1.06s/it]
2026-02-21T18:56:32.2627358Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:32.2627704Z Loading safetensors checkpoint shards:  84% Completed | 137/163 [02:22<00:27,  1.06s/it]
2026-02-21T18:56:33.2431809Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:33.2432270Z Loading safetensors checkpoint shards:  85% Completed | 138/163 [02:23<00:25,  1.04s/it]
2026-02-21T18:56:34.2285784Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:34.2286530Z Loading safetensors checkpoint shards:  85% Completed | 139/163 [02:24<00:24,  1.02s/it]
2026-02-21T18:56:35.2114681Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:35.2115040Z Loading safetensors checkpoint shards:  86% Completed | 140/163 [02:25<00:23,  1.01s/it]
2026-02-21T18:56:36.2736174Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:36.2736535Z Loading safetensors checkpoint shards:  87% Completed | 141/163 [02:26<00:22,  1.03s/it]
2026-02-21T18:56:37.3269156Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:37.3269530Z Loading safetensors checkpoint shards:  87% Completed | 142/163 [02:27<00:21,  1.03s/it]
2026-02-21T18:56:38.3595826Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:38.3596185Z Loading safetensors checkpoint shards:  88% Completed | 143/163 [02:28<00:20,  1.03s/it]
2026-02-21T18:56:39.4449309Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:39.4449653Z Loading safetensors checkpoint shards:  88% Completed | 144/163 [02:29<00:19,  1.05s/it]
2026-02-21T18:56:40.5262330Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:40.5262683Z Loading safetensors checkpoint shards:  89% Completed | 145/163 [02:31<00:19,  1.06s/it]
2026-02-21T18:56:41.4849892Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:41.4850246Z Loading safetensors checkpoint shards:  90% Completed | 146/163 [02:31<00:17,  1.03s/it]
2026-02-21T18:56:42.4400401Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:42.4400758Z Loading safetensors checkpoint shards:  90% Completed | 147/163 [02:32<00:16,  1.01s/it]
2026-02-21T18:56:43.4618349Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:43.4618709Z Loading safetensors checkpoint shards:  91% Completed | 148/163 [02:33<00:15,  1.01s/it]
2026-02-21T18:56:44.5601028Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:44.5601391Z Loading safetensors checkpoint shards:  91% Completed | 149/163 [02:35<00:14,  1.04s/it]
2026-02-21T18:56:49.4034016Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:49.4034495Z Loading safetensors checkpoint shards:  92% Completed | 150/163 [02:39<00:28,  2.18s/it]
2026-02-21T18:56:50.1872402Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:50.1872748Z Loading safetensors checkpoint shards:  93% Completed | 151/163 [02:40<00:21,  1.76s/it]
2026-02-21T18:56:51.1921263Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:51.1921621Z Loading safetensors checkpoint shards:  93% Completed | 152/163 [02:41<00:16,  1.53s/it]
2026-02-21T18:56:52.2123349Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:52.2123718Z Loading safetensors checkpoint shards:  94% Completed | 153/163 [02:42<00:13,  1.38s/it]
2026-02-21T18:56:53.2281464Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:53.2281824Z Loading safetensors checkpoint shards:  94% Completed | 154/163 [02:43<00:11,  1.27s/it]
2026-02-21T18:56:54.3318454Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:54.3318841Z Loading safetensors checkpoint shards:  95% Completed | 155/163 [02:44<00:09,  1.22s/it]
2026-02-21T18:56:55.4787826Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:55.4788186Z Loading safetensors checkpoint shards:  96% Completed | 156/163 [02:45<00:08,  1.20s/it]
2026-02-21T18:56:56.6227481Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:56.6227835Z Loading safetensors checkpoint shards:  96% Completed | 157/163 [02:47<00:07,  1.18s/it]
2026-02-21T18:56:57.6310532Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:57.6310902Z Loading safetensors checkpoint shards:  97% Completed | 158/163 [02:48<00:05,  1.13s/it]
2026-02-21T18:56:58.6913883Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:58.6914259Z Loading safetensors checkpoint shards:  98% Completed | 159/163 [02:49<00:04,  1.11s/it]
2026-02-21T18:56:59.8003179Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:56:59.8003534Z Loading safetensors checkpoint shards:  98% Completed | 160/163 [02:50<00:03,  1.11s/it]
2026-02-21T18:57:00.9228498Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:57:00.9228882Z Loading safetensors checkpoint shards:  99% Completed | 161/163 [02:51<00:02,  1.11s/it]
2026-02-21T18:57:02.0509767Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:57:02.0510120Z Loading safetensors checkpoint shards:  99% Completed | 162/163 [02:52<00:01,  1.12s/it]
2026-02-21T18:57:03.1796451Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:57:03.1796832Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [02:53<00:00,  1.12s/it]
2026-02-21T18:57:03.1842428Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:57:03.1842900Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [02:53<00:00,  1.07s/it]
2026-02-21T18:57:03.1852436Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:57:03.2086673Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m INFO 02-21 18:57:03 [default_loader.py:291] Loading weights took 173.62 seconds
2026-02-21T18:57:03.2605767Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m INFO 02-21 18:57:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-21T18:57:03.4382820Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-21 18:57:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-21T18:57:03.4410090Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-21 18:57:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-21T18:57:03.4436061Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-21 18:57:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-21T18:57:03.4462370Z [0;36m(Worker_DP1_TP3_EP11 pid=476)[0;0m INFO 02-21 18:57:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-21T18:57:03.4473102Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-21 18:57:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-21T18:57:03.4487200Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-21 18:57:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-21T18:57:03.4502493Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-21 18:57:03 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-21T18:57:04.0871490Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m INFO 02-21 18:57:04 [default_loader.py:291] Loading weights took 174.57 seconds
2026-02-21T18:57:04.1183274Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-21 18:57:04 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-21T18:57:04.1209263Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-21 18:57:04 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-21T18:57:04.1255093Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-21 18:57:04 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-21T18:57:04.1264242Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-21 18:57:04 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-21T18:57:04.1274280Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-21 18:57:04 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-21T18:57:04.1284270Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m INFO 02-21 18:57:04 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-21T18:57:04.1294022Z [0;36m(Worker_DP0_TP2_EP2 pid=371)[0;0m INFO 02-21 18:57:04 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-21T18:57:04.1405224Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-21 18:57:04 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-21T18:57:04.5054992Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-21 18:57:04 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-21T18:57:04.8431645Z [0;36m(Worker_DP1_TP3_EP11 pid=476)[0;0m INFO 02-21 18:57:04 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-21T18:57:04.8703752Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-21 18:57:04 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-21T18:57:05.1374687Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-21 18:57:05 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-21T18:57:05.3243943Z [0;36m(Worker_DP0_TP2_EP2 pid=371)[0;0m INFO 02-21 18:57:05 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-21T18:57:05.4080083Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-21 18:57:05 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-21T18:57:05.6106704Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-21 18:57:05 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-21T18:57:05.6212746Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m INFO 02-21 18:57:05 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-21T18:57:05.8017255Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-21 18:57:05 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-21T18:57:06.1593701Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-21 18:57:06 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-21T18:57:06.4488343Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-21 18:57:06 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-21T18:57:06.5358208Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m INFO 02-21 18:57:06 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-21T18:57:06.6108644Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-21 18:57:06 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-21T18:57:06.6160568Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-21 18:57:06 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-21T18:57:06.9551532Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-21 18:57:06 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-21T18:57:07.2612747Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-21 18:57:07 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-21T18:57:12.2310951Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m INFO 02-21 18:57:12 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/5f4e5deba5/rank_0_0/backbone for vLLM's torch.compile
2026-02-21T18:57:12.2334472Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m INFO 02-21 18:57:12 [backends.py:865] Dynamo bytecode transform time: 4.46 s
2026-02-21T18:57:12.3937538Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m INFO 02-21 18:57:12 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/5f4e5deba5/rank_0_1/backbone for vLLM's torch.compile
2026-02-21T18:57:12.3963524Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m INFO 02-21 18:57:12 [backends.py:865] Dynamo bytecode transform time: 4.62 s
2026-02-21T18:57:18.7517533Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-21T18:57:18.7526273Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m   warnings.warn(
2026-02-21T18:57:18.7543513Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-21T18:57:18.7551944Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m   warnings.warn(
2026-02-21T18:57:18.7816530Z [0;36m(Worker_DP0_TP2_EP2 pid=371)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-21T18:57:18.7827055Z [0;36m(Worker_DP0_TP2_EP2 pid=371)[0;0m   warnings.warn(
2026-02-21T18:57:18.7920134Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-21T18:57:18.7928384Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m   warnings.warn(
2026-02-21T18:57:18.8323666Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-21T18:57:18.8330981Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m   warnings.warn(
2026-02-21T18:57:18.8604637Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-21T18:57:18.8612427Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m   warnings.warn(
2026-02-21T18:57:18.9244464Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-21T18:57:18.9270023Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m   warnings.warn(
2026-02-21T18:57:18.9299795Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-21T18:57:18.9307571Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m   warnings.warn(
2026-02-21T18:57:18.9388608Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-21T18:57:18.9397403Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m   warnings.warn(
2026-02-21T18:57:18.9417494Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-21T18:57:18.9426540Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m   warnings.warn(
2026-02-21T18:57:18.9528292Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-21T18:57:18.9536670Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m   warnings.warn(
2026-02-21T18:57:18.9630270Z [0;36m(Worker_DP1_TP3_EP11 pid=476)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-21T18:57:18.9639314Z [0;36m(Worker_DP1_TP3_EP11 pid=476)[0;0m   warnings.warn(
2026-02-21T18:57:19.0106321Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-21T18:57:19.0114616Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m   warnings.warn(
2026-02-21T18:57:19.0306481Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-21T18:57:19.0314355Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m   warnings.warn(
2026-02-21T18:57:19.0357782Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-21T18:57:19.0366424Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m   warnings.warn(
2026-02-21T18:57:19.0939592Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-21T18:57:19.0948292Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m   warnings.warn(
2026-02-21T18:57:31.5480710Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m INFO 02-21 18:57:31 [backends.py:319] Compiling a graph for compile range (1, 4096) takes 12.60 s
2026-02-21T18:57:31.5492179Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m INFO 02-21 18:57:31 [monitor.py:34] torch.compile takes 17.22 s in total
2026-02-21T18:57:32.5884637Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m INFO 02-21 18:57:32 [backends.py:319] Compiling a graph for compile range (1, 4096) takes 13.80 s
2026-02-21T18:57:32.5899567Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m INFO 02-21 18:57:32 [monitor.py:34] torch.compile takes 18.26 s in total
2026-02-21T18:57:37.7980798Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-21 18:57:37 [worker.py:338] Available memory: 18551938764, total memory: 65796046848
2026-02-21T18:57:37.8290367Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-21 18:57:37 [worker.py:338] Available memory: 18254834176, total memory: 65787658240
2026-02-21T18:57:37.8663254Z [0;36m(Worker_DP0_TP2_EP2 pid=371)[0;0m INFO 02-21 18:57:37 [worker.py:338] Available memory: 18269345280, total memory: 65787658240
2026-02-21T18:57:37.9145342Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-21 18:57:37 [worker.py:338] Available memory: 18544184012, total memory: 65796046848
2026-02-21T18:57:37.9224363Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-21 18:57:37 [worker.py:338] Available memory: 18546412236, total memory: 65796046848
2026-02-21T18:57:37.9551315Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m INFO 02-21 18:57:37 [worker.py:338] Available memory: 17856051712, total memory: 65787658240
2026-02-21T18:57:38.2308957Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-21 18:57:38 [worker.py:338] Available memory: 18245022208, total memory: 65787658240
2026-02-21T18:57:38.2847209Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-21 18:57:38 [worker.py:338] Available memory: 18257465856, total memory: 65787658240
2026-02-21T18:57:38.4954970Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-21 18:57:38 [worker.py:338] Available memory: 18255692288, total memory: 65787658240
2026-02-21T18:57:38.5847524Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-21 18:57:38 [worker.py:338] Available memory: 18262828544, total memory: 65787658240
2026-02-21T18:57:38.7616519Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-21 18:57:38 [worker.py:338] Available memory: 18554700492, total memory: 65796046848
2026-02-21T18:57:39.1644154Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-21 18:57:39 [worker.py:338] Available memory: 18535494348, total memory: 65796046848
2026-02-21T18:57:39.3521091Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-21 18:57:39 [worker.py:338] Available memory: 18561211084, total memory: 65796046848
2026-02-21T18:57:39.4618978Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-21 18:57:39 [worker.py:338] Available memory: 18554921676, total memory: 65796046848
2026-02-21T18:57:39.8297505Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m INFO 02-21 18:57:39 [worker.py:338] Available memory: 17859779072, total memory: 65787658240
2026-02-21T18:57:39.8320539Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-21 18:57:39 [kv_cache_utils.py:1307] GPU KV cache size: 204,544 tokens
2026-02-21T18:57:39.8330628Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-21 18:57:39 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 24.97x
2026-02-21T18:57:39.8513112Z [0;36m(Worker_DP1_TP3_EP11 pid=476)[0;0m INFO 02-21 18:57:39 [worker.py:338] Available memory: 18546784972, total memory: 65796046848
2026-02-21T18:57:39.8536362Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-21 18:57:39 [kv_cache_utils.py:1307] GPU KV cache size: 204,544 tokens
2026-02-21T18:57:39.8545943Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-21 18:57:39 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 24.97x
2026-02-21T18:57:56.7423846Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m 
2026-02-21T18:57:56.7424553Z Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s][rank12]:[W221 18:57:56.666756797 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-21T18:57:56.7434755Z [rank4]:[W221 18:57:56.666776067 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-21T18:57:56.7444769Z [rank13]:[W221 18:57:56.666778537 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-21T18:57:56.7455506Z [rank8]:[W221 18:57:56.666774897 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-21T18:57:56.7465060Z [rank11]:[W221 18:57:56.667174040 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-21T18:57:56.7475859Z [rank2]:[W221 18:57:56.667196020 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-21T18:57:56.7486440Z [rank3]:[W221 18:57:56.667196090 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-21T18:57:56.7495958Z [rank5]:[W221 18:57:56.667269701 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-21T18:57:56.7505991Z [rank7]:[W221 18:57:56.667396052 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-21T18:57:56.7515882Z [rank6]:[W221 18:57:56.667399642 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-21T18:57:56.7526548Z [rank14]:[W221 18:57:56.667921416 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-21T18:57:56.7537089Z [rank0]:[W221 18:57:56.668106757 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-21T18:57:56.7546769Z [rank9]:[W221 18:57:56.668472560 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-21T18:57:56.7557456Z [rank1]:[W221 18:57:56.670111383 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-21T18:57:56.7566935Z [rank15]:[W221 18:57:56.670125633 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-21T18:57:56.7577085Z [rank10]:[W221 18:57:56.679774206 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-21T18:57:59.0072913Z [rank4]:[W221 18:57:59.932395006 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-21T18:57:59.0081349Z [rank1]:[W221 18:57:59.932442036 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-21T18:57:59.0093424Z [rank3]:[W221 18:57:59.932448136 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-21T18:57:59.0104327Z [rank5]:[W221 18:57:59.932556287 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-21T18:57:59.0114538Z [rank0]:[W221 18:57:59.932868049 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-21T18:57:59.0124930Z [rank7]:[W221 18:57:59.933330763 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-21T18:57:59.0134436Z [rank2]:[W221 18:57:59.933425193 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-21T18:57:59.0144234Z [rank6]:[W221 18:57:59.933962537 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-21T18:57:59.1228957Z [rank12]:[W221 18:57:59.048400543 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-21T18:57:59.1250447Z [rank10]:[W221 18:57:59.049393181 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-21T18:57:59.1260669Z [rank13]:[W221 18:57:59.050118086 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-21T18:57:59.1270034Z [rank15]:[W221 18:57:59.050186537 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-21T18:57:59.1279287Z [rank9]:[W221 18:57:59.050364948 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-21T18:57:59.1288363Z [rank8]:[W221 18:57:59.050675730 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-21T18:57:59.1298387Z [rank14]:[W221 18:57:59.051252075 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-21T18:57:59.1307376Z [rank11]:[W221 18:57:59.054446539 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-21T18:58:02.1820229Z 
2026-02-21T18:58:02.1821022Z Capturing CUDA graphs (decode, FULL):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:16<00:16, 16.97s/it]
2026-02-21T18:58:02.1821520Z Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:20<00:00,  8.78s/it]
2026-02-21T18:58:02.1821929Z Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:20<00:00, 10.01s/it]
2026-02-21T18:58:02.6979802Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m INFO 02-21 18:58:02 [gpu_model_runner.py:5051] Graph capturing finished in 21 secs, took 0.27 GiB
2026-02-21T18:58:02.7220149Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m INFO 02-21 18:58:02 [gpu_model_runner.py:5051] Graph capturing finished in 21 secs, took 0.27 GiB
2026-02-21T18:58:03.0913129Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-21 18:58:03 [core.py:272] init engine (profile, create kv cache, warmup model) took 56.13 seconds
2026-02-21T18:58:03.2609513Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-21 18:58:03 [core.py:272] init engine (profile, create kv cache, warmup model) took 56.00 seconds
2026-02-21T18:58:04.6945019Z INFO 02-21 18:58:04 [coordinator.py:200] All engine subscriptions received by DP coordinator
2026-02-21T18:58:04.6964439Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-21 18:58:04 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-21T18:58:04.6973659Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-21 18:58:04 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-21T18:58:04.6983622Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-21 18:58:04 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:58:04.6993284Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-21 18:58:04 [ascend_config.py:412] Dynamic EPLB is False
2026-02-21T18:58:04.7002692Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-21 18:58:04 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:58:04.7012361Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-21 18:58:04 [ascend_config.py:413] The number of redundant experts is 0
2026-02-21T18:58:04.7023179Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-21 18:58:04 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:58:04.7034710Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-21 18:58:04 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-21T18:58:04.7043585Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-21 18:58:04 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-21T18:58:04.7053738Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-21 18:58:04 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-21T18:58:04.7063494Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-21 18:58:04 [platform.py:335] [91m
2026-02-21T18:58:04.7073757Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             **********************************************************************************
2026-02-21T18:58:04.7083960Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-21T18:58:04.7093854Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-21T18:58:04.7103687Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-21T18:58:04.7113966Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-21T18:58:04.7125021Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-21T18:58:04.7134610Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             * batch size for graph capture.
2026-02-21T18:58:04.7144549Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             * For more details, please refer to:
2026-02-21T18:58:04.7155292Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-21T18:58:04.7166997Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             **********************************************************************************[0m
2026-02-21T18:58:04.7176453Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             
2026-02-21T18:58:04.7185574Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-21 18:58:04 [platform.py:335] [91m
2026-02-21T18:58:04.7198901Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             **********************************************************************************
2026-02-21T18:58:04.7207768Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-21T18:58:04.7217721Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-21T18:58:04.7227678Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-21T18:58:04.7237750Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-21T18:58:04.7247456Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-21T18:58:04.7256808Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             * batch size for graph capture.
2026-02-21T18:58:04.7266649Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             * For more details, please refer to:
2026-02-21T18:58:04.7276621Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-21T18:58:04.7287005Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             **********************************************************************************[0m
2026-02-21T18:58:04.7296157Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-21 18:58:04 [platform.py:335]             
2026-02-21T18:58:04.7305992Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-21 18:58:04 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-21T18:58:04.7314874Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-21 18:58:04 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-21T18:58:04.7325440Z INFO 02-21 18:58:04 [utils.py:249] Waiting for API servers to complete ...
2026-02-21T18:58:04.7334948Z ERROR 02-21 18:58:04 [utils.py:290] Exception occurred while running API servers: Process ApiServer_0 (PID: 184) died with exit code 1
2026-02-21T18:58:04.7344899Z ERROR 02-21 18:58:04 [utils.py:290] Traceback (most recent call last):
2026-02-21T18:58:04.7355046Z ERROR 02-21 18:58:04 [utils.py:290]   File "/vllm-workspace/vllm/vllm/v1/utils.py", line 277, in wait_for_completion_or_failure
2026-02-21T18:58:04.7364111Z ERROR 02-21 18:58:04 [utils.py:290]     raise RuntimeError(
2026-02-21T18:58:04.7373368Z ERROR 02-21 18:58:04 [utils.py:290] RuntimeError: Process ApiServer_0 (PID: 184) died with exit code 1
2026-02-21T18:58:04.7382598Z INFO 02-21 18:58:04 [utils.py:293] Terminating remaining processes ...
2026-02-21T18:58:04.9906452Z [0;36m(Worker_DP0_TP2_EP2 pid=371)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-21T18:58:04.9916368Z [0;36m(Worker_DP1_TP3_EP11 pid=476)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-21T18:58:04.9928464Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-21T18:58:04.9938345Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-21T18:58:04.9948109Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-21T18:58:04.9958190Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-21T18:58:04.9966958Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-21T18:58:04.9977119Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-21T18:58:04.9986203Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-21T18:58:04.9996205Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-21T18:58:05.0006087Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-21T18:58:05.0015265Z [0;36m(Worker_DP0_TP2_EP2 pid=371)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-21T18:58:05.0025517Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-21T18:58:05.0034452Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-21T18:58:05.0044190Z [0;36m(Worker_DP1_TP0_EP8 pid=234)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-21T18:58:05.0054086Z [0;36m(Worker_DP0_TP3_EP3 pid=473)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-21T18:58:05.0063863Z [0;36m(Worker_DP1_TP3_EP11 pid=476)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-21T18:58:05.0073532Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-21T18:58:05.0083461Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-21T18:58:05.0092772Z [0;36m(Worker_DP0_TP4_EP4 pid=577)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-21T18:58:05.0102449Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-21T18:58:05.0112238Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-21T18:58:05.0121106Z [0;36m(Worker_DP1_TP4_EP12 pid=580)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-21T18:58:05.0131175Z [0;36m(Worker_DP1_TP1_EP9 pid=275)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-21T18:58:05.0140879Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-21T18:58:05.0150146Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-21T18:58:05.0160492Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-21T18:58:05.0170145Z [0;36m(Worker_DP0_TP0_EP0 pid=233)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-21T18:58:05.0179881Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-21T18:58:05.0189369Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-21T18:58:05.0199403Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-21T18:58:05.0209227Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-21 18:58:04 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-21T18:58:10.0268613Z Traceback (most recent call last):
2026-02-21T18:58:10.0275854Z   File "/usr/local/python3.11.14/bin/vllm", line 7, in <module>
2026-02-21T18:58:10.0295222Z     sys.exit(main())
2026-02-21T18:58:10.0305004Z              ^^^^^^
2026-02-21T18:58:10.0314814Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/main.py", line 73, in main
2026-02-21T18:58:10.0323912Z     args.dispatch_function(args)
2026-02-21T18:58:10.0333504Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 108, in cmd
2026-02-21T18:58:10.0342676Z     run_multi_api_server(args)
2026-02-21T18:58:10.0352217Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 282, in run_multi_api_server
2026-02-21T18:58:10.0361955Z     wait_for_completion_or_failure(
2026-02-21T18:58:10.0371926Z   File "/vllm-workspace/vllm/vllm/v1/utils.py", line 277, in wait_for_completion_or_failure
2026-02-21T18:58:10.0381278Z     raise RuntimeError(
2026-02-21T18:58:10.0390759Z RuntimeError: Process ApiServer_0 (PID: 184) died with exit code 1
2026-02-21T18:58:10.1567346Z [ERROR] 2026-02-21-18:58:10 (PID:138, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
2026-02-21T18:58:11.1007425Z sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-21T18:58:12.9006049Z /usr/local/python3.11.14/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked shared_memory objects to clean up at shutdown
2026-02-21T18:58:12.9014605Z   warnings.warn('resource_tracker: There appear to be %d '
2026-02-21T18:58:15.8293917Z FAILED
2026-02-21T18:58:15.8305341Z 
2026-02-21T18:58:15.8317414Z =================================== FAILURES ===================================
2026-02-21T18:58:15.8329048Z _______________________________ test_multi_node ________________________________
2026-02-21T18:58:15.8339138Z 
2026-02-21T18:58:15.8348983Z     @pytest.mark.asyncio
2026-02-21T18:58:15.8358959Z     async def test_multi_node() -> None:
2026-02-21T18:58:15.8369113Z         config = MultiNodeConfigLoader.from_yaml()
2026-02-21T18:58:15.8378195Z     
2026-02-21T18:58:15.8388337Z         with ProxyLauncher(
2026-02-21T18:58:15.8397849Z                 nodes=config.nodes,
2026-02-21T18:58:15.8407799Z                 disagg_cfg=config.disagg_cfg,
2026-02-21T18:58:15.8416966Z                 envs=config.envs,
2026-02-21T18:58:15.8427232Z                 proxy_port=config.proxy_port,
2026-02-21T18:58:15.8438032Z                 cur_index=config.cur_index,
2026-02-21T18:58:15.8448044Z         ) as proxy:
2026-02-21T18:58:15.8459045Z     
2026-02-21T18:58:15.8466642Z >           with RemoteOpenAIServer(
2026-02-21T18:58:15.8475982Z                     model=config.model,
2026-02-21T18:58:15.8486963Z                     vllm_serve_args=config.server_cmd,
2026-02-21T18:58:15.8495026Z                     server_port=config.server_port,
2026-02-21T18:58:15.8504422Z                     server_host=config.master_ip,
2026-02-21T18:58:15.8514953Z                     env_dict=config.envs,
2026-02-21T18:58:15.8525665Z                     auto_port=False,
2026-02-21T18:58:15.8534324Z                     proxy_port=proxy.proxy_port,
2026-02-21T18:58:15.8543679Z                     disaggregated_prefill=config.disagg_cfg,
2026-02-21T18:58:15.8553337Z                     nodes_info=config.nodes,
2026-02-21T18:58:15.8563924Z                     max_wait_seconds=2800,
2026-02-21T18:58:15.8574530Z             ) as server:
2026-02-21T18:58:15.8583107Z 
2026-02-21T18:58:15.8592846Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py:21: 
2026-02-21T18:58:15.8602927Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-21T18:58:15.8612888Z tests/e2e/conftest.py:306: in __init__
2026-02-21T18:58:15.8622295Z     self._wait_for_multiple_servers(
2026-02-21T18:58:15.8631957Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-21T18:58:15.8641528Z 
2026-02-21T18:58:15.8651075Z self = <tests.e2e.conftest.RemoteOpenAIServer object at 0xffff1a95b590>
2026-02-21T18:58:15.8661373Z targets = [('10.0.0.62', 'http://10.0.0.62:8080/health')], timeout = 2800
2026-02-21T18:58:15.8670567Z log_interval = 30.0
2026-02-21T18:58:15.8680667Z 
2026-02-21T18:58:15.8690512Z     def _wait_for_multiple_servers(self,
2026-02-21T18:58:15.8699843Z                                    targets,
2026-02-21T18:58:15.8709778Z                                    timeout: float,
2026-02-21T18:58:15.8719827Z                                    log_interval: float = 30.0):
2026-02-21T18:58:15.8729496Z         """
2026-02-21T18:58:15.8739115Z         targets: List[(node_ip, url)]
2026-02-21T18:58:15.8748694Z         log_interval
2026-02-21T18:58:15.8759841Z         """
2026-02-21T18:58:15.8770959Z         start = time.time()
2026-02-21T18:58:15.8792287Z         client = requests
2026-02-21T18:58:15.8792517Z     
2026-02-21T18:58:15.8802843Z         ready = {node_ip: False for node_ip, _ in targets}
2026-02-21T18:58:15.8812719Z     
2026-02-21T18:58:15.8821796Z         last_log_time = 0.0
2026-02-21T18:58:15.8830683Z     
2026-02-21T18:58:15.8840548Z         while True:
2026-02-21T18:58:15.8850582Z             now = time.time()
2026-02-21T18:58:15.8859974Z             all_ready = True
2026-02-21T18:58:15.8869945Z             should_log = (now - last_log_time) >= log_interval
2026-02-21T18:58:15.8879408Z     
2026-02-21T18:58:15.8889151Z             for node_ip, url in targets:
2026-02-21T18:58:15.8899254Z                 if ready[node_ip]:
2026-02-21T18:58:15.8908434Z                     continue
2026-02-21T18:58:15.8922219Z     
2026-02-21T18:58:15.8931592Z                 try:
2026-02-21T18:58:15.8941138Z                     resp = client.get(url)
2026-02-21T18:58:15.8950528Z                     if resp.status_code == 200:
2026-02-21T18:58:15.8960765Z                         ready[node_ip] = True
2026-02-21T18:58:15.8970696Z                         logger.info(f"[READY] Node {node_ip} is ready.")
2026-02-21T18:58:15.8980209Z                 except RequestException:
2026-02-21T18:58:15.8989100Z                     all_ready = False
2026-02-21T18:58:15.8998602Z                     if should_log:
2026-02-21T18:58:15.9008720Z                         logger.debug(f"[WAIT] {url}: connection failed")
2026-02-21T18:58:15.9018080Z     
2026-02-21T18:58:15.9027802Z                     # check unexpected exit
2026-02-21T18:58:15.9037872Z                     result = self._poll()
2026-02-21T18:58:15.9047487Z                     if result is not None and result != 0:
2026-02-21T18:58:15.9056923Z >                       raise RuntimeError(
2026-02-21T18:58:15.9066765Z                             f"Server at {node_ip} exited unexpectedly."
2026-02-21T18:58:15.9076225Z                         ) from None
2026-02-21T18:58:15.9086724Z E                       RuntimeError: Server at 10.0.0.62 exited unexpectedly.
2026-02-21T18:58:15.9095788Z 
2026-02-21T18:58:15.9105562Z tests/e2e/conftest.py:399: RuntimeError
2026-02-21T18:58:15.9115462Z =============================== warnings summary ===============================
2026-02-21T18:58:15.9125464Z <frozen importlib._bootstrap>:241
2026-02-21T18:58:15.9135366Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute
2026-02-21T18:58:15.9144138Z 
2026-02-21T18:58:15.9154056Z <frozen importlib._bootstrap>:241
2026-02-21T18:58:15.9164238Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute
2026-02-21T18:58:15.9173722Z 
2026-02-21T18:58:15.9183239Z ../../usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647
2026-02-21T18:58:15.9194228Z   /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-21T18:58:15.9202879Z     warnings.warn(
2026-02-21T18:58:15.9212773Z 
2026-02-21T18:58:15.9222433Z ../../usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8
2026-02-21T18:58:15.9232480Z   /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
2026-02-21T18:58:15.9241669Z     import pkg_resources
2026-02-21T18:58:15.9250646Z 
2026-02-21T18:58:15.9260352Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2026-02-21T18:58:15.9270303Z =========================== short test summary info ============================
2026-02-21T18:58:15.9281131Z FAILED tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node
2026-02-21T18:58:15.9292087Z ================== 1 failed, 4 warnings in 810.89s (0:13:30) ===================
2026-02-21T18:58:17.5495600Z [0;31mFAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml âœ— ERROR: Some tests failed, please check the error stack above for details. If this is insufficient to pinpoint the error, please download and review the logs of all other nodes from the job's summary.[0m
2026-02-21T18:58:17.7606342Z Cleaning up background log streams...
2026-02-21T18:58:17.8279577Z ##[error]Error: failed to run script step: Error: command terminated with non-zero exit code: command terminated with exit code 1
2026-02-21T18:58:17.8317903Z ##[error]Process completed with exit code 1.
2026-02-21T18:58:17.8410808Z ##[error]Executing the custom container implementation failed. Please contact your self hosted runner administrator.
2026-02-21T18:58:17.8797313Z ##[group]Run actions/upload-artifact@v6
2026-02-21T18:58:17.8797515Z with:
2026-02-21T18:58:17.8797692Z   name: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs
2026-02-21T18:58:17.8797925Z   path: /tmp/vllm*_logs.txt
2026-02-21T18:58:17.8798190Z   retention-days: 7
2026-02-21T18:58:17.8798342Z   if-no-files-found: warn
2026-02-21T18:58:17.8798510Z   compression-level: 6
2026-02-21T18:58:17.8798657Z   overwrite: false
2026-02-21T18:58:17.8798814Z   include-hidden-files: false
2026-02-21T18:58:17.8798981Z env:
2026-02-21T18:58:17.8799157Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-21T18:58:17.8799385Z ##[endgroup]
2026-02-21T18:58:17.8823204Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-21T18:58:17.8823824Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-21T18:58:17.8824029Z ##[endgroup]
2026-02-21T18:58:18.2471114Z (node:931) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-21T18:58:18.2471753Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-21T18:58:19.2359921Z With the provided path, there will be 1 file uploaded
2026-02-21T18:58:19.2364701Z Artifact name is valid!
2026-02-21T18:58:19.2365656Z Root directory input is valid!
2026-02-21T18:58:20.0806100Z Beginning upload of artifact content to blob storage
2026-02-21T18:58:21.0025484Z Uploaded bytes 12612
2026-02-21T18:58:21.2309528Z Finished uploading artifact content to blob storage!
2026-02-21T18:58:21.2309987Z SHA256 digest of uploaded artifact zip is dda6b66e8755d2b7d00ed0b03c143af38e8ea699e364b3ebf20e77b08400f547
2026-02-21T18:58:21.2310359Z Finalizing artifact upload
2026-02-21T18:58:22.0403265Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs.zip successfully finalized. Artifact ID 5602780626
2026-02-21T18:58:22.0403982Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs has been successfully uploaded! Final size is 12612 bytes. Artifact ID is 5602780626
2026-02-21T18:58:22.0406852Z Artifact download URL: https://github.com/vllm-project/vllm-ascend/actions/runs/22260012431/artifacts/5602780626
2026-02-21T18:58:22.5036998Z ##[group]Run kubectl get pods -n "$NAMESPACE" --ignore-not-found=true
2026-02-21T18:58:22.5037369Z [36;1mkubectl get pods -n "$NAMESPACE" --ignore-not-found=true[0m
2026-02-21T18:58:22.5037720Z [36;1mkubectl delete -f ./lws.yaml --ignore-not-found=true || true[0m
2026-02-21T18:58:22.5038078Z shell: bash -el {0}
2026-02-21T18:58:22.5038234Z env:
2026-02-21T18:58:22.5038438Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-21T18:58:22.5038669Z ##[endgroup]
2026-02-21T18:58:22.5125854Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-21T18:58:22.5126483Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-21T18:58:22.5126692Z ##[endgroup]
2026-02-21T18:58:22.8619871Z (node:1046) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-21T18:58:22.8620515Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-21T18:58:23.4863278Z NAME                                             READY   STATUS    RESTARTS     AGE
2026-02-21T18:58:23.4863664Z linux-aarch64-a3-0-n4cwm-runner-nngvl            1/1     Running   0            15m
2026-02-21T18:58:23.4864059Z linux-aarch64-a3-0-n4cwm-runner-nngvl-workflow   1/1     Running   0            14m
2026-02-21T18:58:23.4864399Z vllm-0                                           1/1     Running   1 (6s ago)   14m
2026-02-21T18:58:23.4864880Z vllm-0-1                                         1/1     Running   0            14m
2026-02-21T18:58:23.5504243Z leaderworkerset.leaderworkerset.x-k8s.io "vllm" deleted from vllm-project namespace
2026-02-21T18:58:23.5728452Z service "vllm-leader" deleted from vllm-project namespace
2026-02-21T18:58:24.0579179Z Post job cleanup.
2026-02-21T18:58:24.0599553Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-21T18:58:24.0600867Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-21T18:58:24.0601083Z ##[endgroup]
2026-02-21T18:58:24.4149182Z (node:1170) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-21T18:58:24.4149987Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-21T18:58:25.0590336Z [command]/usr/bin/git version
2026-02-21T18:58:25.0765462Z git version 2.34.1
2026-02-21T18:58:25.0798836Z Copying '/root/.gitconfig' to '/__w/_temp/b43414e3-8267-463c-aa9d-6115b983dd10/.gitconfig'
2026-02-21T18:58:25.0806498Z Temporarily overriding HOME='/__w/_temp/b43414e3-8267-463c-aa9d-6115b983dd10' before making global git config changes
2026-02-21T18:58:25.0807092Z Adding repository directory to the temporary git global config as a safe directory
2026-02-21T18:58:25.0810654Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-21T18:58:25.0847640Z Removing SSH command configuration
2026-02-21T18:58:25.0852404Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-21T18:58:25.0908102Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-21T18:58:25.1387553Z Removing HTTP extra header
2026-02-21T18:58:25.1392603Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-21T18:58:25.1417657Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-21T18:58:25.1597763Z Removing includeIf entries pointing to credentials config files
2026-02-21T18:58:25.1602324Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-21T18:58:25.1620803Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-21T18:58:25.1621120Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-21T18:58:25.1621398Z includeif.gitdir:/github/workspace/.git.path
2026-02-21T18:58:25.1621728Z includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-21T18:58:25.1628564Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-21T18:58:25.1646421Z /__w/_temp/git-credentials-3840c8d1-6969-4afe-9435-6d2577271682.config
2026-02-21T18:58:25.1654610Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-3840c8d1-6969-4afe-9435-6d2577271682.config
2026-02-21T18:58:25.1686467Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-21T18:58:25.1705227Z /__w/_temp/git-credentials-3840c8d1-6969-4afe-9435-6d2577271682.config
2026-02-21T18:58:25.1712891Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-3840c8d1-6969-4afe-9435-6d2577271682.config
2026-02-21T18:58:25.1742927Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git.path
2026-02-21T18:58:25.1761906Z /github/runner_temp/git-credentials-3840c8d1-6969-4afe-9435-6d2577271682.config
2026-02-21T18:58:25.1768629Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-3840c8d1-6969-4afe-9435-6d2577271682.config
2026-02-21T18:58:25.1795953Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-21T18:58:25.1814575Z /github/runner_temp/git-credentials-3840c8d1-6969-4afe-9435-6d2577271682.config
2026-02-21T18:58:25.1820640Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-3840c8d1-6969-4afe-9435-6d2577271682.config
2026-02-21T18:58:25.1849047Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-21T18:58:25.2022531Z Removing credentials config '/__w/_temp/git-credentials-3840c8d1-6969-4afe-9435-6d2577271682.config'
2026-02-21T18:58:43.9465480Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-21T18:58:43.9466187Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-21T18:58:43.9466425Z ##[endgroup]
2026-02-21T18:58:44.3442982Z Cleaning up orphan processes
