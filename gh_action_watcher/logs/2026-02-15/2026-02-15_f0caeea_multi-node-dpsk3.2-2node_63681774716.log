# Run ID: 22038904393
# Commit: f0caeeadcb37261beebd4a6e32934fa9f460db98
# Job: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
# Date: 2026-02-15
============================================================

ï»¿2026-02-15T18:40:37.8829687Z Current runner version: '2.330.0'
2026-02-15T18:40:37.8834562Z Runner name: 'linux-aarch64-a3-0-n4cwm-runner-9mxjl'
2026-02-15T18:40:37.8835309Z Runner group name: 'Default'
2026-02-15T18:40:37.8836021Z Machine name: 'linux-aarch64-a3-0-n4cwm-runner-9mxjl'
2026-02-15T18:40:37.8839291Z ##[group]GITHUB_TOKEN Permissions
2026-02-15T18:40:37.8841159Z Actions: write
2026-02-15T18:40:37.8841686Z ArtifactMetadata: write
2026-02-15T18:40:37.8842269Z Attestations: write
2026-02-15T18:40:37.8842684Z Checks: write
2026-02-15T18:40:37.8843189Z Contents: write
2026-02-15T18:40:37.8843552Z Deployments: write
2026-02-15T18:40:37.8843932Z Discussions: write
2026-02-15T18:40:37.8844296Z Issues: write
2026-02-15T18:40:37.8844670Z Metadata: read
2026-02-15T18:40:37.8845077Z Models: read
2026-02-15T18:40:37.8845428Z Packages: write
2026-02-15T18:40:37.8845807Z Pages: write
2026-02-15T18:40:37.8846256Z PullRequests: write
2026-02-15T18:40:37.8846636Z RepositoryProjects: write
2026-02-15T18:40:37.8847188Z SecurityEvents: write
2026-02-15T18:40:37.8847614Z Statuses: write
2026-02-15T18:40:37.8847997Z ##[endgroup]
2026-02-15T18:40:37.8849769Z Secret source: Actions
2026-02-15T18:40:37.8850296Z Prepare workflow directory
2026-02-15T18:40:37.9429201Z Prepare all required actions
2026-02-15T18:40:37.9461394Z Getting action download info
2026-02-15T18:40:39.2325479Z Download action repository 'actions/checkout@v6' (SHA:de0fac2e4500dabe0009e67214ff5f5447ce83dd)
2026-02-15T18:40:43.9528380Z Download action repository 'actions/upload-artifact@v6' (SHA:b7c566a772e6b6bfb58ed0dc250532a479d7789f)
2026-02-15T18:40:51.9151150Z Uses: vllm-project/vllm-ascend/.github/workflows/_e2e_nightly_multi_node.yaml@refs/heads/main (f0caeeadcb37261beebd4a6e32934fa9f460db98)
2026-02-15T18:40:51.9154716Z ##[group] Inputs
2026-02-15T18:40:51.9155023Z   soc_version: a3
2026-02-15T18:40:51.9155248Z   runner: linux-aarch64-a3-0
2026-02-15T18:40:51.9155721Z   image: swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3
2026-02-15T18:40:51.9156179Z   config_file_path: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-15T18:40:51.9156460Z   replicas: 1
2026-02-15T18:40:51.9156820Z   size: 2
2026-02-15T18:40:51.9157037Z   vllm_version: v0.15.0
2026-02-15T18:40:51.9157387Z   vllm_ascend_remote_url: https://github.com/vllm-project/vllm-ascend.git
2026-02-15T18:40:51.9157730Z   vllm_ascend_ref: main
2026-02-15T18:40:51.9157968Z ##[endgroup]
2026-02-15T18:40:51.9158431Z Complete job name: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-15T18:40:51.9684176Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-15T18:40:51.9686416Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-15T18:40:51.9687042Z ##[endgroup]
2026-02-15T18:40:59.5283182Z (node:74) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-15T18:40:59.5284032Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-15T18:41:01.3230141Z ##[group]Run # Decode and save kubeconfig
2026-02-15T18:41:01.3230582Z [36;1m# Decode and save kubeconfig[0m
2026-02-15T18:41:01.3265176Z [36;1mecho "***" | base64 -d > "$KUBECONFIG"[0m
2026-02-15T18:41:01.3265714Z shell: bash -el {0}
2026-02-15T18:41:01.3265935Z ##[endgroup]
2026-02-15T18:41:01.3379653Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-15T18:41:01.3380509Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-15T18:41:01.3380810Z ##[endgroup]
2026-02-15T18:41:01.6875764Z (node:403) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-15T18:41:01.6876588Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-15T18:41:02.8763619Z ##[group]Run actions/checkout@v6
2026-02-15T18:41:02.8763981Z with:
2026-02-15T18:41:02.8764285Z   repository: vllm-project/vllm-ascend
2026-02-15T18:41:02.8764941Z   token: ***
2026-02-15T18:41:02.8765175Z   ssh-strict: true
2026-02-15T18:41:02.8765573Z   ssh-user: git
2026-02-15T18:41:02.8765780Z   persist-credentials: true
2026-02-15T18:41:02.8766108Z   clean: true
2026-02-15T18:41:02.8766307Z   sparse-checkout-cone-mode: true
2026-02-15T18:41:02.8766598Z   fetch-depth: 1
2026-02-15T18:41:02.8766826Z   fetch-tags: false
2026-02-15T18:41:02.8767016Z   show-progress: true
2026-02-15T18:41:02.8767252Z   lfs: false
2026-02-15T18:41:02.8767459Z   submodules: false
2026-02-15T18:41:02.8767653Z   set-safe-directory: true
2026-02-15T18:41:02.8767909Z ##[endgroup]
2026-02-15T18:41:02.8808019Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-15T18:41:02.8809071Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-15T18:41:02.8809374Z ##[endgroup]
2026-02-15T18:41:03.2434838Z (node:434) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-15T18:41:03.2435615Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-15T18:41:03.8403557Z Syncing repository: vllm-project/vllm-ascend
2026-02-15T18:41:03.8404624Z ##[group]Getting Git version info
2026-02-15T18:41:03.8404981Z Working directory is '/__w/vllm-ascend/vllm-ascend'
2026-02-15T18:41:03.8405443Z [command]/usr/bin/git version
2026-02-15T18:41:03.8512445Z git version 2.34.1
2026-02-15T18:41:03.8531365Z ##[endgroup]
2026-02-15T18:41:03.8537565Z Copying '/root/.gitconfig' to '/__w/_temp/02bc71f3-5c18-4391-b6f2-56829787d1cb/.gitconfig'
2026-02-15T18:41:03.8546688Z Temporarily overriding HOME='/__w/_temp/02bc71f3-5c18-4391-b6f2-56829787d1cb' before making global git config changes
2026-02-15T18:41:03.8547285Z Adding repository directory to the temporary git global config as a safe directory
2026-02-15T18:41:03.8550548Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-15T18:41:03.8591501Z Deleting the contents of '/__w/vllm-ascend/vllm-ascend'
2026-02-15T18:41:03.8592310Z ##[group]Initializing the repository
2026-02-15T18:41:03.8593433Z [command]/usr/bin/git init /__w/vllm-ascend/vllm-ascend
2026-02-15T18:41:03.8706647Z hint: Using 'master' as the name for the initial branch. This default branch name
2026-02-15T18:41:03.8707232Z hint: is subject to change. To configure the initial branch name to use in all
2026-02-15T18:41:03.8707685Z hint: of your new repositories, which will suppress this warning, call:
2026-02-15T18:41:03.8707984Z hint: 
2026-02-15T18:41:03.8708271Z hint: 	git config --global init.defaultBranch <name>
2026-02-15T18:41:03.8708569Z hint: 
2026-02-15T18:41:03.8708861Z hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and
2026-02-15T18:41:03.8709397Z hint: 'development'. The just-created branch can be renamed via this command:
2026-02-15T18:41:03.8709828Z hint: 
2026-02-15T18:41:03.8710082Z hint: 	git branch -m <name>
2026-02-15T18:41:03.8713784Z Initialized empty Git repository in /__w/vllm-ascend/vllm-ascend/.git/
2026-02-15T18:41:03.8722440Z [command]/usr/bin/git remote add origin https://github.com/vllm-project/vllm-ascend
2026-02-15T18:41:03.8766530Z ##[endgroup]
2026-02-15T18:41:03.8766912Z ##[group]Disabling automatic garbage collection
2026-02-15T18:41:03.8770085Z [command]/usr/bin/git config --local gc.auto 0
2026-02-15T18:41:03.8795912Z ##[endgroup]
2026-02-15T18:41:03.8796266Z ##[group]Setting up auth
2026-02-15T18:41:03.8796825Z Removing SSH command configuration
2026-02-15T18:41:03.8802464Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-15T18:41:03.8829793Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-15T18:41:03.9237579Z Removing HTTP extra header
2026-02-15T18:41:03.9240964Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-15T18:41:03.9265933Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-15T18:41:03.9438058Z Removing includeIf entries pointing to credentials config files
2026-02-15T18:41:03.9442387Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-15T18:41:03.9468717Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-15T18:41:03.9646328Z [command]/usr/bin/git config --file /__w/_temp/git-credentials-2ba3eb75-c086-4bb9-9b51-bf56582906fb.config http.https://github.com/.extraheader AUTHORIZATION: basic ***
2026-02-15T18:41:03.9678311Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-2ba3eb75-c086-4bb9-9b51-bf56582906fb.config
2026-02-15T18:41:03.9704040Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-2ba3eb75-c086-4bb9-9b51-bf56582906fb.config
2026-02-15T18:41:03.9729218Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-2ba3eb75-c086-4bb9-9b51-bf56582906fb.config
2026-02-15T18:41:03.9755179Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-2ba3eb75-c086-4bb9-9b51-bf56582906fb.config
2026-02-15T18:41:03.9778319Z ##[endgroup]
2026-02-15T18:41:03.9778907Z ##[group]Fetching the repository
2026-02-15T18:41:03.9788101Z [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --no-recurse-submodules --depth=1 origin +f0caeeadcb37261beebd4a6e32934fa9f460db98:refs/remotes/origin/main
2026-02-15T18:41:05.6467032Z From https://gh-proxy.test.osinfra.cn/https://github.com/vllm-project/vllm-ascend
2026-02-15T18:41:05.6467602Z  * [new ref]         f0caeeadcb37261beebd4a6e32934fa9f460db98 -> origin/main
2026-02-15T18:41:05.6538636Z [command]/usr/bin/git branch --list --remote origin/main
2026-02-15T18:41:05.6538927Z   origin/main
2026-02-15T18:41:05.6539551Z [command]/usr/bin/git rev-parse refs/remotes/origin/main
2026-02-15T18:41:05.6545119Z f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-15T18:41:05.6548603Z ##[endgroup]
2026-02-15T18:41:05.6548960Z ##[group]Determining the checkout info
2026-02-15T18:41:05.6550630Z ##[endgroup]
2026-02-15T18:41:05.6554621Z [command]/usr/bin/git sparse-checkout disable
2026-02-15T18:41:05.6596648Z [command]/usr/bin/git config --local --unset-all extensions.worktreeConfig
2026-02-15T18:41:05.6629092Z ##[group]Checking out the ref
2026-02-15T18:41:05.6629521Z [command]/usr/bin/git checkout --progress --force -B main refs/remotes/origin/main
2026-02-15T18:41:05.7478714Z Switched to a new branch 'main'
2026-02-15T18:41:05.7479268Z Branch 'main' set up to track remote branch 'main' from 'origin'.
2026-02-15T18:41:05.7487878Z ##[endgroup]
2026-02-15T18:41:05.7524611Z [command]/usr/bin/git log -1 --format=%H
2026-02-15T18:41:05.7546600Z f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-15T18:41:06.1689690Z ##[group]Run # prepare for lws entrypoint scripts
2026-02-15T18:41:06.1690086Z [36;1m# prepare for lws entrypoint scripts[0m
2026-02-15T18:41:06.1690588Z [36;1minstall -D tests/e2e/nightly/multi_node/scripts/run.sh /root/.cache/tests/run.sh[0m
2026-02-15T18:41:06.1691054Z shell: bash -el {0}
2026-02-15T18:41:06.1691308Z ##[endgroup]
2026-02-15T18:41:06.1827444Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-15T18:41:06.1828249Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-15T18:41:06.1828513Z ##[endgroup]
2026-02-15T18:41:06.5383351Z (node:475) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-15T18:41:06.5384160Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-15T18:41:07.4599406Z ##[group]Run set -euo pipefail
2026-02-15T18:41:07.4599759Z [36;1mset -euo pipefail[0m
2026-02-15T18:41:07.4599968Z [36;1m[0m
2026-02-15T18:41:07.4600379Z [36;1mCRD_NAME="${CRD_NAME:-vllm}"[0m
2026-02-15T18:41:07.4600783Z [36;1mTIMEOUT=${TIMEOUT:-120}[0m
2026-02-15T18:41:07.4601027Z [36;1mSLEEP_INTERVAL=2[0m
2026-02-15T18:41:07.4601253Z [36;1m[0m
2026-02-15T18:41:07.4601588Z [36;1mecho "Deleting leaderworkerset [$CRD_NAME] in namespace [$NAMESPACE]..."[0m
2026-02-15T18:41:07.4602281Z [36;1mkubectl delete leaderworkerset "$CRD_NAME" -n "$NAMESPACE" --ignore-not-found[0m
2026-02-15T18:41:07.4602653Z [36;1m[0m
2026-02-15T18:41:07.4602956Z [36;1mecho "Waiting for all pods starting with 'vllm' to be deleted..."[0m
2026-02-15T18:41:07.4603270Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-15T18:41:07.4603504Z [36;1m[0m
2026-02-15T18:41:07.4603726Z [36;1mwhile true; do[0m
2026-02-15T18:41:07.4603940Z [36;1m  NOW=$(date +%s)[0m
2026-02-15T18:41:07.4604294Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-15T18:41:07.4604520Z [36;1m[0m
2026-02-15T18:41:07.4604758Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-15T18:41:07.4605110Z [36;1m    echo "Timeout reached ($TIMEOUT seconds), some pods still exist:"[0m
2026-02-15T18:41:07.4605478Z [36;1m    kubectl get pods -n "$NAMESPACE" | grep '^vllm' || true[0m
2026-02-15T18:41:07.4605862Z [36;1m    exit 1[0m
2026-02-15T18:41:07.4606082Z [36;1m  fi[0m
2026-02-15T18:41:07.4606262Z [36;1m[0m
2026-02-15T18:41:07.4606690Z [36;1m  PODS_EXIST=$(kubectl get pods -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null | tr ' ' '\n' | grep '^vllm' || true)[0m
2026-02-15T18:41:07.4607216Z [36;1m[0m
2026-02-15T18:41:07.4607410Z [36;1m  if [[ -z "$PODS_EXIST" ]]; then[0m
2026-02-15T18:41:07.4607682Z [36;1m    echo "All vllm pods deleted."[0m
2026-02-15T18:41:07.4607900Z [36;1m    break[0m
2026-02-15T18:41:07.4608125Z [36;1m  else[0m
2026-02-15T18:41:07.4608391Z [36;1m    echo "Waiting for pods to be deleted: $PODS_EXIST"[0m
2026-02-15T18:41:07.4608664Z [36;1m    sleep $SLEEP_INTERVAL[0m
2026-02-15T18:41:07.4608989Z [36;1m  fi[0m
2026-02-15T18:41:07.4609193Z [36;1mdone[0m
2026-02-15T18:41:07.4609494Z shell: bash -el {0}
2026-02-15T18:41:07.4609728Z ##[endgroup]
2026-02-15T18:41:07.4710764Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-15T18:41:07.4711594Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-15T18:41:07.4711893Z ##[endgroup]
2026-02-15T18:41:07.8233710Z (node:529) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-15T18:41:07.8234480Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-15T18:41:08.3503946Z Deleting leaderworkerset [vllm] in namespace [vllm-project]...
2026-02-15T18:41:08.6611414Z Waiting for all pods starting with 'vllm' to be deleted...
2026-02-15T18:41:08.7445563Z All vllm pods deleted.
2026-02-15T18:41:09.1514526Z ##[group]Run set -e
2026-02-15T18:41:09.1514818Z [36;1mset -e[0m
2026-02-15T18:41:09.1515032Z [36;1m[0m
2026-02-15T18:41:09.1515221Z [36;1msize="2"[0m
2026-02-15T18:41:09.1515447Z [36;1mreplicas="1"[0m
2026-02-15T18:41:09.1515839Z [36;1mimage="swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3"[0m
2026-02-15T18:41:09.1516318Z [36;1mconfig_file_path="DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-15T18:41:09.1516716Z [36;1mfail_tag=FAIL_TAG_"DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-15T18:41:09.1517056Z [36;1mecho "FAIL_TAG=${fail_tag}" >> "$GITHUB_ENV"[0m
2026-02-15T18:41:09.1517313Z [36;1m[0m
2026-02-15T18:41:09.1517604Z [36;1mrequired_params=("size" "replicas" "image" "config_file_path")[0m
2026-02-15T18:41:09.1518059Z [36;1mfor param in "${required_params[@]}"; do[0m
2026-02-15T18:41:09.1518346Z [36;1m  if [ -z "${!param}" ]; then[0m
2026-02-15T18:41:09.1518901Z [36;1m    echo "Error: Parameter '$param' is required but empty"[0m
2026-02-15T18:41:09.1519211Z [36;1m    exit 1[0m
2026-02-15T18:41:09.1519435Z [36;1m  fi[0m
2026-02-15T18:41:09.1519628Z [36;1mdone[0m
2026-02-15T18:41:09.1519833Z [36;1m[0m
2026-02-15T18:41:09.1520037Z [36;1mif [ "a3" = "a3" ]; then[0m
2026-02-15T18:41:09.1520262Z [36;1m  npu_per_node=16[0m
2026-02-15T18:41:09.1520614Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws.yaml.jinja2"[0m
2026-02-15T18:41:09.1520958Z [36;1melse[0m
2026-02-15T18:41:09.1521257Z [36;1m  npu_per_node=8[0m
2026-02-15T18:41:09.1521618Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws-a2.yaml.jinja2"[0m
2026-02-15T18:41:09.1521976Z [36;1mfi[0m
2026-02-15T18:41:09.1522271Z [36;1m[0m
2026-02-15T18:41:09.1522552Z [36;1mjinja2 $TEMPLATE_FILE \[0m
2026-02-15T18:41:09.1522815Z [36;1m  -D size="$size" \[0m
2026-02-15T18:41:09.1523042Z [36;1m  -D replicas="$replicas" \[0m
2026-02-15T18:41:09.1523319Z [36;1m  -D image="$image" \[0m
2026-02-15T18:41:09.1523625Z [36;1m  -D config_file_path="$config_file_path" \[0m
2026-02-15T18:41:09.1523915Z [36;1m  -D npu_per_node="$npu_per_node" \[0m
2026-02-15T18:41:09.1524201Z [36;1m  -D fail_tag="$fail_tag" \[0m
2026-02-15T18:41:09.1524470Z [36;1m  --outfile lws.yaml[0m
2026-02-15T18:41:09.1524688Z [36;1m[0m
2026-02-15T18:41:09.1524903Z [36;1mkubectl apply -f ./lws.yaml[0m
2026-02-15T18:41:09.1525260Z shell: bash -el {0}
2026-02-15T18:41:09.1525485Z ##[endgroup]
2026-02-15T18:41:09.1598115Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-15T18:41:09.1598904Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-15T18:41:09.1599157Z ##[endgroup]
2026-02-15T18:41:09.5059360Z (node:595) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-15T18:41:09.5060133Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-15T18:41:10.4267827Z leaderworkerset.leaderworkerset.x-k8s.io/vllm created
2026-02-15T18:41:10.4509815Z service/vllm-leader created
2026-02-15T18:41:10.8740515Z ##[group]Run POD_PREFIX="${POD_PREFIX:-vllm-0}"
2026-02-15T18:41:10.8740810Z [36;1mPOD_PREFIX="${POD_PREFIX:-vllm-0}"[0m
2026-02-15T18:41:10.8741008Z [36;1mSIZE="2"[0m
2026-02-15T18:41:10.8741178Z [36;1mTIMEOUT=1200  # default timeout 20 minutes[0m
2026-02-15T18:41:10.8741376Z [36;1m[0m
2026-02-15T18:41:10.8741799Z [36;1mecho "Waiting for Pods in namespace [$NAMESPACE] to become Running and Ready (timeout ${TIMEOUT}s)..."[0m
2026-02-15T18:41:10.8742275Z [36;1m[0m
2026-02-15T18:41:10.8742412Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-15T18:41:10.8742572Z [36;1m[0m
2026-02-15T18:41:10.8742698Z [36;1mwhile true; do[0m
2026-02-15T18:41:10.8742856Z [36;1m  NOW=$(date +%s)[0m
2026-02-15T18:41:10.8743030Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-15T18:41:10.8743243Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-15T18:41:10.8743487Z [36;1m    echo "Timeout reached after ${ELAPSED}s"[0m
2026-02-15T18:41:10.8743759Z [36;1m    echo "Dumping pod status for debugging:"[0m
2026-02-15T18:41:10.8743975Z [36;1m    kubectl get pods -n "$NAMESPACE"[0m
2026-02-15T18:41:10.8744218Z [36;1m    kubectl describe pod "$LEADER_POD" -n "$NAMESPACE"[0m
2026-02-15T18:41:10.8744448Z [36;1m    exit 1[0m
2026-02-15T18:41:10.8744583Z [36;1m  fi[0m
2026-02-15T18:41:10.8744709Z [36;1m[0m
2026-02-15T18:41:10.8744844Z [36;1m  # 1) check follower pods[0m
2026-02-15T18:41:10.8745021Z [36;1m  ALL_FOLLOWERS_READY=true[0m
2026-02-15T18:41:10.8745206Z [36;1m  for ((i=1; i<SIZE; i++)); do[0m
2026-02-15T18:41:10.8745391Z [36;1m    POD="${POD_PREFIX}-${i}"[0m
2026-02-15T18:41:10.8745754Z [36;1m    PHASE=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-15T18:41:10.8746270Z [36;1m    READY=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-15T18:41:10.8746730Z [36;1m[0m
2026-02-15T18:41:10.8746908Z [36;1m    echo "Follower [$POD] phase=$PHASE ready=$READY"[0m
2026-02-15T18:41:10.8747126Z [36;1m[0m
2026-02-15T18:41:10.8747297Z [36;1m    if [[ "$PHASE" != "Running" || "$READY" != "true" ]]; then[0m
2026-02-15T18:41:10.8747560Z [36;1m      echo "Follower [$POD] not Ready yet..."[0m
2026-02-15T18:41:10.8747778Z [36;1m      ALL_FOLLOWERS_READY=false[0m
2026-02-15T18:41:10.8747956Z [36;1m      break[0m
2026-02-15T18:41:10.8748096Z [36;1m    fi[0m
2026-02-15T18:41:10.8748227Z [36;1m  done[0m
2026-02-15T18:41:10.8748346Z [36;1m[0m
2026-02-15T18:41:10.8748480Z [36;1m  # 2) check leader pod[0m
2026-02-15T18:41:10.8748845Z [36;1m  LEADER_PHASE=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-15T18:41:10.8749405Z [36;1m  LEADER_READY=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-15T18:41:10.8749777Z [36;1m[0m
2026-02-15T18:41:10.8749983Z [36;1m  echo "Leader [$LEADER_POD] phase=$LEADER_PHASE ready=$LEADER_READY"[0m
2026-02-15T18:41:10.8750225Z [36;1m[0m
2026-02-15T18:41:10.8750419Z [36;1m  if [[ "$LEADER_PHASE" != "Running" || "$LEADER_READY" != "true" ]]; then[0m
2026-02-15T18:41:10.8750684Z [36;1m    echo "Leader not Ready yet..."[0m
2026-02-15T18:41:10.8750875Z [36;1m    ALL_FOLLOWERS_READY=false[0m
2026-02-15T18:41:10.8751050Z [36;1m  fi[0m
2026-02-15T18:41:10.8751177Z [36;1m[0m
2026-02-15T18:41:10.8751333Z [36;1m  if [[ "$ALL_FOLLOWERS_READY" == "true" ]]; then[0m
2026-02-15T18:41:10.8751703Z [36;1m    echo "All follower pods and leader pod are Running and Ready â€” continuing."[0m
2026-02-15T18:41:10.8752101Z [36;1m    break[0m
2026-02-15T18:41:10.8752255Z [36;1m  fi[0m
2026-02-15T18:41:10.8752379Z [36;1m[0m
2026-02-15T18:41:10.8752495Z [36;1m  sleep 2[0m
2026-02-15T18:41:10.8752631Z [36;1mdone[0m
2026-02-15T18:41:10.8752908Z shell: bash -el {0}
2026-02-15T18:41:10.8753043Z env:
2026-02-15T18:41:10.8753364Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-15T18:41:10.8753582Z ##[endgroup]
2026-02-15T18:41:10.8957149Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-15T18:41:10.8957756Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-15T18:41:10.8957954Z ##[endgroup]
2026-02-15T18:41:11.2437871Z (node:673) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-15T18:41:11.2438530Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-15T18:41:11.7761470Z Waiting for Pods in namespace [vllm-project] to become Running and Ready (timeout 1200s)...
2026-02-15T18:41:11.8987636Z Follower [vllm-0-1] phase=Pending ready=
2026-02-15T18:41:11.8987871Z Follower [vllm-0-1] not Ready yet...
2026-02-15T18:41:12.0103575Z Leader [vllm-0] phase=Pending ready=
2026-02-15T18:41:12.0103799Z Leader not Ready yet...
2026-02-15T18:41:14.1312210Z Follower [vllm-0-1] phase=Pending ready=
2026-02-15T18:41:14.1312553Z Follower [vllm-0-1] not Ready yet...
2026-02-15T18:41:14.2456826Z Leader [vllm-0] phase=Pending ready=
2026-02-15T18:41:14.2457136Z Leader not Ready yet...
2026-02-15T18:41:16.3595532Z Follower [vllm-0-1] phase=Pending ready=
2026-02-15T18:41:16.3595848Z Follower [vllm-0-1] not Ready yet...
2026-02-15T18:41:16.4689207Z Leader [vllm-0] phase=Pending ready=
2026-02-15T18:41:16.4689457Z Leader not Ready yet...
2026-02-15T18:41:18.5877278Z Follower [vllm-0-1] phase=Pending ready=
2026-02-15T18:41:18.5877549Z Follower [vllm-0-1] not Ready yet...
2026-02-15T18:41:18.7040289Z Leader [vllm-0] phase=Pending ready=
2026-02-15T18:41:18.7040532Z Leader not Ready yet...
2026-02-15T18:41:20.8158337Z Follower [vllm-0-1] phase=Pending ready=
2026-02-15T18:41:20.8158598Z Follower [vllm-0-1] not Ready yet...
2026-02-15T18:41:20.9345643Z Leader [vllm-0] phase=Pending ready=
2026-02-15T18:41:20.9346228Z Leader not Ready yet...
2026-02-15T18:41:23.0453071Z Follower [vllm-0-1] phase=Pending ready=
2026-02-15T18:41:23.0453354Z Follower [vllm-0-1] not Ready yet...
2026-02-15T18:41:23.1567242Z Leader [vllm-0] phase=Pending ready=
2026-02-15T18:41:23.1567476Z Leader not Ready yet...
2026-02-15T18:41:25.2769059Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-15T18:41:25.2769325Z Follower [vllm-0-1] not Ready yet...
2026-02-15T18:41:25.3966865Z Leader [vllm-0] phase=Pending ready=false
2026-02-15T18:41:25.3967134Z Leader not Ready yet...
2026-02-15T18:41:27.5105114Z Follower [vllm-0-1] phase=Running ready=true
2026-02-15T18:41:27.6255430Z Leader [vllm-0] phase=Running ready=true
2026-02-15T18:41:27.6256839Z All follower pods and leader pod are Running and Ready â€” continuing.
2026-02-15T18:41:28.0409406Z ##[group]Run set -euo pipefail
2026-02-15T18:41:28.0409773Z [36;1mset -euo pipefail[0m
2026-02-15T18:41:28.0410017Z [36;1m[0m
2026-02-15T18:41:28.0410192Z [36;1msize="2"[0m
2026-02-15T18:41:28.0410443Z [36;1mpids=()[0m
2026-02-15T18:41:28.0410650Z [36;1m[0m
2026-02-15T18:41:28.0410821Z [36;1mcleanup() {[0m
2026-02-15T18:41:28.0411215Z [36;1m  echo "Cleaning up background log streams..."[0m
2026-02-15T18:41:28.0411493Z [36;1m  for pid in "${pids[@]}"; do[0m
2026-02-15T18:41:28.0411775Z [36;1m    kill "$pid" 2>/dev/null || true[0m
2026-02-15T18:41:28.0412150Z [36;1m  done[0m
2026-02-15T18:41:28.0412331Z [36;1m}[0m
2026-02-15T18:41:28.0412555Z [36;1mtrap cleanup EXIT[0m
2026-02-15T18:41:28.0412793Z [36;1m[0m
2026-02-15T18:41:28.0412988Z [36;1mfor i in $(seq 1 $((size - 1))); do[0m
2026-02-15T18:41:28.0413261Z [36;1m  POD="vllm-0-${i}"[0m
2026-02-15T18:41:28.0413496Z [36;1m[0m
2026-02-15T18:41:28.0413823Z [36;1m  echo "==== Collecting logs from worker pod: $POD ===="[0m
2026-02-15T18:41:28.0414164Z [36;1m  kubectl logs -f "$POD" -n "$NAMESPACE" \[0m
2026-02-15T18:41:28.0414448Z [36;1m    > "/tmp/${POD}_logs.txt" 2>&1 &[0m
2026-02-15T18:41:28.0414718Z [36;1m[0m
2026-02-15T18:41:28.0414922Z [36;1m  pids+=($!)[0m
2026-02-15T18:41:28.0415120Z [36;1mdone[0m
2026-02-15T18:41:28.0415328Z [36;1m[0m
2026-02-15T18:41:28.0415547Z [36;1mecho "==== Streaming logs from leader pod: $LEADER_POD ===="[0m
2026-02-15T18:41:28.0415912Z [36;1mecho "Looking for logs containing: $FAIL_TAG"[0m
2026-02-15T18:41:28.0416193Z [36;1m[0m
2026-02-15T18:41:28.0416530Z [36;1mkubectl logs -f "$LEADER_POD" -n "$NAMESPACE" | while IFS= read -r line; do[0m
2026-02-15T18:41:28.0416909Z [36;1m  echo "$line"[0m
2026-02-15T18:41:28.0417187Z [36;1m  if echo "$line" | grep -q "$FAIL_TAG"; then[0m
2026-02-15T18:41:28.0417421Z [36;1m    exit 1[0m
2026-02-15T18:41:28.0417660Z [36;1m  fi[0m
2026-02-15T18:41:28.0417934Z [36;1mdone[0m
2026-02-15T18:41:28.0418281Z shell: bash -el {0}
2026-02-15T18:41:28.0418521Z env:
2026-02-15T18:41:28.0418780Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-15T18:41:28.0419031Z ##[endgroup]
2026-02-15T18:41:28.0517928Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-15T18:41:28.0518935Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-15T18:41:28.0519180Z ##[endgroup]
2026-02-15T18:41:28.4043781Z (node:767) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-15T18:41:28.4044516Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-15T18:41:28.9296259Z ==== Collecting logs from worker pod: vllm-0-1 ====
2026-02-15T18:41:28.9297510Z ==== Streaming logs from leader pod: vllm-0 ====
2026-02-15T18:41:28.9297993Z Looking for logs containing: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-15T18:41:29.0065443Z /usr/local/Ascend/ascend-toolkit/set_env.sh: line 31: CMAKE_PREFIX_PATH: unbound variable
2026-02-15T18:41:29.0075414Z ====> Check NPU info
2026-02-15T18:41:29.0084763Z +------------------------------------------------------------------------------------------------+
2026-02-15T18:41:29.0096168Z | npu-smi 25.5.0                   Version: 25.5.0                                               |
2026-02-15T18:41:29.0104804Z +---------------------------+---------------+----------------------------------------------------+
2026-02-15T18:41:29.0114319Z | NPU   Name                | Health        | Power(W)    Temp(C)           Hugepages-Usage(page)|
2026-02-15T18:41:29.0123858Z | Chip  Phy-ID              | Bus-Id        | AICore(%)   Memory-Usage(MB)  HBM-Usage(MB)        |
2026-02-15T18:41:29.0133952Z +===========================+===============+====================================================+
2026-02-15T18:41:29.0142652Z | 0     Ascend910           | OK            | 166.7       36                0    / 0             |
2026-02-15T18:41:29.0152669Z | 0     0                   | 0000:9D:00.0  | 0           0    / 0          3165 / 65536         |
2026-02-15T18:41:29.0162150Z +------------------------------------------------------------------------------------------------+
2026-02-15T18:41:29.0171671Z | 0     Ascend910           | OK            | -           36                0    / 0             |
2026-02-15T18:41:29.0180933Z | 1     1                   | 0000:9F:00.0  | 0           0    / 0          2882 / 65536         |
2026-02-15T18:41:29.0190125Z +===========================+===============+====================================================+
2026-02-15T18:41:29.0199687Z | 1     Ascend910           | OK            | 167.7       35                0    / 0             |
2026-02-15T18:41:29.0209438Z | 0     2                   | 0000:99:00.0  | 0           0    / 0          3152 / 65536         |
2026-02-15T18:41:29.0218852Z +------------------------------------------------------------------------------------------------+
2026-02-15T18:41:29.0228189Z | 1     Ascend910           | OK            | -           37                0    / 0             |
2026-02-15T18:41:29.0237857Z | 1     3                   | 0000:9B:00.0  | 0           0    / 0          2890 / 65536         |
2026-02-15T18:41:29.0247458Z +===========================+===============+====================================================+
2026-02-15T18:41:29.0256152Z | 2     Ascend910           | OK            | 166.5       37                0    / 0             |
2026-02-15T18:41:29.0265701Z | 0     4                   | 0000:95:00.0  | 0           0    / 0          3160 / 65536         |
2026-02-15T18:41:29.0275012Z +------------------------------------------------------------------------------------------------+
2026-02-15T18:41:29.0284729Z | 2     Ascend910           | OK            | -           34                0    / 0             |
2026-02-15T18:41:29.0294070Z | 1     5                   | 0000:97:00.0  | 0           0    / 0          2884 / 65536         |
2026-02-15T18:41:29.0302940Z +===========================+===============+====================================================+
2026-02-15T18:41:29.0312238Z | 3     Ascend910           | OK            | 156.4       36                0    / 0             |
2026-02-15T18:41:29.0322346Z | 0     6                   | 0000:91:00.0  | 0           0    / 0          3157 / 65536         |
2026-02-15T18:41:29.0330925Z +------------------------------------------------------------------------------------------------+
2026-02-15T18:41:29.0339963Z | 3     Ascend910           | OK            | -           34                0    / 0             |
2026-02-15T18:41:29.0349381Z | 1     7                   | 0000:93:00.0  | 0           0    / 0          2885 / 65536         |
2026-02-15T18:41:29.0358736Z +===========================+===============+====================================================+
2026-02-15T18:41:29.0367680Z | 4     Ascend910           | OK            | 165.2       36                0    / 0             |
2026-02-15T18:41:29.0377025Z | 0     8                   | 0000:8D:00.0  | 0           0    / 0          3167 / 65536         |
2026-02-15T18:41:29.0386189Z +------------------------------------------------------------------------------------------------+
2026-02-15T18:41:29.0395783Z | 4     Ascend910           | OK            | -           35                0    / 0             |
2026-02-15T18:41:29.0405918Z | 1     9                   | 0000:8F:00.0  | 0           0    / 0          2885 / 65536         |
2026-02-15T18:41:29.0414861Z +===========================+===============+====================================================+
2026-02-15T18:41:29.0425039Z | 5     Ascend910           | OK            | 169.3       36                0    / 0             |
2026-02-15T18:41:29.0434009Z | 0     10                  | 0000:89:00.0  | 0           0    / 0          3147 / 65536         |
2026-02-15T18:41:29.0442894Z +------------------------------------------------------------------------------------------------+
2026-02-15T18:41:29.0452700Z | 5     Ascend910           | OK            | -           37                0    / 0             |
2026-02-15T18:41:29.0461160Z | 1     11                  | 0000:8B:00.0  | 0           0    / 0          2896 / 65536         |
2026-02-15T18:41:29.0470369Z +===========================+===============+====================================================+
2026-02-15T18:41:29.0480284Z | 6     Ascend910           | OK            | 164.1       36                0    / 0             |
2026-02-15T18:41:29.0491894Z | 0     12                  | 0000:85:00.0  | 0           0    / 0          3163 / 65536         |
2026-02-15T18:41:29.0498324Z +------------------------------------------------------------------------------------------------+
2026-02-15T18:41:29.0507456Z | 6     Ascend910           | OK            | -           35                0    / 0             |
2026-02-15T18:41:29.0516277Z | 1     13                  | 0000:87:00.0  | 0           0    / 0          2884 / 65536         |
2026-02-15T18:41:29.0526163Z +===========================+===============+====================================================+
2026-02-15T18:41:29.0535831Z | 7     Ascend910           | OK            | 164.0       34                0    / 0             |
2026-02-15T18:41:29.0579872Z | 0     14                  | 0000:81:00.0  | 0           0    / 0          3146 / 65536         |
2026-02-15T18:41:29.0580208Z +------------------------------------------------------------------------------------------------+
2026-02-15T18:41:29.0580524Z | 7     Ascend910           | OK            | -           36                0    / 0             |
2026-02-15T18:41:29.0580813Z | 1     15                  | 0000:83:00.0  | 0           0    / 0          2896 / 65536         |
2026-02-15T18:41:29.0583915Z +===========================+===============+====================================================+
2026-02-15T18:41:29.0591703Z +---------------------------+---------------+----------------------------------------------------+
2026-02-15T18:41:29.0601117Z | NPU     Chip              | Process id    | Process name             | Process memory(MB)      |
2026-02-15T18:41:29.0610641Z +===========================+===============+====================================================+
2026-02-15T18:41:29.0620122Z | No running processes found in NPU 0                                                            |
2026-02-15T18:41:29.0629730Z +===========================+===============+====================================================+
2026-02-15T18:41:29.0639349Z | No running processes found in NPU 1                                                            |
2026-02-15T18:41:29.0648244Z +===========================+===============+====================================================+
2026-02-15T18:41:29.0657267Z | No running processes found in NPU 2                                                            |
2026-02-15T18:41:29.0666639Z +===========================+===============+====================================================+
2026-02-15T18:41:29.0676241Z | No running processes found in NPU 3                                                            |
2026-02-15T18:41:29.0685639Z +===========================+===============+====================================================+
2026-02-15T18:41:29.0694883Z | No running processes found in NPU 4                                                            |
2026-02-15T18:41:29.0704723Z +===========================+===============+====================================================+
2026-02-15T18:41:29.0713116Z | No running processes found in NPU 5                                                            |
2026-02-15T18:41:29.0722584Z +===========================+===============+====================================================+
2026-02-15T18:41:29.0732236Z | No running processes found in NPU 6                                                            |
2026-02-15T18:41:29.0741268Z +===========================+===============+====================================================+
2026-02-15T18:41:29.0750290Z | No running processes found in NPU 7                                                            |
2026-02-15T18:41:29.0760186Z +===========================+===============+====================================================+
2026-02-15T18:41:29.0768916Z package_name=Ascend-cann-toolkit
2026-02-15T18:41:29.0777896Z version=8.5.0
2026-02-15T18:41:29.0786552Z innerversion=V100R001C25SPC001B232
2026-02-15T18:41:29.0795716Z compatible_version=[V100R001C15],[V100R001C18],[V100R001C19],[V100R001C20],[V100R001C21],[V100R001C23]
2026-02-15T18:41:29.0804834Z arch=aarch64
2026-02-15T18:41:29.0813768Z os=linux
2026-02-15T18:41:29.0822452Z path=/usr/local/Ascend/cann-8.5.0
2026-02-15T18:41:29.0831139Z ====> Configure mirrors and git proxy
2026-02-15T18:41:29.1518994Z Writing to /root/.config/pip/pip.conf
2026-02-15T18:41:29.1872204Z Installed vLLM-related Python packages:
2026-02-15T18:41:30.1963498Z ais_bench_benchmark               3.0.20250930                /vllm-workspace/vllm-ascend/benchmark
2026-02-15T18:41:30.1972559Z vllm                              0.15.0+empty                /vllm-workspace/vllm
2026-02-15T18:41:30.1981178Z vllm_ascend                       0.14.0rc2.dev171+gf0caeeadc /vllm-workspace/vllm-ascend
2026-02-15T18:41:30.1993700Z 
2026-02-15T18:41:30.2003799Z ============================
2026-02-15T18:41:30.2015296Z vLLM Git information
2026-02-15T18:41:30.2028985Z ============================
2026-02-15T18:41:30.2091654Z Branch:      HEAD
2026-02-15T18:41:30.2123548Z Commit hash: f176443446f659dbab5315e056e605d8984fd976
2026-02-15T18:41:30.2183061Z Author:      TJian <tunjian.tan@embeddedllm.com>
2026-02-15T18:41:30.2208336Z Date:        2026-01-29 14:45:42 +0800
2026-02-15T18:41:30.2231837Z Message:     [Release] [CI] Optim release pipeline (#33156)
2026-02-15T18:41:30.2254808Z Tags:        v0.15.0
2026-02-15T18:41:30.2285557Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm.git (fetch)
2026-02-15T18:41:30.2295433Z 
2026-02-15T18:41:30.2305434Z 
2026-02-15T18:41:30.2315289Z ============================
2026-02-15T18:41:30.2325127Z vLLM-Ascend Git information
2026-02-15T18:41:30.2334700Z ============================
2026-02-15T18:41:30.2351755Z Branch:      main
2026-02-15T18:41:30.2374897Z Commit hash: f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-15T18:41:30.2463418Z Author:      Nengjun Ma <nengjunma@outlook.com>
2026-02-15T18:41:30.2490283Z Date:        2026-02-14 18:54:04 +0800
2026-02-15T18:41:30.2512801Z Message:     [CI] unlock when load model (#6771)
2026-02-15T18:41:30.2829860Z Tags:        
2026-02-15T18:41:30.2860631Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm-ascend (fetch)
2026-02-15T18:41:30.2870543Z 
2026-02-15T18:41:30.2880452Z ====> Check triton ascend info
2026-02-15T18:41:30.6143189Z Ubuntu clang version 15.0.7
2026-02-15T18:41:30.6152100Z Target: aarch64-unknown-linux-gnu
2026-02-15T18:41:30.6162205Z Thread model: posix
2026-02-15T18:41:30.6171702Z InstalledDir: /usr/bin
2026-02-15T18:41:30.6182563Z Found candidate GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-15T18:41:30.6192775Z Selected GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-15T18:41:30.6202735Z Candidate multilib: .;@m64
2026-02-15T18:41:30.6212505Z Selected multilib: .;@m64
2026-02-15T18:41:30.6297966Z /usr/local/Ascend/cann-8.5.0/tools/bishengir/bin/bishengir-compile
2026-02-15T18:41:31.2453554Z Name: triton-ascend
2026-02-15T18:41:31.2461838Z Version: 3.2.0
2026-02-15T18:41:31.2476375Z Summary: A language and compiler for custom Deep Learning operations on Ascend hardwares
2026-02-15T18:41:31.2485891Z Home-page: https://gitcode.com/Ascend/triton-ascend/
2026-02-15T18:41:31.2494734Z Author: 
2026-02-15T18:41:31.2503955Z Author-email: 
2026-02-15T18:41:31.2512897Z License: 
2026-02-15T18:41:31.2522351Z Location: /usr/local/python3.11.14/lib/python3.11/site-packages
2026-02-15T18:41:31.2531728Z Requires: 
2026-02-15T18:41:31.2540590Z Required-by: vllm_ascend
2026-02-15T18:41:49.4993782Z INFO 02-15 18:41:49 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:41:49.5001930Z INFO 02-15 18:41:49 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:41:49.5013767Z INFO 02-15 18:41:49 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:41:49.7152159Z INFO 02-15 18:41:49 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:41:56.0406648Z ============================= test session starts ==============================
2026-02-15T18:41:56.0414482Z platform linux -- Python 3.11.14, pytest-8.4.2, pluggy-1.6.0 -- /usr/local/python3.11.14/bin/python3
2026-02-15T18:41:56.0422728Z cachedir: .pytest_cache
2026-02-15T18:41:56.0434741Z rootdir: /vllm-workspace/vllm-ascend
2026-02-15T18:41:56.0442479Z configfile: pyproject.toml
2026-02-15T18:41:56.0451491Z plugins: asyncio-1.3.0, cov-7.0.0, mock-3.15.1, anyio-4.12.1
2026-02-15T18:41:56.0461300Z asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
2026-02-15T18:41:56.8864523Z collecting ... collected 1 item
2026-02-15T18:41:56.8872477Z 
2026-02-15T18:41:56.8888231Z [2026-02-15 18:41:56] INFO multi_node_config.py:294: Loading config yaml: tests/e2e/nightly/multi_node/config/DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-15T18:41:56.8928591Z [2026-02-15 18:41:56] INFO multi_node_config.py:351: Resolving cluster IPs via DNS...
2026-02-15T18:41:56.8991107Z [2026-02-15 18:41:56] INFO multi_node_config.py:212: Node 0 envs: {'HCCL_OP_EXPANSION_MODE': 'AIV', 'VLLM_USE_MODELSCOPE': 'True', 'HCCL_BUFFSIZE': '1024', 'SERVER_PORT': '8080', 'OMP_PROC_BIND': 'False', 'OMP_NUM_THREADS': '1', 'PYTORCH_NPU_ALLOC_CONF': 'expandable_segments:True', 'VLLM_ASCEND_ENABLE_FLASHCOMM1': '1', 'ASCEND_A3_EBA_ENABLE': '1', 'HCCL_IF_IP': '10.0.0.51', 'HCCL_SOCKET_IFNAME': 'eth0', 'GLOO_SOCKET_IFNAME': 'eth0', 'TP_SOCKET_IFNAME': 'eth0', 'LOCAL_IP': '10.0.0.51', 'NIC_NAME': 'eth0', 'MASTER_IP': '10.0.0.51'}
2026-02-15T18:41:56.9011527Z [2026-02-15 18:41:56] INFO multi_node_config.py:137: Not launching proxy on non-master node
2026-02-15T18:41:56.9025916Z [2026-02-15 18:41:56] INFO conftest.py:241: Starting server with command: vllm serve vllm-ascend/DeepSeek-V3.2-W8A8 --host 0.0.0.0 --port 8080 --data-parallel-size 4 --data-parallel-size-local 2 --data-parallel-address 10.0.0.51 --data-parallel-rpc-port 13399 --tensor-parallel-size 8 --quantization ascend --seed 1024 --enable-expert-parallel --max-num-seqs 16 --max-model-len 8192 --max-num-batched-tokens 4096 --no-enable-prefix-caching --gpu-memory-utilization 0.85 --trust-remote-code --speculative-config {"num_speculative_tokens": 2, "method":"deepseek_mtp"} --compilation-config {"cudagraph_capture_sizes": [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], "cudagraph_mode": "FULL_DECODE_ONLY"} --additional-config {"layer_sharding": ["q_b_proj", "o_proj"]} --tokenizer-mode deepseek_v32 --reasoning-parser deepseek_v3
2026-02-15T18:42:01.4523477Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node INFO 02-15 18:42:01 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:42:01.4527120Z INFO 02-15 18:42:01 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:42:01.4536687Z INFO 02-15 18:42:01 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:42:01.4600045Z INFO 02-15 18:42:01 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:42:07.8007736Z 2026-02-15 18:42:07,798 - 138 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:42:07.8311097Z INFO 02-15 18:42:07 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:42:07.9723725Z INFO 02-15 18:42:07 [serve.py:100] Defaulting api_server_count to data_parallel_size (4).
2026-02-15T18:42:07.9756825Z INFO 02-15 18:42:07 [utils.py:325] 
2026-02-15T18:42:07.9767303Z INFO 02-15 18:42:07 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
2026-02-15T18:42:07.9777602Z INFO 02-15 18:42:07 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
2026-02-15T18:42:07.9786192Z INFO 02-15 18:42:07 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   vllm-ascend/DeepSeek-V3.2-W8A8
2026-02-15T18:42:07.9796921Z INFO 02-15 18:42:07 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
2026-02-15T18:42:07.9806162Z INFO 02-15 18:42:07 [utils.py:325] 
2026-02-15T18:42:07.9825920Z INFO 02-15 18:42:07 [utils.py:261] non-default args: {'model_tag': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'api_server_count': 4, 'host': '0.0.0.0', 'port': 8080, 'model': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'tokenizer_mode': 'deepseek_v32', 'trust_remote_code': True, 'seed': 1024, 'max_model_len': 8192, 'quantization': 'ascend', 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 8, 'data_parallel_size': 4, 'data_parallel_size_local': 2, 'data_parallel_address': '10.0.0.51', 'data_parallel_rpc_port': 13399, 'enable_expert_parallel': True, 'gpu_memory_utilization': 0.85, 'enable_prefix_caching': False, 'max_num_batched_tokens': 4096, 'max_num_seqs': 16, 'speculative_config': {'num_speculative_tokens': 2, 'method': 'deepseek_mtp'}, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}, 'additional_config': {'layer_sharding': ['q_b_proj', 'o_proj']}}
2026-02-15T18:42:08.0158767Z 2026-02-15 18:42:08,014 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-15T18:42:08.0205863Z INFO 02-15 18:42:08 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-15T18:42:08.0223206Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:08.0254847Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:08.0274016Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:08.0284205Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-15T18:42:08.0466559Z INFO 02-15 18:42:08 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-15T18:42:08.0484944Z INFO 02-15 18:42:08 [model.py:1561] Using max model len 8192
2026-02-15T18:42:08.3551843Z WARNING 02-15 18:42:08 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-15T18:42:08.3573556Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:08.3583397Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:08.3592183Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-15T18:42:08.3664191Z INFO 02-15 18:42:08 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-15T18:42:08.3684323Z INFO 02-15 18:42:08 [model.py:1561] Using max model len 163840
2026-02-15T18:42:08.3693443Z WARNING 02-15 18:42:08 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-15T18:42:08.3701188Z INFO 02-15 18:42:08 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-15T18:42:08.6869472Z INFO 02-15 18:42:08 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-15T18:42:08.6876734Z INFO 02-15 18:42:08 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-15T18:42:08.6887069Z WARNING 02-15 18:42:08 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-15T18:42:08.6896698Z WARNING 02-15 18:42:08 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-15T18:42:08.6905123Z INFO 02-15 18:42:08 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:42:08.6914219Z INFO 02-15 18:42:08 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:42:08.6924007Z INFO 02-15 18:42:08 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:42:08.6933129Z WARNING 02-15 18:42:08 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-15T18:42:08.6942960Z INFO 02-15 18:42:08 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-15T18:42:08.6952758Z WARNING 02-15 18:42:08 [platform.py:335] [91m
2026-02-15T18:42:08.6963071Z WARNING 02-15 18:42:08 [platform.py:335]             **********************************************************************************
2026-02-15T18:42:08.6972944Z WARNING 02-15 18:42:08 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-15T18:42:08.6982939Z WARNING 02-15 18:42:08 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-15T18:42:08.6992602Z WARNING 02-15 18:42:08 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-15T18:42:08.7002917Z WARNING 02-15 18:42:08 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-15T18:42:08.7012503Z WARNING 02-15 18:42:08 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-15T18:42:08.7020467Z WARNING 02-15 18:42:08 [platform.py:335]             * batch size for graph capture.
2026-02-15T18:42:08.7030542Z WARNING 02-15 18:42:08 [platform.py:335]             * For more details, please refer to:
2026-02-15T18:42:08.7040159Z WARNING 02-15 18:42:08 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-15T18:42:08.7051807Z WARNING 02-15 18:42:08 [platform.py:335]             **********************************************************************************[0m
2026-02-15T18:42:08.7060125Z WARNING 02-15 18:42:08 [platform.py:335]             
2026-02-15T18:42:08.7069305Z INFO 02-15 18:42:08 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-15T18:42:08.7080815Z INFO 02-15 18:42:08 [utils.py:851] Started DP Coordinator process (PID: 151)
2026-02-15T18:42:13.2839824Z INFO 02-15 18:42:13 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:42:13.2848536Z INFO 02-15 18:42:13 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:42:13.2858748Z INFO 02-15 18:42:13 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:42:13.2910773Z INFO 02-15 18:42:13 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:42:13.4661572Z INFO 02-15 18:42:13 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:42:13.4670442Z INFO 02-15 18:42:13 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:42:13.4679148Z INFO 02-15 18:42:13 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:42:13.4747907Z INFO 02-15 18:42:13 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:42:23.2942241Z INFO 02-15 18:42:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:42:23.2951531Z INFO 02-15 18:42:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:42:23.2962097Z INFO 02-15 18:42:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:42:23.3049560Z INFO 02-15 18:42:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:42:28.5445617Z INFO 02-15 18:42:28 [utils.py:218] Started 4 API server processes
2026-02-15T18:42:28.7388054Z [0;36m(EngineCore_DP0 pid=154)[0;0m 2026-02-15 18:42:28,736 - 154 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:42:28.7425091Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-15 18:42:28 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:42:28.7466244Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-15 18:42:28 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', speculative_config=SpeculativeConfig(method='mtp', model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', num_spec_tokens=2), tokenizer='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', skip_tokenizer_init=False, tokenizer_mode=deepseek_v32, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=4, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=bfloat16, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=1024, served_model_name=/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [24, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 48, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
2026-02-15T18:42:28.7473906Z [0;36m(EngineCore_DP1 pid=173)[0;0m 2026-02-15 18:42:28,741 - 173 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:42:28.7491787Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-15 18:42:28 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:42:33.0263301Z INFO 02-15 18:42:33 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:42:33.0271294Z INFO 02-15 18:42:33 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:42:33.0282876Z INFO 02-15 18:42:33 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:42:33.0333031Z INFO 02-15 18:42:33 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:42:33.6308943Z INFO 02-15 18:42:33 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:42:33.6316157Z INFO 02-15 18:42:33 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:42:33.6328253Z INFO 02-15 18:42:33 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:42:33.6389353Z INFO 02-15 18:42:33 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:42:33.8075822Z INFO 02-15 18:42:33 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:42:33.8084879Z INFO 02-15 18:42:33 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:42:33.8094465Z INFO 02-15 18:42:33 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:42:33.8104202Z INFO 02-15 18:42:33 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:42:33.8115348Z INFO 02-15 18:42:33 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:42:33.8125475Z INFO 02-15 18:42:33 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:42:33.8170807Z INFO 02-15 18:42:33 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:42:33.8191799Z INFO 02-15 18:42:33 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:42:33.9856219Z INFO 02-15 18:42:33 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:42:33.9863287Z INFO 02-15 18:42:33 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:42:33.9872458Z INFO 02-15 18:42:33 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:42:33.9932271Z INFO 02-15 18:42:33 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:42:34.0347009Z INFO 02-15 18:42:34 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:42:34.0370932Z INFO 02-15 18:42:34 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:42:34.0380871Z INFO 02-15 18:42:34 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:42:34.0439762Z INFO 02-15 18:42:34 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:42:38.8308598Z [0;36m(ApiServer_0 pid=184)[0;0m 2026-02-15 18:42:38,828 - 184 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:42:38.8449196Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-15 18:42:38 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:42:38.8683144Z [0;36m(ApiServer_0 pid=184)[0;0m 2026-02-15 18:42:38,866 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-15T18:42:38.8733170Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-15 18:42:38 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-15T18:42:38.9714815Z [0;36m(ApiServer_0 pid=184)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:38.9730750Z [0;36m(ApiServer_0 pid=184)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:38.9749963Z [0;36m(ApiServer_0 pid=184)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:38.9761479Z [0;36m(ApiServer_0 pid=184)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-15T18:42:38.9824385Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-15 18:42:38 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-15T18:42:38.9843063Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-15 18:42:38 [model.py:1561] Using max model len 8192
2026-02-15T18:42:39.0882594Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-15 18:42:39 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-15T18:42:39.0905044Z [0;36m(ApiServer_0 pid=184)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:39.0914843Z [0;36m(ApiServer_0 pid=184)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:39.0925606Z [0;36m(ApiServer_0 pid=184)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-15T18:42:39.0963320Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-15 18:42:39 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-15T18:42:39.0985117Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-15 18:42:39 [model.py:1561] Using max model len 163840
2026-02-15T18:42:39.0995879Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-15 18:42:39 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-15T18:42:39.1006742Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-15 18:42:39 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-15T18:42:39.2165547Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-15 18:42:39 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-15T18:42:39.2173397Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-15 18:42:39 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-15T18:42:39.2213414Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-15 18:42:39 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-15T18:42:39.2222726Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-15 18:42:39 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-15T18:42:39.2231752Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-15 18:42:39 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:42:39.2241826Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-15 18:42:39 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:42:39.2252643Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-15 18:42:39 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:42:39.2262316Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-15 18:42:39 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-15T18:42:39.2272875Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-15 18:42:39 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-15T18:42:39.2282608Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-15 18:42:39 [platform.py:335] [91m
2026-02-15T18:42:39.2291453Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-15 18:42:39 [platform.py:335]             **********************************************************************************
2026-02-15T18:42:39.2300754Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-15 18:42:39 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-15T18:42:39.2310693Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-15 18:42:39 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-15T18:42:39.2320565Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-15 18:42:39 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-15T18:42:39.2330287Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-15 18:42:39 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-15T18:42:39.2340080Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-15 18:42:39 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-15T18:42:39.2349659Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-15 18:42:39 [platform.py:335]             * batch size for graph capture.
2026-02-15T18:42:39.2359807Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-15 18:42:39 [platform.py:335]             * For more details, please refer to:
2026-02-15T18:42:39.2369918Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-15 18:42:39 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-15T18:42:39.2379738Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-15 18:42:39 [platform.py:335]             **********************************************************************************[0m
2026-02-15T18:42:39.2388962Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-15 18:42:39 [platform.py:335]             
2026-02-15T18:42:39.2399342Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-15 18:42:39 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-15T18:42:39.2409241Z 2026-02-15 18:42:39,221 - 202 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:42:39.2419258Z INFO 02-15 18:42:39 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:42:39.4958526Z 2026-02-15 18:42:39,493 - 201 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:42:39.5021389Z INFO 02-15 18:42:39 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:42:39.6321845Z [0;36m(ApiServer_2 pid=186)[0;0m 2026-02-15 18:42:39,630 - 186 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:42:39.6471321Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-15 18:42:39 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:42:39.6641389Z [0;36m(ApiServer_2 pid=186)[0;0m 2026-02-15 18:42:39,660 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-15T18:42:39.6676236Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-15 18:42:39 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-15T18:42:39.7715215Z [0;36m(ApiServer_2 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:39.7758491Z [0;36m(ApiServer_2 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:39.7767836Z [0;36m(ApiServer_2 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:39.7777651Z [0;36m(ApiServer_2 pid=186)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-15T18:42:39.7818339Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-15 18:42:39 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-15T18:42:39.7828701Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-15 18:42:39 [model.py:1561] Using max model len 8192
2026-02-15T18:42:39.8848809Z [0;36m(ApiServer_1 pid=185)[0;0m 2026-02-15 18:42:39,883 - 185 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:42:39.8941534Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-15 18:42:39 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-15T18:42:39.8962329Z [0;36m(ApiServer_2 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:39.8972743Z [0;36m(ApiServer_2 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:39.8981634Z [0;36m(ApiServer_2 pid=186)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-15T18:42:39.9014015Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-15 18:42:39 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:42:39.9034213Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-15 18:42:39 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-15T18:42:39.9043935Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-15 18:42:39 [model.py:1561] Using max model len 163840
2026-02-15T18:42:39.9054021Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-15 18:42:39 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-15T18:42:39.9063764Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-15 18:42:39 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-15T18:42:39.9112670Z [0;36m(ApiServer_3 pid=187)[0;0m 2026-02-15 18:42:39,909 - 187 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:42:39.9179718Z [0;36m(ApiServer_1 pid=185)[0;0m 2026-02-15 18:42:39,916 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-15T18:42:39.9248427Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-15 18:42:39 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-15T18:42:39.9278252Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-15 18:42:39 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:42:39.9403494Z [0;36m(ApiServer_3 pid=187)[0;0m 2026-02-15 18:42:39,938 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-15T18:42:39.9465893Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-15 18:42:39 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-15T18:42:40.0218197Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-15 18:42:40 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-15T18:42:40.0226572Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-15 18:42:40 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-15T18:42:40.0238756Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-15 18:42:40 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-15T18:42:40.0248495Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-15 18:42:40 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-15T18:42:40.0257882Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-15 18:42:40 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:42:40.0267614Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-15 18:42:40 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:42:40.0278646Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-15 18:42:40 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:42:40.0288640Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-15 18:42:40 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-15T18:42:40.0297779Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-15 18:42:40 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-15T18:42:40.0307529Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-15 18:42:40 [platform.py:335] [91m
2026-02-15T18:42:40.0316956Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             **********************************************************************************
2026-02-15T18:42:40.0327316Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-15T18:42:40.0337206Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-15T18:42:40.0346969Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-15T18:42:40.0356193Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-15T18:42:40.0367505Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-15T18:42:40.0377011Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * batch size for graph capture.
2026-02-15T18:42:40.0386640Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * For more details, please refer to:
2026-02-15T18:42:40.0397461Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-15T18:42:40.0408809Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             **********************************************************************************[0m
2026-02-15T18:42:40.0421164Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             
2026-02-15T18:42:40.0430040Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-15 18:42:40 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-15T18:42:40.0439897Z [0;36m(ApiServer_1 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:40.0449990Z [0;36m(ApiServer_1 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:40.0459367Z [0;36m(ApiServer_1 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:40.0469311Z [0;36m(ApiServer_1 pid=185)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-15T18:42:40.0480045Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-15 18:42:40 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-15T18:42:40.0489751Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-15 18:42:40 [model.py:1561] Using max model len 8192
2026-02-15T18:42:40.0498535Z [0;36m(ApiServer_3 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:40.0509242Z [0;36m(ApiServer_3 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:40.0539361Z [0;36m(ApiServer_3 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:40.0549351Z [0;36m(ApiServer_3 pid=187)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-15T18:42:40.0611372Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-15 18:42:40 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-15T18:42:40.0611868Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-15 18:42:40 [model.py:1561] Using max model len 8192
2026-02-15T18:42:40.1461810Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-15 18:42:40 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-15T18:42:40.1484663Z [0;36m(ApiServer_1 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:40.1493007Z [0;36m(ApiServer_1 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:40.1502521Z [0;36m(ApiServer_1 pid=185)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-15T18:42:40.1542169Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-15 18:42:40 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-15T18:42:40.1563891Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-15 18:42:40 [model.py:1561] Using max model len 163840
2026-02-15T18:42:40.1573728Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-15 18:42:40 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-15T18:42:40.1582365Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-15 18:42:40 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-15T18:42:40.1721931Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-15 18:42:40 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-15T18:42:40.1743092Z [0;36m(ApiServer_3 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:40.1752896Z [0;36m(ApiServer_3 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-15T18:42:40.1762508Z [0;36m(ApiServer_3 pid=187)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-15T18:42:40.1801733Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-15 18:42:40 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-15T18:42:40.1823210Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-15 18:42:40 [model.py:1561] Using max model len 163840
2026-02-15T18:42:40.1833462Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-15 18:42:40 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-15T18:42:40.1842987Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-15 18:42:40 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-15T18:42:40.2697289Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-15 18:42:40 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-15T18:42:40.2705279Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-15 18:42:40 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-15T18:42:40.2720903Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-15 18:42:40 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-15T18:42:40.2730220Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-15 18:42:40 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-15T18:42:40.2739220Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-15 18:42:40 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:42:40.2748528Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-15 18:42:40 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:42:40.2758725Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-15 18:42:40 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:42:40.2769109Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-15 18:42:40 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-15T18:42:40.2778480Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-15 18:42:40 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-15T18:42:40.2787184Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-15 18:42:40 [platform.py:335] [91m
2026-02-15T18:42:40.2796766Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             **********************************************************************************
2026-02-15T18:42:40.2806909Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-15T18:42:40.2816523Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-15T18:42:40.2825742Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-15T18:42:40.2835073Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-15T18:42:40.2844981Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-15T18:42:40.2854787Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * batch size for graph capture.
2026-02-15T18:42:40.2863847Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * For more details, please refer to:
2026-02-15T18:42:40.2873825Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-15T18:42:40.2883500Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             **********************************************************************************[0m
2026-02-15T18:42:40.2892892Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             
2026-02-15T18:42:40.2902328Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-15 18:42:40 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-15T18:42:40.2999279Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-15 18:42:40 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-15T18:42:40.3008476Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-15 18:42:40 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-15T18:42:40.3020108Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-15 18:42:40 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-15T18:42:40.3030122Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-15 18:42:40 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-15T18:42:40.3038432Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-15 18:42:40 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:42:40.3048308Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-15 18:42:40 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:42:40.3058450Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-15 18:42:40 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:42:40.3068456Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-15 18:42:40 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-15T18:42:40.3078528Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-15 18:42:40 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-15T18:42:40.3088228Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-15 18:42:40 [platform.py:335] [91m
2026-02-15T18:42:40.3097909Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             **********************************************************************************
2026-02-15T18:42:40.3107262Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-15T18:42:40.3117184Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-15T18:42:40.3128491Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-15T18:42:40.3137577Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-15T18:42:40.3146668Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-15T18:42:40.3156260Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * batch size for graph capture.
2026-02-15T18:42:40.3166443Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * For more details, please refer to:
2026-02-15T18:42:40.3176410Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-15T18:42:40.3185958Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             **********************************************************************************[0m
2026-02-15T18:42:40.3194988Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-15 18:42:40 [platform.py:335]             
2026-02-15T18:42:40.3206143Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-15 18:42:40 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-15T18:42:41.1860659Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-15T18:42:41.1868160Z   warnings.warn(
2026-02-15T18:42:41.1986684Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-15T18:42:41.1994795Z   warnings.warn(
2026-02-15T18:42:43.9372165Z INFO 02-15 18:42:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:42:43.9381202Z INFO 02-15 18:42:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:42:43.9391013Z INFO 02-15 18:42:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:42:43.9450290Z INFO 02-15 18:42:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:42:44.2611659Z INFO 02-15 18:42:44 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:42:44.2621692Z INFO 02-15 18:42:44 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:42:44.2632380Z INFO 02-15 18:42:44 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:42:44.2691079Z INFO 02-15 18:42:44 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:42:44.8037109Z INFO 02-15 18:42:44 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:42:44.8045701Z INFO 02-15 18:42:44 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:42:44.8057666Z INFO 02-15 18:42:44 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:42:44.8067904Z INFO 02-15 18:42:44 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:42:44.8077991Z INFO 02-15 18:42:44 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:42:44.8089186Z INFO 02-15 18:42:44 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:42:44.8618422Z INFO 02-15 18:42:44 [parallel_state.py:1212] world_size=32 rank=8 local_rank=0 distributed_init_method=tcp://10.0.0.51:46893 backend=hccl
2026-02-15T18:42:44.8626844Z INFO 02-15 18:42:44 [parallel_state.py:1212] world_size=32 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.51:46893 backend=hccl
2026-02-15T18:42:49.0172328Z 2026-02-15 18:42:49,014 - 254 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:42:49.0229400Z INFO 02-15 18:42:49 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:42:49.2639200Z 2026-02-15 18:42:49,261 - 257 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:42:49.2691908Z INFO 02-15 18:42:49 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:42:50.3478760Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-15T18:42:50.3485973Z   warnings.warn(
2026-02-15T18:42:50.6239135Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-15T18:42:50.6246085Z   warnings.warn(
2026-02-15T18:42:52.4541554Z INFO 02-15 18:42:52 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:42:52.4560642Z INFO 02-15 18:42:52 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:42:52.4561413Z INFO 02-15 18:42:52 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:42:52.7725956Z INFO 02-15 18:42:52 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:42:52.7733604Z INFO 02-15 18:42:52 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:42:52.7745512Z INFO 02-15 18:42:52 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:42:52.8709828Z INFO 02-15 18:42:52 [parallel_state.py:1212] world_size=32 rank=9 local_rank=1 distributed_init_method=tcp://10.0.0.51:46893 backend=hccl
2026-02-15T18:42:53.1934091Z INFO 02-15 18:42:53 [parallel_state.py:1212] world_size=32 rank=1 local_rank=1 distributed_init_method=tcp://10.0.0.51:46893 backend=hccl
2026-02-15T18:42:53.5261899Z INFO 02-15 18:42:53 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:42:53.5270590Z INFO 02-15 18:42:53 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:42:53.5279960Z INFO 02-15 18:42:53 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:42:53.5337274Z INFO 02-15 18:42:53 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:42:53.9308270Z INFO 02-15 18:42:53 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:42:53.9320103Z INFO 02-15 18:42:53 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:42:53.9330259Z INFO 02-15 18:42:53 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:42:53.9373518Z INFO 02-15 18:42:53 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:42:58.5060807Z 2026-02-15 18:42:58,503 - 369 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:42:58.5093454Z INFO 02-15 18:42:58 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:42:58.9024143Z 2026-02-15 18:42:58,900 - 372 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:42:58.9087002Z INFO 02-15 18:42:58 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:42:59.8122502Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-15T18:42:59.8128742Z   warnings.warn(
2026-02-15T18:43:00.1904100Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-15T18:43:00.1910200Z   warnings.warn(
2026-02-15T18:43:01.9055229Z INFO 02-15 18:43:01 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:43:01.9068493Z INFO 02-15 18:43:01 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:43:01.9075911Z INFO 02-15 18:43:01 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:43:02.3096275Z INFO 02-15 18:43:02 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:43:02.3104671Z INFO 02-15 18:43:02 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:43:02.3116011Z INFO 02-15 18:43:02 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:43:02.3331876Z INFO 02-15 18:43:02 [parallel_state.py:1212] world_size=32 rank=10 local_rank=2 distributed_init_method=tcp://10.0.0.51:46893 backend=hccl
2026-02-15T18:43:02.7541930Z INFO 02-15 18:43:02 [parallel_state.py:1212] world_size=32 rank=2 local_rank=2 distributed_init_method=tcp://10.0.0.51:46893 backend=hccl
2026-02-15T18:43:02.9824965Z INFO 02-15 18:43:02 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:43:02.9832613Z INFO 02-15 18:43:02 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:43:02.9843784Z INFO 02-15 18:43:02 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:43:02.9900897Z INFO 02-15 18:43:02 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:43:03.3577267Z INFO 02-15 18:43:03 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:43:03.3588532Z INFO 02-15 18:43:03 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:43:03.3595967Z INFO 02-15 18:43:03 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:43:03.3651765Z INFO 02-15 18:43:03 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:43:08.0095163Z 2026-02-15 18:43:08,007 - 473 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:43:08.0151307Z INFO 02-15 18:43:08 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:43:08.3251358Z 2026-02-15 18:43:08,322 - 476 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:43:08.3286413Z INFO 02-15 18:43:08 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:43:09.3127574Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-15T18:43:09.3134050Z   warnings.warn(
2026-02-15T18:43:09.6055638Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-15T18:43:09.6062497Z   warnings.warn(
2026-02-15T18:43:11.4765464Z INFO 02-15 18:43:11 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:43:11.4802797Z INFO 02-15 18:43:11 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:43:11.4803573Z INFO 02-15 18:43:11 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:43:11.6380046Z INFO 02-15 18:43:11 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:43:11.6388036Z INFO 02-15 18:43:11 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:43:11.6399585Z INFO 02-15 18:43:11 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:43:11.9151485Z INFO 02-15 18:43:11 [parallel_state.py:1212] world_size=32 rank=11 local_rank=3 distributed_init_method=tcp://10.0.0.51:46893 backend=hccl
2026-02-15T18:43:12.0925062Z INFO 02-15 18:43:12 [parallel_state.py:1212] world_size=32 rank=3 local_rank=3 distributed_init_method=tcp://10.0.0.51:46893 backend=hccl
2026-02-15T18:43:12.5834866Z INFO 02-15 18:43:12 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:43:12.5844966Z INFO 02-15 18:43:12 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:43:12.5855425Z INFO 02-15 18:43:12 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:43:12.5910221Z INFO 02-15 18:43:12 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:43:12.6745575Z INFO 02-15 18:43:12 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:43:12.6750610Z INFO 02-15 18:43:12 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:43:12.6762512Z INFO 02-15 18:43:12 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:43:12.6787024Z INFO 02-15 18:43:12 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:43:17.5458936Z 2026-02-15 18:43:17,543 - 580 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:43:17.5514773Z INFO 02-15 18:43:17 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:43:17.7041249Z 2026-02-15 18:43:17,702 - 577 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:43:17.7100256Z INFO 02-15 18:43:17 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:43:18.9684800Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-15T18:43:18.9693051Z   warnings.warn(
2026-02-15T18:43:19.1245217Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-15T18:43:19.1255277Z   warnings.warn(
2026-02-15T18:43:21.0884293Z INFO 02-15 18:43:21 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:43:21.0892283Z INFO 02-15 18:43:21 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:43:21.0902804Z INFO 02-15 18:43:21 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:43:21.2058363Z INFO 02-15 18:43:21 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:43:21.2067461Z INFO 02-15 18:43:21 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:43:21.2078096Z INFO 02-15 18:43:21 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:43:21.5222232Z INFO 02-15 18:43:21 [parallel_state.py:1212] world_size=32 rank=4 local_rank=4 distributed_init_method=tcp://10.0.0.51:46893 backend=hccl
2026-02-15T18:43:21.6228489Z INFO 02-15 18:43:21 [parallel_state.py:1212] world_size=32 rank=12 local_rank=4 distributed_init_method=tcp://10.0.0.51:46893 backend=hccl
2026-02-15T18:43:22.2119444Z INFO 02-15 18:43:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:43:22.2128295Z INFO 02-15 18:43:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:43:22.2137749Z INFO 02-15 18:43:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:43:22.2195435Z INFO 02-15 18:43:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:43:22.4608395Z INFO 02-15 18:43:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:43:22.4616647Z INFO 02-15 18:43:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:43:22.4630422Z INFO 02-15 18:43:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:43:22.4686522Z INFO 02-15 18:43:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:43:27.2288567Z 2026-02-15 18:43:27,226 - 681 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:43:27.2321052Z INFO 02-15 18:43:27 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:43:27.5216804Z 2026-02-15 18:43:27,519 - 684 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:43:27.5276832Z INFO 02-15 18:43:27 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:43:28.5508962Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-15T18:43:28.5516194Z   warnings.warn(
2026-02-15T18:43:28.8337070Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-15T18:43:28.8344012Z   warnings.warn(
2026-02-15T18:43:30.7149509Z INFO 02-15 18:43:30 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:43:30.7160402Z INFO 02-15 18:43:30 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:43:30.7171656Z INFO 02-15 18:43:30 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:43:31.0522078Z INFO 02-15 18:43:31 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:43:31.0531169Z INFO 02-15 18:43:31 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:43:31.0541769Z INFO 02-15 18:43:31 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:43:31.1610497Z INFO 02-15 18:43:31 [parallel_state.py:1212] world_size=32 rank=5 local_rank=5 distributed_init_method=tcp://10.0.0.51:46893 backend=hccl
2026-02-15T18:43:31.4956649Z INFO 02-15 18:43:31 [parallel_state.py:1212] world_size=32 rank=13 local_rank=5 distributed_init_method=tcp://10.0.0.51:46893 backend=hccl
2026-02-15T18:43:31.5510257Z INFO 02-15 18:43:31 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:43:31.5519771Z INFO 02-15 18:43:31 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:43:31.5529301Z INFO 02-15 18:43:31 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:43:31.5585263Z INFO 02-15 18:43:31 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:43:32.0332106Z INFO 02-15 18:43:32 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:43:32.0340564Z INFO 02-15 18:43:32 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:43:32.0350900Z INFO 02-15 18:43:32 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:43:32.0407170Z INFO 02-15 18:43:32 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:43:36.4154149Z 2026-02-15 18:43:36,412 - 785 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:43:36.4181359Z INFO 02-15 18:43:36 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:43:37.0002786Z 2026-02-15 18:43:36,998 - 788 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:43:37.0063908Z INFO 02-15 18:43:37 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:43:37.7003855Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-15T18:43:37.7005747Z   warnings.warn(
2026-02-15T18:43:38.3131720Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-15T18:43:38.3139680Z   warnings.warn(
2026-02-15T18:43:39.7970719Z INFO 02-15 18:43:39 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:43:39.7980237Z INFO 02-15 18:43:39 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:43:39.7989463Z INFO 02-15 18:43:39 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:43:40.2301147Z INFO 02-15 18:43:40 [parallel_state.py:1212] world_size=32 rank=6 local_rank=6 distributed_init_method=tcp://10.0.0.51:46893 backend=hccl
2026-02-15T18:43:40.3899121Z INFO 02-15 18:43:40 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:43:40.3907438Z INFO 02-15 18:43:40 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:43:40.3917833Z INFO 02-15 18:43:40 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:43:40.6213523Z INFO 02-15 18:43:40 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:43:40.6222575Z INFO 02-15 18:43:40 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:43:40.6231388Z INFO 02-15 18:43:40 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:43:40.6282377Z INFO 02-15 18:43:40 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:43:40.8246137Z INFO 02-15 18:43:40 [parallel_state.py:1212] world_size=32 rank=14 local_rank=6 distributed_init_method=tcp://10.0.0.51:46893 backend=hccl
2026-02-15T18:43:41.4788724Z INFO 02-15 18:43:41 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:43:41.4796604Z INFO 02-15 18:43:41 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:43:41.4806115Z INFO 02-15 18:43:41 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:43:41.4862095Z INFO 02-15 18:43:41 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:43:45.4465963Z 2026-02-15 18:43:45,444 - 889 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:43:45.4522846Z INFO 02-15 18:43:45 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:43:46.4504392Z 2026-02-15 18:43:46,448 - 892 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-15T18:43:46.4538266Z INFO 02-15 18:43:46 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-15T18:43:46.7111386Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-15T18:43:46.7119465Z   warnings.warn(
2026-02-15T18:43:47.7311226Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-15T18:43:47.7319742Z   warnings.warn(
2026-02-15T18:43:48.7422695Z INFO 02-15 18:43:48 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:43:48.7431716Z INFO 02-15 18:43:48 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:43:48.7442968Z INFO 02-15 18:43:48 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:43:49.1681972Z INFO 02-15 18:43:49 [parallel_state.py:1212] world_size=32 rank=7 local_rank=7 distributed_init_method=tcp://10.0.0.51:46893 backend=hccl
2026-02-15T18:43:49.7734936Z INFO 02-15 18:43:49 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:43:49.7743329Z INFO 02-15 18:43:49 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:43:49.7753940Z INFO 02-15 18:43:49 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:43:50.2011250Z INFO 02-15 18:43:50 [parallel_state.py:1212] world_size=32 rank=15 local_rank=7 distributed_init_method=tcp://10.0.0.51:46893 backend=hccl
2026-02-15T18:43:50.2411160Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.3194242Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.3215557Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.3223917Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.3233630Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.3243480Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.3254291Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.3263935Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.3274153Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.3673502Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.3699134Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.3708183Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.3717065Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.3727577Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.3736886Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.3747050Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.3971073Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-15T18:43:50.3999433Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-15T18:43:50.4009984Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-15T18:43:50.4018107Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-15T18:43:50.4028936Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-15T18:43:50.4037798Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-15T18:43:50.4048125Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-15T18:43:50.4058503Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-15T18:43:50.4074663Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-15T18:43:50.4083573Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-15T18:43:50.4093217Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-15T18:43:50.4102623Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-15T18:43:50.4112431Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-15T18:43:50.4121937Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-15T18:43:50.4132303Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-15T18:43:50.4142089Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-15T18:43:50.4153113Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4163185Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4172808Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4182620Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4192426Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4202764Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4212897Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4222337Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4232204Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4242756Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4252590Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4262609Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4272656Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4283247Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4293547Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4303352Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4313141Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4323565Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4333472Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4343435Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4352400Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4362257Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4372857Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4383428Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4703701Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4716520Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4726811Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4736760Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4746602Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4756227Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4766261Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4775836Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4785764Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4795243Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4806172Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4815738Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4825204Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4835012Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4845013Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4855423Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4864765Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4874374Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4884686Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4894741Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4904112Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4913683Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4923770Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.4934097Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4943932Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-15T18:43:50.4954594Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.4969000Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.4980224Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.4989678Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.5000462Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.5010146Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.5020449Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.5029942Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.5040318Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.5050080Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.5060526Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.5070654Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.5081038Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.5091272Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.5100965Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.5177769Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.5195106Z INFO 02-15 18:43:50 [parallel_state.py:1423] rank 0 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
2026-02-15T18:43:50.5814454Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.5830753Z INFO 02-15 18:43:50 [parallel_state.py:1423] rank 1 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
2026-02-15T18:43:50.5975075Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.5986382Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.5994339Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6004902Z INFO 02-15 18:43:50 [parallel_state.py:1423] rank 3 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
2026-02-15T18:43:50.6012800Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6022378Z INFO 02-15 18:43:50 [parallel_state.py:1423] rank 4 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 4, EP rank 4
2026-02-15T18:43:50.6031549Z INFO 02-15 18:43:50 [parallel_state.py:1423] rank 6 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 6, EP rank 6
2026-02-15T18:43:50.6040517Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6050662Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6060112Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6069683Z INFO 02-15 18:43:50 [parallel_state.py:1423] rank 5 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 5, EP rank 5
2026-02-15T18:43:50.6079860Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6088608Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6097873Z INFO 02-15 18:43:50 [parallel_state.py:1423] rank 9 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 1, EP rank 9
2026-02-15T18:43:50.6107135Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6117282Z INFO 02-15 18:43:50 [parallel_state.py:1423] rank 8 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 0, EP rank 8
2026-02-15T18:43:50.6127548Z INFO 02-15 18:43:50 [parallel_state.py:1423] rank 7 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 7, EP rank 7
2026-02-15T18:43:50.6137660Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6146669Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6156096Z INFO 02-15 18:43:50 [parallel_state.py:1423] rank 10 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 2, EP rank 10
2026-02-15T18:43:50.6165778Z INFO 02-15 18:43:50 [parallel_state.py:1423] rank 11 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 3, EP rank 11
2026-02-15T18:43:50.6174985Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6184561Z INFO 02-15 18:43:50 [parallel_state.py:1423] rank 13 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 5, EP rank 13
2026-02-15T18:43:50.6194108Z INFO 02-15 18:43:50 [parallel_state.py:1423] rank 12 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 4, EP rank 12
2026-02-15T18:43:50.6204704Z INFO 02-15 18:43:50 [parallel_state.py:1423] rank 14 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 6, EP rank 14
2026-02-15T18:43:50.6214436Z INFO 02-15 18:43:50 [parallel_state.py:1423] rank 15 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 7, EP rank 15
2026-02-15T18:43:50.6223794Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6233795Z INFO 02-15 18:43:50 [parallel_state.py:1423] rank 2 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
2026-02-15T18:43:50.6400568Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6419689Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6428736Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6437915Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6448986Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6457789Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6466613Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6476238Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6486329Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6495641Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6505531Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6515023Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6524955Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6535478Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6545012Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6554189Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-15T18:43:50.6564058Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.6573616Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.6582570Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.6592355Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.6601604Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.6611473Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.6621347Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.6630190Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.6640214Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.6649179Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.6658294Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.6667882Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.6677300Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.6686810Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.6696498Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.6706454Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-15T18:43:50.7567307Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.7588338Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.7596842Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.7607233Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.7616044Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.7626394Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.7634651Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.7644193Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.7653936Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.7663503Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.7673100Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.7682621Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.7692568Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.7701643Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.7711326Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.7720855Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.8113369Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.8124245Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.8133717Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.8143157Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.8152986Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.8162664Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.8172285Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.8181713Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.8192827Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.8200296Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.8210295Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.8219341Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.8229888Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-15 18:43:50 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-15T18:43:50.8239024Z [0;36m(Worker_DP0_TP1_EP1 pid=257)[0;0m INFO 02-15 18:43:50 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-15T18:43:50.8250505Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-15 18:43:50 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-15T18:43:50.8259324Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.8268478Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-15 18:43:50 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-15T18:43:50.8277990Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.8288052Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-15 18:43:50 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-15T18:43:50.8297886Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-15 18:43:50 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-15T18:43:50.8306653Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.8317486Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-15 18:43:50 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-15T18:43:50.8327190Z [0;36m(Worker_DP0_TP3_EP3 pid=476)[0;0m INFO 02-15 18:43:50 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-15T18:43:50.8336285Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-15 18:43:50 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-15T18:43:50.8346166Z [0;36m(Worker_DP0_TP2_EP2 pid=372)[0;0m INFO 02-15 18:43:50 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-15T18:43:50.8367390Z [0;36m(Worker_DP1_TP1_EP9 pid=254)[0;0m INFO 02-15 18:43:50 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-15T18:43:50.8368350Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-15 18:43:50 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-15T18:43:50.8374666Z WARNING 02-15 18:43:50 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-15T18:43:50.8384065Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-15 18:43:50 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-15T18:43:50.8394005Z [0;36m(Worker_DP0_TP4_EP4 pid=580)[0;0m INFO 02-15 18:43:50 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-15T18:43:50.8404198Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-15 18:43:50 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-15T18:43:50.8831839Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m INFO 02-15 18:43:50 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-15T18:43:51.1892839Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m [2026-02-15 18:43:51] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-15T18:43:51.2084050Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m [2026-02-15 18:43:51] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-15T18:43:51.2130595Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m [2026-02-15 18:43:51] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-15T18:43:51.2190383Z [0;36m(Worker_DP0_TP3_EP3 pid=476)[0;0m [2026-02-15 18:43:51] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-15T18:43:51.2430192Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m [2026-02-15 18:43:51] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-15T18:43:51.2487634Z [0;36m(Worker_DP1_TP1_EP9 pid=254)[0;0m [2026-02-15 18:43:51] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-15T18:43:51.2671550Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m [2026-02-15 18:43:51] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-15T18:43:51.2796194Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m [2026-02-15 18:43:51] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-15T18:43:51.2827835Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m [2026-02-15 18:43:51] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-15T18:43:51.2860003Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m [2026-02-15 18:43:51] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-15T18:43:51.3961448Z [0;36m(Worker_DP1_TP1_EP9 pid=254)[0;0m INFO 02-15 18:43:51 [layer.py:475] [EP Rank 9/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-15T18:43:51.3970835Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-15 18:43:51 [layer.py:475] [EP Rank 7/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-15T18:43:51.3979986Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-15 18:43:51 [layer.py:475] [EP Rank 5/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-15T18:43:51.3989952Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-15 18:43:51 [layer.py:475] [EP Rank 14/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-15T18:43:51.3999920Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-15 18:43:51 [layer.py:475] [EP Rank 11/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-15T18:43:51.4010026Z [0;36m(Worker_DP0_TP3_EP3 pid=476)[0;0m INFO 02-15 18:43:51 [layer.py:475] [EP Rank 3/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-15T18:43:51.4019167Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-15 18:43:51 [layer.py:475] [EP Rank 15/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-15T18:43:51.4060667Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-15 18:43:51 [layer.py:475] [EP Rank 10/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-15T18:43:51.4090637Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-15 18:43:51 [layer.py:475] [EP Rank 6/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-15T18:43:51.4693192Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m INFO 02-15 18:43:51 [layer.py:475] [EP Rank 0/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-15T18:43:51.5475761Z [0;36m(Worker_DP0_TP1_EP1 pid=257)[0;0m [2026-02-15 18:43:51] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-15T18:43:51.5779899Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m [2026-02-15 18:43:51] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-15T18:43:51.5887880Z [0;36m(Worker_DP0_TP2_EP2 pid=372)[0;0m [2026-02-15 18:43:51] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-15T18:43:51.6306610Z [0;36m(Worker_DP0_TP4_EP4 pid=580)[0;0m [2026-02-15 18:43:51] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-15T18:43:51.6590050Z [0;36m(Worker_DP0_TP1_EP1 pid=257)[0;0m INFO 02-15 18:43:51 [layer.py:475] [EP Rank 1/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-15T18:43:51.6918620Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-15 18:43:51 [layer.py:475] [EP Rank 13/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-15T18:43:51.7075217Z [0;36m(Worker_DP0_TP2_EP2 pid=372)[0;0m INFO 02-15 18:43:51 [layer.py:475] [EP Rank 2/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-15T18:43:51.7374827Z [0;36m(Worker_DP0_TP4_EP4 pid=580)[0;0m INFO 02-15 18:43:51 [layer.py:475] [EP Rank 4/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-15T18:43:51.8882427Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m [2026-02-15 18:43:51] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-15T18:43:51.9817563Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m [2026-02-15 18:43:51] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-15T18:43:52.0498610Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-15 18:43:52 [layer.py:475] [EP Rank 8/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-15T18:43:52.0999727Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-15 18:43:52 [layer.py:475] [EP Rank 12/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-15T18:43:52.8225917Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-15T18:43:52.8234257Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m   return func(*args, **kwargs)
2026-02-15T18:43:52.8945569Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m INFO 02-15 18:43:52 [fused_moe.py:207] [EP Rank 0/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-15T18:43:52.8983751Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-15T18:43:52.8992283Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m   return func(*args, **kwargs)
2026-02-15T18:43:52.9553164Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-15 18:43:52 [fused_moe.py:207] [EP Rank 8/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-15T18:43:52.9843306Z [0;36m(Worker_DP0_TP1_EP1 pid=257)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-15T18:43:52.9851339Z [0;36m(Worker_DP0_TP1_EP1 pid=257)[0;0m   return func(*args, **kwargs)
2026-02-15T18:43:53.0099351Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-15T18:43:53.0107882Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m   return func(*args, **kwargs)
2026-02-15T18:43:53.0460380Z [0;36m(Worker_DP0_TP1_EP1 pid=257)[0;0m INFO 02-15 18:43:53 [fused_moe.py:207] [EP Rank 1/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-15T18:43:53.0567162Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-15T18:43:53.0575044Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m   return func(*args, **kwargs)
2026-02-15T18:43:53.0589174Z [0;36m(Worker_DP0_TP3_EP3 pid=476)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-15T18:43:53.0610493Z [0;36m(Worker_DP0_TP3_EP3 pid=476)[0;0m   return func(*args, **kwargs)
2026-02-15T18:43:53.0619283Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-15T18:43:53.0627938Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m   return func(*args, **kwargs)
2026-02-15T18:43:53.0640223Z [0;36m(Worker_DP1_TP1_EP9 pid=254)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-15T18:43:53.0648541Z [0;36m(Worker_DP1_TP1_EP9 pid=254)[0;0m   return func(*args, **kwargs)
2026-02-15T18:43:53.0660053Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-15T18:43:53.0669394Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m   return func(*args, **kwargs)
2026-02-15T18:43:53.0741823Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-15T18:43:53.0750437Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m   return func(*args, **kwargs)
2026-02-15T18:43:53.0761743Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-15 18:43:53 [fused_moe.py:207] [EP Rank 13/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-15T18:43:53.0789335Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-15T18:43:53.0799691Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m   return func(*args, **kwargs)
2026-02-15T18:43:53.1147290Z [0;36m(Worker_DP0_TP4_EP4 pid=580)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-15T18:43:53.1154764Z [0;36m(Worker_DP0_TP4_EP4 pid=580)[0;0m   return func(*args, **kwargs)
2026-02-15T18:43:53.1218610Z [0;36m(Worker_DP0_TP2_EP2 pid=372)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-15T18:43:53.1226728Z [0;36m(Worker_DP0_TP2_EP2 pid=372)[0;0m   return func(*args, **kwargs)
2026-02-15T18:43:53.1238583Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-15T18:43:53.1246405Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m   return func(*args, **kwargs)
2026-02-15T18:43:53.1257401Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-15T18:43:53.1268228Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m   return func(*args, **kwargs)
2026-02-15T18:43:53.1277684Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-15T18:43:53.1288327Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m   return func(*args, **kwargs)
2026-02-15T18:43:53.1297776Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-15 18:43:53 [fused_moe.py:207] [EP Rank 7/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-15T18:43:53.1307740Z [0;36m(Worker_DP1_TP1_EP9 pid=254)[0;0m INFO 02-15 18:43:53 [fused_moe.py:207] [EP Rank 9/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-15T18:43:53.1318161Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-15 18:43:53 [fused_moe.py:207] [EP Rank 15/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-15T18:43:53.1327489Z [0;36m(Worker_DP0_TP3_EP3 pid=476)[0;0m INFO 02-15 18:43:53 [fused_moe.py:207] [EP Rank 3/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-15T18:43:53.1336890Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-15 18:43:53 [fused_moe.py:207] [EP Rank 5/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-15T18:43:53.1347679Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-15 18:43:53 [fused_moe.py:207] [EP Rank 10/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-15T18:43:53.1391275Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-15 18:43:53 [fused_moe.py:207] [EP Rank 12/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-15T18:43:53.1747692Z [0;36m(Worker_DP0_TP4_EP4 pid=580)[0;0m INFO 02-15 18:43:53 [fused_moe.py:207] [EP Rank 4/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-15T18:43:53.1798604Z [0;36m(Worker_DP0_TP2_EP2 pid=372)[0;0m INFO 02-15 18:43:53 [fused_moe.py:207] [EP Rank 2/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-15T18:43:53.1816116Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-15 18:43:53 [fused_moe.py:207] [EP Rank 14/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-15T18:43:53.1836188Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-15 18:43:53 [fused_moe.py:207] [EP Rank 6/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-15T18:43:53.1845704Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-15 18:43:53 [fused_moe.py:207] [EP Rank 11/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-15T18:43:53.3794041Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-15 18:43:53 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-15T18:43:53.4255678Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-15 18:43:53 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-15T18:43:53.4275652Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-15 18:43:53 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-15T18:43:53.4284744Z [0;36m(Worker_DP1_TP1_EP9 pid=254)[0;0m INFO 02-15 18:43:53 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-15T18:43:53.4297279Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-15 18:43:53 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-15T18:43:53.4307119Z [0;36m(Worker_DP0_TP3_EP3 pid=476)[0;0m INFO 02-15 18:43:53 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-15T18:43:53.4438876Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-15 18:43:53 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-15T18:43:53.4503384Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-15 18:43:53 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-15T18:43:53.4962286Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-15 18:43:53 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-15T18:43:53.4975901Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-15 18:43:53 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-15T18:43:53.5027701Z [0;36m(Worker_DP0_TP4_EP4 pid=580)[0;0m INFO 02-15 18:43:53 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-15T18:43:53.5091168Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-15 18:43:53 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-15T18:43:53.5117917Z [0;36m(Worker_DP0_TP2_EP2 pid=372)[0;0m INFO 02-15 18:43:53 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-15T18:43:53.5146253Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-15 18:43:53 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-15T18:43:53.5406230Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m INFO 02-15 18:43:53 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-15T18:43:53.6104359Z [0;36m(Worker_DP0_TP1_EP1 pid=257)[0;0m INFO 02-15 18:43:53 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-15T18:43:56.8017127Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-15 18:43:56 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-15T18:43:56.8053597Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-15 18:43:56 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-15T18:43:56.8276004Z [0;36m(Worker_DP0_TP3_EP3 pid=476)[0;0m INFO 02-15 18:43:56 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-15T18:43:56.8374346Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-15 18:43:56 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-15T18:43:56.8508272Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-15 18:43:56 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-15T18:43:56.8599329Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-15 18:43:56 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-15T18:43:56.8650436Z [0;36m(Worker_DP0_TP4_EP4 pid=580)[0;0m INFO 02-15 18:43:56 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-15T18:43:56.8773443Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-15 18:43:56 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-15T18:43:56.8993045Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-15 18:43:56 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-15T18:43:56.9069637Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-15 18:43:56 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-15T18:43:56.9198918Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-15 18:43:56 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-15T18:43:56.9457280Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m INFO 02-15 18:43:56 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-15T18:43:57.0037768Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:43:57.0038291Z Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
2026-02-15T18:43:57.0109787Z [0;36m(Worker_DP1_TP1_EP9 pid=254)[0;0m INFO 02-15 18:43:57 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-15T18:43:57.0939052Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-15 18:43:57 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-15T18:43:57.1037454Z [0;36m(Worker_DP0_TP2_EP2 pid=372)[0;0m INFO 02-15 18:43:57 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-15T18:43:57.1232798Z [0;36m(Worker_DP0_TP1_EP1 pid=257)[0;0m INFO 02-15 18:43:57 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-15T18:43:58.2181844Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:43:58.2182352Z Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:01<03:16,  1.21s/it]
2026-02-15T18:44:01.7133860Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:01.7134227Z Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:04<06:51,  2.56s/it]
2026-02-15T18:44:02.8656469Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:02.8656994Z Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:05<05:06,  1.91s/it]
2026-02-15T18:44:04.2303460Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:04.2303813Z Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:07<04:29,  1.70s/it]
2026-02-15T18:44:06.2625653Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:06.2626263Z Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:09<04:47,  1.82s/it]
2026-02-15T18:44:09.1596853Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:09.1597240Z Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:12<05:42,  2.18s/it]
2026-02-15T18:44:11.4170427Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:11.4170810Z Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:14<05:44,  2.21s/it]
2026-02-15T18:44:13.3378301Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:13.3378652Z Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:16<05:28,  2.12s/it]
2026-02-15T18:44:14.9435120Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:14.9435751Z Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:17<05:01,  1.96s/it]
2026-02-15T18:44:16.8649271Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:16.8649647Z Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:19<04:57,  1.95s/it]
2026-02-15T18:44:18.5520006Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:18.5520382Z Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:21<04:43,  1.87s/it]
2026-02-15T18:44:20.8493211Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:20.8493595Z Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:23<05:01,  2.00s/it]
2026-02-15T18:44:22.8611538Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:22.8611911Z Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:25<05:00,  2.00s/it]
2026-02-15T18:44:24.9753563Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:24.9754055Z Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:27<05:03,  2.04s/it]
2026-02-15T18:44:28.6200965Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:28.6201335Z Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:31<06:13,  2.52s/it]
2026-02-15T18:44:30.6606285Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:30.6606689Z Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:33<05:49,  2.38s/it]
2026-02-15T18:44:32.8146696Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:32.8147062Z Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:35<05:37,  2.31s/it]
2026-02-15T18:44:34.9570892Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:34.9571245Z Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:37<05:27,  2.26s/it]
2026-02-15T18:44:36.0708442Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:36.0708839Z Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:39<04:35,  1.92s/it]
2026-02-15T18:44:37.9190266Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:37.9190667Z Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:40<04:30,  1.90s/it]
2026-02-15T18:44:40.6317961Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:40.6318418Z Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:43<05:03,  2.14s/it]
2026-02-15T18:44:42.2537273Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:42.2537714Z Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:45<04:39,  1.98s/it]
2026-02-15T18:44:44.1450527Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:44.1450969Z Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:47<04:33,  1.96s/it]
2026-02-15T18:44:45.9998594Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:45.9998977Z Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:48<04:27,  1.93s/it]
2026-02-15T18:44:48.9259941Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:48.9260445Z Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:51<05:07,  2.23s/it]
2026-02-15T18:44:49.4496043Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:49.4496409Z Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:52<03:55,  1.72s/it]
2026-02-15T18:44:50.5255810Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:50.5256161Z Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:53<03:27,  1.52s/it]
2026-02-15T18:44:53.3751049Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:53.3751410Z Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:56<04:19,  1.92s/it]
2026-02-15T18:44:54.6967026Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:54.6967387Z Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:57<03:53,  1.74s/it]
2026-02-15T18:44:55.9745251Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:55.9745604Z Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:58<03:33,  1.60s/it]
2026-02-15T18:44:57.1408152Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:57.1408527Z Loading safetensors checkpoint shards:  19% Completed | 31/163 [01:00<03:14,  1.47s/it]
2026-02-15T18:44:58.8534608Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:44:58.8535001Z Loading safetensors checkpoint shards:  20% Completed | 32/163 [01:01<03:22,  1.54s/it]
2026-02-15T18:45:00.8526160Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:00.8526530Z Loading safetensors checkpoint shards:  20% Completed | 33/163 [01:03<03:38,  1.68s/it]
2026-02-15T18:45:03.0327283Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:03.0327652Z Loading safetensors checkpoint shards:  21% Completed | 34/163 [01:06<03:56,  1.83s/it]
2026-02-15T18:45:04.9162386Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:04.9162759Z Loading safetensors checkpoint shards:  21% Completed | 35/163 [01:07<03:56,  1.85s/it]
2026-02-15T18:45:09.9365607Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:09.9366006Z Loading safetensors checkpoint shards:  22% Completed | 36/163 [01:12<05:55,  2.80s/it]
2026-02-15T18:45:11.0230536Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:11.0230899Z Loading safetensors checkpoint shards:  23% Completed | 37/163 [01:14<04:47,  2.28s/it]
2026-02-15T18:45:12.7169662Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:12.7170026Z Loading safetensors checkpoint shards:  23% Completed | 38/163 [01:15<04:23,  2.11s/it]
2026-02-15T18:45:14.7081814Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:14.7082282Z Loading safetensors checkpoint shards:  24% Completed | 39/163 [01:17<04:17,  2.07s/it]
2026-02-15T18:45:19.2167617Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:19.2168012Z Loading safetensors checkpoint shards:  25% Completed | 40/163 [01:22<05:44,  2.80s/it]
2026-02-15T18:45:20.0095435Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:20.0095812Z Loading safetensors checkpoint shards:  25% Completed | 41/163 [01:23<04:28,  2.20s/it]
2026-02-15T18:45:21.0026883Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:21.0027240Z Loading safetensors checkpoint shards:  26% Completed | 42/163 [01:23<03:42,  1.84s/it]
2026-02-15T18:45:24.9718004Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:24.9718409Z Loading safetensors checkpoint shards:  26% Completed | 43/163 [01:27<04:57,  2.48s/it]
2026-02-15T18:45:26.2756986Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:26.2757348Z Loading safetensors checkpoint shards:  27% Completed | 44/163 [01:29<04:12,  2.13s/it]
2026-02-15T18:45:27.9530717Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:27.9531139Z Loading safetensors checkpoint shards:  28% Completed | 45/163 [01:30<03:54,  1.99s/it]
2026-02-15T18:45:29.5193466Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:29.5193831Z Loading safetensors checkpoint shards:  28% Completed | 46/163 [01:32<03:38,  1.86s/it]
2026-02-15T18:45:36.8437723Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:36.8438149Z Loading safetensors checkpoint shards:  29% Completed | 47/163 [01:39<06:46,  3.50s/it]
2026-02-15T18:45:39.5309839Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:39.5310210Z Loading safetensors checkpoint shards:  29% Completed | 48/163 [01:42<06:14,  3.26s/it]
2026-02-15T18:45:39.8276184Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:39.8276531Z Loading safetensors checkpoint shards:  30% Completed | 49/163 [01:42<04:30,  2.37s/it]
2026-02-15T18:45:41.8330919Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:41.8331296Z Loading safetensors checkpoint shards:  31% Completed | 50/163 [01:44<04:15,  2.26s/it]
2026-02-15T18:45:44.5162975Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:44.5163354Z Loading safetensors checkpoint shards:  31% Completed | 51/163 [01:47<04:27,  2.39s/it]
2026-02-15T18:45:45.2668564Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:45.2669276Z Loading safetensors checkpoint shards:  32% Completed | 52/163 [01:48<03:30,  1.90s/it]
2026-02-15T18:45:49.1932645Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:49.1933020Z Loading safetensors checkpoint shards:  33% Completed | 53/163 [01:52<04:35,  2.51s/it]
2026-02-15T18:45:50.2443980Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:50.2444386Z Loading safetensors checkpoint shards:  33% Completed | 54/163 [01:53<03:45,  2.07s/it]
2026-02-15T18:45:51.3636269Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:51.3636619Z Loading safetensors checkpoint shards:  34% Completed | 55/163 [01:54<03:12,  1.78s/it]
2026-02-15T18:45:52.9790678Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:52.9791035Z Loading safetensors checkpoint shards:  34% Completed | 56/163 [01:55<03:05,  1.73s/it]
2026-02-15T18:45:54.8629734Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:54.8630097Z Loading safetensors checkpoint shards:  35% Completed | 57/163 [01:57<03:08,  1.78s/it]
2026-02-15T18:45:56.7938297Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:45:56.7938771Z Loading safetensors checkpoint shards:  36% Completed | 58/163 [01:59<03:11,  1.82s/it]
2026-02-15T18:46:01.4222474Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:01.4222880Z Loading safetensors checkpoint shards:  36% Completed | 59/163 [02:04<04:37,  2.67s/it]
2026-02-15T18:46:02.1159188Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:02.1159581Z Loading safetensors checkpoint shards:  37% Completed | 60/163 [02:05<03:33,  2.07s/it]
2026-02-15T18:46:02.8398795Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:02.8399147Z Loading safetensors checkpoint shards:  37% Completed | 61/163 [02:05<02:50,  1.67s/it]
2026-02-15T18:46:05.9979753Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:05.9980124Z Loading safetensors checkpoint shards:  38% Completed | 62/163 [02:08<03:33,  2.12s/it]
2026-02-15T18:46:07.2772980Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:07.2773405Z Loading safetensors checkpoint shards:  39% Completed | 63/163 [02:10<03:06,  1.86s/it]
2026-02-15T18:46:09.0817632Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:09.0817989Z Loading safetensors checkpoint shards:  39% Completed | 64/163 [02:12<03:02,  1.85s/it]
2026-02-15T18:46:13.4429215Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:13.4429594Z Loading safetensors checkpoint shards:  40% Completed | 65/163 [02:16<04:14,  2.60s/it]
2026-02-15T18:46:15.8107788Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:15.8108158Z Loading safetensors checkpoint shards:  40% Completed | 66/163 [02:18<04:05,  2.53s/it]
2026-02-15T18:46:16.9980662Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:16.9981039Z Loading safetensors checkpoint shards:  41% Completed | 67/163 [02:19<03:24,  2.13s/it]
2026-02-15T18:46:19.9117167Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:19.9117518Z Loading safetensors checkpoint shards:  42% Completed | 68/163 [02:22<03:44,  2.36s/it]
2026-02-15T18:46:20.8339922Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:20.8340290Z Loading safetensors checkpoint shards:  42% Completed | 69/163 [02:23<03:01,  1.93s/it]
2026-02-15T18:46:21.8767418Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:21.8768069Z Loading safetensors checkpoint shards:  43% Completed | 70/163 [02:24<02:34,  1.67s/it]
2026-02-15T18:46:23.3717386Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:23.3717800Z Loading safetensors checkpoint shards:  44% Completed | 71/163 [02:26<02:28,  1.61s/it]
2026-02-15T18:46:26.1081060Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:26.1081430Z Loading safetensors checkpoint shards:  44% Completed | 72/163 [02:29<02:57,  1.95s/it]
2026-02-15T18:46:26.7858976Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:26.7859318Z Loading safetensors checkpoint shards:  45% Completed | 73/163 [02:29<02:21,  1.57s/it]
2026-02-15T18:46:28.1079431Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:28.1080185Z Loading safetensors checkpoint shards:  45% Completed | 74/163 [02:31<02:13,  1.50s/it]
2026-02-15T18:46:29.8032387Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:29.8032742Z Loading safetensors checkpoint shards:  46% Completed | 75/163 [02:32<02:16,  1.55s/it]
2026-02-15T18:46:30.9313465Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:30.9313832Z Loading safetensors checkpoint shards:  47% Completed | 76/163 [02:33<02:04,  1.43s/it]
2026-02-15T18:46:33.6736719Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:33.6737105Z Loading safetensors checkpoint shards:  47% Completed | 77/163 [02:36<02:36,  1.82s/it]
2026-02-15T18:46:34.7504926Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:34.7505290Z Loading safetensors checkpoint shards:  48% Completed | 78/163 [02:37<02:15,  1.60s/it]
2026-02-15T18:46:35.7796116Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:35.7796472Z Loading safetensors checkpoint shards:  48% Completed | 79/163 [02:38<01:59,  1.43s/it]
2026-02-15T18:46:37.9414202Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:37.9414569Z Loading safetensors checkpoint shards:  49% Completed | 80/163 [02:40<02:16,  1.65s/it]
2026-02-15T18:46:41.1819030Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:42.0267257Z Loading safetensors checkpoint shards:  50% Completed | 81/163 [02:44<02:54,  2.13s/it]
2026-02-15T18:46:42.0267790Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:42.0268093Z Loading safetensors checkpoint shards:  50% Completed | 82/163 [02:45<02:21,  1.74s/it]
2026-02-15T18:46:43.5666988Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:43.5667344Z Loading safetensors checkpoint shards:  51% Completed | 83/163 [02:46<02:14,  1.68s/it]
2026-02-15T18:46:44.9414054Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:44.9414414Z Loading safetensors checkpoint shards:  52% Completed | 84/163 [02:47<02:05,  1.59s/it]
2026-02-15T18:46:47.1108091Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:47.1108475Z Loading safetensors checkpoint shards:  52% Completed | 85/163 [02:50<02:17,  1.76s/it]
2026-02-15T18:46:48.2038947Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:48.2039323Z Loading safetensors checkpoint shards:  53% Completed | 86/163 [02:51<02:00,  1.56s/it]
2026-02-15T18:46:49.6579612Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:49.6579962Z Loading safetensors checkpoint shards:  53% Completed | 87/163 [02:52<01:56,  1.53s/it]
2026-02-15T18:46:53.1007210Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:53.1007564Z Loading safetensors checkpoint shards:  54% Completed | 88/163 [02:56<02:37,  2.10s/it]
2026-02-15T18:46:54.6716399Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:54.6716772Z Loading safetensors checkpoint shards:  55% Completed | 89/163 [02:57<02:23,  1.94s/it]
2026-02-15T18:46:58.2790474Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:46:58.2790862Z Loading safetensors checkpoint shards:  55% Completed | 90/163 [03:01<02:58,  2.44s/it]
2026-02-15T18:47:00.0230954Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:00.0231307Z Loading safetensors checkpoint shards:  56% Completed | 91/163 [03:03<02:40,  2.23s/it]
2026-02-15T18:47:04.1516459Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:04.1516930Z Loading safetensors checkpoint shards:  56% Completed | 92/163 [03:07<03:18,  2.80s/it]
2026-02-15T18:47:04.8130836Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:04.8131198Z Loading safetensors checkpoint shards:  57% Completed | 93/163 [03:07<02:31,  2.16s/it]
2026-02-15T18:47:05.6578799Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:05.6579156Z Loading safetensors checkpoint shards:  58% Completed | 94/163 [03:08<02:01,  1.76s/it]
2026-02-15T18:47:07.4433290Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:07.4433656Z Loading safetensors checkpoint shards:  58% Completed | 95/163 [03:10<02:00,  1.77s/it]
2026-02-15T18:47:10.1095527Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:10.1095904Z Loading safetensors checkpoint shards:  59% Completed | 96/163 [03:13<02:16,  2.04s/it]
2026-02-15T18:47:11.4761635Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:11.4762121Z Loading safetensors checkpoint shards:  60% Completed | 97/163 [03:14<02:01,  1.84s/it]
2026-02-15T18:47:12.8565558Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:12.8565913Z Loading safetensors checkpoint shards:  60% Completed | 98/163 [03:15<01:50,  1.70s/it]
2026-02-15T18:47:14.5666126Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:14.5666520Z Loading safetensors checkpoint shards:  61% Completed | 99/163 [03:17<01:49,  1.70s/it]
2026-02-15T18:47:16.2342647Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:16.2343018Z Loading safetensors checkpoint shards:  61% Completed | 100/163 [03:19<01:46,  1.69s/it]
2026-02-15T18:47:17.3547964Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:17.3548337Z Loading safetensors checkpoint shards:  62% Completed | 101/163 [03:20<01:34,  1.52s/it]
2026-02-15T18:47:20.3825443Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:20.3825811Z Loading safetensors checkpoint shards:  63% Completed | 102/163 [03:23<02:00,  1.97s/it]
2026-02-15T18:47:22.6816280Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:22.6816650Z Loading safetensors checkpoint shards:  63% Completed | 103/163 [03:25<02:04,  2.07s/it]
2026-02-15T18:47:25.5118062Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:25.5118428Z Loading safetensors checkpoint shards:  64% Completed | 104/163 [03:28<02:15,  2.30s/it]
2026-02-15T18:47:26.6015242Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:26.6015608Z Loading safetensors checkpoint shards:  64% Completed | 105/163 [03:29<01:52,  1.94s/it]
2026-02-15T18:47:29.6206054Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:29.6206422Z Loading safetensors checkpoint shards:  65% Completed | 106/163 [03:32<02:08,  2.26s/it]
2026-02-15T18:47:30.4318639Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:30.4318989Z Loading safetensors checkpoint shards:  66% Completed | 107/163 [03:33<01:42,  1.83s/it]
2026-02-15T18:47:33.1561323Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:33.1561709Z Loading safetensors checkpoint shards:  66% Completed | 108/163 [03:36<01:55,  2.10s/it]
2026-02-15T18:47:33.9171726Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:33.9172160Z Loading safetensors checkpoint shards:  67% Completed | 109/163 [03:36<01:31,  1.70s/it]
2026-02-15T18:47:36.3454544Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:36.3454910Z Loading safetensors checkpoint shards:  67% Completed | 110/163 [03:39<01:41,  1.91s/it]
2026-02-15T18:47:39.6573721Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:39.6574115Z Loading safetensors checkpoint shards:  68% Completed | 111/163 [03:42<02:01,  2.33s/it]
2026-02-15T18:47:41.4166622Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:41.4167089Z Loading safetensors checkpoint shards:  69% Completed | 112/163 [03:44<01:50,  2.16s/it]
2026-02-15T18:47:43.3647084Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:43.3647796Z Loading safetensors checkpoint shards:  69% Completed | 113/163 [03:46<01:44,  2.10s/it]
2026-02-15T18:47:44.4403353Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:44.4403717Z Loading safetensors checkpoint shards:  70% Completed | 114/163 [03:47<01:27,  1.79s/it]
2026-02-15T18:47:45.8040596Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:45.8040963Z Loading safetensors checkpoint shards:  71% Completed | 115/163 [03:48<01:19,  1.66s/it]
2026-02-15T18:47:49.0672166Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:49.0672548Z Loading safetensors checkpoint shards:  71% Completed | 116/163 [03:52<01:40,  2.14s/it]
2026-02-15T18:47:50.6748084Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:50.6748764Z Loading safetensors checkpoint shards:  72% Completed | 117/163 [03:53<01:31,  1.98s/it]
2026-02-15T18:47:53.7517601Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:53.7517961Z Loading safetensors checkpoint shards:  72% Completed | 118/163 [03:56<01:43,  2.31s/it]
2026-02-15T18:47:55.1361371Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:55.1361755Z Loading safetensors checkpoint shards:  73% Completed | 119/163 [03:58<01:29,  2.03s/it]
2026-02-15T18:47:56.9251373Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:56.9251765Z Loading safetensors checkpoint shards:  74% Completed | 120/163 [03:59<01:24,  1.96s/it]
2026-02-15T18:47:58.7709460Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:47:58.7709854Z Loading safetensors checkpoint shards:  74% Completed | 121/163 [04:01<01:20,  1.93s/it]
2026-02-15T18:48:00.0904158Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:00.0904591Z Loading safetensors checkpoint shards:  75% Completed | 122/163 [04:03<01:11,  1.74s/it]
2026-02-15T18:48:02.4738280Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:02.4738653Z Loading safetensors checkpoint shards:  75% Completed | 123/163 [04:05<01:17,  1.94s/it]
2026-02-15T18:48:03.5664942Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:03.5665308Z Loading safetensors checkpoint shards:  76% Completed | 124/163 [04:06<01:05,  1.68s/it]
2026-02-15T18:48:06.4834388Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:06.4834757Z Loading safetensors checkpoint shards:  77% Completed | 125/163 [04:09<01:18,  2.05s/it]
2026-02-15T18:48:07.7384173Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:07.7384538Z Loading safetensors checkpoint shards:  77% Completed | 126/163 [04:10<01:07,  1.81s/it]
2026-02-15T18:48:09.1280895Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:09.1281278Z Loading safetensors checkpoint shards:  78% Completed | 127/163 [04:12<01:00,  1.69s/it]
2026-02-15T18:48:11.8443944Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:11.8444336Z Loading safetensors checkpoint shards:  79% Completed | 128/163 [04:14<01:09,  2.00s/it]
2026-02-15T18:48:14.5699127Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:14.5699491Z Loading safetensors checkpoint shards:  79% Completed | 129/163 [04:17<01:15,  2.21s/it]
2026-02-15T18:48:15.5263901Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:15.5264273Z Loading safetensors checkpoint shards:  80% Completed | 130/163 [04:18<01:00,  1.84s/it]
2026-02-15T18:48:16.7533740Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:16.7534107Z Loading safetensors checkpoint shards:  80% Completed | 131/163 [04:19<00:52,  1.65s/it]
2026-02-15T18:48:18.3468176Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:18.3468544Z Loading safetensors checkpoint shards:  81% Completed | 132/163 [04:21<00:50,  1.64s/it]
2026-02-15T18:48:20.7133139Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:20.7133522Z Loading safetensors checkpoint shards:  82% Completed | 133/163 [04:23<00:55,  1.85s/it]
2026-02-15T18:48:22.4413080Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:22.4413439Z Loading safetensors checkpoint shards:  82% Completed | 134/163 [04:25<00:52,  1.82s/it]
2026-02-15T18:48:26.3741592Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:26.3741970Z Loading safetensors checkpoint shards:  83% Completed | 135/163 [04:29<01:08,  2.45s/it]
2026-02-15T18:48:27.7175114Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:27.7175483Z Loading safetensors checkpoint shards:  83% Completed | 136/163 [04:30<00:57,  2.12s/it]
2026-02-15T18:48:29.4457572Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:29.4457935Z Loading safetensors checkpoint shards:  84% Completed | 137/163 [04:32<00:52,  2.00s/it]
2026-02-15T18:48:33.5794700Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:33.5795120Z Loading safetensors checkpoint shards:  85% Completed | 138/163 [04:36<01:06,  2.64s/it]
2026-02-15T18:48:35.9775374Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:35.9776127Z Loading safetensors checkpoint shards:  85% Completed | 139/163 [04:38<01:01,  2.57s/it]
2026-02-15T18:48:36.6856122Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:36.6856621Z Loading safetensors checkpoint shards:  86% Completed | 140/163 [04:39<00:46,  2.01s/it]
2026-02-15T18:48:38.3089243Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:38.3089743Z Loading safetensors checkpoint shards:  87% Completed | 141/163 [04:41<00:41,  1.89s/it]
2026-02-15T18:48:39.9137380Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:39.9137895Z Loading safetensors checkpoint shards:  87% Completed | 142/163 [04:42<00:37,  1.81s/it]
2026-02-15T18:48:42.4587999Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:42.4588453Z Loading safetensors checkpoint shards:  88% Completed | 143/163 [04:45<00:40,  2.03s/it]
2026-02-15T18:48:45.8109733Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:45.8110159Z Loading safetensors checkpoint shards:  88% Completed | 144/163 [04:48<00:46,  2.43s/it]
2026-02-15T18:48:47.8668241Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:47.8668691Z Loading safetensors checkpoint shards:  89% Completed | 145/163 [04:50<00:41,  2.31s/it]
2026-02-15T18:48:50.0384636Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:50.0385051Z Loading safetensors checkpoint shards:  90% Completed | 146/163 [04:53<00:38,  2.27s/it]
2026-02-15T18:48:51.0698734Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:51.0699176Z Loading safetensors checkpoint shards:  90% Completed | 147/163 [04:54<00:30,  1.90s/it]
2026-02-15T18:48:52.1883898Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:52.1884379Z Loading safetensors checkpoint shards:  91% Completed | 148/163 [04:55<00:24,  1.67s/it]
2026-02-15T18:48:53.2356346Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:53.2356846Z Loading safetensors checkpoint shards:  91% Completed | 149/163 [04:56<00:20,  1.48s/it]
2026-02-15T18:48:56.4084945Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:56.4085455Z Loading safetensors checkpoint shards:  93% Completed | 151/163 [04:59<00:18,  1.53s/it]
2026-02-15T18:48:57.7477169Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:57.7477726Z Loading safetensors checkpoint shards:  93% Completed | 152/163 [05:00<00:16,  1.48s/it]
2026-02-15T18:48:59.2214679Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:48:59.2215170Z Loading safetensors checkpoint shards:  94% Completed | 153/163 [05:02<00:14,  1.48s/it]
2026-02-15T18:49:00.9845356Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:49:00.9845877Z Loading safetensors checkpoint shards:  94% Completed | 154/163 [05:03<00:14,  1.56s/it]
2026-02-15T18:49:02.8232352Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:49:02.8232844Z Loading safetensors checkpoint shards:  95% Completed | 155/163 [05:05<00:13,  1.64s/it]
2026-02-15T18:49:04.3448816Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:49:04.3449364Z Loading safetensors checkpoint shards:  96% Completed | 156/163 [05:07<00:11,  1.60s/it]
2026-02-15T18:49:05.9181448Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:49:05.9181897Z Loading safetensors checkpoint shards:  96% Completed | 157/163 [05:08<00:09,  1.59s/it]
2026-02-15T18:49:09.6462644Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:49:09.6463055Z Loading safetensors checkpoint shards:  97% Completed | 158/163 [05:12<00:11,  2.22s/it]
2026-02-15T18:49:10.9959329Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:49:10.9959802Z Loading safetensors checkpoint shards:  98% Completed | 159/163 [05:13<00:07,  1.96s/it]
2026-02-15T18:49:12.7775598Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:49:12.7776116Z Loading safetensors checkpoint shards:  98% Completed | 160/163 [05:15<00:05,  1.91s/it]
2026-02-15T18:49:14.0841690Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:49:14.0842579Z Loading safetensors checkpoint shards:  99% Completed | 161/163 [05:17<00:03,  1.73s/it]
2026-02-15T18:49:15.9188403Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:49:15.9188897Z Loading safetensors checkpoint shards:  99% Completed | 162/163 [05:18<00:01,  1.76s/it]
2026-02-15T18:49:17.3971647Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:49:17.3972221Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [05:20<00:00,  1.68s/it]
2026-02-15T18:49:17.3995340Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:49:17.3995726Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [05:20<00:00,  1.97s/it]
2026-02-15T18:49:17.4004658Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:49:17.4144616Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m INFO 02-15 18:49:17 [default_loader.py:291] Loading weights took 320.41 seconds
2026-02-15T18:49:17.7346698Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-15 18:49:17 [default_loader.py:291] Loading weights took 320.58 seconds
2026-02-15T18:49:30.6668672Z INFO 02-15 18:49:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:49:30.6678010Z INFO 02-15 18:49:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:49:30.6689225Z INFO 02-15 18:49:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:49:30.6700401Z INFO 02-15 18:49:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:49:30.6710692Z INFO 02-15 18:49:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:49:30.6721392Z INFO 02-15 18:49:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:49:30.6732208Z INFO 02-15 18:49:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:49:30.6741554Z INFO 02-15 18:49:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:49:30.6751121Z INFO 02-15 18:49:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:49:30.6765989Z INFO 02-15 18:49:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:49:30.6778345Z INFO 02-15 18:49:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:49:30.6787436Z INFO 02-15 18:49:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:49:30.6814393Z INFO 02-15 18:49:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:49:30.6824239Z INFO 02-15 18:49:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:49:30.6834412Z INFO 02-15 18:49:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:49:30.6845045Z INFO 02-15 18:49:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:49:30.7091122Z INFO 02-15 18:49:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:49:30.7104781Z INFO 02-15 18:49:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:49:30.7116648Z INFO 02-15 18:49:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:49:30.7131766Z INFO 02-15 18:49:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:49:30.7138576Z INFO 02-15 18:49:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:49:30.7148879Z INFO 02-15 18:49:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:49:30.7160417Z INFO 02-15 18:49:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:49:30.7169980Z INFO 02-15 18:49:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:49:30.7179802Z INFO 02-15 18:49:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:49:30.7193144Z INFO 02-15 18:49:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:49:30.7204761Z INFO 02-15 18:49:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:49:30.7215576Z INFO 02-15 18:49:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:49:30.7225145Z INFO 02-15 18:49:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:49:30.7234453Z INFO 02-15 18:49:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:49:30.7245098Z INFO 02-15 18:49:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:49:30.7254991Z INFO 02-15 18:49:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:49:30.7264778Z INFO 02-15 18:49:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:49:30.7274253Z INFO 02-15 18:49:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:49:30.7284665Z INFO 02-15 18:49:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:49:30.7294887Z INFO 02-15 18:49:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:49:30.7304753Z INFO 02-15 18:49:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:49:30.7315916Z INFO 02-15 18:49:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:49:30.7325620Z INFO 02-15 18:49:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:49:30.7334852Z INFO 02-15 18:49:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:49:30.7344885Z INFO 02-15 18:49:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:49:30.7355254Z INFO 02-15 18:49:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:49:30.7364157Z INFO 02-15 18:49:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:49:30.7374222Z INFO 02-15 18:49:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:49:30.8054025Z INFO 02-15 18:49:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:49:30.8063738Z INFO 02-15 18:49:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:49:30.8074069Z INFO 02-15 18:49:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:49:30.8084396Z INFO 02-15 18:49:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:49:30.8093004Z INFO 02-15 18:49:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:49:30.8102762Z INFO 02-15 18:49:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:49:30.8111430Z INFO 02-15 18:49:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:49:30.8166305Z INFO 02-15 18:49:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:49:30.8166849Z INFO 02-15 18:49:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:49:30.8196724Z INFO 02-15 18:49:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:49:30.8209717Z INFO 02-15 18:49:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:49:30.8218400Z INFO 02-15 18:49:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:49:30.8915424Z INFO 02-15 18:49:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:49:30.8925092Z INFO 02-15 18:49:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-15T18:49:30.8935856Z INFO 02-15 18:49:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:49:30.8950531Z INFO 02-15 18:49:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-15T18:49:30.8964678Z INFO 02-15 18:49:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:49:30.8973216Z INFO 02-15 18:49:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-15T18:49:30.8994899Z INFO 02-15 18:49:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:49:30.9007784Z INFO 02-15 18:49:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-15T18:49:53.9630455Z [0;36m(Worker_DP0_TP1_EP1 pid=257)[0;0m WARNING 02-15 18:49:53 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-15T18:49:53.9640369Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m WARNING 02-15 18:49:53 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-15T18:49:53.9748058Z [0;36m(Worker_DP0_TP2_EP2 pid=372)[0;0m WARNING 02-15 18:49:53 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-15T18:49:53.9772158Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m WARNING 02-15 18:49:53 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-15T18:49:53.9782431Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m WARNING 02-15 18:49:53 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-15T18:49:53.9864178Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m WARNING 02-15 18:49:53 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-15T18:49:53.9885884Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m WARNING 02-15 18:49:53 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-15T18:49:53.9909412Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m WARNING 02-15 18:49:53 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-15T18:49:53.9937994Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m WARNING 02-15 18:49:53 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-15T18:49:53.9963783Z [0;36m(Worker_DP1_TP1_EP9 pid=254)[0;0m WARNING 02-15 18:49:53 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-15T18:49:54.0045008Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m WARNING 02-15 18:49:53 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-15T18:49:54.0120556Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m WARNING 02-15 18:49:54 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-15T18:49:54.0129421Z [0;36m(Worker_DP0_TP3_EP3 pid=476)[0;0m WARNING 02-15 18:49:54 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-15T18:49:54.0217609Z [0;36m(Worker_DP0_TP1_EP1 pid=257)[0;0m INFO 02-15 18:49:54 [model_runner_v1.py:2315] Loading drafter model...
2026-02-15T18:49:54.0253713Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-15 18:49:54 [model_runner_v1.py:2315] Loading drafter model...
2026-02-15T18:49:54.0293602Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m WARNING 02-15 18:49:54 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-15T18:49:54.0325254Z [0;36m(Worker_DP0_TP2_EP2 pid=372)[0;0m INFO 02-15 18:49:54 [model_runner_v1.py:2315] Loading drafter model...
2026-02-15T18:49:54.0350994Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-15 18:49:54 [model_runner_v1.py:2315] Loading drafter model...
2026-02-15T18:49:54.0374188Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-15 18:49:54 [model_runner_v1.py:2315] Loading drafter model...
2026-02-15T18:49:54.0437190Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m INFO 02-15 18:49:54 [model_runner_v1.py:2315] Loading drafter model...
2026-02-15T18:49:54.0474146Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-15 18:49:54 [model_runner_v1.py:2315] Loading drafter model...
2026-02-15T18:49:54.0558743Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-15 18:49:54 [model_runner_v1.py:2315] Loading drafter model...
2026-02-15T18:49:54.0569440Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-15 18:49:54 [model_runner_v1.py:2315] Loading drafter model...
2026-02-15T18:49:54.0580798Z [0;36m(Worker_DP1_TP1_EP9 pid=254)[0;0m INFO 02-15 18:49:54 [model_runner_v1.py:2315] Loading drafter model...
2026-02-15T18:49:54.0616152Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-15 18:49:54 [model_runner_v1.py:2315] Loading drafter model...
2026-02-15T18:49:54.0698144Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-15 18:49:54 [model_runner_v1.py:2315] Loading drafter model...
2026-02-15T18:49:54.0762192Z [0;36m(Worker_DP0_TP4_EP4 pid=580)[0;0m WARNING 02-15 18:49:54 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-15T18:49:54.0865131Z [0;36m(Worker_DP0_TP3_EP3 pid=476)[0;0m INFO 02-15 18:49:54 [model_runner_v1.py:2315] Loading drafter model...
2026-02-15T18:49:54.1196385Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m WARNING 02-15 18:49:54 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-15T18:49:54.1356417Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-15 18:49:54 [model_runner_v1.py:2315] Loading drafter model...
2026-02-15T18:49:54.1759693Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:49:54.1760222Z Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
2026-02-15T18:49:54.1831463Z [0;36m(Worker_DP0_TP4_EP4 pid=580)[0;0m INFO 02-15 18:49:54 [model_runner_v1.py:2315] Loading drafter model...
2026-02-15T18:49:54.1865425Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-15 18:49:54 [model_runner_v1.py:2315] Loading drafter model...
2026-02-15T18:49:54.7274485Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:49:54.7274956Z Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<01:29,  1.81it/s]
2026-02-15T18:49:55.8146885Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:49:55.8147296Z Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:01<02:19,  1.15it/s]
2026-02-15T18:49:57.0246512Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:49:57.0246956Z Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:02<02:43,  1.02s/it]
2026-02-15T18:49:58.2070170Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:49:58.2070688Z Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:04<02:52,  1.09s/it]
2026-02-15T18:49:59.3237551Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:49:59.3238042Z Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:05<02:53,  1.10s/it]
2026-02-15T18:50:00.3976706Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:00.3977205Z Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:06<02:50,  1.09s/it]
2026-02-15T18:50:01.5247209Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:01.5247708Z Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:07<02:51,  1.10s/it]
2026-02-15T18:50:02.6075077Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:02.6075498Z Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:08<02:49,  1.10s/it]
2026-02-15T18:50:03.8010086Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:03.8010507Z Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:09<02:53,  1.13s/it]
2026-02-15T18:50:04.9495437Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:04.9495891Z Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:10<02:53,  1.13s/it]
2026-02-15T18:50:06.0630888Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:06.0631292Z Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:11<02:51,  1.13s/it]
2026-02-15T18:50:07.1535840Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:07.1536539Z Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:12<02:48,  1.12s/it]
2026-02-15T18:50:08.3034703Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:08.3035180Z Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:14<02:48,  1.13s/it]
2026-02-15T18:50:09.4158592Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:09.4159121Z Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:15<02:47,  1.12s/it]
2026-02-15T18:50:10.5155284Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:10.5155728Z Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:16<02:44,  1.11s/it]
2026-02-15T18:50:11.5242554Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:11.5243077Z Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:17<02:39,  1.08s/it]
2026-02-15T18:50:12.5266945Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:12.5267370Z Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:18<02:34,  1.06s/it]
2026-02-15T18:50:13.5247697Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:13.5248162Z Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:19<02:30,  1.04s/it]
2026-02-15T18:50:14.6373776Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:14.6375104Z Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:20<02:32,  1.06s/it]
2026-02-15T18:50:15.7562504Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:15.7562912Z Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:21<02:34,  1.08s/it]
2026-02-15T18:50:16.8594912Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:16.8595342Z Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:22<02:34,  1.09s/it]
2026-02-15T18:50:17.9785302Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:17.9785701Z Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:23<02:34,  1.10s/it]
2026-02-15T18:50:19.0913644Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:19.0914050Z Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:24<02:34,  1.10s/it]
2026-02-15T18:50:20.1877734Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:20.1878184Z Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:26<02:32,  1.10s/it]
2026-02-15T18:50:21.2663644Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:21.2664030Z Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:27<02:30,  1.09s/it]
2026-02-15T18:50:22.7243190Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:22.7243676Z Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:28<02:44,  1.20s/it]
2026-02-15T18:50:23.9285055Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:23.9285597Z Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:29<02:43,  1.20s/it]
2026-02-15T18:50:25.0075013Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:25.0075492Z Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:30<02:37,  1.17s/it]
2026-02-15T18:50:26.1564004Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:26.1564469Z Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:31<02:35,  1.16s/it]
2026-02-15T18:50:27.2690947Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:27.2691421Z Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:33<02:32,  1.15s/it]
2026-02-15T18:50:28.4446430Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:28.4446820Z Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:34<02:32,  1.16s/it]
2026-02-15T18:50:29.5826708Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:29.5827218Z Loading safetensors checkpoint shards:  20% Completed | 32/163 [00:35<02:30,  1.15s/it]
2026-02-15T18:50:30.6831226Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:30.6831653Z Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:36<02:27,  1.14s/it]
2026-02-15T18:50:31.8017697Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:31.8018210Z Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:37<02:25,  1.13s/it]
2026-02-15T18:50:33.0040824Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:33.0041309Z Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:38<02:27,  1.15s/it]
2026-02-15T18:50:34.0304273Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:34.0304695Z Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:39<02:21,  1.11s/it]
2026-02-15T18:50:35.0526926Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:35.0527364Z Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:40<02:16,  1.09s/it]
2026-02-15T18:50:36.0482761Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:36.0483173Z Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:41<02:12,  1.06s/it]
2026-02-15T18:50:37.1993528Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:37.1993934Z Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:43<02:14,  1.09s/it]
2026-02-15T18:50:38.2699007Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:38.2699556Z Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:44<02:13,  1.08s/it]
2026-02-15T18:50:39.3743512Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:39.3743926Z Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:45<02:12,  1.09s/it]
2026-02-15T18:50:40.6062324Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:40.6062853Z Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:46<02:16,  1.13s/it]
2026-02-15T18:50:41.6729322Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:41.6729836Z Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:47<02:13,  1.11s/it]
2026-02-15T18:50:42.8177953Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:42.8178409Z Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:48<02:13,  1.12s/it]
2026-02-15T18:50:43.9589850Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:43.9590246Z Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:49<02:13,  1.13s/it]
2026-02-15T18:50:45.1144821Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:45.1145248Z Loading safetensors checkpoint shards:  28% Completed | 46/163 [00:50<02:12,  1.14s/it]
2026-02-15T18:50:45.7914760Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:45.7915172Z Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:51<01:55,  1.01it/s]
2026-02-15T18:50:46.9820774Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:46.9821203Z Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:52<01:32,  1.23it/s]
2026-02-15T18:50:48.0410249Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:48.0410720Z Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:53<01:38,  1.14it/s]
2026-02-15T18:50:49.1156447Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:49.1156846Z Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:54<01:43,  1.08it/s]
2026-02-15T18:50:50.2632734Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:50.2633344Z Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:56<01:49,  1.01it/s]
2026-02-15T18:50:51.3664242Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:51.3664682Z Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:57<01:52,  1.02s/it]
2026-02-15T18:50:52.4901181Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:52.4901653Z Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:58<01:54,  1.05s/it]
2026-02-15T18:50:53.6259529Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:53.6259966Z Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:59<01:56,  1.07s/it]
2026-02-15T18:50:54.7909746Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:54.7910451Z Loading safetensors checkpoint shards:  34% Completed | 56/163 [01:00<01:57,  1.10s/it]
2026-02-15T18:50:55.8146451Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:55.8146854Z Loading safetensors checkpoint shards:  35% Completed | 57/163 [01:01<01:54,  1.08s/it]
2026-02-15T18:50:57.0835459Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:57.0835882Z Loading safetensors checkpoint shards:  36% Completed | 58/163 [01:02<01:59,  1.13s/it]
2026-02-15T18:50:58.2371108Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:58.2371598Z Loading safetensors checkpoint shards:  36% Completed | 59/163 [01:04<01:58,  1.14s/it]
2026-02-15T18:50:59.3385169Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:50:59.3385591Z Loading safetensors checkpoint shards:  37% Completed | 60/163 [01:05<01:56,  1.13s/it]
2026-02-15T18:51:00.4883720Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:00.4884139Z Loading safetensors checkpoint shards:  37% Completed | 61/163 [01:06<01:55,  1.14s/it]
2026-02-15T18:51:01.5864108Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:01.5864520Z Loading safetensors checkpoint shards:  38% Completed | 62/163 [01:07<01:53,  1.12s/it]
2026-02-15T18:51:02.7482716Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:02.7483182Z Loading safetensors checkpoint shards:  39% Completed | 63/163 [01:08<01:53,  1.14s/it]
2026-02-15T18:51:03.9092833Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:03.9093242Z Loading safetensors checkpoint shards:  39% Completed | 64/163 [01:09<01:53,  1.14s/it]
2026-02-15T18:51:04.9352853Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:04.9353258Z Loading safetensors checkpoint shards:  40% Completed | 65/163 [01:10<01:48,  1.11s/it]
2026-02-15T18:51:06.0783426Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:06.0783931Z Loading safetensors checkpoint shards:  40% Completed | 66/163 [01:11<01:48,  1.12s/it]
2026-02-15T18:51:07.0675616Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:07.0676140Z Loading safetensors checkpoint shards:  41% Completed | 67/163 [01:12<01:43,  1.08s/it]
2026-02-15T18:51:08.1696843Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:08.1697353Z Loading safetensors checkpoint shards:  42% Completed | 68/163 [01:13<01:43,  1.09s/it]
2026-02-15T18:51:09.3790327Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:09.3790828Z Loading safetensors checkpoint shards:  42% Completed | 69/163 [01:15<01:45,  1.12s/it]
2026-02-15T18:51:10.5390141Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:10.5390526Z Loading safetensors checkpoint shards:  43% Completed | 70/163 [01:16<01:45,  1.13s/it]
2026-02-15T18:51:11.6946308Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:11.6946743Z Loading safetensors checkpoint shards:  44% Completed | 71/163 [01:17<01:44,  1.14s/it]
2026-02-15T18:51:12.6808760Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:12.6809182Z Loading safetensors checkpoint shards:  44% Completed | 72/163 [01:18<01:39,  1.09s/it]
2026-02-15T18:51:13.8183846Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:13.8184457Z Loading safetensors checkpoint shards:  45% Completed | 73/163 [01:19<01:39,  1.11s/it]
2026-02-15T18:51:14.9494634Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:14.9495032Z Loading safetensors checkpoint shards:  45% Completed | 74/163 [01:20<01:39,  1.11s/it]
2026-02-15T18:51:16.0041052Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:16.0041493Z Loading safetensors checkpoint shards:  46% Completed | 75/163 [01:21<01:36,  1.10s/it]
2026-02-15T18:51:17.1669336Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:17.1669736Z Loading safetensors checkpoint shards:  47% Completed | 76/163 [01:22<01:37,  1.12s/it]
2026-02-15T18:51:18.1969388Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:18.1969841Z Loading safetensors checkpoint shards:  47% Completed | 77/163 [01:24<01:33,  1.09s/it]
2026-02-15T18:51:19.3248732Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:19.3249389Z Loading safetensors checkpoint shards:  48% Completed | 78/163 [01:25<01:33,  1.10s/it]
2026-02-15T18:51:20.4698188Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:20.4698629Z Loading safetensors checkpoint shards:  48% Completed | 79/163 [01:26<01:33,  1.11s/it]
2026-02-15T18:51:21.4406957Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:21.4407446Z Loading safetensors checkpoint shards:  49% Completed | 80/163 [01:27<01:28,  1.07s/it]
2026-02-15T18:51:22.4496747Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:22.4497170Z Loading safetensors checkpoint shards:  50% Completed | 81/163 [01:28<01:26,  1.05s/it]
2026-02-15T18:51:23.5178950Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:23.5179423Z Loading safetensors checkpoint shards:  50% Completed | 82/163 [01:29<01:25,  1.06s/it]
2026-02-15T18:51:24.5961840Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:24.5962387Z Loading safetensors checkpoint shards:  51% Completed | 83/163 [01:30<01:25,  1.06s/it]
2026-02-15T18:51:25.6830984Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:25.6831382Z Loading safetensors checkpoint shards:  52% Completed | 84/163 [01:31<01:24,  1.07s/it]
2026-02-15T18:51:26.7691795Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:26.7692333Z Loading safetensors checkpoint shards:  52% Completed | 85/163 [01:32<01:23,  1.08s/it]
2026-02-15T18:51:27.8808595Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:27.8809051Z Loading safetensors checkpoint shards:  53% Completed | 86/163 [01:33<01:23,  1.09s/it]
2026-02-15T18:51:28.9344869Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:28.9345355Z Loading safetensors checkpoint shards:  53% Completed | 87/163 [01:34<01:21,  1.08s/it]
2026-02-15T18:51:30.1873335Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:30.1873778Z Loading safetensors checkpoint shards:  54% Completed | 88/163 [01:36<01:24,  1.13s/it]
2026-02-15T18:51:31.1788953Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:31.1789410Z Loading safetensors checkpoint shards:  55% Completed | 89/163 [01:37<01:20,  1.09s/it]
2026-02-15T18:51:32.1799871Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:32.1800371Z Loading safetensors checkpoint shards:  55% Completed | 90/163 [01:38<01:17,  1.06s/it]
2026-02-15T18:51:33.2594410Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:33.2594880Z Loading safetensors checkpoint shards:  56% Completed | 91/163 [01:39<01:16,  1.07s/it]
2026-02-15T18:51:34.2071027Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:34.2071408Z Loading safetensors checkpoint shards:  56% Completed | 92/163 [01:40<01:13,  1.03s/it]
2026-02-15T18:51:35.3203796Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:35.3204214Z Loading safetensors checkpoint shards:  57% Completed | 93/163 [01:41<01:13,  1.05s/it]
2026-02-15T18:51:36.3902270Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:36.3902697Z Loading safetensors checkpoint shards:  58% Completed | 94/163 [01:42<01:13,  1.06s/it]
2026-02-15T18:51:37.4246570Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:37.4246994Z Loading safetensors checkpoint shards:  58% Completed | 95/163 [01:43<01:11,  1.05s/it]
2026-02-15T18:51:38.5102218Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:38.5102620Z Loading safetensors checkpoint shards:  59% Completed | 96/163 [01:44<01:11,  1.06s/it]
2026-02-15T18:51:39.6248079Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:39.6248481Z Loading safetensors checkpoint shards:  60% Completed | 97/163 [01:45<01:11,  1.08s/it]
2026-02-15T18:51:40.6766181Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:40.6766608Z Loading safetensors checkpoint shards:  60% Completed | 98/163 [01:46<01:09,  1.07s/it]
2026-02-15T18:51:41.6876081Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:41.6876514Z Loading safetensors checkpoint shards:  61% Completed | 99/163 [01:47<01:07,  1.05s/it]
2026-02-15T18:51:42.7743806Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:42.7744335Z Loading safetensors checkpoint shards:  61% Completed | 100/163 [01:48<01:06,  1.06s/it]
2026-02-15T18:51:44.0183762Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:44.0184300Z Loading safetensors checkpoint shards:  62% Completed | 101/163 [01:49<01:09,  1.12s/it]
2026-02-15T18:51:45.0291786Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:45.0292390Z Loading safetensors checkpoint shards:  63% Completed | 102/163 [01:50<01:06,  1.09s/it]
2026-02-15T18:51:45.9823405Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:45.9823830Z Loading safetensors checkpoint shards:  63% Completed | 103/163 [01:51<01:02,  1.05s/it]
2026-02-15T18:51:46.9838608Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:46.9839115Z Loading safetensors checkpoint shards:  64% Completed | 104/163 [01:52<01:00,  1.03s/it]
2026-02-15T18:51:48.0415211Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:48.0415718Z Loading safetensors checkpoint shards:  64% Completed | 105/163 [01:53<01:00,  1.04s/it]
2026-02-15T18:51:49.0993941Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:49.0994332Z Loading safetensors checkpoint shards:  65% Completed | 106/163 [01:54<00:59,  1.05s/it]
2026-02-15T18:51:51.7523985Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:51.7524576Z Loading safetensors checkpoint shards:  66% Completed | 107/163 [01:57<01:25,  1.53s/it]
2026-02-15T18:51:52.4909263Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:52.4909699Z Loading safetensors checkpoint shards:  66% Completed | 108/163 [01:58<01:11,  1.29s/it]
2026-02-15T18:51:53.5471936Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:53.5472423Z Loading safetensors checkpoint shards:  67% Completed | 109/163 [01:59<01:05,  1.22s/it]
2026-02-15T18:51:54.5247238Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:54.5247698Z Loading safetensors checkpoint shards:  67% Completed | 110/163 [02:00<01:00,  1.15s/it]
2026-02-15T18:51:55.5068802Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:55.5069274Z Loading safetensors checkpoint shards:  68% Completed | 111/163 [02:01<00:57,  1.10s/it]
2026-02-15T18:51:56.5237635Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:56.5238039Z Loading safetensors checkpoint shards:  69% Completed | 112/163 [02:02<00:54,  1.07s/it]
2026-02-15T18:51:57.5458997Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:57.5459433Z Loading safetensors checkpoint shards:  69% Completed | 113/163 [02:03<00:52,  1.06s/it]
2026-02-15T18:51:58.5871121Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:58.5871537Z Loading safetensors checkpoint shards:  70% Completed | 114/163 [02:04<00:51,  1.05s/it]
2026-02-15T18:51:59.7046533Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:51:59.7047068Z Loading safetensors checkpoint shards:  71% Completed | 115/163 [02:05<00:51,  1.07s/it]
2026-02-15T18:52:00.7312417Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:00.7312889Z Loading safetensors checkpoint shards:  71% Completed | 116/163 [02:06<00:49,  1.06s/it]
2026-02-15T18:52:01.7771692Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:01.7772566Z Loading safetensors checkpoint shards:  72% Completed | 117/163 [02:07<00:48,  1.06s/it]
2026-02-15T18:52:02.7891391Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:02.7891839Z Loading safetensors checkpoint shards:  72% Completed | 118/163 [02:08<00:46,  1.04s/it]
2026-02-15T18:52:03.8279625Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:03.8280022Z Loading safetensors checkpoint shards:  73% Completed | 119/163 [02:09<00:45,  1.04s/it]
2026-02-15T18:52:04.9147826Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:04.9148298Z Loading safetensors checkpoint shards:  74% Completed | 120/163 [02:10<00:45,  1.05s/it]
2026-02-15T18:52:06.0012205Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:06.0012864Z Loading safetensors checkpoint shards:  74% Completed | 121/163 [02:11<00:44,  1.06s/it]
2026-02-15T18:52:07.1267385Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:07.1267788Z Loading safetensors checkpoint shards:  75% Completed | 122/163 [02:12<00:44,  1.08s/it]
2026-02-15T18:52:08.1504520Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:08.1504974Z Loading safetensors checkpoint shards:  75% Completed | 123/163 [02:13<00:42,  1.07s/it]
2026-02-15T18:52:09.1750209Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:09.1750610Z Loading safetensors checkpoint shards:  76% Completed | 124/163 [02:14<00:41,  1.05s/it]
2026-02-15T18:52:10.1671570Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:10.1672106Z Loading safetensors checkpoint shards:  77% Completed | 125/163 [02:15<00:39,  1.03s/it]
2026-02-15T18:52:11.2456959Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:11.2457369Z Loading safetensors checkpoint shards:  77% Completed | 126/163 [02:17<00:38,  1.05s/it]
2026-02-15T18:52:12.2992205Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:12.2992697Z Loading safetensors checkpoint shards:  78% Completed | 127/163 [02:18<00:37,  1.05s/it]
2026-02-15T18:52:13.2983676Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:13.2984097Z Loading safetensors checkpoint shards:  79% Completed | 128/163 [02:19<00:36,  1.03s/it]
2026-02-15T18:52:14.3918592Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:14.3918999Z Loading safetensors checkpoint shards:  80% Completed | 130/163 [02:20<00:26,  1.24it/s]
2026-02-15T18:52:15.4720262Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:15.4720669Z Loading safetensors checkpoint shards:  80% Completed | 131/163 [02:21<00:28,  1.14it/s]
2026-02-15T18:52:16.4789814Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:16.4790254Z Loading safetensors checkpoint shards:  81% Completed | 132/163 [02:22<00:28,  1.10it/s]
2026-02-15T18:52:17.6235392Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:17.6235824Z Loading safetensors checkpoint shards:  82% Completed | 133/163 [02:23<00:29,  1.03it/s]
2026-02-15T18:52:18.6342245Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:18.6342737Z Loading safetensors checkpoint shards:  82% Completed | 134/163 [02:24<00:28,  1.02it/s]
2026-02-15T18:52:19.6124843Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:19.6125266Z Loading safetensors checkpoint shards:  83% Completed | 135/163 [02:25<00:27,  1.02it/s]
2026-02-15T18:52:20.6646280Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:20.6646731Z Loading safetensors checkpoint shards:  83% Completed | 136/163 [02:26<00:27,  1.00s/it]
2026-02-15T18:52:21.7337752Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:21.7338161Z Loading safetensors checkpoint shards:  84% Completed | 137/163 [02:27<00:26,  1.02s/it]
2026-02-15T18:52:22.7329588Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:22.7330110Z Loading safetensors checkpoint shards:  85% Completed | 138/163 [02:28<00:25,  1.02s/it]
2026-02-15T18:52:23.7074983Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:23.7075491Z Loading safetensors checkpoint shards:  85% Completed | 139/163 [02:29<00:24,  1.00s/it]
2026-02-15T18:52:24.7937778Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:24.7938236Z Loading safetensors checkpoint shards:  86% Completed | 140/163 [02:30<00:23,  1.03s/it]
2026-02-15T18:52:25.9153054Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:25.9153560Z Loading safetensors checkpoint shards:  87% Completed | 141/163 [02:31<00:23,  1.06s/it]
2026-02-15T18:52:27.0146398Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:27.0146782Z Loading safetensors checkpoint shards:  87% Completed | 142/163 [02:32<00:22,  1.07s/it]
2026-02-15T18:52:27.9436093Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:27.9436505Z Loading safetensors checkpoint shards:  88% Completed | 143/163 [02:33<00:20,  1.03s/it]
2026-02-15T18:52:28.9298415Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:28.9298822Z Loading safetensors checkpoint shards:  88% Completed | 144/163 [02:34<00:19,  1.01s/it]
2026-02-15T18:52:29.8993349Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:29.8993802Z Loading safetensors checkpoint shards:  89% Completed | 145/163 [02:35<00:18,  1.00s/it]
2026-02-15T18:52:30.9584482Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:30.9585000Z Loading safetensors checkpoint shards:  90% Completed | 146/163 [02:36<00:17,  1.02s/it]
2026-02-15T18:52:32.0392619Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:32.0393117Z Loading safetensors checkpoint shards:  90% Completed | 147/163 [02:37<00:16,  1.04s/it]
2026-02-15T18:52:33.0829691Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:33.0830141Z Loading safetensors checkpoint shards:  91% Completed | 148/163 [02:38<00:15,  1.04s/it]
2026-02-15T18:52:34.1768633Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:34.1769252Z Loading safetensors checkpoint shards:  91% Completed | 149/163 [02:40<00:14,  1.06s/it]
2026-02-15T18:52:37.7393160Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:37.7393694Z Loading safetensors checkpoint shards:  92% Completed | 150/163 [02:43<00:23,  1.81s/it]
2026-02-15T18:52:38.5058119Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:38.5058679Z Loading safetensors checkpoint shards:  93% Completed | 151/163 [02:44<00:17,  1.50s/it]
2026-02-15T18:52:39.5531227Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:39.5531727Z Loading safetensors checkpoint shards:  93% Completed | 152/163 [02:45<00:14,  1.36s/it]
2026-02-15T18:52:39.9844119Z [0;36m(ApiServer_0 pid=184)[0;0m Process ApiServer_0:
2026-02-15T18:52:39.9988230Z [0;36m(ApiServer_0 pid=184)[0;0m Traceback (most recent call last):
2026-02-15T18:52:40.0010703Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-15T18:52:40.0019788Z [0;36m(ApiServer_0 pid=184)[0;0m     self.run()
2026-02-15T18:52:40.0030804Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-15T18:52:40.0040984Z [0;36m(ApiServer_0 pid=184)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-15T18:52:40.0050681Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-15T18:52:40.0060720Z [0;36m(ApiServer_0 pid=184)[0;0m     uvloop.run(
2026-02-15T18:52:40.0070609Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-15T18:52:40.0080087Z [0;36m(ApiServer_0 pid=184)[0;0m     return runner.run(wrapper())
2026-02-15T18:52:40.0090084Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.0099133Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-15T18:52:40.0109151Z [0;36m(ApiServer_0 pid=184)[0;0m     return self._loop.run_until_complete(task)
2026-02-15T18:52:40.0119715Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.0129594Z [0;36m(ApiServer_0 pid=184)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-15T18:52:40.0139420Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-15T18:52:40.0148892Z [0;36m(ApiServer_0 pid=184)[0;0m     return await main
2026-02-15T18:52:40.0158493Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^
2026-02-15T18:52:40.0169403Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-15T18:52:40.0178871Z [0;36m(ApiServer_0 pid=184)[0;0m     async with build_async_engine_client(
2026-02-15T18:52:40.0189299Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-15T18:52:40.0199197Z [0;36m(ApiServer_0 pid=184)[0;0m     return await anext(self.gen)
2026-02-15T18:52:40.0208750Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.0218550Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-15T18:52:40.0228199Z [0;36m(ApiServer_0 pid=184)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-15T18:52:40.0238096Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-15T18:52:40.0248357Z [0;36m(ApiServer_0 pid=184)[0;0m     return await anext(self.gen)
2026-02-15T18:52:40.0257900Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.0267948Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-15T18:52:40.0278441Z [0;36m(ApiServer_0 pid=184)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-15T18:52:40.0288984Z [0;36m(ApiServer_0 pid=184)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.0298476Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-15T18:52:40.0307445Z [0;36m(ApiServer_0 pid=184)[0;0m     return cls(
2026-02-15T18:52:40.0316706Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^
2026-02-15T18:52:40.0327405Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-15T18:52:40.0337501Z [0;36m(ApiServer_0 pid=184)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-15T18:52:40.0345954Z [0;36m(ApiServer_0 pid=184)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.0356442Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-15T18:52:40.0367044Z [0;36m(ApiServer_0 pid=184)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-15T18:52:40.0375820Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.0385554Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-15T18:52:40.0394874Z [0;36m(ApiServer_0 pid=184)[0;0m     super().__init__(
2026-02-15T18:52:40.0404984Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-15T18:52:40.0414260Z [0;36m(ApiServer_0 pid=184)[0;0m     super().__init__(
2026-02-15T18:52:40.0426028Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-15T18:52:40.0432844Z [0;36m(ApiServer_0 pid=184)[0;0m     super().__init__(
2026-02-15T18:52:40.0443103Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-15T18:52:40.0452675Z [0;36m(ApiServer_0 pid=184)[0;0m     raise TimeoutError(
2026-02-15T18:52:40.0462220Z [0;36m(ApiServer_0 pid=184)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-15T18:52:40.3522726Z [0;36m(ApiServer_0 pid=184)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-15T18:52:40.6035741Z [0;36m(ApiServer_2 pid=186)[0;0m Process ApiServer_2:
2026-02-15T18:52:40.6055978Z [0;36m(ApiServer_2 pid=186)[0;0m Traceback (most recent call last):
2026-02-15T18:52:40.6065140Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-15T18:52:40.6074525Z [0;36m(ApiServer_2 pid=186)[0;0m     self.run()
2026-02-15T18:52:40.6084331Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-15T18:52:40.6094070Z [0;36m(ApiServer_2 pid=186)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-15T18:52:40.6104753Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-15T18:52:40.6114351Z [0;36m(ApiServer_2 pid=186)[0;0m     uvloop.run(
2026-02-15T18:52:40.6125641Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-15T18:52:40.6135514Z [0;36m(ApiServer_2 pid=186)[0;0m     return runner.run(wrapper())
2026-02-15T18:52:40.6145298Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.6158635Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-15T18:52:40.6168196Z [0;36m(ApiServer_2 pid=186)[0;0m     return self._loop.run_until_complete(task)
2026-02-15T18:52:40.6177456Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.6187570Z [0;36m(ApiServer_2 pid=186)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-15T18:52:40.6196765Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-15T18:52:40.6206993Z [0;36m(ApiServer_2 pid=186)[0;0m     return await main
2026-02-15T18:52:40.6216056Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^
2026-02-15T18:52:40.6226421Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-15T18:52:40.6234933Z [0;36m(ApiServer_2 pid=186)[0;0m     async with build_async_engine_client(
2026-02-15T18:52:40.6244865Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-15T18:52:40.6254559Z [0;36m(ApiServer_2 pid=186)[0;0m     return await anext(self.gen)
2026-02-15T18:52:40.6263651Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.6273586Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-15T18:52:40.6284372Z [0;36m(ApiServer_2 pid=186)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-15T18:52:40.6292587Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-15T18:52:40.6301610Z [0;36m(ApiServer_2 pid=186)[0;0m     return await anext(self.gen)
2026-02-15T18:52:40.6311169Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.6321724Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-15T18:52:40.6331208Z [0;36m(ApiServer_2 pid=186)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-15T18:52:40.6340391Z [0;36m(ApiServer_2 pid=186)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.6350480Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-15T18:52:40.6359759Z [0;36m(ApiServer_2 pid=186)[0;0m     return cls(
2026-02-15T18:52:40.6369383Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^
2026-02-15T18:52:40.6379410Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-15T18:52:40.6388359Z [0;36m(ApiServer_2 pid=186)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-15T18:52:40.6397795Z [0;36m(ApiServer_2 pid=186)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.6407915Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-15T18:52:40.6417206Z [0;36m(ApiServer_2 pid=186)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-15T18:52:40.6426462Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.6436167Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-15T18:52:40.6446153Z [0;36m(ApiServer_2 pid=186)[0;0m     super().__init__(
2026-02-15T18:52:40.6454439Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-15T18:52:40.6463946Z [0;36m(ApiServer_2 pid=186)[0;0m     super().__init__(
2026-02-15T18:52:40.6473253Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-15T18:52:40.6483005Z [0;36m(ApiServer_2 pid=186)[0;0m     super().__init__(
2026-02-15T18:52:40.6493103Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-15T18:52:40.6502491Z [0;36m(ApiServer_2 pid=186)[0;0m     raise TimeoutError(
2026-02-15T18:52:40.6511655Z [0;36m(ApiServer_2 pid=186)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-15T18:52:40.6720026Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:40.6720447Z Loading safetensors checkpoint shards:  94% Completed | 153/163 [02:46<00:12,  1.29s/it]
2026-02-15T18:52:40.8354638Z [0;36m(ApiServer_1 pid=185)[0;0m Process ApiServer_1:
2026-02-15T18:52:40.8377794Z [0;36m(ApiServer_1 pid=185)[0;0m Traceback (most recent call last):
2026-02-15T18:52:40.8401799Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-15T18:52:40.8402462Z [0;36m(ApiServer_1 pid=185)[0;0m     self.run()
2026-02-15T18:52:40.8406295Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-15T18:52:40.8415283Z [0;36m(ApiServer_1 pid=185)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-15T18:52:40.8425671Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-15T18:52:40.8434859Z [0;36m(ApiServer_1 pid=185)[0;0m     uvloop.run(
2026-02-15T18:52:40.8444867Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-15T18:52:40.8454404Z [0;36m(ApiServer_1 pid=185)[0;0m     return runner.run(wrapper())
2026-02-15T18:52:40.8464305Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.8499511Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-15T18:52:40.8500057Z [0;36m(ApiServer_1 pid=185)[0;0m     return self._loop.run_until_complete(task)
2026-02-15T18:52:40.8500574Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.8502849Z [0;36m(ApiServer_1 pid=185)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-15T18:52:40.8512518Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-15T18:52:40.8521386Z [0;36m(ApiServer_1 pid=185)[0;0m     return await main
2026-02-15T18:52:40.8531277Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^
2026-02-15T18:52:40.8540913Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-15T18:52:40.8550861Z [0;36m(ApiServer_1 pid=185)[0;0m     async with build_async_engine_client(
2026-02-15T18:52:40.8560255Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-15T18:52:40.8569607Z [0;36m(ApiServer_1 pid=185)[0;0m     return await anext(self.gen)
2026-02-15T18:52:40.8578484Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.8588271Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-15T18:52:40.8645928Z [0;36m(ApiServer_1 pid=185)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-15T18:52:40.8646497Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-15T18:52:40.8647010Z [0;36m(ApiServer_1 pid=185)[0;0m     return await anext(self.gen)
2026-02-15T18:52:40.8647389Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.8648102Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-15T18:52:40.8648729Z [0;36m(ApiServer_1 pid=185)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-15T18:52:40.8656343Z [0;36m(ApiServer_1 pid=185)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.8664737Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-15T18:52:40.8674126Z [0;36m(ApiServer_1 pid=185)[0;0m     return cls(
2026-02-15T18:52:40.8683234Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^
2026-02-15T18:52:40.8694109Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-15T18:52:40.8705196Z [0;36m(ApiServer_1 pid=185)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-15T18:52:40.8712388Z [0;36m(ApiServer_1 pid=185)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.8722425Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-15T18:52:40.8731408Z [0;36m(ApiServer_1 pid=185)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-15T18:52:40.8741129Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.8750958Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-15T18:52:40.8759909Z [0;36m(ApiServer_1 pid=185)[0;0m     super().__init__(
2026-02-15T18:52:40.8770287Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-15T18:52:40.8779927Z [0;36m(ApiServer_1 pid=185)[0;0m     super().__init__(
2026-02-15T18:52:40.8789609Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-15T18:52:40.8799694Z [0;36m(ApiServer_1 pid=185)[0;0m     super().__init__(
2026-02-15T18:52:40.8811531Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-15T18:52:40.8818795Z [0;36m(ApiServer_1 pid=185)[0;0m     raise TimeoutError(
2026-02-15T18:52:40.8828779Z [0;36m(ApiServer_1 pid=185)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-15T18:52:40.8837900Z [0;36m(ApiServer_3 pid=187)[0;0m Process ApiServer_3:
2026-02-15T18:52:40.8847701Z [0;36m(ApiServer_3 pid=187)[0;0m Traceback (most recent call last):
2026-02-15T18:52:40.8857819Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-15T18:52:40.8866462Z [0;36m(ApiServer_3 pid=187)[0;0m     self.run()
2026-02-15T18:52:40.8875858Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-15T18:52:40.8886251Z [0;36m(ApiServer_3 pid=187)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-15T18:52:40.8894796Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-15T18:52:40.8903657Z [0;36m(ApiServer_3 pid=187)[0;0m     uvloop.run(
2026-02-15T18:52:40.8913612Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-15T18:52:40.8923027Z [0;36m(ApiServer_3 pid=187)[0;0m     return runner.run(wrapper())
2026-02-15T18:52:40.8932893Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.8942416Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-15T18:52:40.8951461Z [0;36m(ApiServer_3 pid=187)[0;0m     return self._loop.run_until_complete(task)
2026-02-15T18:52:40.8961342Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.8970798Z [0;36m(ApiServer_3 pid=187)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-15T18:52:40.8980924Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-15T18:52:40.8989906Z [0;36m(ApiServer_3 pid=187)[0;0m     return await main
2026-02-15T18:52:40.8999157Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^
2026-02-15T18:52:40.9009037Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-15T18:52:40.9018204Z [0;36m(ApiServer_3 pid=187)[0;0m     async with build_async_engine_client(
2026-02-15T18:52:40.9028047Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-15T18:52:40.9037337Z [0;36m(ApiServer_3 pid=187)[0;0m     return await anext(self.gen)
2026-02-15T18:52:40.9047894Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.9056986Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-15T18:52:40.9066325Z [0;36m(ApiServer_3 pid=187)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-15T18:52:40.9075730Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-15T18:52:40.9086297Z [0;36m(ApiServer_3 pid=187)[0;0m     return await anext(self.gen)
2026-02-15T18:52:40.9095691Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.9105560Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-15T18:52:40.9116062Z [0;36m(ApiServer_3 pid=187)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-15T18:52:40.9124330Z [0;36m(ApiServer_3 pid=187)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.9134281Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-15T18:52:40.9143437Z [0;36m(ApiServer_3 pid=187)[0;0m     return cls(
2026-02-15T18:52:40.9154073Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^
2026-02-15T18:52:40.9165619Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-15T18:52:40.9176542Z [0;36m(ApiServer_3 pid=187)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-15T18:52:40.9186707Z [0;36m(ApiServer_3 pid=187)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.9196194Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-15T18:52:40.9208648Z [0;36m(ApiServer_3 pid=187)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-15T18:52:40.9220388Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-15T18:52:40.9231563Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-15T18:52:40.9242340Z [0;36m(ApiServer_3 pid=187)[0;0m     super().__init__(
2026-02-15T18:52:40.9254049Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-15T18:52:40.9263721Z [0;36m(ApiServer_3 pid=187)[0;0m     super().__init__(
2026-02-15T18:52:40.9274098Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-15T18:52:40.9284129Z [0;36m(ApiServer_3 pid=187)[0;0m     super().__init__(
2026-02-15T18:52:40.9293996Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-15T18:52:40.9303527Z [0;36m(ApiServer_3 pid=187)[0;0m     raise TimeoutError(
2026-02-15T18:52:40.9313282Z [0;36m(ApiServer_3 pid=187)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-15T18:52:41.0358082Z [0;36m(ApiServer_2 pid=186)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-15T18:52:41.2157378Z [0;36m(ApiServer_1 pid=185)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-15T18:52:41.2701909Z [0;36m(ApiServer_3 pid=187)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-15T18:52:41.7635213Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:41.7635759Z Loading safetensors checkpoint shards:  94% Completed | 154/163 [02:47<00:11,  1.23s/it]
2026-02-15T18:52:42.9000322Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:42.9000753Z Loading safetensors checkpoint shards:  95% Completed | 155/163 [02:48<00:09,  1.20s/it]
2026-02-15T18:52:44.0071765Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:44.0072302Z Loading safetensors checkpoint shards:  96% Completed | 156/163 [02:49<00:08,  1.17s/it]
2026-02-15T18:52:45.1198784Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:45.1199266Z Loading safetensors checkpoint shards:  96% Completed | 157/163 [02:50<00:06,  1.16s/it]
2026-02-15T18:52:46.0803155Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:46.0803564Z Loading safetensors checkpoint shards:  97% Completed | 158/163 [02:51<00:05,  1.10s/it]
2026-02-15T18:52:47.1281892Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:47.1282446Z Loading safetensors checkpoint shards:  98% Completed | 159/163 [02:52<00:04,  1.08s/it]
2026-02-15T18:52:48.2343801Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:48.2344227Z Loading safetensors checkpoint shards:  98% Completed | 160/163 [02:54<00:03,  1.09s/it]
2026-02-15T18:52:49.2873455Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:49.2873913Z Loading safetensors checkpoint shards:  99% Completed | 161/163 [02:55<00:02,  1.08s/it]
2026-02-15T18:52:50.3322722Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:50.3323136Z Loading safetensors checkpoint shards:  99% Completed | 162/163 [02:56<00:01,  1.07s/it]
2026-02-15T18:52:51.3308286Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:51.3308713Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [02:57<00:00,  1.05s/it]
2026-02-15T18:52:51.3331109Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:51.3331555Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [02:57<00:00,  1.09s/it]
2026-02-15T18:52:51.3339927Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:52:51.4271041Z [0;36m(Worker_DP1_TP1_EP9 pid=254)[0;0m INFO 02-15 18:52:51 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-15T18:52:51.5936486Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-15 18:52:51 [default_loader.py:291] Loading weights took 177.40 seconds
2026-02-15T18:52:51.6291670Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-15 18:52:51 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-15T18:52:51.6311885Z [0;36m(Worker_DP0_TP3_EP3 pid=476)[0;0m INFO 02-15 18:52:51 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-15T18:52:51.6335660Z [0;36m(Worker_DP0_TP4_EP4 pid=580)[0;0m INFO 02-15 18:52:51 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-15T18:52:51.6414900Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-15 18:52:51 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-15T18:52:51.6469382Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-15 18:52:51 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-15T18:52:51.6610504Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-15 18:52:51 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-15T18:52:51.6623999Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-15 18:52:51 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-15T18:52:52.3286014Z [0;36m(Worker_DP1_TP1_EP9 pid=254)[0;0m INFO 02-15 18:52:52 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-15T18:52:52.3584732Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m INFO 02-15 18:52:52 [default_loader.py:291] Loading weights took 178.18 seconds
2026-02-15T18:52:52.3671176Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-15 18:52:52 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-15T18:52:52.3917258Z [0;36m(Worker_DP0_TP1_EP1 pid=257)[0;0m INFO 02-15 18:52:52 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-15T18:52:52.3957403Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-15 18:52:52 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-15T18:52:52.4068751Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-15 18:52:52 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-15T18:52:52.4073701Z [0;36m(Worker_DP0_TP2_EP2 pid=372)[0;0m INFO 02-15 18:52:52 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-15T18:52:52.4084670Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m INFO 02-15 18:52:52 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-15T18:52:52.4095342Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-15 18:52:52 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-15T18:52:52.4106033Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-15 18:52:52 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-15T18:52:52.5891604Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-15 18:52:52 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-15T18:52:52.7544329Z [0;36m(Worker_DP0_TP3_EP3 pid=476)[0;0m INFO 02-15 18:52:52 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-15T18:52:53.2533334Z [0;36m(Worker_DP0_TP4_EP4 pid=580)[0;0m INFO 02-15 18:52:53 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-15T18:52:53.2837114Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-15 18:52:53 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-15T18:52:53.4007283Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-15 18:52:53 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-15T18:52:53.4416134Z [0;36m(Worker_DP0_TP1_EP1 pid=257)[0;0m INFO 02-15 18:52:53 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-15T18:52:53.4423692Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-15 18:52:53 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-15T18:52:53.6573262Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m INFO 02-15 18:52:53 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-15T18:52:53.8399698Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-15 18:52:53 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-15T18:52:53.8667070Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-15 18:52:53 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-15T18:52:53.8929381Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-15 18:52:53 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-15T18:52:54.0748298Z [0;36m(Worker_DP0_TP2_EP2 pid=372)[0;0m INFO 02-15 18:52:54 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-15T18:52:54.1274420Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-15 18:52:54 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-15T18:52:54.1423074Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-15 18:52:54 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-15T18:52:54.3486273Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-15 18:52:54 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-15T18:53:00.3385254Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m INFO 02-15 18:53:00 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/5f4e5deba5/rank_0_0/backbone for vLLM's torch.compile
2026-02-15T18:53:00.3418042Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m INFO 02-15 18:53:00 [backends.py:865] Dynamo bytecode transform time: 4.65 s
2026-02-15T18:53:00.3863289Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-15 18:53:00 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/5f4e5deba5/rank_0_1/backbone for vLLM's torch.compile
2026-02-15T18:53:00.3889930Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-15 18:53:00 [backends.py:865] Dynamo bytecode transform time: 4.70 s
2026-02-15T18:53:06.4739349Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-15T18:53:06.4748592Z [0;36m(Worker_DP1_TP1_EP9 pid=254)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-15T18:53:06.4758655Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m   warnings.warn(
2026-02-15T18:53:06.4768393Z [0;36m(Worker_DP1_TP1_EP9 pid=254)[0;0m   warnings.warn(
2026-02-15T18:53:06.4930127Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-15T18:53:06.4937733Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m   warnings.warn(
2026-02-15T18:53:06.5021951Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-15T18:53:06.5162210Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m   warnings.warn(
2026-02-15T18:53:06.5228021Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-15T18:53:06.5236783Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m   warnings.warn(
2026-02-15T18:53:06.5297179Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-15T18:53:06.5306228Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m   warnings.warn(
2026-02-15T18:53:06.5388650Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-15T18:53:06.5397486Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m   warnings.warn(
2026-02-15T18:53:06.5890597Z [0;36m(Worker_DP0_TP1_EP1 pid=257)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-15T18:53:06.5899153Z [0;36m(Worker_DP0_TP1_EP1 pid=257)[0;0m   warnings.warn(
2026-02-15T18:53:06.6145273Z [0;36m(Worker_DP0_TP3_EP3 pid=476)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-15T18:53:06.6153635Z [0;36m(Worker_DP0_TP3_EP3 pid=476)[0;0m   warnings.warn(
2026-02-15T18:53:06.6247173Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-15T18:53:06.6255391Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m   warnings.warn(
2026-02-15T18:53:06.6320399Z [0;36m(Worker_DP0_TP4_EP4 pid=580)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-15T18:53:06.6328366Z [0;36m(Worker_DP0_TP4_EP4 pid=580)[0;0m   warnings.warn(
2026-02-15T18:53:06.6682643Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-15T18:53:06.6690229Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m   warnings.warn(
2026-02-15T18:53:06.6755525Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-15T18:53:06.6764193Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m   warnings.warn(
2026-02-15T18:53:06.6900451Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-15T18:53:06.6907705Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m   warnings.warn(
2026-02-15T18:53:06.6961790Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-15T18:53:06.6969422Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m   warnings.warn(
2026-02-15T18:53:06.7087241Z [0;36m(Worker_DP0_TP2_EP2 pid=372)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-15T18:53:06.7094903Z [0;36m(Worker_DP0_TP2_EP2 pid=372)[0;0m   warnings.warn(
2026-02-15T18:53:19.4438470Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-15 18:53:19 [backends.py:319] Compiling a graph for compile range (1, 4096) takes 12.97 s
2026-02-15T18:53:19.4448453Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-15 18:53:19 [monitor.py:34] torch.compile takes 17.67 s in total
2026-02-15T18:53:20.8542420Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m INFO 02-15 18:53:20 [backends.py:319] Compiling a graph for compile range (1, 4096) takes 14.16 s
2026-02-15T18:53:20.8550050Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m INFO 02-15 18:53:20 [monitor.py:34] torch.compile takes 18.80 s in total
2026-02-15T18:53:25.4191345Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-15 18:53:25 [worker.py:338] Available memory: 18549770956, total memory: 65796046848
2026-02-15T18:53:25.4953352Z [0;36m(Worker_DP0_TP4_EP4 pid=580)[0;0m INFO 02-15 18:53:25 [worker.py:338] Available memory: 18248212992, total memory: 65787658240
2026-02-15T18:53:25.5014191Z [0;36m(Worker_DP0_TP1_EP1 pid=257)[0;0m INFO 02-15 18:53:25 [worker.py:338] Available memory: 18568145612, total memory: 65796046848
2026-02-15T18:53:25.5094553Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-15 18:53:25 [worker.py:338] Available memory: 18263290368, total memory: 65787658240
2026-02-15T18:53:25.5286865Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-15 18:53:25 [worker.py:338] Available memory: 17858951680, total memory: 65787658240
2026-02-15T18:53:25.5316111Z [0;36m(Worker_DP1_TP1_EP9 pid=254)[0;0m INFO 02-15 18:53:25 [worker.py:338] Available memory: 18557915852, total memory: 65796046848
2026-02-15T18:53:25.5406412Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-15 18:53:25 [worker.py:338] Available memory: 18555940556, total memory: 65796046848
2026-02-15T18:53:25.5557631Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-15 18:53:25 [worker.py:338] Available memory: 18558464716, total memory: 65796046848
2026-02-15T18:53:25.5567076Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-15 18:53:25 [worker.py:338] Available memory: 18260498944, total memory: 65787658240
2026-02-15T18:53:25.5954334Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m INFO 02-15 18:53:25 [worker.py:338] Available memory: 17863453184, total memory: 65787658240
2026-02-15T18:53:25.9448391Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-15 18:53:25 [worker.py:338] Available memory: 18262643200, total memory: 65787658240
2026-02-15T18:53:25.9706484Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-15 18:53:25 [worker.py:338] Available memory: 18552130252, total memory: 65796046848
2026-02-15T18:53:25.9859334Z [0;36m(Worker_DP0_TP2_EP2 pid=372)[0;0m INFO 02-15 18:53:25 [worker.py:338] Available memory: 18272343552, total memory: 65787658240
2026-02-15T18:53:26.0231155Z [0;36m(Worker_DP0_TP3_EP3 pid=476)[0;0m INFO 02-15 18:53:26 [worker.py:338] Available memory: 18536207052, total memory: 65796046848
2026-02-15T18:53:26.0259240Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-15 18:53:26 [kv_cache_utils.py:1307] GPU KV cache size: 204,544 tokens
2026-02-15T18:53:26.0281512Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-15 18:53:26 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 24.97x
2026-02-15T18:53:26.2849701Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-15 18:53:26 [worker.py:338] Available memory: 18268177920, total memory: 65787658240
2026-02-15T18:53:26.3466126Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-15 18:53:26 [worker.py:338] Available memory: 18550479564, total memory: 65796046848
2026-02-15T18:53:26.3523686Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-15 18:53:26 [kv_cache_utils.py:1307] GPU KV cache size: 204,544 tokens
2026-02-15T18:53:26.3531874Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-15 18:53:26 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 24.97x
2026-02-15T18:53:44.1878127Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m 
2026-02-15T18:53:44.1879023Z Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s][rank11]:[W215 18:53:44.112323276 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-15T18:53:44.1887900Z [rank12]:[W215 18:53:44.112342637 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-15T18:53:44.1898856Z [rank13]:[W215 18:53:44.112342697 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-15T18:53:44.1909693Z [rank8]:[W215 18:53:44.112356677 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-15T18:53:44.1922550Z [rank6]:[W215 18:53:44.112367907 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-15T18:53:44.1931488Z [rank9]:[W215 18:53:44.112372667 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-15T18:53:44.1940517Z [rank15]:[W215 18:53:44.112389797 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-15T18:53:44.1950316Z [rank10]:[W215 18:53:44.112533508 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-15T18:53:44.1963806Z [rank7]:[W215 18:53:44.112651869 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-15T18:53:44.1975168Z [rank3]:[W215 18:53:44.112897371 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-15T18:53:44.1990669Z [rank14]:[W215 18:53:44.113259214 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-15T18:53:44.2002447Z [rank4]:[W215 18:53:44.113708677 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-15T18:53:44.2015811Z [rank5]:[W215 18:53:44.113714317 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-15T18:53:44.2032948Z [rank0]:[W215 18:53:44.113836148 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-15T18:53:44.2051613Z [rank1]:[W215 18:53:44.115780763 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-15T18:53:44.2063686Z [rank2]:[W215 18:53:44.116407378 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-15T18:53:46.3177385Z [rank7]:[W215 18:53:46.242328795 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-15T18:53:46.3185770Z [rank6]:[W215 18:53:46.242328755 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-15T18:53:46.3202559Z [rank3]:[W215 18:53:46.243241642 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-15T18:53:46.3212162Z [rank5]:[W215 18:53:46.243403754 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-15T18:53:46.3221770Z [rank0]:[W215 18:53:46.243582435 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-15T18:53:46.3231085Z [rank2]:[W215 18:53:46.243858317 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-15T18:53:46.3240980Z [rank4]:[W215 18:53:46.244627703 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-15T18:53:46.3251238Z [rank1]:[W215 18:53:46.245462269 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-15T18:53:46.3482107Z [rank11]:[W215 18:53:46.273664546 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-15T18:53:46.3503530Z [rank10]:[W215 18:53:46.274575033 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-15T18:53:46.3512593Z [rank9]:[W215 18:53:46.275040907 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-15T18:53:46.3522222Z [rank12]:[W215 18:53:46.275369369 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-15T18:53:46.3531646Z [rank14]:[W215 18:53:46.275402339 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-15T18:53:46.3542420Z [rank8]:[W215 18:53:46.276066224 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-15T18:53:46.3551923Z [rank15]:[W215 18:53:46.276131365 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-15T18:53:46.3563250Z [rank13]:[W215 18:53:46.276413907 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-15T18:53:49.3262590Z 
2026-02-15T18:53:49.3263353Z Capturing CUDA graphs (decode, FULL):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:18<00:18, 18.06s/it]
2026-02-15T18:53:49.3264185Z Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:21<00:00,  9.18s/it]
2026-02-15T18:53:49.3264692Z Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:21<00:00, 10.51s/it]
2026-02-15T18:53:49.8433455Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-15 18:53:49 [gpu_model_runner.py:5051] Graph capturing finished in 22 secs, took 0.27 GiB
2026-02-15T18:53:50.0192527Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m INFO 02-15 18:53:50 [gpu_model_runner.py:5051] Graph capturing finished in 22 secs, took 0.27 GiB
2026-02-15T18:53:50.6274059Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-15 18:53:50 [core.py:272] init engine (profile, create kv cache, warmup model) took 56.41 seconds
2026-02-15T18:53:50.8901206Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-15 18:53:50 [core.py:272] init engine (profile, create kv cache, warmup model) took 56.54 seconds
2026-02-15T18:53:51.7727130Z INFO 02-15 18:53:51 [coordinator.py:200] All engine subscriptions received by DP coordinator
2026-02-15T18:53:51.7750557Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-15 18:53:51 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-15T18:53:51.7761588Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-15 18:53:51 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-15T18:53:51.7769737Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-15 18:53:51 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:53:51.7781310Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-15 18:53:51 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:53:51.7790708Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-15 18:53:51 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:53:51.7800238Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-15 18:53:51 [ascend_config.py:412] Dynamic EPLB is False
2026-02-15T18:53:51.7811176Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-15 18:53:51 [ascend_config.py:413] The number of redundant experts is 0
2026-02-15T18:53:51.7823139Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-15 18:53:51 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-15T18:53:51.7832726Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-15 18:53:51 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-15T18:53:51.7842922Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-15 18:53:51 [platform.py:335] [91m
2026-02-15T18:53:51.7853073Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             **********************************************************************************
2026-02-15T18:53:51.7863429Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-15T18:53:51.7873491Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-15T18:53:51.7884304Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-15T18:53:51.7894835Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-15T18:53:51.7904494Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-15T18:53:51.7915213Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             * batch size for graph capture.
2026-02-15T18:53:51.7925293Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             * For more details, please refer to:
2026-02-15T18:53:51.7935353Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-15T18:53:51.7944163Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             **********************************************************************************[0m
2026-02-15T18:53:51.7953673Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             
2026-02-15T18:53:51.7964523Z INFO 02-15 18:53:51 [utils.py:249] Waiting for API servers to complete ...
2026-02-15T18:53:51.7973404Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-15 18:53:51 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-15T18:53:51.7982664Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-15 18:53:51 [platform.py:335] [91m
2026-02-15T18:53:51.7995453Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             **********************************************************************************
2026-02-15T18:53:51.8003201Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-15T18:53:51.8013885Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-15T18:53:51.8023225Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-15T18:53:51.8033860Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-15T18:53:51.8044103Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-15T18:53:51.8053635Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             * batch size for graph capture.
2026-02-15T18:53:51.8063389Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             * For more details, please refer to:
2026-02-15T18:53:51.8072899Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-15T18:53:51.8085615Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             **********************************************************************************[0m
2026-02-15T18:53:51.8092972Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-15 18:53:51 [platform.py:335]             
2026-02-15T18:53:51.8101955Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-15 18:53:51 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-15T18:53:51.8117737Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-15 18:53:51 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-15T18:53:51.8136305Z ERROR 02-15 18:53:51 [utils.py:290] Exception occurred while running API servers: Process ApiServer_0 (PID: 184) died with exit code 1
2026-02-15T18:53:51.8145799Z ERROR 02-15 18:53:51 [utils.py:290] Traceback (most recent call last):
2026-02-15T18:53:51.8156123Z ERROR 02-15 18:53:51 [utils.py:290]   File "/vllm-workspace/vllm/vllm/v1/utils.py", line 277, in wait_for_completion_or_failure
2026-02-15T18:53:51.8166013Z ERROR 02-15 18:53:51 [utils.py:290]     raise RuntimeError(
2026-02-15T18:53:51.8175541Z ERROR 02-15 18:53:51 [utils.py:290] RuntimeError: Process ApiServer_0 (PID: 184) died with exit code 1
2026-02-15T18:53:51.8185228Z INFO 02-15 18:53:51 [utils.py:293] Terminating remaining processes ...
2026-02-15T18:53:52.0629425Z [0;36m(Worker_DP0_TP1_EP1 pid=257)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-15T18:53:52.0638063Z [0;36m(Worker_DP0_TP3_EP3 pid=476)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-15T18:53:52.0647278Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-15T18:53:52.0657816Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-15T18:53:52.0668410Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-15T18:53:52.0678052Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-15T18:53:52.0688797Z [0;36m(Worker_DP0_TP2_EP2 pid=372)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-15T18:53:52.0698933Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-15T18:53:52.0709096Z [0;36m(Worker_DP1_TP1_EP9 pid=254)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-15T18:53:52.0719737Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-15T18:53:52.0730926Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-15T18:53:52.0739998Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-15T18:53:52.0749715Z [0;36m(Worker_DP0_TP3_EP3 pid=476)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-15T18:53:52.0759216Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-15T18:53:52.0770184Z [0;36m(Worker_DP0_TP1_EP1 pid=257)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-15T18:53:52.0780842Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-15T18:53:52.0791640Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-15T18:53:52.0801616Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-15T18:53:52.0811608Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-15T18:53:52.0821319Z [0;36m(Worker_DP1_TP1_EP9 pid=254)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-15T18:53:52.0831306Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-15T18:53:52.0841880Z [0;36m(Worker_DP0_TP4_EP4 pid=580)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-15T18:53:52.0851978Z [0;36m(Worker_DP0_TP2_EP2 pid=372)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-15T18:53:52.0862294Z [0;36m(Worker_DP1_TP2_EP10 pid=369)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-15T18:53:52.0872330Z [0;36m(Worker_DP0_TP6_EP6 pid=785)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-15T18:53:52.0882953Z [0;36m(Worker_DP0_TP0_EP0 pid=201)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-15T18:53:52.0893009Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-15T18:53:52.0903142Z [0;36m(Worker_DP1_TP7_EP15 pid=892)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-15T18:53:52.0913734Z [0;36m(Worker_DP1_TP0_EP8 pid=202)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-15T18:53:52.0930594Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-15T18:53:52.0936183Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-15T18:53:52.0947674Z [0;36m(Worker_DP0_TP4_EP4 pid=580)[0;0m INFO 02-15 18:53:52 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-15T18:53:57.0880171Z Traceback (most recent call last):
2026-02-15T18:53:57.0888481Z   File "/usr/local/python3.11.14/bin/vllm", line 7, in <module>
2026-02-15T18:53:57.0900416Z     sys.exit(main())
2026-02-15T18:53:57.0909204Z              ^^^^^^
2026-02-15T18:53:57.0920177Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/main.py", line 73, in main
2026-02-15T18:53:57.0930299Z     args.dispatch_function(args)
2026-02-15T18:53:57.0938899Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 108, in cmd
2026-02-15T18:53:57.0948068Z     run_multi_api_server(args)
2026-02-15T18:53:57.0958927Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 282, in run_multi_api_server
2026-02-15T18:53:57.0968633Z     wait_for_completion_or_failure(
2026-02-15T18:53:57.0977681Z   File "/vllm-workspace/vllm/vllm/v1/utils.py", line 277, in wait_for_completion_or_failure
2026-02-15T18:53:57.0987557Z     raise RuntimeError(
2026-02-15T18:53:57.0998443Z RuntimeError: Process ApiServer_0 (PID: 184) died with exit code 1
2026-02-15T18:53:57.1872517Z [ERROR] 2026-02-15-18:53:57 (PID:138, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
2026-02-15T18:53:57.4936137Z sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-15T18:53:59.1354891Z /usr/local/python3.11.14/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked shared_memory objects to clean up at shutdown
2026-02-15T18:53:59.1363016Z   warnings.warn('resource_tracker: There appear to be %d '
2026-02-15T18:54:03.7634581Z FAILED
2026-02-15T18:54:03.7643918Z 
2026-02-15T18:54:03.7656222Z =================================== FAILURES ===================================
2026-02-15T18:54:03.7664817Z _______________________________ test_multi_node ________________________________
2026-02-15T18:54:03.7674988Z 
2026-02-15T18:54:03.7685881Z     @pytest.mark.asyncio
2026-02-15T18:54:03.7695348Z     async def test_multi_node() -> None:
2026-02-15T18:54:03.7704256Z         config = MultiNodeConfigLoader.from_yaml()
2026-02-15T18:54:03.7714320Z     
2026-02-15T18:54:03.7724359Z         with ProxyLauncher(
2026-02-15T18:54:03.7734738Z                 nodes=config.nodes,
2026-02-15T18:54:03.7745125Z                 disagg_cfg=config.disagg_cfg,
2026-02-15T18:54:03.7754623Z                 envs=config.envs,
2026-02-15T18:54:03.7765361Z                 proxy_port=config.proxy_port,
2026-02-15T18:54:03.7775007Z                 cur_index=config.cur_index,
2026-02-15T18:54:03.7784058Z         ) as proxy:
2026-02-15T18:54:03.7793747Z     
2026-02-15T18:54:03.7803396Z >           with RemoteOpenAIServer(
2026-02-15T18:54:03.7813506Z                     model=config.model,
2026-02-15T18:54:03.7823028Z                     vllm_serve_args=config.server_cmd,
2026-02-15T18:54:03.7833698Z                     server_port=config.server_port,
2026-02-15T18:54:03.7843847Z                     server_host=config.master_ip,
2026-02-15T18:54:03.7853802Z                     env_dict=config.envs,
2026-02-15T18:54:03.7863727Z                     auto_port=False,
2026-02-15T18:54:03.7874615Z                     proxy_port=proxy.proxy_port,
2026-02-15T18:54:03.7885219Z                     disaggregated_prefill=config.disagg_cfg,
2026-02-15T18:54:03.7894774Z                     nodes_info=config.nodes,
2026-02-15T18:54:03.7904327Z                     max_wait_seconds=2800,
2026-02-15T18:54:03.7914494Z             ) as server:
2026-02-15T18:54:03.7923573Z 
2026-02-15T18:54:03.7933244Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py:21: 
2026-02-15T18:54:03.7942848Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-15T18:54:03.7952338Z tests/e2e/conftest.py:306: in __init__
2026-02-15T18:54:03.7961813Z     self._wait_for_multiple_servers(
2026-02-15T18:54:03.7971529Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-15T18:54:03.7980943Z 
2026-02-15T18:54:03.7991518Z self = <tests.e2e.conftest.RemoteOpenAIServer object at 0xffff15db4dd0>
2026-02-15T18:54:03.8001442Z targets = [('10.0.0.51', 'http://10.0.0.51:8080/health')], timeout = 2800
2026-02-15T18:54:03.8010948Z log_interval = 30.0
2026-02-15T18:54:03.8020039Z 
2026-02-15T18:54:03.8030692Z     def _wait_for_multiple_servers(self,
2026-02-15T18:54:03.8040831Z                                    targets,
2026-02-15T18:54:03.8050328Z                                    timeout: float,
2026-02-15T18:54:03.8059622Z                                    log_interval: float = 30.0):
2026-02-15T18:54:03.8068773Z         """
2026-02-15T18:54:03.8079381Z         targets: List[(node_ip, url)]
2026-02-15T18:54:03.8088588Z         log_interval
2026-02-15T18:54:03.8098227Z         """
2026-02-15T18:54:03.8107842Z         start = time.time()
2026-02-15T18:54:03.8117498Z         client = requests
2026-02-15T18:54:03.8162659Z     
2026-02-15T18:54:03.8162923Z         ready = {node_ip: False for node_ip, _ in targets}
2026-02-15T18:54:03.8163209Z     
2026-02-15T18:54:03.8163385Z         last_log_time = 0.0
2026-02-15T18:54:03.8167157Z     
2026-02-15T18:54:03.8176201Z         while True:
2026-02-15T18:54:03.8185572Z             now = time.time()
2026-02-15T18:54:03.8196501Z             all_ready = True
2026-02-15T18:54:03.8204263Z             should_log = (now - last_log_time) >= log_interval
2026-02-15T18:54:03.8213613Z     
2026-02-15T18:54:03.8223802Z             for node_ip, url in targets:
2026-02-15T18:54:03.8233735Z                 if ready[node_ip]:
2026-02-15T18:54:03.8243739Z                     continue
2026-02-15T18:54:03.8253600Z     
2026-02-15T18:54:03.8263365Z                 try:
2026-02-15T18:54:03.8273265Z                     resp = client.get(url)
2026-02-15T18:54:03.8283069Z                     if resp.status_code == 200:
2026-02-15T18:54:03.8293071Z                         ready[node_ip] = True
2026-02-15T18:54:03.8302784Z                         logger.info(f"[READY] Node {node_ip} is ready.")
2026-02-15T18:54:03.8312908Z                 except RequestException:
2026-02-15T18:54:03.8381684Z                     all_ready = False
2026-02-15T18:54:03.8382069Z                     if should_log:
2026-02-15T18:54:03.8382398Z                         logger.debug(f"[WAIT] {url}: connection failed")
2026-02-15T18:54:03.8382669Z     
2026-02-15T18:54:03.8383018Z                     # check unexpected exit
2026-02-15T18:54:03.8383276Z                     result = self._poll()
2026-02-15T18:54:03.8383585Z                     if result is not None and result != 0:
2026-02-15T18:54:03.8390662Z >                       raise RuntimeError(
2026-02-15T18:54:03.8400750Z                             f"Server at {node_ip} exited unexpectedly."
2026-02-15T18:54:03.8410473Z                         ) from None
2026-02-15T18:54:03.8420503Z E                       RuntimeError: Server at 10.0.0.51 exited unexpectedly.
2026-02-15T18:54:03.8430416Z 
2026-02-15T18:54:03.8439990Z tests/e2e/conftest.py:399: RuntimeError
2026-02-15T18:54:03.8450282Z =============================== warnings summary ===============================
2026-02-15T18:54:03.8459889Z <frozen importlib._bootstrap>:241
2026-02-15T18:54:03.8469618Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute
2026-02-15T18:54:03.8479807Z 
2026-02-15T18:54:03.8490258Z <frozen importlib._bootstrap>:241
2026-02-15T18:54:03.8500607Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute
2026-02-15T18:54:03.8511216Z 
2026-02-15T18:54:03.8519819Z ../../usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647
2026-02-15T18:54:03.8531018Z   /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-15T18:54:03.8539494Z     warnings.warn(
2026-02-15T18:54:03.8548857Z 
2026-02-15T18:54:03.8560319Z ../../usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8
2026-02-15T18:54:03.8571422Z   /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
2026-02-15T18:54:03.8580441Z     import pkg_resources
2026-02-15T18:54:03.8589256Z 
2026-02-15T18:54:03.8599775Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2026-02-15T18:54:03.8609532Z =========================== short test summary info ============================
2026-02-15T18:54:03.8619524Z FAILED tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node
2026-02-15T18:54:03.8628965Z ================== 1 failed, 4 warnings in 726.41s (0:12:06) ===================
2026-02-15T18:54:05.4982464Z [0;31mFAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml âœ— ERROR: Some tests failed, please check the error stack above for details. If this is insufficient to pinpoint the error, please download and review the logs of all other nodes from the job's summary.[0m
2026-02-15T18:54:05.6861969Z Cleaning up background log streams...
2026-02-15T18:54:05.7686157Z ##[error]Error: failed to run script step: Error: command terminated with non-zero exit code: command terminated with exit code 1
2026-02-15T18:54:05.7725414Z ##[error]Process completed with exit code 1.
2026-02-15T18:54:05.7979813Z ##[error]Executing the custom container implementation failed. Please contact your self hosted runner administrator.
2026-02-15T18:54:05.8757093Z ##[group]Run actions/upload-artifact@v6
2026-02-15T18:54:05.8757335Z with:
2026-02-15T18:54:05.8757607Z   name: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs
2026-02-15T18:54:05.8757925Z   path: /tmp/vllm*_logs.txt
2026-02-15T18:54:05.8758182Z   retention-days: 7
2026-02-15T18:54:05.8758411Z   if-no-files-found: warn
2026-02-15T18:54:05.8758612Z   compression-level: 6
2026-02-15T18:54:05.8758855Z   overwrite: false
2026-02-15T18:54:05.8759078Z   include-hidden-files: false
2026-02-15T18:54:05.8759281Z env:
2026-02-15T18:54:05.8759533Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-15T18:54:05.8759794Z ##[endgroup]
2026-02-15T18:54:05.8785901Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-15T18:54:05.8786729Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-15T18:54:05.8787028Z ##[endgroup]
2026-02-15T18:54:06.2506010Z (node:921) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-15T18:54:06.2507026Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-15T18:54:07.2276685Z With the provided path, there will be 1 file uploaded
2026-02-15T18:54:07.2281154Z Artifact name is valid!
2026-02-15T18:54:07.2281473Z Root directory input is valid!
2026-02-15T18:54:08.1691265Z Beginning upload of artifact content to blob storage
2026-02-15T18:54:09.1052347Z Uploaded bytes 12591
2026-02-15T18:54:09.3342385Z Finished uploading artifact content to blob storage!
2026-02-15T18:54:09.3343088Z SHA256 digest of uploaded artifact zip is 419b98a1308022dd92b15ce8eaa023c8a37c80959d1ee7d0edff08f69f7b3f94
2026-02-15T18:54:09.3343546Z Finalizing artifact upload
2026-02-15T18:54:10.1496063Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs.zip successfully finalized. Artifact ID 5517831193
2026-02-15T18:54:10.1496818Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs has been successfully uploaded! Final size is 12591 bytes. Artifact ID is 5517831193
2026-02-15T18:54:10.1499436Z Artifact download URL: https://github.com/vllm-project/vllm-ascend/actions/runs/22038904393/artifacts/5517831193
2026-02-15T18:54:11.8120810Z ##[group]Run kubectl get pods -n "$NAMESPACE" --ignore-not-found=true
2026-02-15T18:54:11.8121283Z [36;1mkubectl get pods -n "$NAMESPACE" --ignore-not-found=true[0m
2026-02-15T18:54:11.8121683Z [36;1mkubectl delete -f ./lws.yaml --ignore-not-found=true || true[0m
2026-02-15T18:54:11.8122248Z shell: bash -el {0}
2026-02-15T18:54:11.8122475Z env:
2026-02-15T18:54:11.8122732Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-15T18:54:11.8123066Z ##[endgroup]
2026-02-15T18:54:11.8216403Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-15T18:54:11.8217229Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-15T18:54:11.8217518Z ##[endgroup]
2026-02-15T18:54:12.1718773Z (node:1083) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-15T18:54:12.1719614Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-15T18:54:12.8984115Z NAME                                             READY   STATUS    RESTARTS     AGE
2026-02-15T18:54:12.8984603Z linux-aarch64-a3-0-n4cwm-runner-9mxjl            1/1     Running   0            13m
2026-02-15T18:54:12.8985083Z linux-aarch64-a3-0-n4cwm-runner-9mxjl-workflow   1/1     Running   0            13m
2026-02-15T18:54:12.8985477Z vllm-0                                           1/1     Running   1 (7s ago)   13m
2026-02-15T18:54:12.8985815Z vllm-0-1                                         1/1     Running   0            13m
2026-02-15T18:54:12.9680841Z leaderworkerset.leaderworkerset.x-k8s.io "vllm" deleted from vllm-project namespace
2026-02-15T18:54:12.9878791Z service "vllm-leader" deleted from vllm-project namespace
2026-02-15T18:54:13.6258330Z Post job cleanup.
2026-02-15T18:54:13.6283457Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-15T18:54:13.6284246Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-15T18:54:13.6284613Z ##[endgroup]
2026-02-15T18:54:13.9822280Z (node:1207) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-15T18:54:13.9823090Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-15T18:54:14.6331076Z [command]/usr/bin/git version
2026-02-15T18:54:14.6526719Z git version 2.34.1
2026-02-15T18:54:14.6557333Z Copying '/root/.gitconfig' to '/__w/_temp/bc9af593-2792-4866-b03a-b03a9da37dd6/.gitconfig'
2026-02-15T18:54:14.6564895Z Temporarily overriding HOME='/__w/_temp/bc9af593-2792-4866-b03a-b03a9da37dd6' before making global git config changes
2026-02-15T18:54:14.6565679Z Adding repository directory to the temporary git global config as a safe directory
2026-02-15T18:54:14.6569000Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-15T18:54:14.6606212Z Removing SSH command configuration
2026-02-15T18:54:14.6611143Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-15T18:54:14.6664625Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-15T18:54:14.7332682Z Removing HTTP extra header
2026-02-15T18:54:14.7333143Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-15T18:54:14.7334248Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-15T18:54:14.7351295Z Removing includeIf entries pointing to credentials config files
2026-02-15T18:54:14.7355385Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-15T18:54:14.7373907Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-15T18:54:14.7374233Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-15T18:54:14.7374523Z includeif.gitdir:/github/workspace/.git.path
2026-02-15T18:54:14.7374784Z includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-15T18:54:14.7381405Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-15T18:54:14.7399843Z /__w/_temp/git-credentials-2ba3eb75-c086-4bb9-9b51-bf56582906fb.config
2026-02-15T18:54:14.7408666Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-2ba3eb75-c086-4bb9-9b51-bf56582906fb.config
2026-02-15T18:54:14.7444707Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-15T18:54:14.7463729Z /__w/_temp/git-credentials-2ba3eb75-c086-4bb9-9b51-bf56582906fb.config
2026-02-15T18:54:14.7470079Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-2ba3eb75-c086-4bb9-9b51-bf56582906fb.config
2026-02-15T18:54:14.7504767Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git.path
2026-02-15T18:54:14.7523114Z /github/runner_temp/git-credentials-2ba3eb75-c086-4bb9-9b51-bf56582906fb.config
2026-02-15T18:54:14.7531494Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-2ba3eb75-c086-4bb9-9b51-bf56582906fb.config
2026-02-15T18:54:14.7563171Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-15T18:54:14.7581085Z /github/runner_temp/git-credentials-2ba3eb75-c086-4bb9-9b51-bf56582906fb.config
2026-02-15T18:54:14.7587465Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-2ba3eb75-c086-4bb9-9b51-bf56582906fb.config
2026-02-15T18:54:14.7615724Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-15T18:54:14.7790390Z Removing credentials config '/__w/_temp/git-credentials-2ba3eb75-c086-4bb9-9b51-bf56582906fb.config'
2026-02-15T18:54:33.2974662Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-15T18:54:33.2975458Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-15T18:54:33.2975741Z ##[endgroup]
2026-02-15T18:54:33.6833805Z Cleaning up orphan processes
