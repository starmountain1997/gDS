# Run ID: 21994466849
# Commit: e2175d9c7e62b437391dfee996b1375674ba7c18
# Job: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
# Date: 2026-02-13
============================================================

ï»¿2026-02-13T18:22:08.8031385Z Current runner version: '2.330.0'
2026-02-13T18:22:08.8036171Z Runner name: 'linux-aarch64-a3-0-n4cwm-runner-jbfld'
2026-02-13T18:22:08.8036937Z Runner group name: 'Default'
2026-02-13T18:22:08.8037632Z Machine name: 'linux-aarch64-a3-0-n4cwm-runner-jbfld'
2026-02-13T18:22:08.8041113Z ##[group]GITHUB_TOKEN Permissions
2026-02-13T18:22:08.8043202Z Actions: write
2026-02-13T18:22:08.8043600Z ArtifactMetadata: write
2026-02-13T18:22:08.8044060Z Attestations: write
2026-02-13T18:22:08.8044456Z Checks: write
2026-02-13T18:22:08.8044792Z Contents: write
2026-02-13T18:22:08.8045205Z Deployments: write
2026-02-13T18:22:08.8045641Z Discussions: write
2026-02-13T18:22:08.8046008Z Issues: write
2026-02-13T18:22:08.8046399Z Metadata: read
2026-02-13T18:22:08.8046739Z Models: read
2026-02-13T18:22:08.8047094Z Packages: write
2026-02-13T18:22:08.8047535Z Pages: write
2026-02-13T18:22:08.8047883Z PullRequests: write
2026-02-13T18:22:08.8048271Z RepositoryProjects: write
2026-02-13T18:22:08.8048817Z SecurityEvents: write
2026-02-13T18:22:08.8049267Z Statuses: write
2026-02-13T18:22:08.8049631Z ##[endgroup]
2026-02-13T18:22:08.8051507Z Secret source: Actions
2026-02-13T18:22:08.8052225Z Prepare workflow directory
2026-02-13T18:22:08.8589793Z Prepare all required actions
2026-02-13T18:22:08.8620808Z Getting action download info
2026-02-13T18:22:09.9461463Z Download action repository 'actions/checkout@v6' (SHA:de0fac2e4500dabe0009e67214ff5f5447ce83dd)
2026-02-13T18:22:14.2978604Z Download action repository 'actions/upload-artifact@v6' (SHA:b7c566a772e6b6bfb58ed0dc250532a479d7789f)
2026-02-13T18:22:22.7153836Z Uses: vllm-project/vllm-ascend/.github/workflows/_e2e_nightly_multi_node.yaml@refs/heads/main (e2175d9c7e62b437391dfee996b1375674ba7c18)
2026-02-13T18:22:22.7157177Z ##[group] Inputs
2026-02-13T18:22:22.7157514Z   soc_version: a3
2026-02-13T18:22:22.7157762Z   runner: linux-aarch64-a3-0
2026-02-13T18:22:22.7158198Z   image: swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3
2026-02-13T18:22:22.7158675Z   config_file_path: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-13T18:22:22.7158981Z   replicas: 1
2026-02-13T18:22:22.7159186Z   size: 2
2026-02-13T18:22:22.7159405Z   vllm_version: v0.15.0
2026-02-13T18:22:22.7159761Z   vllm_ascend_remote_url: https://github.com/vllm-project/vllm-ascend.git
2026-02-13T18:22:22.7160112Z   vllm_ascend_ref: main
2026-02-13T18:22:22.7160360Z ##[endgroup]
2026-02-13T18:22:22.7160892Z Complete job name: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-13T18:22:22.7659060Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-13T18:22:22.7661434Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-13T18:22:22.7661946Z ##[endgroup]
2026-02-13T18:22:38.3392235Z (node:71) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-13T18:22:38.3393126Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-13T18:22:40.1917014Z ##[group]Run # Decode and save kubeconfig
2026-02-13T18:22:40.1917522Z [36;1m# Decode and save kubeconfig[0m
2026-02-13T18:22:40.1949758Z [36;1mecho "***" | base64 -d > "$KUBECONFIG"[0m
2026-02-13T18:22:40.1950301Z shell: bash -el {0}
2026-02-13T18:22:40.1950512Z ##[endgroup]
2026-02-13T18:22:40.2061171Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-13T18:22:40.2062455Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-13T18:22:40.2062855Z ##[endgroup]
2026-02-13T18:22:40.5601832Z (node:402) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-13T18:22:40.5602788Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-13T18:22:41.4209983Z ##[group]Run actions/checkout@v6
2026-02-13T18:22:41.4210304Z with:
2026-02-13T18:22:41.4210681Z   repository: vllm-project/vllm-ascend
2026-02-13T18:22:41.4211391Z   token: ***
2026-02-13T18:22:41.4211592Z   ssh-strict: true
2026-02-13T18:22:41.4211845Z   ssh-user: git
2026-02-13T18:22:41.4212240Z   persist-credentials: true
2026-02-13T18:22:41.4212477Z   clean: true
2026-02-13T18:22:41.4212738Z   sparse-checkout-cone-mode: true
2026-02-13T18:22:41.4213022Z   fetch-depth: 1
2026-02-13T18:22:41.4213214Z   fetch-tags: false
2026-02-13T18:22:41.4213461Z   show-progress: true
2026-02-13T18:22:41.4213664Z   lfs: false
2026-02-13T18:22:41.4213938Z   submodules: false
2026-02-13T18:22:41.4214189Z   set-safe-directory: true
2026-02-13T18:22:41.4214414Z ##[endgroup]
2026-02-13T18:22:41.4256147Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-13T18:22:41.4257092Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-13T18:22:41.4257400Z ##[endgroup]
2026-02-13T18:22:41.7763027Z (node:432) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-13T18:22:41.7764018Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-13T18:22:42.3295014Z Syncing repository: vllm-project/vllm-ascend
2026-02-13T18:22:42.3296133Z ##[group]Getting Git version info
2026-02-13T18:22:42.3296500Z Working directory is '/__w/vllm-ascend/vllm-ascend'
2026-02-13T18:22:42.3297112Z [command]/usr/bin/git version
2026-02-13T18:22:42.3297437Z git version 2.34.1
2026-02-13T18:22:42.3302527Z ##[endgroup]
2026-02-13T18:22:42.3310448Z Copying '/root/.gitconfig' to '/__w/_temp/885f640e-bfc2-47cf-95e1-b2c5975ea201/.gitconfig'
2026-02-13T18:22:42.3322491Z Temporarily overriding HOME='/__w/_temp/885f640e-bfc2-47cf-95e1-b2c5975ea201' before making global git config changes
2026-02-13T18:22:42.3323106Z Adding repository directory to the temporary git global config as a safe directory
2026-02-13T18:22:42.3327218Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-13T18:22:42.3364893Z Deleting the contents of '/__w/vllm-ascend/vllm-ascend'
2026-02-13T18:22:42.3367658Z ##[group]Initializing the repository
2026-02-13T18:22:42.3371681Z [command]/usr/bin/git init /__w/vllm-ascend/vllm-ascend
2026-02-13T18:22:42.3466403Z hint: Using 'master' as the name for the initial branch. This default branch name
2026-02-13T18:22:42.3466907Z hint: is subject to change. To configure the initial branch name to use in all
2026-02-13T18:22:42.3467311Z hint: of your new repositories, which will suppress this warning, call:
2026-02-13T18:22:42.3467707Z hint: 
2026-02-13T18:22:42.3468031Z hint: 	git config --global init.defaultBranch <name>
2026-02-13T18:22:42.3468295Z hint: 
2026-02-13T18:22:42.3468570Z hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and
2026-02-13T18:22:42.3469067Z hint: 'development'. The just-created branch can be renamed via this command:
2026-02-13T18:22:42.3469422Z hint: 
2026-02-13T18:22:42.3469632Z hint: 	git branch -m <name>
2026-02-13T18:22:42.3473045Z Initialized empty Git repository in /__w/vllm-ascend/vllm-ascend/.git/
2026-02-13T18:22:42.3481893Z [command]/usr/bin/git remote add origin https://github.com/vllm-project/vllm-ascend
2026-02-13T18:22:42.3520900Z ##[endgroup]
2026-02-13T18:22:42.3521389Z ##[group]Disabling automatic garbage collection
2026-02-13T18:22:42.3524586Z [command]/usr/bin/git config --local gc.auto 0
2026-02-13T18:22:42.3554636Z ##[endgroup]
2026-02-13T18:22:42.3554987Z ##[group]Setting up auth
2026-02-13T18:22:42.3556043Z Removing SSH command configuration
2026-02-13T18:22:42.3561436Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-13T18:22:42.3590239Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-13T18:22:42.3782093Z Removing HTTP extra header
2026-02-13T18:22:42.3785359Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-13T18:22:42.3810943Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-13T18:22:42.4004360Z Removing includeIf entries pointing to credentials config files
2026-02-13T18:22:42.4008677Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-13T18:22:42.4035883Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-13T18:22:42.4222870Z [command]/usr/bin/git config --file /__w/_temp/git-credentials-0cd6c63f-61a2-4d77-8ad7-9a62b26dae91.config http.https://github.com/.extraheader AUTHORIZATION: basic ***
2026-02-13T18:22:42.4261720Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-0cd6c63f-61a2-4d77-8ad7-9a62b26dae91.config
2026-02-13T18:22:42.4288005Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-0cd6c63f-61a2-4d77-8ad7-9a62b26dae91.config
2026-02-13T18:22:42.4314564Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-0cd6c63f-61a2-4d77-8ad7-9a62b26dae91.config
2026-02-13T18:22:42.4341160Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-0cd6c63f-61a2-4d77-8ad7-9a62b26dae91.config
2026-02-13T18:22:42.4368249Z ##[endgroup]
2026-02-13T18:22:42.4368745Z ##[group]Fetching the repository
2026-02-13T18:22:42.4375919Z [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --no-recurse-submodules --depth=1 origin +e2175d9c7e62b437391dfee996b1375674ba7c18:refs/remotes/origin/main
2026-02-13T18:22:44.0920136Z From https://gh-proxy.test.osinfra.cn/https://github.com/vllm-project/vllm-ascend
2026-02-13T18:22:44.0920723Z  * [new ref]         e2175d9c7e62b437391dfee996b1375674ba7c18 -> origin/main
2026-02-13T18:22:44.0946680Z [command]/usr/bin/git branch --list --remote origin/main
2026-02-13T18:22:44.0971959Z   origin/main
2026-02-13T18:22:44.0981787Z [command]/usr/bin/git rev-parse refs/remotes/origin/main
2026-02-13T18:22:44.1004507Z e2175d9c7e62b437391dfee996b1375674ba7c18
2026-02-13T18:22:44.1009942Z ##[endgroup]
2026-02-13T18:22:44.1010291Z ##[group]Determining the checkout info
2026-02-13T18:22:44.1011855Z ##[endgroup]
2026-02-13T18:22:44.1015977Z [command]/usr/bin/git sparse-checkout disable
2026-02-13T18:22:44.1062824Z [command]/usr/bin/git config --local --unset-all extensions.worktreeConfig
2026-02-13T18:22:44.1106402Z ##[group]Checking out the ref
2026-02-13T18:22:44.1106786Z [command]/usr/bin/git checkout --progress --force -B main refs/remotes/origin/main
2026-02-13T18:22:44.1955920Z Switched to a new branch 'main'
2026-02-13T18:22:44.1957258Z Branch 'main' set up to track remote branch 'main' from 'origin'.
2026-02-13T18:22:44.1966955Z ##[endgroup]
2026-02-13T18:22:44.2014519Z [command]/usr/bin/git log -1 --format=%H
2026-02-13T18:22:44.2038281Z e2175d9c7e62b437391dfee996b1375674ba7c18
2026-02-13T18:22:44.8534239Z ##[group]Run # prepare for lws entrypoint scripts
2026-02-13T18:22:44.8534640Z [36;1m# prepare for lws entrypoint scripts[0m
2026-02-13T18:22:44.8535063Z [36;1minstall -D tests/e2e/nightly/multi_node/scripts/run.sh /root/.cache/tests/run.sh[0m
2026-02-13T18:22:44.8535527Z shell: bash -el {0}
2026-02-13T18:22:44.8535780Z ##[endgroup]
2026-02-13T18:22:44.8650853Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-13T18:22:44.8651792Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-13T18:22:44.8652290Z ##[endgroup]
2026-02-13T18:22:45.2137427Z (node:473) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-13T18:22:45.2138192Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-13T18:22:46.3720215Z ##[group]Run set -euo pipefail
2026-02-13T18:22:46.3720592Z [36;1mset -euo pipefail[0m
2026-02-13T18:22:46.3720863Z [36;1m[0m
2026-02-13T18:22:46.3721081Z [36;1mCRD_NAME="${CRD_NAME:-vllm}"[0m
2026-02-13T18:22:46.3721376Z [36;1mTIMEOUT=${TIMEOUT:-120}[0m
2026-02-13T18:22:46.3721650Z [36;1mSLEEP_INTERVAL=2[0m
2026-02-13T18:22:46.3721877Z [36;1m[0m
2026-02-13T18:22:46.3722364Z [36;1mecho "Deleting leaderworkerset [$CRD_NAME] in namespace [$NAMESPACE]..."[0m
2026-02-13T18:22:46.3722909Z [36;1mkubectl delete leaderworkerset "$CRD_NAME" -n "$NAMESPACE" --ignore-not-found[0m
2026-02-13T18:22:46.3723304Z [36;1m[0m
2026-02-13T18:22:46.3723641Z [36;1mecho "Waiting for all pods starting with 'vllm' to be deleted..."[0m
2026-02-13T18:22:46.3724006Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-13T18:22:46.3724220Z [36;1m[0m
2026-02-13T18:22:46.3724524Z [36;1mwhile true; do[0m
2026-02-13T18:22:46.3724768Z [36;1m  NOW=$(date +%s)[0m
2026-02-13T18:22:46.3725118Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-13T18:22:46.3725371Z [36;1m[0m
2026-02-13T18:22:46.3725592Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-13T18:22:46.3726009Z [36;1m    echo "Timeout reached ($TIMEOUT seconds), some pods still exist:"[0m
2026-02-13T18:22:46.3726415Z [36;1m    kubectl get pods -n "$NAMESPACE" | grep '^vllm' || true[0m
2026-02-13T18:22:46.3726717Z [36;1m    exit 1[0m
2026-02-13T18:22:46.3726961Z [36;1m  fi[0m
2026-02-13T18:22:46.3727173Z [36;1m[0m
2026-02-13T18:22:46.3727612Z [36;1m  PODS_EXIST=$(kubectl get pods -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null | tr ' ' '\n' | grep '^vllm' || true)[0m
2026-02-13T18:22:46.3728190Z [36;1m[0m
2026-02-13T18:22:46.3728414Z [36;1m  if [[ -z "$PODS_EXIST" ]]; then[0m
2026-02-13T18:22:46.3728659Z [36;1m    echo "All vllm pods deleted."[0m
2026-02-13T18:22:46.3728982Z [36;1m    break[0m
2026-02-13T18:22:46.3729203Z [36;1m  else[0m
2026-02-13T18:22:46.3729441Z [36;1m    echo "Waiting for pods to be deleted: $PODS_EXIST"[0m
2026-02-13T18:22:46.3729814Z [36;1m    sleep $SLEEP_INTERVAL[0m
2026-02-13T18:22:46.3730112Z [36;1m  fi[0m
2026-02-13T18:22:46.3730330Z [36;1mdone[0m
2026-02-13T18:22:46.3730743Z shell: bash -el {0}
2026-02-13T18:22:46.3730938Z ##[endgroup]
2026-02-13T18:22:46.3901420Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-13T18:22:46.3902453Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-13T18:22:46.3902714Z ##[endgroup]
2026-02-13T18:22:46.7398209Z (node:527) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-13T18:22:46.7398932Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-13T18:22:47.2259827Z Deleting leaderworkerset [vllm] in namespace [vllm-project]...
2026-02-13T18:22:47.2973631Z Waiting for all pods starting with 'vllm' to be deleted...
2026-02-13T18:22:47.3608890Z Waiting for pods to be deleted: vllm-0-1
2026-02-13T18:22:49.4306556Z Waiting for pods to be deleted: vllm-0-1
2026-02-13T18:22:51.4951648Z Waiting for pods to be deleted: vllm-0-1
2026-02-13T18:22:53.5615848Z Waiting for pods to be deleted: vllm-0-1
2026-02-13T18:22:55.6271238Z Waiting for pods to be deleted: vllm-0-1
2026-02-13T18:22:57.6964398Z Waiting for pods to be deleted: vllm-0-1
2026-02-13T18:22:59.7595869Z Waiting for pods to be deleted: vllm-0-1
2026-02-13T18:23:01.8585859Z Waiting for pods to be deleted: vllm-0-1
2026-02-13T18:23:03.9268275Z Waiting for pods to be deleted: vllm-0-1
2026-02-13T18:23:06.0520555Z Waiting for pods to be deleted: vllm-0-1
2026-02-13T18:23:08.1187774Z All vllm pods deleted.
2026-02-13T18:23:08.5778767Z ##[group]Run set -e
2026-02-13T18:23:08.5779096Z [36;1mset -e[0m
2026-02-13T18:23:08.5779347Z [36;1m[0m
2026-02-13T18:23:08.5779564Z [36;1msize="2"[0m
2026-02-13T18:23:08.5779785Z [36;1mreplicas="1"[0m
2026-02-13T18:23:08.5780346Z [36;1mimage="swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3"[0m
2026-02-13T18:23:08.5780835Z [36;1mconfig_file_path="DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-13T18:23:08.5781221Z [36;1mfail_tag=FAIL_TAG_"DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-13T18:23:08.5781658Z [36;1mecho "FAIL_TAG=${fail_tag}" >> "$GITHUB_ENV"[0m
2026-02-13T18:23:08.5781945Z [36;1m[0m
2026-02-13T18:23:08.5782340Z [36;1mrequired_params=("size" "replicas" "image" "config_file_path")[0m
2026-02-13T18:23:08.5782737Z [36;1mfor param in "${required_params[@]}"; do[0m
2026-02-13T18:23:08.5783033Z [36;1m  if [ -z "${!param}" ]; then[0m
2026-02-13T18:23:08.5783452Z [36;1m    echo "Error: Parameter '$param' is required but empty"[0m
2026-02-13T18:23:08.5783740Z [36;1m    exit 1[0m
2026-02-13T18:23:08.5784222Z [36;1m  fi[0m
2026-02-13T18:23:08.5784449Z [36;1mdone[0m
2026-02-13T18:23:08.5784662Z [36;1m[0m
2026-02-13T18:23:08.5784842Z [36;1mif [ "a3" = "a3" ]; then[0m
2026-02-13T18:23:08.5785152Z [36;1m  npu_per_node=16[0m
2026-02-13T18:23:08.5785508Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws.yaml.jinja2"[0m
2026-02-13T18:23:08.5785821Z [36;1melse[0m
2026-02-13T18:23:08.5786147Z [36;1m  npu_per_node=8[0m
2026-02-13T18:23:08.5786501Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws-a2.yaml.jinja2"[0m
2026-02-13T18:23:08.5786826Z [36;1mfi[0m
2026-02-13T18:23:08.5787051Z [36;1m[0m
2026-02-13T18:23:08.5787249Z [36;1mjinja2 $TEMPLATE_FILE \[0m
2026-02-13T18:23:08.5787507Z [36;1m  -D size="$size" \[0m
2026-02-13T18:23:08.5787830Z [36;1m  -D replicas="$replicas" \[0m
2026-02-13T18:23:08.5788074Z [36;1m  -D image="$image" \[0m
2026-02-13T18:23:08.5788349Z [36;1m  -D config_file_path="$config_file_path" \[0m
2026-02-13T18:23:08.5788740Z [36;1m  -D npu_per_node="$npu_per_node" \[0m
2026-02-13T18:23:08.5788992Z [36;1m  -D fail_tag="$fail_tag" \[0m
2026-02-13T18:23:08.5789255Z [36;1m  --outfile lws.yaml[0m
2026-02-13T18:23:08.5789525Z [36;1m[0m
2026-02-13T18:23:08.5789722Z [36;1mkubectl apply -f ./lws.yaml[0m
2026-02-13T18:23:08.5790106Z shell: bash -el {0}
2026-02-13T18:23:08.5790354Z ##[endgroup]
2026-02-13T18:23:08.5866334Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-13T18:23:08.5867230Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-13T18:23:08.5867534Z ##[endgroup]
2026-02-13T18:23:08.9439824Z (node:597) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-13T18:23:08.9440564Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-13T18:23:09.8899455Z leaderworkerset.leaderworkerset.x-k8s.io/vllm created
2026-02-13T18:23:09.9174512Z service/vllm-leader created
2026-02-13T18:23:10.3237992Z ##[group]Run POD_PREFIX="${POD_PREFIX:-vllm-0}"
2026-02-13T18:23:10.3238423Z [36;1mPOD_PREFIX="${POD_PREFIX:-vllm-0}"[0m
2026-02-13T18:23:10.3238727Z [36;1mSIZE="2"[0m
2026-02-13T18:23:10.3239017Z [36;1mTIMEOUT=1200  # default timeout 20 minutes[0m
2026-02-13T18:23:10.3239282Z [36;1m[0m
2026-02-13T18:23:10.3239662Z [36;1mecho "Waiting for Pods in namespace [$NAMESPACE] to become Running and Ready (timeout ${TIMEOUT}s)..."[0m
2026-02-13T18:23:10.3240106Z [36;1m[0m
2026-02-13T18:23:10.3240301Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-13T18:23:10.3240555Z [36;1m[0m
2026-02-13T18:23:10.3240784Z [36;1mwhile true; do[0m
2026-02-13T18:23:10.3241001Z [36;1m  NOW=$(date +%s)[0m
2026-02-13T18:23:10.3241271Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-13T18:23:10.3241579Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-13T18:23:10.3241870Z [36;1m    echo "Timeout reached after ${ELAPSED}s"[0m
2026-02-13T18:23:10.3242424Z [36;1m    echo "Dumping pod status for debugging:"[0m
2026-02-13T18:23:10.3242712Z [36;1m    kubectl get pods -n "$NAMESPACE"[0m
2026-02-13T18:23:10.3243047Z [36;1m    kubectl describe pod "$LEADER_POD" -n "$NAMESPACE"[0m
2026-02-13T18:23:10.3243607Z [36;1m    exit 1[0m
2026-02-13T18:23:10.3243807Z [36;1m  fi[0m
2026-02-13T18:23:10.3244014Z [36;1m[0m
2026-02-13T18:23:10.3244228Z [36;1m  # 1) check follower pods[0m
2026-02-13T18:23:10.3244508Z [36;1m  ALL_FOLLOWERS_READY=true[0m
2026-02-13T18:23:10.3244770Z [36;1m  for ((i=1; i<SIZE; i++)); do[0m
2026-02-13T18:23:10.3245130Z [36;1m    POD="${POD_PREFIX}-${i}"[0m
2026-02-13T18:23:10.3245591Z [36;1m    PHASE=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-13T18:23:10.3246198Z [36;1m    READY=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-13T18:23:10.3246728Z [36;1m[0m
2026-02-13T18:23:10.3246975Z [36;1m    echo "Follower [$POD] phase=$PHASE ready=$READY"[0m
2026-02-13T18:23:10.3247261Z [36;1m[0m
2026-02-13T18:23:10.3247503Z [36;1m    if [[ "$PHASE" != "Running" || "$READY" != "true" ]]; then[0m
2026-02-13T18:23:10.3247862Z [36;1m      echo "Follower [$POD] not Ready yet..."[0m
2026-02-13T18:23:10.3248150Z [36;1m      ALL_FOLLOWERS_READY=false[0m
2026-02-13T18:23:10.3248401Z [36;1m      break[0m
2026-02-13T18:23:10.3248627Z [36;1m    fi[0m
2026-02-13T18:23:10.3248965Z [36;1m  done[0m
2026-02-13T18:23:10.3249157Z [36;1m[0m
2026-02-13T18:23:10.3249381Z [36;1m  # 2) check leader pod[0m
2026-02-13T18:23:10.3249828Z [36;1m  LEADER_PHASE=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-13T18:23:10.3250489Z [36;1m  LEADER_READY=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-13T18:23:10.3251037Z [36;1m[0m
2026-02-13T18:23:10.3251329Z [36;1m  echo "Leader [$LEADER_POD] phase=$LEADER_PHASE ready=$LEADER_READY"[0m
2026-02-13T18:23:10.3251664Z [36;1m[0m
2026-02-13T18:23:10.3251944Z [36;1m  if [[ "$LEADER_PHASE" != "Running" || "$LEADER_READY" != "true" ]]; then[0m
2026-02-13T18:23:10.3252420Z [36;1m    echo "Leader not Ready yet..."[0m
2026-02-13T18:23:10.3252712Z [36;1m    ALL_FOLLOWERS_READY=false[0m
2026-02-13T18:23:10.3252968Z [36;1m  fi[0m
2026-02-13T18:23:10.3253249Z [36;1m[0m
2026-02-13T18:23:10.3253482Z [36;1m  if [[ "$ALL_FOLLOWERS_READY" == "true" ]]; then[0m
2026-02-13T18:23:10.3253876Z [36;1m    echo "All follower pods and leader pod are Running and Ready â€” continuing."[0m
2026-02-13T18:23:10.3254223Z [36;1m    break[0m
2026-02-13T18:23:10.3254439Z [36;1m  fi[0m
2026-02-13T18:23:10.3254643Z [36;1m[0m
2026-02-13T18:23:10.3254819Z [36;1m  sleep 2[0m
2026-02-13T18:23:10.3255266Z [36;1mdone[0m
2026-02-13T18:23:10.3255613Z shell: bash -el {0}
2026-02-13T18:23:10.3255812Z env:
2026-02-13T18:23:10.3256227Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-13T18:23:10.3256613Z ##[endgroup]
2026-02-13T18:23:10.3388062Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-13T18:23:10.3388860Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-13T18:23:10.3389251Z ##[endgroup]
2026-02-13T18:23:10.6961460Z (node:676) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-13T18:23:10.6962275Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-13T18:23:11.2270108Z Waiting for Pods in namespace [vllm-project] to become Running and Ready (timeout 1200s)...
2026-02-13T18:23:11.3424049Z Follower [vllm-0-1] phase=Pending ready=
2026-02-13T18:23:11.3424274Z Follower [vllm-0-1] not Ready yet...
2026-02-13T18:23:11.4560422Z Leader [vllm-0] phase=Pending ready=
2026-02-13T18:23:11.4560653Z Leader not Ready yet...
2026-02-13T18:23:13.5759994Z Follower [vllm-0-1] phase=Pending ready=
2026-02-13T18:23:13.5760298Z Follower [vllm-0-1] not Ready yet...
2026-02-13T18:23:13.7100467Z Leader [vllm-0] phase=Pending ready=
2026-02-13T18:23:13.7100794Z Leader not Ready yet...
2026-02-13T18:23:15.8224248Z Follower [vllm-0-1] phase=Pending ready=
2026-02-13T18:23:15.8224529Z Follower [vllm-0-1] not Ready yet...
2026-02-13T18:23:15.9329843Z Leader [vllm-0] phase=Pending ready=
2026-02-13T18:23:15.9330092Z Leader not Ready yet...
2026-02-13T18:23:18.0498002Z Follower [vllm-0-1] phase=Pending ready=
2026-02-13T18:23:18.0498274Z Follower [vllm-0-1] not Ready yet...
2026-02-13T18:23:18.1672618Z Leader [vllm-0] phase=Pending ready=
2026-02-13T18:23:18.1672849Z Leader not Ready yet...
2026-02-13T18:23:20.2833074Z Follower [vllm-0-1] phase=Pending ready=
2026-02-13T18:23:20.2833345Z Follower [vllm-0-1] not Ready yet...
2026-02-13T18:23:20.3949650Z Leader [vllm-0] phase=Pending ready=
2026-02-13T18:23:20.3949922Z Leader not Ready yet...
2026-02-13T18:23:22.5113843Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-13T18:23:22.5114132Z Follower [vllm-0-1] not Ready yet...
2026-02-13T18:23:22.6325777Z Leader [vllm-0] phase=Pending ready=false
2026-02-13T18:23:22.6326032Z Leader not Ready yet...
2026-02-13T18:23:24.7545390Z Follower [vllm-0-1] phase=Running ready=true
2026-02-13T18:23:24.8759590Z Leader [vllm-0] phase=Running ready=true
2026-02-13T18:23:24.8760161Z All follower pods and leader pod are Running and Ready â€” continuing.
2026-02-13T18:23:25.2996781Z ##[group]Run set -euo pipefail
2026-02-13T18:23:25.2997030Z [36;1mset -euo pipefail[0m
2026-02-13T18:23:25.2997204Z [36;1m[0m
2026-02-13T18:23:25.2997338Z [36;1msize="2"[0m
2026-02-13T18:23:25.2997489Z [36;1mpids=()[0m
2026-02-13T18:23:25.2997628Z [36;1m[0m
2026-02-13T18:23:25.2997755Z [36;1mcleanup() {[0m
2026-02-13T18:23:25.2997957Z [36;1m  echo "Cleaning up background log streams..."[0m
2026-02-13T18:23:25.2998194Z [36;1m  for pid in "${pids[@]}"; do[0m
2026-02-13T18:23:25.2998406Z [36;1m    kill "$pid" 2>/dev/null || true[0m
2026-02-13T18:23:25.2998600Z [36;1m  done[0m
2026-02-13T18:23:25.2998733Z [36;1m}[0m
2026-02-13T18:23:25.2998881Z [36;1mtrap cleanup EXIT[0m
2026-02-13T18:23:25.2999046Z [36;1m[0m
2026-02-13T18:23:25.2999207Z [36;1mfor i in $(seq 1 $((size - 1))); do[0m
2026-02-13T18:23:25.2999412Z [36;1m  POD="vllm-0-${i}"[0m
2026-02-13T18:23:25.2999571Z [36;1m[0m
2026-02-13T18:23:25.2999792Z [36;1m  echo "==== Collecting logs from worker pod: $POD ===="[0m
2026-02-13T18:23:25.3000068Z [36;1m  kubectl logs -f "$POD" -n "$NAMESPACE" \[0m
2026-02-13T18:23:25.3000301Z [36;1m    > "/tmp/${POD}_logs.txt" 2>&1 &[0m
2026-02-13T18:23:25.3000594Z [36;1m[0m
2026-02-13T18:23:25.3000729Z [36;1m  pids+=($!)[0m
2026-02-13T18:23:25.3000886Z [36;1mdone[0m
2026-02-13T18:23:25.3001021Z [36;1m[0m
2026-02-13T18:23:25.3001212Z [36;1mecho "==== Streaming logs from leader pod: $LEADER_POD ===="[0m
2026-02-13T18:23:25.3001493Z [36;1mecho "Looking for logs containing: $FAIL_TAG"[0m
2026-02-13T18:23:25.3001697Z [36;1m[0m
2026-02-13T18:23:25.3001927Z [36;1mkubectl logs -f "$LEADER_POD" -n "$NAMESPACE" | while IFS= read -r line; do[0m
2026-02-13T18:23:25.3002327Z [36;1m  echo "$line"[0m
2026-02-13T18:23:25.3002531Z [36;1m  if echo "$line" | grep -q "$FAIL_TAG"; then[0m
2026-02-13T18:23:25.3002745Z [36;1m    exit 1[0m
2026-02-13T18:23:25.3002893Z [36;1m  fi[0m
2026-02-13T18:23:25.3003025Z [36;1mdone[0m
2026-02-13T18:23:25.3003315Z shell: bash -el {0}
2026-02-13T18:23:25.3003463Z env:
2026-02-13T18:23:25.3003649Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-13T18:23:25.3003880Z ##[endgroup]
2026-02-13T18:23:25.3083589Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-13T18:23:25.3084352Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-13T18:23:25.3084615Z ##[endgroup]
2026-02-13T18:23:25.6614729Z (node:769) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-13T18:23:25.6615416Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-13T18:23:26.1889025Z ==== Collecting logs from worker pod: vllm-0-1 ====
2026-02-13T18:23:26.1889597Z ==== Streaming logs from leader pod: vllm-0 ====
2026-02-13T18:23:26.1889901Z Looking for logs containing: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-13T18:23:26.2719267Z /usr/local/Ascend/ascend-toolkit/set_env.sh: line 31: CMAKE_PREFIX_PATH: unbound variable
2026-02-13T18:23:26.2732892Z ====> Check NPU info
2026-02-13T18:23:26.2743426Z +------------------------------------------------------------------------------------------------+
2026-02-13T18:23:26.2758220Z | npu-smi 25.5.0                   Version: 25.5.0                                               |
2026-02-13T18:23:26.2763681Z +---------------------------+---------------+----------------------------------------------------+
2026-02-13T18:23:26.2774507Z | NPU   Name                | Health        | Power(W)    Temp(C)           Hugepages-Usage(page)|
2026-02-13T18:23:26.2783669Z | Chip  Phy-ID              | Bus-Id        | AICore(%)   Memory-Usage(MB)  HBM-Usage(MB)        |
2026-02-13T18:23:26.2795615Z +===========================+===============+====================================================+
2026-02-13T18:23:26.2804291Z | 0     Ascend910           | OK            | 163.9       35                0    / 0             |
2026-02-13T18:23:26.2813446Z | 0     0                   | 0000:9D:00.0  | 0           0    / 0          3152 / 65536         |
2026-02-13T18:23:26.2823666Z +------------------------------------------------------------------------------------------------+
2026-02-13T18:23:26.2832902Z | 0     Ascend910           | OK            | -           36                0    / 0             |
2026-02-13T18:23:26.2843333Z | 1     1                   | 0000:9F:00.0  | 0           0    / 0          2893 / 65536         |
2026-02-13T18:23:26.2854009Z +===========================+===============+====================================================+
2026-02-13T18:23:26.2862782Z | 1     Ascend910           | OK            | 162.4       35                0    / 0             |
2026-02-13T18:23:26.2872379Z | 0     2                   | 0000:99:00.0  | 0           0    / 0          3162 / 65536         |
2026-02-13T18:23:26.2882581Z +------------------------------------------------------------------------------------------------+
2026-02-13T18:23:26.2892933Z | 1     Ascend910           | OK            | -           36                0    / 0             |
2026-02-13T18:23:26.2902330Z | 1     3                   | 0000:9B:00.0  | 0           0    / 0          2881 / 65536         |
2026-02-13T18:23:26.2912527Z +===========================+===============+====================================================+
2026-02-13T18:23:26.2922267Z | 2     Ascend910           | OK            | 163.8       37                0    / 0             |
2026-02-13T18:23:26.2931431Z | 0     4                   | 0000:95:00.0  | 0           0    / 0          3161 / 65536         |
2026-02-13T18:23:26.2954840Z +------------------------------------------------------------------------------------------------+
2026-02-13T18:23:26.2955182Z | 2     Ascend910           | OK            | -           37                0    / 0             |
2026-02-13T18:23:26.2960683Z | 1     5                   | 0000:97:00.0  | 0           0    / 0          2881 / 65536         |
2026-02-13T18:23:26.2969795Z +===========================+===============+====================================================+
2026-02-13T18:23:26.2979280Z | 3     Ascend910           | OK            | 172.1       35                0    / 0             |
2026-02-13T18:23:26.2988717Z | 0     6                   | 0000:91:00.0  | 0           0    / 0          3150 / 65536         |
2026-02-13T18:23:26.2998428Z +------------------------------------------------------------------------------------------------+
2026-02-13T18:23:26.3007581Z | 3     Ascend910           | OK            | -           36                0    / 0             |
2026-02-13T18:23:26.3017250Z | 1     7                   | 0000:93:00.0  | 0           0    / 0          2893 / 65536         |
2026-02-13T18:23:26.3026463Z +===========================+===============+====================================================+
2026-02-13T18:23:26.3035487Z | 4     Ascend910           | OK            | 163.3       36                0    / 0             |
2026-02-13T18:23:26.3045506Z | 0     8                   | 0000:8D:00.0  | 0           0    / 0          3150 / 65536         |
2026-02-13T18:23:26.3055131Z +------------------------------------------------------------------------------------------------+
2026-02-13T18:23:26.3065167Z | 4     Ascend910           | OK            | -           35                0    / 0             |
2026-02-13T18:23:26.3073733Z | 1     9                   | 0000:8F:00.0  | 0           0    / 0          2896 / 65536         |
2026-02-13T18:23:26.3083721Z +===========================+===============+====================================================+
2026-02-13T18:23:26.3093512Z | 5     Ascend910           | OK            | 164.4       34                0    / 0             |
2026-02-13T18:23:26.3102835Z | 0     10                  | 0000:89:00.0  | 0           0    / 0          3154 / 65536         |
2026-02-13T18:23:26.3111938Z +------------------------------------------------------------------------------------------------+
2026-02-13T18:23:26.3121761Z | 5     Ascend910           | OK            | -           36                0    / 0             |
2026-02-13T18:23:26.3131414Z | 1     11                  | 0000:8B:00.0  | 0           0    / 0          2895 / 65536         |
2026-02-13T18:23:26.3141349Z +===========================+===============+====================================================+
2026-02-13T18:23:26.3150287Z | 6     Ascend910           | OK            | 165.7       35                0    / 0             |
2026-02-13T18:23:26.3160581Z | 0     12                  | 0000:85:00.0  | 0           0    / 0          3150 / 65536         |
2026-02-13T18:23:26.3169764Z +------------------------------------------------------------------------------------------------+
2026-02-13T18:23:26.3179717Z | 6     Ascend910           | OK            | -           34                0    / 0             |
2026-02-13T18:23:26.3188620Z | 1     13                  | 0000:87:00.0  | 0           0    / 0          2891 / 65536         |
2026-02-13T18:23:26.3198677Z +===========================+===============+====================================================+
2026-02-13T18:23:26.3208572Z | 7     Ascend910           | OK            | 163.2       36                0    / 0             |
2026-02-13T18:23:26.3217868Z | 0     14                  | 0000:81:00.0  | 0           0    / 0          3158 / 65536         |
2026-02-13T18:23:26.3226834Z +------------------------------------------------------------------------------------------------+
2026-02-13T18:23:26.3235864Z | 7     Ascend910           | OK            | -           36                0    / 0             |
2026-02-13T18:23:26.3245044Z | 1     15                  | 0000:83:00.0  | 0           0    / 0          2878 / 65536         |
2026-02-13T18:23:26.3254649Z +===========================+===============+====================================================+
2026-02-13T18:23:26.3264257Z +---------------------------+---------------+----------------------------------------------------+
2026-02-13T18:23:26.3273655Z | NPU     Chip              | Process id    | Process name             | Process memory(MB)      |
2026-02-13T18:23:26.3284546Z +===========================+===============+====================================================+
2026-02-13T18:23:26.3293956Z | No running processes found in NPU 0                                                            |
2026-02-13T18:23:26.3304043Z +===========================+===============+====================================================+
2026-02-13T18:23:26.3317358Z | No running processes found in NPU 1                                                            |
2026-02-13T18:23:26.3327523Z +===========================+===============+====================================================+
2026-02-13T18:23:26.3337002Z | No running processes found in NPU 2                                                            |
2026-02-13T18:23:26.3346499Z +===========================+===============+====================================================+
2026-02-13T18:23:26.3356391Z | No running processes found in NPU 3                                                            |
2026-02-13T18:23:26.3366448Z +===========================+===============+====================================================+
2026-02-13T18:23:26.3375814Z | No running processes found in NPU 4                                                            |
2026-02-13T18:23:26.3385296Z +===========================+===============+====================================================+
2026-02-13T18:23:26.3395352Z | No running processes found in NPU 5                                                            |
2026-02-13T18:23:26.3405034Z +===========================+===============+====================================================+
2026-02-13T18:23:26.3414679Z | No running processes found in NPU 6                                                            |
2026-02-13T18:23:26.3424002Z +===========================+===============+====================================================+
2026-02-13T18:23:26.3433684Z | No running processes found in NPU 7                                                            |
2026-02-13T18:23:26.3442849Z +===========================+===============+====================================================+
2026-02-13T18:23:26.3451723Z package_name=Ascend-cann-toolkit
2026-02-13T18:23:26.3461299Z version=8.5.0
2026-02-13T18:23:26.3470242Z innerversion=V100R001C25SPC001B232
2026-02-13T18:23:26.3480177Z compatible_version=[V100R001C15],[V100R001C18],[V100R001C19],[V100R001C20],[V100R001C21],[V100R001C23]
2026-02-13T18:23:26.3488873Z arch=aarch64
2026-02-13T18:23:26.3497952Z os=linux
2026-02-13T18:23:26.3507503Z path=/usr/local/Ascend/cann-8.5.0
2026-02-13T18:23:26.3517260Z ====> Configure mirrors and git proxy
2026-02-13T18:23:26.3526475Z Writing to /root/.config/pip/pip.conf
2026-02-13T18:23:26.3625268Z Installed vLLM-related Python packages:
2026-02-13T18:23:27.1381899Z ais_bench_benchmark               3.0.20250930                /vllm-workspace/vllm-ascend/benchmark
2026-02-13T18:23:27.1389947Z vllm                              0.15.0+empty                /vllm-workspace/vllm
2026-02-13T18:23:27.1399961Z vllm_ascend                       0.14.0rc2.dev166+ge2175d9c7 /vllm-workspace/vllm-ascend
2026-02-13T18:23:27.1408738Z 
2026-02-13T18:23:27.1418599Z ============================
2026-02-13T18:23:27.1427980Z vLLM Git information
2026-02-13T18:23:27.1436723Z ============================
2026-02-13T18:23:27.1488102Z Branch:      HEAD
2026-02-13T18:23:27.1527730Z Commit hash: f176443446f659dbab5315e056e605d8984fd976
2026-02-13T18:23:27.1611189Z Author:      TJian <tunjian.tan@embeddedllm.com>
2026-02-13T18:23:27.1671093Z Date:        2026-01-29 14:45:42 +0800
2026-02-13T18:23:27.1681972Z Message:     [Release] [CI] Optim release pipeline (#33156)
2026-02-13T18:23:27.1690908Z Tags:        v0.15.0
2026-02-13T18:23:27.1731884Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm.git (fetch)
2026-02-13T18:23:27.1740366Z 
2026-02-13T18:23:27.1749784Z 
2026-02-13T18:23:27.1759591Z ============================
2026-02-13T18:23:27.1769401Z vLLM-Ascend Git information
2026-02-13T18:23:27.1778517Z ============================
2026-02-13T18:23:27.1811620Z Branch:      main
2026-02-13T18:23:27.1820443Z Commit hash: e2175d9c7e62b437391dfee996b1375674ba7c18
2026-02-13T18:23:27.1931498Z Author:      whx <56632993+whx-sjtu@users.noreply.github.com>
2026-02-13T18:23:27.1940476Z Date:        2026-02-13 15:53:16 +0800
2026-02-13T18:23:27.1949343Z Message:     [Lint] Adapt lint tools for windows (#6727)
2026-02-13T18:23:27.2232770Z Tags:        
2026-02-13T18:23:27.2261753Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm-ascend (fetch)
2026-02-13T18:23:27.2270251Z 
2026-02-13T18:23:27.2279219Z ====> Check triton ascend info
2026-02-13T18:23:27.6236639Z Ubuntu clang version 15.0.7
2026-02-13T18:23:27.6244617Z Target: aarch64-unknown-linux-gnu
2026-02-13T18:23:27.6253861Z Thread model: posix
2026-02-13T18:23:27.6262199Z InstalledDir: /usr/bin
2026-02-13T18:23:27.6271675Z Found candidate GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-13T18:23:27.6281247Z Selected GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-13T18:23:27.6290264Z Candidate multilib: .;@m64
2026-02-13T18:23:27.6299662Z Selected multilib: .;@m64
2026-02-13T18:23:27.6397624Z /usr/local/Ascend/cann-8.5.0/tools/bishengir/bin/bishengir-compile
2026-02-13T18:23:28.2524999Z Name: triton-ascend
2026-02-13T18:23:28.2532541Z Version: 3.2.0
2026-02-13T18:23:28.2549715Z Summary: A language and compiler for custom Deep Learning operations on Ascend hardwares
2026-02-13T18:23:28.2559348Z Home-page: https://gitcode.com/Ascend/triton-ascend/
2026-02-13T18:23:28.2567868Z Author: 
2026-02-13T18:23:28.2577696Z Author-email: 
2026-02-13T18:23:28.2586671Z License: 
2026-02-13T18:23:28.2596135Z Location: /usr/local/python3.11.14/lib/python3.11/site-packages
2026-02-13T18:23:28.2605649Z Requires: 
2026-02-13T18:23:28.2614807Z Required-by: vllm_ascend
2026-02-13T18:23:40.1389291Z INFO 02-13 18:23:40 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:23:40.1400140Z INFO 02-13 18:23:40 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:23:40.1410901Z INFO 02-13 18:23:40 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:23:40.3935442Z INFO 02-13 18:23:40 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:23:45.4189400Z ============================= test session starts ==============================
2026-02-13T18:23:45.4200641Z platform linux -- Python 3.11.14, pytest-8.4.2, pluggy-1.6.0 -- /usr/local/python3.11.14/bin/python3
2026-02-13T18:23:45.4211274Z cachedir: .pytest_cache
2026-02-13T18:23:45.4220956Z rootdir: /vllm-workspace/vllm-ascend
2026-02-13T18:23:45.4231497Z configfile: pyproject.toml
2026-02-13T18:23:45.4241535Z plugins: cov-7.0.0, asyncio-1.3.0, mock-3.15.1, anyio-4.12.1
2026-02-13T18:23:45.4251744Z asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
2026-02-13T18:23:45.7560873Z collecting ... collected 1 item
2026-02-13T18:23:45.7568016Z 
2026-02-13T18:23:45.7582489Z [2026-02-13 18:23:45] INFO multi_node_config.py:294: Loading config yaml: tests/e2e/nightly/multi_node/config/DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-13T18:23:45.7635921Z [2026-02-13 18:23:45] INFO multi_node_config.py:351: Resolving cluster IPs via DNS...
2026-02-13T18:23:45.7699777Z [2026-02-13 18:23:45] INFO multi_node_config.py:212: Node 0 envs: {'HCCL_OP_EXPANSION_MODE': 'AIV', 'VLLM_USE_MODELSCOPE': 'True', 'HCCL_BUFFSIZE': '1024', 'SERVER_PORT': '8080', 'OMP_PROC_BIND': 'False', 'OMP_NUM_THREADS': '1', 'PYTORCH_NPU_ALLOC_CONF': 'expandable_segments:True', 'VLLM_ASCEND_ENABLE_FLASHCOMM1': '1', 'ASCEND_A3_EBA_ENABLE': '1', 'HCCL_IF_IP': '10.0.0.74', 'HCCL_SOCKET_IFNAME': 'eth0', 'GLOO_SOCKET_IFNAME': 'eth0', 'TP_SOCKET_IFNAME': 'eth0', 'LOCAL_IP': '10.0.0.74', 'NIC_NAME': 'eth0', 'MASTER_IP': '10.0.0.74'}
2026-02-13T18:23:45.7721634Z [2026-02-13 18:23:45] INFO multi_node_config.py:137: Not launching proxy on non-master node
2026-02-13T18:23:45.7751248Z [2026-02-13 18:23:45] INFO conftest.py:241: Starting server with command: vllm serve vllm-ascend/DeepSeek-V3.2-W8A8 --host 0.0.0.0 --port 8080 --data-parallel-size 4 --data-parallel-size-local 2 --data-parallel-address 10.0.0.74 --data-parallel-rpc-port 13399 --tensor-parallel-size 8 --quantization ascend --seed 1024 --enable-expert-parallel --max-num-seqs 16 --max-model-len 8192 --max-num-batched-tokens 4096 --no-enable-prefix-caching --gpu-memory-utilization 0.85 --trust-remote-code --speculative-config {"num_speculative_tokens": 2, "method":"deepseek_mtp"} --compilation-config {"cudagraph_capture_sizes": [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], "cudagraph_mode": "FULL_DECODE_ONLY"} --additional-config {"layer_sharding": ["q_b_proj", "o_proj"]} --tokenizer-mode deepseek_v32 --reasoning-parser deepseek_v3
2026-02-13T18:23:50.3344828Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node INFO 02-13 18:23:50 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:23:50.3348640Z INFO 02-13 18:23:50 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:23:50.3359273Z INFO 02-13 18:23:50 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:23:50.3410521Z INFO 02-13 18:23:50 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:23:56.3219769Z 2026-02-13 18:23:56,319 - 139 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:23:56.3480016Z INFO 02-13 18:23:56 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:23:56.4894651Z INFO 02-13 18:23:56 [serve.py:100] Defaulting api_server_count to data_parallel_size (4).
2026-02-13T18:23:56.4919408Z INFO 02-13 18:23:56 [utils.py:325] 
2026-02-13T18:23:56.4928993Z INFO 02-13 18:23:56 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
2026-02-13T18:23:56.4938500Z INFO 02-13 18:23:56 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
2026-02-13T18:23:56.4949082Z INFO 02-13 18:23:56 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   vllm-ascend/DeepSeek-V3.2-W8A8
2026-02-13T18:23:56.4958687Z INFO 02-13 18:23:56 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
2026-02-13T18:23:56.4967773Z INFO 02-13 18:23:56 [utils.py:325] 
2026-02-13T18:23:56.4985956Z INFO 02-13 18:23:56 [utils.py:261] non-default args: {'model_tag': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'api_server_count': 4, 'host': '0.0.0.0', 'port': 8080, 'model': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'tokenizer_mode': 'deepseek_v32', 'trust_remote_code': True, 'seed': 1024, 'max_model_len': 8192, 'quantization': 'ascend', 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 8, 'data_parallel_size': 4, 'data_parallel_size_local': 2, 'data_parallel_address': '10.0.0.74', 'data_parallel_rpc_port': 13399, 'enable_expert_parallel': True, 'gpu_memory_utilization': 0.85, 'enable_prefix_caching': False, 'max_num_batched_tokens': 4096, 'max_num_seqs': 16, 'speculative_config': {'num_speculative_tokens': 2, 'method': 'deepseek_mtp'}, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}, 'additional_config': {'layer_sharding': ['q_b_proj', 'o_proj']}}
2026-02-13T18:23:56.5427891Z 2026-02-13 18:23:56,541 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-13T18:23:56.5458757Z INFO 02-13 18:23:56 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-13T18:23:56.5482411Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:23:56.5551898Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:23:56.5561659Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:23:56.5572768Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-13T18:23:56.5759913Z INFO 02-13 18:23:56 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-13T18:23:56.5772290Z INFO 02-13 18:23:56 [model.py:1561] Using max model len 8192
2026-02-13T18:23:56.7938755Z WARNING 02-13 18:23:56 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-13T18:23:56.7967709Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:23:56.7977683Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:23:56.7986526Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-13T18:23:56.8059333Z INFO 02-13 18:23:56 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-13T18:23:56.8111784Z INFO 02-13 18:23:56 [model.py:1561] Using max model len 163840
2026-02-13T18:23:56.8121122Z WARNING 02-13 18:23:56 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-13T18:23:56.8130622Z INFO 02-13 18:23:56 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-13T18:23:57.4361837Z INFO 02-13 18:23:57 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-13T18:23:57.4369319Z INFO 02-13 18:23:57 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-13T18:23:57.4388340Z WARNING 02-13 18:23:57 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-13T18:23:57.4397918Z WARNING 02-13 18:23:57 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-13T18:23:57.4406929Z INFO 02-13 18:23:57 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:23:57.4415944Z INFO 02-13 18:23:57 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:23:57.4425645Z INFO 02-13 18:23:57 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:23:57.4434945Z WARNING 02-13 18:23:57 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-13T18:23:57.4444892Z INFO 02-13 18:23:57 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-13T18:23:57.4454305Z WARNING 02-13 18:23:57 [platform.py:335] [91m
2026-02-13T18:23:57.4463249Z WARNING 02-13 18:23:57 [platform.py:335]             **********************************************************************************
2026-02-13T18:23:57.4472585Z WARNING 02-13 18:23:57 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-13T18:23:57.4482689Z WARNING 02-13 18:23:57 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-13T18:23:57.4492286Z WARNING 02-13 18:23:57 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-13T18:23:57.4501879Z WARNING 02-13 18:23:57 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-13T18:23:57.4510853Z WARNING 02-13 18:23:57 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-13T18:23:57.4521040Z WARNING 02-13 18:23:57 [platform.py:335]             * batch size for graph capture.
2026-02-13T18:23:57.4530696Z WARNING 02-13 18:23:57 [platform.py:335]             * For more details, please refer to:
2026-02-13T18:23:57.4539987Z WARNING 02-13 18:23:57 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-13T18:23:57.4549720Z WARNING 02-13 18:23:57 [platform.py:335]             **********************************************************************************[0m
2026-02-13T18:23:57.4559297Z WARNING 02-13 18:23:57 [platform.py:335]             
2026-02-13T18:23:57.4568785Z INFO 02-13 18:23:57 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-13T18:23:57.4578232Z INFO 02-13 18:23:57 [utils.py:851] Started DP Coordinator process (PID: 152)
2026-02-13T18:24:02.0359153Z INFO 02-13 18:24:02 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:24:02.0368583Z INFO 02-13 18:24:02 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:24:02.0378989Z INFO 02-13 18:24:02 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:24:02.0428917Z INFO 02-13 18:24:02 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:24:02.0633281Z INFO 02-13 18:24:02 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:24:02.0642146Z INFO 02-13 18:24:02 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:24:02.0652666Z INFO 02-13 18:24:02 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:24:02.0703877Z INFO 02-13 18:24:02 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:24:11.8820868Z INFO 02-13 18:24:11 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:24:11.8831291Z INFO 02-13 18:24:11 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:24:11.8843323Z INFO 02-13 18:24:11 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:24:11.8887451Z INFO 02-13 18:24:11 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:24:17.1433794Z INFO 02-13 18:24:17 [utils.py:218] Started 4 API server processes
2026-02-13T18:24:21.9435729Z INFO 02-13 18:24:21 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:24:21.9447026Z INFO 02-13 18:24:21 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:24:21.9457563Z INFO 02-13 18:24:21 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:24:21.9510429Z INFO 02-13 18:24:21 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:24:22.0940936Z INFO 02-13 18:24:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:24:22.0949702Z INFO 02-13 18:24:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:24:22.0959955Z INFO 02-13 18:24:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:24:22.1069148Z INFO 02-13 18:24:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:24:22.2795321Z INFO 02-13 18:24:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:24:22.2803883Z INFO 02-13 18:24:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:24:22.2813855Z INFO 02-13 18:24:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:24:22.2871754Z INFO 02-13 18:24:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:24:22.3844361Z INFO 02-13 18:24:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:24:22.3851007Z INFO 02-13 18:24:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:24:22.3860400Z INFO 02-13 18:24:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:24:22.3966555Z INFO 02-13 18:24:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:24:22.8432464Z [0;36m(EngineCore_DP0 pid=155)[0;0m 2026-02-13 18:24:22,841 - 155 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:24:22.8473261Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-13 18:24:22 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:24:22.8513515Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-13 18:24:22 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', speculative_config=SpeculativeConfig(method='mtp', model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', num_spec_tokens=2), tokenizer='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', skip_tokenizer_init=False, tokenizer_mode=deepseek_v32, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=4, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=bfloat16, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=1024, served_model_name=/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [24, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 48, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
2026-02-13T18:24:22.8634069Z [0;36m(EngineCore_DP1 pid=174)[0;0m 2026-02-13 18:24:22,861 - 174 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:24:22.8678321Z [0;36m(EngineCore_DP1 pid=174)[0;0m INFO 02-13 18:24:22 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:24:27.5881349Z INFO 02-13 18:24:27 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:24:27.5889797Z INFO 02-13 18:24:27 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:24:27.5900239Z INFO 02-13 18:24:27 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:24:27.5955439Z INFO 02-13 18:24:27 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:24:27.9802642Z INFO 02-13 18:24:27 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:24:27.9810435Z INFO 02-13 18:24:27 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:24:27.9819779Z INFO 02-13 18:24:27 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:24:27.9871227Z INFO 02-13 18:24:27 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:24:28.0214963Z [0;36m(ApiServer_3 pid=188)[0;0m 2026-02-13 18:24:28,020 - 188 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:24:28.0365300Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-13 18:24:28 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:24:28.0726811Z [0;36m(ApiServer_3 pid=188)[0;0m 2026-02-13 18:24:28,070 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-13T18:24:28.0771459Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-13 18:24:28 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-13T18:24:28.1341758Z [0;36m(ApiServer_1 pid=186)[0;0m 2026-02-13 18:24:28,132 - 186 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:24:28.1493088Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-13 18:24:28 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:24:28.1653172Z [0;36m(ApiServer_1 pid=186)[0;0m 2026-02-13 18:24:28,164 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-13T18:24:28.1711767Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-13 18:24:28 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-13T18:24:28.1800541Z [0;36m(ApiServer_3 pid=188)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:24:28.1813034Z [0;36m(ApiServer_3 pid=188)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:24:28.1831667Z [0;36m(ApiServer_3 pid=188)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:24:28.1841226Z [0;36m(ApiServer_3 pid=188)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-13T18:24:28.1915958Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-13 18:24:28 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-13T18:24:28.1936092Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-13 18:24:28 [model.py:1561] Using max model len 8192
2026-02-13T18:24:28.2774403Z [0;36m(ApiServer_1 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:24:28.2777705Z [0;36m(ApiServer_1 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:24:28.2797574Z [0;36m(ApiServer_1 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:24:28.2807909Z [0;36m(ApiServer_1 pid=186)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-13T18:24:28.2854202Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-13 18:24:28 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-13T18:24:28.2874501Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-13 18:24:28 [model.py:1561] Using max model len 8192
2026-02-13T18:24:28.3053083Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-13 18:24:28 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-13T18:24:28.3053737Z [0;36m(ApiServer_3 pid=188)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:24:28.3061645Z [0;36m(ApiServer_3 pid=188)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:24:28.3095659Z [0;36m(ApiServer_3 pid=188)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-13T18:24:28.3115288Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-13 18:24:28 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-13T18:24:28.3138912Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-13 18:24:28 [model.py:1561] Using max model len 163840
2026-02-13T18:24:28.3146169Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-13 18:24:28 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-13T18:24:28.3189009Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-13 18:24:28 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-13T18:24:28.3428989Z [0;36m(ApiServer_0 pid=185)[0;0m 2026-02-13 18:24:28,341 - 185 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:24:28.3582900Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-13 18:24:28 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:24:28.3733884Z [0;36m(ApiServer_0 pid=185)[0;0m 2026-02-13 18:24:28,372 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-13T18:24:28.3790664Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-13 18:24:28 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-13T18:24:28.3970311Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-13 18:24:28 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-13T18:24:28.3990407Z [0;36m(ApiServer_1 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:24:28.4000024Z [0;36m(ApiServer_1 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:24:28.4009769Z [0;36m(ApiServer_1 pid=186)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-13T18:24:28.4053076Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-13 18:24:28 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-13T18:24:28.4074566Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-13 18:24:28 [model.py:1561] Using max model len 163840
2026-02-13T18:24:28.4085729Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-13 18:24:28 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-13T18:24:28.4095584Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-13 18:24:28 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-13T18:24:28.4323452Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-13 18:24:28 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-13T18:24:28.4333442Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-13 18:24:28 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-13T18:24:28.4345010Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-13 18:24:28 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-13T18:24:28.4354758Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-13 18:24:28 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-13T18:24:28.4364706Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-13 18:24:28 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:24:28.4374247Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-13 18:24:28 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:24:28.4386668Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-13 18:24:28 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:24:28.4395162Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-13 18:24:28 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-13T18:24:28.4405646Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-13 18:24:28 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-13T18:24:28.4414596Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-13 18:24:28 [platform.py:335] [91m
2026-02-13T18:24:28.4424601Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             **********************************************************************************
2026-02-13T18:24:28.4434490Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-13T18:24:28.4444641Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-13T18:24:28.4454334Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-13T18:24:28.4463900Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-13T18:24:28.4473810Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-13T18:24:28.4484515Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * batch size for graph capture.
2026-02-13T18:24:28.4493915Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * For more details, please refer to:
2026-02-13T18:24:28.4506091Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-13T18:24:28.4514077Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             **********************************************************************************[0m
2026-02-13T18:24:28.4523542Z [0;36m(ApiServer_3 pid=188)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             
2026-02-13T18:24:28.4533367Z [0;36m(ApiServer_3 pid=188)[0;0m INFO 02-13 18:24:28 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-13T18:24:28.4820514Z [0;36m(ApiServer_0 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:24:28.4846558Z [0;36m(ApiServer_0 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:24:28.4856084Z [0;36m(ApiServer_0 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:24:28.4868846Z [0;36m(ApiServer_0 pid=185)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-13T18:24:28.4917099Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-13 18:24:28 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-13T18:24:28.4936000Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-13 18:24:28 [model.py:1561] Using max model len 8192
2026-02-13T18:24:28.5224727Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-13 18:24:28 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-13T18:24:28.5233973Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-13 18:24:28 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-13T18:24:28.5245376Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-13 18:24:28 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-13T18:24:28.5254875Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-13 18:24:28 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-13T18:24:28.5264068Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-13 18:24:28 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:24:28.5273251Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-13 18:24:28 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:24:28.5283892Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-13 18:24:28 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:24:28.5293182Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-13 18:24:28 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-13T18:24:28.5302424Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-13 18:24:28 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-13T18:24:28.5311408Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-13 18:24:28 [platform.py:335] [91m
2026-02-13T18:24:28.5321578Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             **********************************************************************************
2026-02-13T18:24:28.5332304Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-13T18:24:28.5341321Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-13T18:24:28.5350864Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-13T18:24:28.5360558Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-13T18:24:28.5369863Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-13T18:24:28.5379104Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * batch size for graph capture.
2026-02-13T18:24:28.5388547Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * For more details, please refer to:
2026-02-13T18:24:28.5399417Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-13T18:24:28.5409025Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             **********************************************************************************[0m
2026-02-13T18:24:28.5416970Z [0;36m(ApiServer_1 pid=186)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             
2026-02-13T18:24:28.5426497Z [0;36m(ApiServer_1 pid=186)[0;0m INFO 02-13 18:24:28 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-13T18:24:28.6025527Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-13 18:24:28 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-13T18:24:28.6050148Z [0;36m(ApiServer_0 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:24:28.6062739Z [0;36m(ApiServer_0 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:24:28.6073164Z [0;36m(ApiServer_0 pid=185)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-13T18:24:28.6103821Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-13 18:24:28 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-13T18:24:28.6124921Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-13 18:24:28 [model.py:1561] Using max model len 163840
2026-02-13T18:24:28.6134589Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-13 18:24:28 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-13T18:24:28.6143566Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-13 18:24:28 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-13T18:24:28.6186790Z [0;36m(ApiServer_2 pid=187)[0;0m 2026-02-13 18:24:28,616 - 187 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:24:28.6338931Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-13 18:24:28 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:24:28.6528095Z [0;36m(ApiServer_2 pid=187)[0;0m 2026-02-13 18:24:28,651 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-13T18:24:28.6576202Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-13 18:24:28 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-13T18:24:28.7317416Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-13 18:24:28 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-13T18:24:28.7325615Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-13 18:24:28 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-13T18:24:28.7339046Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-13 18:24:28 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-13T18:24:28.7349108Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-13 18:24:28 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-13T18:24:28.7357813Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-13 18:24:28 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:24:28.7367612Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-13 18:24:28 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:24:28.7379789Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-13 18:24:28 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:24:28.7388683Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-13 18:24:28 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-13T18:24:28.7397109Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-13 18:24:28 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-13T18:24:28.7406616Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-13 18:24:28 [platform.py:335] [91m
2026-02-13T18:24:28.7416836Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             **********************************************************************************
2026-02-13T18:24:28.7425159Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-13T18:24:28.7435206Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-13T18:24:28.7444740Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-13T18:24:28.7453971Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-13T18:24:28.7463703Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-13T18:24:28.7472704Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * batch size for graph capture.
2026-02-13T18:24:28.7482387Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * For more details, please refer to:
2026-02-13T18:24:28.7492324Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-13T18:24:28.7501615Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             **********************************************************************************[0m
2026-02-13T18:24:28.7510887Z [0;36m(ApiServer_0 pid=185)[0;0m WARNING 02-13 18:24:28 [platform.py:335]             
2026-02-13T18:24:28.7520728Z [0;36m(ApiServer_0 pid=185)[0;0m INFO 02-13 18:24:28 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-13T18:24:28.7570637Z [0;36m(ApiServer_2 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:24:28.7594554Z [0;36m(ApiServer_2 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:24:28.7605247Z [0;36m(ApiServer_2 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:24:28.7648251Z [0;36m(ApiServer_2 pid=187)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-13T18:24:28.7677786Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-13 18:24:28 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-13T18:24:28.7687497Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-13 18:24:28 [model.py:1561] Using max model len 8192
2026-02-13T18:24:28.8761105Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-13 18:24:28 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-13T18:24:28.8774851Z [0;36m(ApiServer_2 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:24:28.8784644Z [0;36m(ApiServer_2 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-13T18:24:28.8795124Z [0;36m(ApiServer_2 pid=187)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-13T18:24:28.8833371Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-13 18:24:28 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-13T18:24:28.8854081Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-13 18:24:28 [model.py:1561] Using max model len 163840
2026-02-13T18:24:28.8867832Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-13 18:24:28 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-13T18:24:28.8876738Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-13 18:24:28 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-13T18:24:29.0033965Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-13 18:24:29 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-13T18:24:29.0046255Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-13 18:24:29 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-13T18:24:29.0055465Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-13 18:24:29 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-13T18:24:29.0065808Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-13 18:24:29 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-13T18:24:29.0074570Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-13 18:24:29 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:24:29.0085507Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-13 18:24:29 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:24:29.0096574Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-13 18:24:29 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:24:29.0106608Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-13 18:24:29 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-13T18:24:29.0116835Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-13 18:24:29 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-13T18:24:29.0126138Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-13 18:24:29 [platform.py:335] [91m
2026-02-13T18:24:29.0136403Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-13 18:24:29 [platform.py:335]             **********************************************************************************
2026-02-13T18:24:29.0145738Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-13 18:24:29 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-13T18:24:29.0155461Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-13 18:24:29 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-13T18:24:29.0166048Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-13 18:24:29 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-13T18:24:29.0176847Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-13 18:24:29 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-13T18:24:29.0187913Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-13 18:24:29 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-13T18:24:29.0197796Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-13 18:24:29 [platform.py:335]             * batch size for graph capture.
2026-02-13T18:24:29.0207665Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-13 18:24:29 [platform.py:335]             * For more details, please refer to:
2026-02-13T18:24:29.0218067Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-13 18:24:29 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-13T18:24:29.0228194Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-13 18:24:29 [platform.py:335]             **********************************************************************************[0m
2026-02-13T18:24:29.0238577Z [0;36m(ApiServer_2 pid=187)[0;0m WARNING 02-13 18:24:29 [platform.py:335]             
2026-02-13T18:24:29.0247933Z [0;36m(ApiServer_2 pid=187)[0;0m INFO 02-13 18:24:29 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-13T18:24:32.8860907Z 2026-02-13 18:24:32,884 - 214 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:24:32.8921293Z INFO 02-13 18:24:32 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:24:33.1283075Z 2026-02-13 18:24:33,126 - 215 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:24:33.1341576Z INFO 02-13 18:24:33 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:24:34.4269340Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-13T18:24:34.4278211Z   warnings.warn(
2026-02-13T18:24:34.5178828Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-13T18:24:34.5186398Z   warnings.warn(
2026-02-13T18:24:36.6830793Z INFO 02-13 18:24:36 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:24:36.6840717Z INFO 02-13 18:24:36 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:24:36.6851912Z INFO 02-13 18:24:36 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:24:36.7054576Z INFO 02-13 18:24:36 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:24:36.7064104Z INFO 02-13 18:24:36 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:24:36.7076063Z INFO 02-13 18:24:36 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:24:36.8102564Z INFO 02-13 18:24:36 [parallel_state.py:1212] world_size=32 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.74:35371 backend=hccl
2026-02-13T18:24:36.8336022Z INFO 02-13 18:24:36 [parallel_state.py:1212] world_size=32 rank=8 local_rank=0 distributed_init_method=tcp://10.0.0.74:35371 backend=hccl
2026-02-13T18:24:37.4028037Z INFO 02-13 18:24:37 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:24:37.4035292Z INFO 02-13 18:24:37 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:24:37.4045211Z INFO 02-13 18:24:37 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:24:37.4110800Z INFO 02-13 18:24:37 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:24:37.7535794Z INFO 02-13 18:24:37 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:24:37.7543328Z INFO 02-13 18:24:37 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:24:37.7553448Z INFO 02-13 18:24:37 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:24:37.7610347Z INFO 02-13 18:24:37 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:24:42.4049055Z 2026-02-13 18:24:42,402 - 276 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:24:42.4103682Z INFO 02-13 18:24:42 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:24:42.8202335Z 2026-02-13 18:24:42,818 - 279 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:24:42.8258915Z INFO 02-13 18:24:42 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:24:43.7494505Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-13T18:24:43.7503874Z   warnings.warn(
2026-02-13T18:24:44.3438825Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-13T18:24:44.3445685Z   warnings.warn(
2026-02-13T18:24:45.9613929Z INFO 02-13 18:24:45 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:24:45.9623684Z INFO 02-13 18:24:45 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:24:45.9635942Z INFO 02-13 18:24:45 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:24:46.4039074Z INFO 02-13 18:24:46 [parallel_state.py:1212] world_size=32 rank=1 local_rank=1 distributed_init_method=tcp://10.0.0.74:35371 backend=hccl
2026-02-13T18:24:46.7911965Z INFO 02-13 18:24:46 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:24:46.7919736Z INFO 02-13 18:24:46 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:24:46.7931226Z INFO 02-13 18:24:46 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:24:47.1459241Z INFO 02-13 18:24:47 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:24:47.1466317Z INFO 02-13 18:24:47 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:24:47.1476721Z INFO 02-13 18:24:47 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:24:47.1534052Z INFO 02-13 18:24:47 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:24:47.2583272Z INFO 02-13 18:24:47 [parallel_state.py:1212] world_size=32 rank=9 local_rank=1 distributed_init_method=tcp://10.0.0.74:35371 backend=hccl
2026-02-13T18:24:47.4367540Z INFO 02-13 18:24:47 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:24:47.4374967Z INFO 02-13 18:24:47 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:24:47.4385043Z INFO 02-13 18:24:47 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:24:47.4445163Z INFO 02-13 18:24:47 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:24:52.2477678Z 2026-02-13 18:24:52,245 - 370 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:24:52.2541158Z INFO 02-13 18:24:52 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:24:52.5867208Z 2026-02-13 18:24:52,585 - 373 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:24:52.5926799Z INFO 02-13 18:24:52 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:24:53.5457715Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-13T18:24:53.5464286Z   warnings.warn(
2026-02-13T18:24:53.9212989Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-13T18:24:53.9220203Z   warnings.warn(
2026-02-13T18:24:55.6718481Z INFO 02-13 18:24:55 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:24:55.6728593Z INFO 02-13 18:24:55 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:24:55.6739496Z INFO 02-13 18:24:55 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:24:56.1037821Z INFO 02-13 18:24:56 [parallel_state.py:1212] world_size=32 rank=2 local_rank=2 distributed_init_method=tcp://10.0.0.74:35371 backend=hccl
2026-02-13T18:24:56.8180196Z INFO 02-13 18:24:56 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:24:56.8180688Z INFO 02-13 18:24:56 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:24:56.8181126Z INFO 02-13 18:24:56 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:24:56.8222366Z INFO 02-13 18:24:56 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:24:56.8369655Z INFO 02-13 18:24:56 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:24:56.8379516Z INFO 02-13 18:24:56 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:24:56.8388509Z INFO 02-13 18:24:56 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:24:57.1019007Z INFO 02-13 18:24:57 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:24:57.1026977Z INFO 02-13 18:24:57 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:24:57.1035131Z INFO 02-13 18:24:57 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:24:57.1094854Z INFO 02-13 18:24:57 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:24:57.2857401Z INFO 02-13 18:24:57 [parallel_state.py:1212] world_size=32 rank=10 local_rank=2 distributed_init_method=tcp://10.0.0.74:35371 backend=hccl
2026-02-13T18:25:01.9917646Z 2026-02-13 18:25:01,989 - 474 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:25:02.0076442Z INFO 02-13 18:25:02 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:25:02.5887497Z 2026-02-13 18:25:02,586 - 477 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:25:02.6040234Z INFO 02-13 18:25:02 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:25:04.2688985Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-13T18:25:04.2698068Z   warnings.warn(
2026-02-13T18:25:04.2707714Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-13T18:25:04.2716040Z   warnings.warn(
2026-02-13T18:25:07.3287461Z INFO 02-13 18:25:07 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:25:07.3298036Z INFO 02-13 18:25:07 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:25:07.3308301Z INFO 02-13 18:25:07 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:25:07.3350933Z INFO 02-13 18:25:07 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:25:07.3360658Z INFO 02-13 18:25:07 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:25:07.3372430Z INFO 02-13 18:25:07 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:25:07.7593306Z INFO 02-13 18:25:07 [parallel_state.py:1212] world_size=32 rank=11 local_rank=3 distributed_init_method=tcp://10.0.0.74:35371 backend=hccl
2026-02-13T18:25:07.7742472Z INFO 02-13 18:25:07 [parallel_state.py:1212] world_size=32 rank=3 local_rank=3 distributed_init_method=tcp://10.0.0.74:35371 backend=hccl
2026-02-13T18:25:09.7613197Z INFO 02-13 18:25:09 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:25:09.7622149Z INFO 02-13 18:25:09 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:25:09.7633061Z INFO 02-13 18:25:09 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:25:09.7667901Z INFO 02-13 18:25:09 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:25:09.7678268Z INFO 02-13 18:25:09 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:25:09.7688402Z INFO 02-13 18:25:09 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:25:09.7748627Z INFO 02-13 18:25:09 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:25:09.7802333Z INFO 02-13 18:25:09 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:25:18.1636946Z 2026-02-13 18:25:18,160 - 578 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:25:18.1699415Z INFO 02-13 18:25:18 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:25:18.1735624Z 2026-02-13 18:25:18,168 - 581 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:25:18.1790406Z INFO 02-13 18:25:18 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:25:19.7289641Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-13T18:25:19.7295991Z   warnings.warn(
2026-02-13T18:25:19.7963153Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-13T18:25:19.7970249Z   warnings.warn(
2026-02-13T18:25:21.9876385Z INFO 02-13 18:25:21 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:25:21.9884642Z INFO 02-13 18:25:21 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:25:21.9895467Z INFO 02-13 18:25:21 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:25:22.0135280Z INFO 02-13 18:25:22 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:25:22.0137515Z INFO 02-13 18:25:22 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:25:22.0143219Z INFO 02-13 18:25:22 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:25:22.4333512Z INFO 02-13 18:25:22 [parallel_state.py:1212] world_size=32 rank=4 local_rank=4 distributed_init_method=tcp://10.0.0.74:35371 backend=hccl
2026-02-13T18:25:22.4484769Z INFO 02-13 18:25:22 [parallel_state.py:1212] world_size=32 rank=12 local_rank=4 distributed_init_method=tcp://10.0.0.74:35371 backend=hccl
2026-02-13T18:25:22.8892332Z INFO 02-13 18:25:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:25:22.8899070Z INFO 02-13 18:25:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:25:22.8908756Z INFO 02-13 18:25:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:25:22.8965604Z INFO 02-13 18:25:22 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:25:23.0043334Z INFO 02-13 18:25:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:25:23.0044573Z INFO 02-13 18:25:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:25:23.0055163Z INFO 02-13 18:25:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:25:23.0117115Z INFO 02-13 18:25:23 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:25:28.0419163Z 2026-02-13 18:25:28,040 - 682 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:25:28.0492117Z INFO 02-13 18:25:28 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:25:28.1883967Z 2026-02-13 18:25:28,186 - 683 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:25:28.1941878Z INFO 02-13 18:25:28 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:25:29.4837005Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-13T18:25:29.4847105Z   warnings.warn(
2026-02-13T18:25:29.5640717Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-13T18:25:29.5648040Z   warnings.warn(
2026-02-13T18:25:31.6962753Z INFO 02-13 18:25:31 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:25:31.6974720Z INFO 02-13 18:25:31 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:25:31.6985686Z INFO 02-13 18:25:31 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:25:31.7066622Z INFO 02-13 18:25:31 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:25:31.7076168Z INFO 02-13 18:25:31 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:25:31.7088316Z INFO 02-13 18:25:31 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:25:32.1200866Z INFO 02-13 18:25:32 [parallel_state.py:1212] world_size=32 rank=5 local_rank=5 distributed_init_method=tcp://10.0.0.74:35371 backend=hccl
2026-02-13T18:25:32.1295860Z INFO 02-13 18:25:32 [parallel_state.py:1212] world_size=32 rank=13 local_rank=5 distributed_init_method=tcp://10.0.0.74:35371 backend=hccl
2026-02-13T18:25:32.7676488Z INFO 02-13 18:25:32 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:25:32.7685473Z INFO 02-13 18:25:32 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:25:32.7696329Z INFO 02-13 18:25:32 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:25:32.7749863Z INFO 02-13 18:25:32 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:25:32.8036374Z INFO 02-13 18:25:32 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:25:32.8047065Z INFO 02-13 18:25:32 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:25:32.8056204Z INFO 02-13 18:25:32 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:25:32.8115281Z INFO 02-13 18:25:32 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:25:38.3108451Z 2026-02-13 18:25:38,308 - 786 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:25:38.3118631Z 2026-02-13 18:25:38,308 - 788 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:25:38.3168155Z INFO 02-13 18:25:38 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:25:38.3179637Z INFO 02-13 18:25:38 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:25:39.6920949Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-13T18:25:39.6928115Z   warnings.warn(
2026-02-13T18:25:39.7014815Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-13T18:25:39.7023241Z   warnings.warn(
2026-02-13T18:25:41.8514998Z INFO 02-13 18:25:41 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:25:41.8523302Z INFO 02-13 18:25:41 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:25:41.8534856Z INFO 02-13 18:25:41 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:25:41.9128356Z INFO 02-13 18:25:41 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:25:41.9137245Z INFO 02-13 18:25:41 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:25:41.9149239Z INFO 02-13 18:25:41 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:25:42.2986708Z INFO 02-13 18:25:42 [parallel_state.py:1212] world_size=32 rank=14 local_rank=6 distributed_init_method=tcp://10.0.0.74:35371 backend=hccl
2026-02-13T18:25:42.3502414Z INFO 02-13 18:25:42 [parallel_state.py:1212] world_size=32 rank=6 local_rank=6 distributed_init_method=tcp://10.0.0.74:35371 backend=hccl
2026-02-13T18:25:42.9530862Z INFO 02-13 18:25:42 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:25:42.9538541Z INFO 02-13 18:25:42 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:25:42.9548926Z INFO 02-13 18:25:42 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:25:42.9604738Z INFO 02-13 18:25:42 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:25:43.1823598Z INFO 02-13 18:25:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:25:43.1833014Z INFO 02-13 18:25:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:25:43.1843279Z INFO 02-13 18:25:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:25:43.1897630Z INFO 02-13 18:25:43 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:25:47.9695340Z 2026-02-13 18:25:47,967 - 891 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:25:47.9747265Z INFO 02-13 18:25:47 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:25:48.2736854Z 2026-02-13 18:25:48,272 - 890 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-13T18:25:48.2795360Z INFO 02-13 18:25:48 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-13T18:25:49.3052464Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-13T18:25:49.3059440Z   warnings.warn(
2026-02-13T18:25:49.5667494Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-13T18:25:49.5674419Z   warnings.warn(
2026-02-13T18:25:51.4018317Z INFO 02-13 18:25:51 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:25:51.4025854Z INFO 02-13 18:25:51 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:25:51.4037607Z INFO 02-13 18:25:51 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:25:51.6570236Z INFO 02-13 18:25:51 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:25:51.6579207Z INFO 02-13 18:25:51 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:25:51.6588989Z INFO 02-13 18:25:51 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:25:51.8152847Z INFO 02-13 18:25:51 [parallel_state.py:1212] world_size=32 rank=15 local_rank=7 distributed_init_method=tcp://10.0.0.74:35371 backend=hccl
2026-02-13T18:25:52.0902571Z INFO 02-13 18:25:52 [parallel_state.py:1212] world_size=32 rank=7 local_rank=7 distributed_init_method=tcp://10.0.0.74:35371 backend=hccl
2026-02-13T18:25:52.1934362Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.1960025Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.1970027Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.1978938Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.1988899Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.1999443Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.2249783Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.2258887Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.2276823Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.2287164Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.2299141Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.2357114Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.2379202Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.2389196Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.2398400Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.2408265Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.2772076Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-13T18:25:52.2785670Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-13T18:25:52.2797712Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-13T18:25:52.2806387Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-13T18:25:52.2816317Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-13T18:25:52.2826090Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-13T18:25:52.2836047Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-13T18:25:52.2845667Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-13T18:25:52.2855173Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-13T18:25:52.2864230Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-13T18:25:52.2874615Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-13T18:25:52.2884377Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-13T18:25:52.2894559Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-13T18:25:52.2903613Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-13T18:25:52.2913505Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-13T18:25:52.2923976Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-13T18:25:52.2933651Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.2942752Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.2952379Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.2962325Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.2971367Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.2981121Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.2991012Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3001944Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3011266Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3020100Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3029537Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3040017Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3049004Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3058721Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3068378Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3078441Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3092169Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3098444Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3107818Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3117512Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3127016Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3137234Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3150329Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3165797Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3438666Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3459420Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3468891Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3479271Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3489104Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3499905Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3509231Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3520574Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3532695Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3542276Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3552583Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3562635Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3572272Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3583436Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3592302Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3602102Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3612179Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3622267Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3633969Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3643051Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3652912Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3661861Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3671766Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3681174Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-13T18:25:52.3691834Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.3701487Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.3712369Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.3721911Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.3731917Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.3741822Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.3751133Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.3760844Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.3770780Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.3780107Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.3789671Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.3798846Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.3808901Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.3823627Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.3854973Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.3866145Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.3932346Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.3956929Z INFO 02-13 18:25:52 [parallel_state.py:1423] rank 0 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
2026-02-13T18:25:52.4676351Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.4699881Z INFO 02-13 18:25:52 [parallel_state.py:1423] rank 1 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
2026-02-13T18:25:52.4761799Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.4785366Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.4795392Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.4804435Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.4814473Z INFO 02-13 18:25:52 [parallel_state.py:1423] rank 5 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 5, EP rank 5
2026-02-13T18:25:52.4824958Z INFO 02-13 18:25:52 [parallel_state.py:1423] rank 6 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 6, EP rank 6
2026-02-13T18:25:52.4834444Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.4844514Z INFO 02-13 18:25:52 [parallel_state.py:1423] rank 7 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 7, EP rank 7
2026-02-13T18:25:52.4853587Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.4863629Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.4873514Z INFO 02-13 18:25:52 [parallel_state.py:1423] rank 8 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 0, EP rank 8
2026-02-13T18:25:52.4883521Z INFO 02-13 18:25:52 [parallel_state.py:1423] rank 11 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 3, EP rank 11
2026-02-13T18:25:52.4894122Z INFO 02-13 18:25:52 [parallel_state.py:1423] rank 9 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 1, EP rank 9
2026-02-13T18:25:52.4903851Z INFO 02-13 18:25:52 [parallel_state.py:1423] rank 10 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 2, EP rank 10
2026-02-13T18:25:52.4913221Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.4923750Z INFO 02-13 18:25:52 [parallel_state.py:1423] rank 13 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 5, EP rank 13
2026-02-13T18:25:52.4932645Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.4951269Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.4961892Z INFO 02-13 18:25:52 [parallel_state.py:1423] rank 2 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
2026-02-13T18:25:52.4972138Z INFO 02-13 18:25:52 [parallel_state.py:1423] rank 14 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 6, EP rank 14
2026-02-13T18:25:52.4981280Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.4990975Z INFO 02-13 18:25:52 [parallel_state.py:1423] rank 3 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
2026-02-13T18:25:52.5000946Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.5011503Z INFO 02-13 18:25:52 [parallel_state.py:1423] rank 15 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 7, EP rank 15
2026-02-13T18:25:52.5021274Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.5031619Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.5041668Z INFO 02-13 18:25:52 [parallel_state.py:1423] rank 4 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 4, EP rank 4
2026-02-13T18:25:52.5051359Z INFO 02-13 18:25:52 [parallel_state.py:1423] rank 12 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 4, EP rank 12
2026-02-13T18:25:52.5299235Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.5320405Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.5330062Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.5339259Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.5348366Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.5357562Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.5368041Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.5376720Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.5386246Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.5395372Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.5405017Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.5414966Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.5424078Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.5433912Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.5444291Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.5453628Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-13T18:25:52.5463085Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.5502774Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.5511759Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.5521481Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.5531294Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.5540899Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.5550599Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.5560578Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.5570224Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.5580360Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.5590239Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.5600161Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.5609051Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.5618352Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.5627771Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.5637616Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-13T18:25:52.6484479Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.6503803Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.6572552Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.6582162Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.6591615Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.6601184Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.6610648Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.6620369Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.6629336Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.6639333Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.6648698Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.6659281Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.6668529Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.6678051Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.6686987Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.6697390Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.7033701Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.7046749Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.7056295Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.7065809Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.7075614Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.7085068Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.7095194Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.7104024Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.7114380Z [0;36m(Worker_DP1_TP4_EP12 pid=581)[0;0m INFO 02-13 18:25:52 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-13T18:25:52.7124373Z [0;36m(Worker_DP1_TP5_EP13 pid=683)[0;0m INFO 02-13 18:25:52 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-13T18:25:52.7134812Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-13 18:25:52 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-13T18:25:52.7143286Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.7152948Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.7162253Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.7172147Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m INFO 02-13 18:25:52 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-13T18:25:52.7181311Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-13 18:25:52 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-13T18:25:52.7191002Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-13 18:25:52 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-13T18:25:52.7201006Z [0;36m(Worker_DP1_TP7_EP15 pid=891)[0;0m INFO 02-13 18:25:52 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-13T18:25:52.7210918Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-13 18:25:52 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-13T18:25:52.7220419Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-13 18:25:52 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-13T18:25:52.7229341Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.7239131Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-13 18:25:52 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-13T18:25:52.7248173Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.7257765Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.7267961Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.7277826Z WARNING 02-13 18:25:52 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-13T18:25:52.7288011Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-13 18:25:52 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-13T18:25:52.7297592Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-13 18:25:52 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-13T18:25:52.7307351Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-13 18:25:52 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-13T18:25:52.7317030Z [0;36m(Worker_DP1_TP1_EP9 pid=279)[0;0m INFO 02-13 18:25:52 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-13T18:25:52.7326893Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-13 18:25:52 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-13T18:25:52.7813793Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m INFO 02-13 18:25:52 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-13T18:25:53.0945819Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m [2026-02-13 18:25:53] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-13T18:25:53.1047622Z [0;36m(Worker_DP1_TP4_EP12 pid=581)[0;0m [2026-02-13 18:25:53] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-13T18:25:53.1124622Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m [2026-02-13 18:25:53] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-13T18:25:53.1607916Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m [2026-02-13 18:25:53] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-13T18:25:53.1749865Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m [2026-02-13 18:25:53] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-13T18:25:53.1870460Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m [2026-02-13 18:25:53] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-13T18:25:53.1950146Z [0;36m(Worker_DP1_TP5_EP13 pid=683)[0;0m [2026-02-13 18:25:53] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-13T18:25:53.2240080Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m [2026-02-13 18:25:53] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-13T18:25:53.3247806Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-13 18:25:53 [layer.py:475] [EP Rank 3/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-13T18:25:53.3260518Z [0;36m(Worker_DP1_TP4_EP12 pid=581)[0;0m INFO 02-13 18:25:53 [layer.py:475] [EP Rank 12/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-13T18:25:53.3271167Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-13 18:25:53 [layer.py:475] [EP Rank 10/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-13T18:25:53.3281841Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-13 18:25:53 [layer.py:475] [EP Rank 2/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-13T18:25:53.3292706Z [0;36m(Worker_DP1_TP5_EP13 pid=683)[0;0m INFO 02-13 18:25:53 [layer.py:475] [EP Rank 13/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-13T18:25:53.3302939Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-13 18:25:53 [layer.py:475] [EP Rank 4/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-13T18:25:53.3389223Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m INFO 02-13 18:25:53 [layer.py:475] [EP Rank 0/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-13T18:25:53.3412850Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-13 18:25:53 [layer.py:475] [EP Rank 7/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-13T18:25:53.4431177Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m [2026-02-13 18:25:53] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-13T18:25:53.5074943Z [0;36m(Worker_DP1_TP1_EP9 pid=279)[0;0m [2026-02-13 18:25:53] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-13T18:25:53.5325840Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m [2026-02-13 18:25:53] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-13T18:25:53.5440944Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m [2026-02-13 18:25:53] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-13T18:25:53.5558613Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-13 18:25:53 [layer.py:475] [EP Rank 1/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-13T18:25:53.5720045Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m [2026-02-13 18:25:53] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-13T18:25:53.5743062Z [0;36m(Worker_DP1_TP7_EP15 pid=891)[0;0m [2026-02-13 18:25:53] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-13T18:25:53.6205792Z [0;36m(Worker_DP1_TP1_EP9 pid=279)[0;0m INFO 02-13 18:25:53 [layer.py:475] [EP Rank 9/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-13T18:25:53.6271389Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m [2026-02-13 18:25:53] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-13T18:25:53.6538916Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-13 18:25:53 [layer.py:475] [EP Rank 6/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-13T18:25:53.6577993Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-13 18:25:53 [layer.py:475] [EP Rank 14/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-13T18:25:53.6620860Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m [2026-02-13 18:25:53] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-13T18:25:53.6986963Z [0;36m(Worker_DP1_TP7_EP15 pid=891)[0;0m INFO 02-13 18:25:53 [layer.py:475] [EP Rank 15/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-13T18:25:53.7037419Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-13 18:25:53 [layer.py:475] [EP Rank 5/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-13T18:25:53.7572273Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-13 18:25:53 [layer.py:475] [EP Rank 11/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-13T18:25:53.8221683Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m INFO 02-13 18:25:53 [layer.py:475] [EP Rank 8/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-13T18:25:55.1247471Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-13T18:25:55.1259319Z [0;36m(Worker_DP1_TP4_EP12 pid=581)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-13T18:25:55.1266766Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m   return func(*args, **kwargs)
2026-02-13T18:25:55.1275902Z [0;36m(Worker_DP1_TP4_EP12 pid=581)[0;0m   return func(*args, **kwargs)
2026-02-13T18:25:55.1286139Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-13T18:25:55.1294867Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m   return func(*args, **kwargs)
2026-02-13T18:25:55.1338156Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-13T18:25:55.1347183Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m   return func(*args, **kwargs)
2026-02-13T18:25:55.1393033Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-13T18:25:55.1402168Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m   return func(*args, **kwargs)
2026-02-13T18:25:55.1451971Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-13T18:25:55.1460889Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m   return func(*args, **kwargs)
2026-02-13T18:25:55.1608920Z [0;36m(Worker_DP1_TP1_EP9 pid=279)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-13T18:25:55.1619348Z [0;36m(Worker_DP1_TP1_EP9 pid=279)[0;0m   return func(*args, **kwargs)
2026-02-13T18:25:55.1634356Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-13T18:25:55.1642858Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m   return func(*args, **kwargs)
2026-02-13T18:25:55.1740890Z [0;36m(Worker_DP1_TP5_EP13 pid=683)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-13T18:25:55.1748921Z [0;36m(Worker_DP1_TP5_EP13 pid=683)[0;0m   return func(*args, **kwargs)
2026-02-13T18:25:55.1818745Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-13T18:25:55.1827759Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m   return func(*args, **kwargs)
2026-02-13T18:25:55.1917929Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-13T18:25:55.1926320Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m   return func(*args, **kwargs)
2026-02-13T18:25:55.1951459Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-13T18:25:55.1960356Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m   return func(*args, **kwargs)
2026-02-13T18:25:55.2042761Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m INFO 02-13 18:25:55 [fused_moe.py:207] [EP Rank 0/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-13T18:25:55.2061576Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-13 18:25:55 [fused_moe.py:207] [EP Rank 7/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-13T18:25:55.2089330Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-13 18:25:55 [fused_moe.py:207] [EP Rank 3/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-13T18:25:55.2116307Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-13 18:25:55 [fused_moe.py:207] [EP Rank 10/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-13T18:25:55.2137929Z [0;36m(Worker_DP1_TP4_EP12 pid=581)[0;0m INFO 02-13 18:25:55 [fused_moe.py:207] [EP Rank 12/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-13T18:25:55.2222799Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m INFO 02-13 18:25:55 [fused_moe.py:207] [EP Rank 8/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-13T18:25:55.2244154Z [0;36m(Worker_DP1_TP1_EP9 pid=279)[0;0m INFO 02-13 18:25:55 [fused_moe.py:207] [EP Rank 9/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-13T18:25:55.2359274Z [0;36m(Worker_DP1_TP5_EP13 pid=683)[0;0m INFO 02-13 18:25:55 [fused_moe.py:207] [EP Rank 13/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-13T18:25:55.2464443Z [0;36m(Worker_DP1_TP7_EP15 pid=891)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-13T18:25:55.2472355Z [0;36m(Worker_DP1_TP7_EP15 pid=891)[0;0m   return func(*args, **kwargs)
2026-02-13T18:25:55.2483646Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-13 18:25:55 [fused_moe.py:207] [EP Rank 2/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-13T18:25:55.2579093Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-13 18:25:55 [fused_moe.py:207] [EP Rank 6/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-13T18:25:55.2589357Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-13 18:25:55 [fused_moe.py:207] [EP Rank 14/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-13T18:25:55.2599669Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-13T18:25:55.2609067Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m   return func(*args, **kwargs)
2026-02-13T18:25:55.2618654Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-13 18:25:55 [fused_moe.py:207] [EP Rank 4/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-13T18:25:55.2967315Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-13T18:25:55.2975031Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m   return func(*args, **kwargs)
2026-02-13T18:25:55.2999939Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-13T18:25:55.3008278Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m   return func(*args, **kwargs)
2026-02-13T18:25:55.3077785Z [0;36m(Worker_DP1_TP7_EP15 pid=891)[0;0m INFO 02-13 18:25:55 [fused_moe.py:207] [EP Rank 15/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-13T18:25:55.3193037Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-13 18:25:55 [fused_moe.py:207] [EP Rank 5/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-13T18:25:55.3606567Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-13 18:25:55 [fused_moe.py:207] [EP Rank 1/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-13T18:25:55.3639037Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-13 18:25:55 [fused_moe.py:207] [EP Rank 11/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-13T18:25:55.5450945Z [0;36m(Worker_DP1_TP5_EP13 pid=683)[0;0m INFO 02-13 18:25:55 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-13T18:25:55.5478200Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-13 18:25:55 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-13T18:25:55.5604946Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-13 18:25:55 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-13T18:25:55.5647439Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-13 18:25:55 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-13T18:25:55.5673381Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-13 18:25:55 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-13T18:25:55.6341322Z [0;36m(Worker_DP1_TP7_EP15 pid=891)[0;0m INFO 02-13 18:25:55 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-13T18:25:55.6424761Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m INFO 02-13 18:25:55 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-13T18:25:55.6500072Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-13 18:25:55 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-13T18:25:55.6620818Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-13 18:25:55 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-13T18:25:55.7079760Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-13 18:25:55 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-13T18:25:55.7235466Z [0;36m(Worker_DP1_TP4_EP12 pid=581)[0;0m INFO 02-13 18:25:55 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-13T18:25:55.7259187Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-13 18:25:55 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-13T18:25:55.7268347Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-13 18:25:55 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-13T18:25:55.7426363Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m INFO 02-13 18:25:55 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-13T18:25:55.7510231Z [0;36m(Worker_DP1_TP1_EP9 pid=279)[0;0m INFO 02-13 18:25:55 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-13T18:25:55.7520455Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-13 18:25:55 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-13T18:25:58.8288976Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-13 18:25:58 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-13T18:25:59.0084519Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-13 18:25:59 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-13T18:25:59.0173279Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-13 18:25:59 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-13T18:25:59.0290219Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-13 18:25:59 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-13T18:25:59.0993183Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-13 18:25:59 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-13T18:25:59.1128860Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-13 18:25:59 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-13T18:25:59.1258452Z [0;36m(Worker_DP1_TP4_EP12 pid=581)[0;0m INFO 02-13 18:25:59 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-13T18:25:59.1292864Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-13 18:25:59 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-13T18:25:59.1684934Z [0;36m(Worker_DP1_TP7_EP15 pid=891)[0;0m INFO 02-13 18:25:59 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-13T18:25:59.1779555Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-13 18:25:59 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-13T18:25:59.2049177Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m INFO 02-13 18:25:59 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-13T18:25:59.2074988Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-13 18:25:59 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-13T18:25:59.2184435Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-13 18:25:59 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-13T18:25:59.2263047Z [0;36m(Worker_DP1_TP5_EP13 pid=683)[0;0m INFO 02-13 18:25:59 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-13T18:25:59.2598062Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:25:59.2598373Z Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
2026-02-13T18:25:59.2806439Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m INFO 02-13 18:25:59 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-13T18:25:59.3605989Z [0;36m(Worker_DP1_TP1_EP9 pid=279)[0;0m INFO 02-13 18:25:59 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-13T18:26:00.3390650Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:00.3391021Z Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:01<02:54,  1.08s/it]
2026-02-13T18:26:04.1316512Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:04.1316891Z Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:04<07:10,  2.68s/it]
2026-02-13T18:26:05.1515637Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:05.1516011Z Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:05<05:07,  1.92s/it]
2026-02-13T18:26:06.6054423Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:06.6054790Z Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:07<04:35,  1.74s/it]
2026-02-13T18:26:09.5467618Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:09.5467995Z Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:10<05:42,  2.17s/it]
2026-02-13T18:26:12.9965039Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:12.9965508Z Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:13<06:49,  2.61s/it]
2026-02-13T18:26:16.3036369Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:16.3036783Z Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:17<07:21,  2.83s/it]
2026-02-13T18:26:19.2830113Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:19.2830511Z Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:20<07:26,  2.88s/it]
2026-02-13T18:26:20.6394915Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:20.6395546Z Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:21<06:10,  2.40s/it]
2026-02-13T18:26:22.0040681Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:22.0041049Z Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:22<05:18,  2.08s/it]
2026-02-13T18:26:23.3828821Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:23.3829187Z Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:24<04:43,  1.87s/it]
2026-02-13T18:26:24.8192339Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:24.8192790Z Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:25<04:22,  1.74s/it]
2026-02-13T18:26:26.3781671Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:26.3782168Z Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:27<04:12,  1.68s/it]
2026-02-13T18:26:30.5664860Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:30.5665284Z Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:31<06:03,  2.44s/it]
2026-02-13T18:26:34.6619385Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:34.6619813Z Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:35<07:14,  2.94s/it]
2026-02-13T18:26:36.7658637Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:36.7659021Z Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:37<06:34,  2.69s/it]
2026-02-13T18:26:38.7804934Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:38.7805313Z Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:39<06:02,  2.49s/it]
2026-02-13T18:26:41.1546784Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:41.1547161Z Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:41<05:55,  2.45s/it]
2026-02-13T18:26:42.4513824Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:42.4514204Z Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:43<05:03,  2.11s/it]
2026-02-13T18:26:44.0300443Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:44.0300818Z Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:44<04:38,  1.95s/it]
2026-02-13T18:26:47.5773788Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:47.5774177Z Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:48<05:44,  2.43s/it]
2026-02-13T18:26:48.8609874Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:48.8610239Z Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:49<04:53,  2.08s/it]
2026-02-13T18:26:50.4338611Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:50.4338975Z Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:51<04:30,  1.93s/it]
2026-02-13T18:26:51.7212078Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:51.7212744Z Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:52<04:01,  1.74s/it]
2026-02-13T18:26:55.7467668Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:55.7468074Z Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:56<05:34,  2.42s/it]
2026-02-13T18:26:56.6381276Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:56.6381726Z Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:57<04:29,  1.96s/it]
2026-02-13T18:26:57.9555012Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:26:57.9555385Z Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:58<04:00,  1.77s/it]
2026-02-13T18:27:01.3066502Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:01.3066876Z Loading safetensors checkpoint shards:  17% Completed | 28/163 [01:02<05:03,  2.24s/it]
2026-02-13T18:27:03.0458930Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:03.0459348Z Loading safetensors checkpoint shards:  18% Completed | 29/163 [01:03<04:40,  2.09s/it]
2026-02-13T18:27:04.7289262Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:04.7289636Z Loading safetensors checkpoint shards:  18% Completed | 30/163 [01:05<04:22,  1.97s/it]
2026-02-13T18:27:06.2460496Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:06.2460884Z Loading safetensors checkpoint shards:  19% Completed | 31/163 [01:06<04:02,  1.83s/it]
2026-02-13T18:27:08.0229519Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:08.0229889Z Loading safetensors checkpoint shards:  20% Completed | 32/163 [01:08<03:58,  1.82s/it]
2026-02-13T18:27:09.7937754Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:09.7938177Z Loading safetensors checkpoint shards:  20% Completed | 33/163 [01:10<03:54,  1.80s/it]
2026-02-13T18:27:11.5584618Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:11.5585054Z Loading safetensors checkpoint shards:  21% Completed | 34/163 [01:12<03:51,  1.79s/it]
2026-02-13T18:27:12.9251039Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:12.9251505Z Loading safetensors checkpoint shards:  21% Completed | 35/163 [01:13<03:33,  1.66s/it]
2026-02-13T18:27:18.7875646Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:18.7876059Z Loading safetensors checkpoint shards:  22% Completed | 36/163 [01:19<06:11,  2.92s/it]
2026-02-13T18:27:20.3705319Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:20.3705700Z Loading safetensors checkpoint shards:  23% Completed | 37/163 [01:21<05:17,  2.52s/it]
2026-02-13T18:27:21.8949499Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:21.8949861Z Loading safetensors checkpoint shards:  23% Completed | 38/163 [01:22<04:37,  2.22s/it]
2026-02-13T18:27:23.8167806Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:23.8168166Z Loading safetensors checkpoint shards:  24% Completed | 39/163 [01:24<04:24,  2.13s/it]
2026-02-13T18:27:29.2003539Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:29.2003925Z Loading safetensors checkpoint shards:  25% Completed | 40/163 [01:29<06:22,  3.11s/it]
2026-02-13T18:27:30.4427802Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:30.4428226Z Loading safetensors checkpoint shards:  25% Completed | 41/163 [01:31<05:10,  2.55s/it]
2026-02-13T18:27:31.9485937Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:31.9486313Z Loading safetensors checkpoint shards:  26% Completed | 42/163 [01:32<04:30,  2.24s/it]
2026-02-13T18:27:36.4144795Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:36.4145170Z Loading safetensors checkpoint shards:  26% Completed | 43/163 [01:37<05:48,  2.90s/it]
2026-02-13T18:27:37.7417973Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:37.7418475Z Loading safetensors checkpoint shards:  27% Completed | 44/163 [01:38<04:49,  2.43s/it]
2026-02-13T18:27:39.1224864Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:39.1225233Z Loading safetensors checkpoint shards:  28% Completed | 45/163 [01:39<04:09,  2.12s/it]
2026-02-13T18:27:40.5170605Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:40.5170990Z Loading safetensors checkpoint shards:  28% Completed | 46/163 [01:41<03:42,  1.90s/it]
2026-02-13T18:27:49.7938546Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:49.7938956Z Loading safetensors checkpoint shards:  29% Completed | 47/163 [01:50<07:57,  4.11s/it]
2026-02-13T18:27:52.8713326Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:52.8713720Z Loading safetensors checkpoint shards:  29% Completed | 48/163 [01:53<07:17,  3.80s/it]
2026-02-13T18:27:53.0394566Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:53.0394969Z Loading safetensors checkpoint shards:  30% Completed | 49/163 [01:53<05:09,  2.71s/it]
2026-02-13T18:27:55.0673001Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:55.0673368Z Loading safetensors checkpoint shards:  31% Completed | 50/163 [01:55<04:43,  2.51s/it]
2026-02-13T18:27:57.9857842Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:57.9858255Z Loading safetensors checkpoint shards:  31% Completed | 51/163 [01:58<04:54,  2.63s/it]
2026-02-13T18:27:59.5852645Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:27:59.5853309Z Loading safetensors checkpoint shards:  32% Completed | 52/163 [02:00<04:17,  2.32s/it]
2026-02-13T18:28:03.4022524Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:03.4022893Z Loading safetensors checkpoint shards:  33% Completed | 53/163 [02:04<05:04,  2.77s/it]
2026-02-13T18:28:04.6761759Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:04.6762232Z Loading safetensors checkpoint shards:  33% Completed | 54/163 [02:05<04:13,  2.32s/it]
2026-02-13T18:28:05.6926201Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:05.6926563Z Loading safetensors checkpoint shards:  34% Completed | 55/163 [02:06<03:28,  1.93s/it]
2026-02-13T18:28:07.2317761Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:07.2318138Z Loading safetensors checkpoint shards:  34% Completed | 56/163 [02:07<03:12,  1.80s/it]
2026-02-13T18:28:08.9858197Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:08.9858558Z Loading safetensors checkpoint shards:  35% Completed | 57/163 [02:09<03:10,  1.80s/it]
2026-02-13T18:28:11.5186934Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:11.5187390Z Loading safetensors checkpoint shards:  36% Completed | 58/163 [02:12<03:32,  2.02s/it]
2026-02-13T18:28:16.9747186Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:16.9747568Z Loading safetensors checkpoint shards:  36% Completed | 59/163 [02:17<05:17,  3.05s/it]
2026-02-13T18:28:18.1063188Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:18.1063565Z Loading safetensors checkpoint shards:  37% Completed | 60/163 [02:18<04:14,  2.47s/it]
2026-02-13T18:28:19.3355258Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:19.3355624Z Loading safetensors checkpoint shards:  37% Completed | 61/163 [02:20<03:34,  2.10s/it]
2026-02-13T18:28:22.5341935Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:22.5342445Z Loading safetensors checkpoint shards:  38% Completed | 62/163 [02:23<04:05,  2.43s/it]
2026-02-13T18:28:23.3514013Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:23.3514380Z Loading safetensors checkpoint shards:  39% Completed | 63/163 [02:24<03:14,  1.95s/it]
2026-02-13T18:28:24.5818109Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:24.5818521Z Loading safetensors checkpoint shards:  39% Completed | 64/163 [02:25<02:51,  1.73s/it]
2026-02-13T18:28:30.3882449Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:30.3882840Z Loading safetensors checkpoint shards:  40% Completed | 65/163 [02:31<04:49,  2.95s/it]
2026-02-13T18:28:33.0100747Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:33.0101107Z Loading safetensors checkpoint shards:  40% Completed | 66/163 [02:33<04:36,  2.85s/it]
2026-02-13T18:28:34.1804490Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:34.1804866Z Loading safetensors checkpoint shards:  41% Completed | 67/163 [02:34<03:45,  2.35s/it]
2026-02-13T18:28:37.3481455Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:37.3481831Z Loading safetensors checkpoint shards:  42% Completed | 68/163 [02:38<04:06,  2.59s/it]
2026-02-13T18:28:38.0737412Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:38.0737787Z Loading safetensors checkpoint shards:  42% Completed | 69/163 [02:38<03:11,  2.03s/it]
2026-02-13T18:28:38.9576726Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:38.9577100Z Loading safetensors checkpoint shards:  43% Completed | 70/163 [02:39<02:37,  1.69s/it]
2026-02-13T18:28:41.6813779Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:41.6814138Z Loading safetensors checkpoint shards:  44% Completed | 71/163 [02:42<03:03,  2.00s/it]
2026-02-13T18:28:44.9911965Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:44.9912465Z Loading safetensors checkpoint shards:  44% Completed | 72/163 [02:45<03:37,  2.39s/it]
2026-02-13T18:28:46.3835967Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:46.3836369Z Loading safetensors checkpoint shards:  45% Completed | 73/163 [02:47<03:08,  2.09s/it]
2026-02-13T18:28:47.9776981Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:47.9777603Z Loading safetensors checkpoint shards:  45% Completed | 74/163 [02:48<02:52,  1.94s/it]
2026-02-13T18:28:49.4452509Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:49.4452880Z Loading safetensors checkpoint shards:  46% Completed | 75/163 [02:50<02:38,  1.80s/it]
2026-02-13T18:28:50.8014133Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:50.8014497Z Loading safetensors checkpoint shards:  47% Completed | 76/163 [02:51<02:25,  1.67s/it]
2026-02-13T18:28:54.6343763Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:54.6344151Z Loading safetensors checkpoint shards:  47% Completed | 77/163 [02:55<03:19,  2.32s/it]
2026-02-13T18:28:55.9583569Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:55.9583945Z Loading safetensors checkpoint shards:  48% Completed | 78/163 [02:56<02:51,  2.02s/it]
2026-02-13T18:28:57.3315919Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:28:57.3316315Z Loading safetensors checkpoint shards:  48% Completed | 79/163 [02:58<02:33,  1.83s/it]
2026-02-13T18:29:00.7209430Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:00.7209803Z Loading safetensors checkpoint shards:  49% Completed | 80/163 [03:01<03:10,  2.29s/it]
2026-02-13T18:29:04.0564836Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:04.0565218Z Loading safetensors checkpoint shards:  50% Completed | 81/163 [03:04<03:33,  2.61s/it]
2026-02-13T18:29:05.0400304Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:05.0400669Z Loading safetensors checkpoint shards:  50% Completed | 82/163 [03:05<02:51,  2.12s/it]
2026-02-13T18:29:08.1223050Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:08.1223431Z Loading safetensors checkpoint shards:  51% Completed | 83/163 [03:08<03:12,  2.41s/it]
2026-02-13T18:29:09.7289600Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:09.7289976Z Loading safetensors checkpoint shards:  52% Completed | 84/163 [03:10<02:51,  2.17s/it]
2026-02-13T18:29:13.0843192Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:13.0843615Z Loading safetensors checkpoint shards:  52% Completed | 85/163 [03:13<03:16,  2.52s/it]
2026-02-13T18:29:14.4272561Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:14.4273026Z Loading safetensors checkpoint shards:  53% Completed | 86/163 [03:15<02:47,  2.17s/it]
2026-02-13T18:29:16.4938210Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:16.4938579Z Loading safetensors checkpoint shards:  53% Completed | 87/163 [03:17<02:42,  2.14s/it]
2026-02-13T18:29:20.4197856Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:20.4198238Z Loading safetensors checkpoint shards:  54% Completed | 88/163 [03:21<03:20,  2.68s/it]
2026-02-13T18:29:21.7181030Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:21.7181828Z Loading safetensors checkpoint shards:  55% Completed | 89/163 [03:22<02:47,  2.26s/it]
2026-02-13T18:29:25.5410912Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:25.5411325Z Loading safetensors checkpoint shards:  55% Completed | 90/163 [03:26<03:19,  2.73s/it]
2026-02-13T18:29:26.5321137Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:26.5321514Z Loading safetensors checkpoint shards:  56% Completed | 91/163 [03:27<02:39,  2.21s/it]
2026-02-13T18:29:30.6872721Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:30.6873206Z Loading safetensors checkpoint shards:  56% Completed | 92/163 [03:31<03:18,  2.79s/it]
2026-02-13T18:29:31.4911663Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:31.4912149Z Loading safetensors checkpoint shards:  57% Completed | 93/163 [03:32<02:33,  2.20s/it]
2026-02-13T18:29:32.7268773Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:32.7269311Z Loading safetensors checkpoint shards:  58% Completed | 94/163 [03:33<02:11,  1.91s/it]
2026-02-13T18:29:34.2628720Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:34.2629101Z Loading safetensors checkpoint shards:  58% Completed | 95/163 [03:35<02:02,  1.80s/it]
2026-02-13T18:29:37.9091958Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:37.9092460Z Loading safetensors checkpoint shards:  59% Completed | 96/163 [03:38<02:37,  2.35s/it]
2026-02-13T18:29:39.1532378Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:39.1532774Z Loading safetensors checkpoint shards:  60% Completed | 97/163 [03:39<02:13,  2.02s/it]
2026-02-13T18:29:40.4457083Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:40.4457474Z Loading safetensors checkpoint shards:  60% Completed | 98/163 [03:41<01:57,  1.80s/it]
2026-02-13T18:29:41.9392334Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:41.9392741Z Loading safetensors checkpoint shards:  61% Completed | 99/163 [03:42<01:49,  1.71s/it]
2026-02-13T18:29:43.3816292Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:43.3816690Z Loading safetensors checkpoint shards:  61% Completed | 100/163 [03:44<01:42,  1.63s/it]
2026-02-13T18:29:44.6695371Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:44.6695755Z Loading safetensors checkpoint shards:  62% Completed | 101/163 [03:45<01:34,  1.53s/it]
2026-02-13T18:29:48.2407484Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:48.2407866Z Loading safetensors checkpoint shards:  63% Completed | 102/163 [03:48<02:10,  2.14s/it]
2026-02-13T18:29:50.6916130Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:50.6916510Z Loading safetensors checkpoint shards:  63% Completed | 103/163 [03:51<02:13,  2.23s/it]
2026-02-13T18:29:53.6010824Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:53.6011201Z Loading safetensors checkpoint shards:  64% Completed | 104/163 [03:54<02:23,  2.44s/it]
2026-02-13T18:29:54.4524677Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:54.4525070Z Loading safetensors checkpoint shards:  64% Completed | 105/163 [03:55<01:53,  1.96s/it]
2026-02-13T18:29:57.8336139Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:57.8336559Z Loading safetensors checkpoint shards:  65% Completed | 106/163 [03:58<02:16,  2.39s/it]
2026-02-13T18:29:58.9870237Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:29:58.9870610Z Loading safetensors checkpoint shards:  66% Completed | 107/163 [03:59<01:52,  2.02s/it]
2026-02-13T18:30:01.8882215Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:01.8882590Z Loading safetensors checkpoint shards:  66% Completed | 108/163 [04:02<02:05,  2.28s/it]
2026-02-13T18:30:03.1263752Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:03.1264144Z Loading safetensors checkpoint shards:  67% Completed | 109/163 [04:03<01:46,  1.97s/it]
2026-02-13T18:30:06.3412582Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:06.3412951Z Loading safetensors checkpoint shards:  67% Completed | 110/163 [04:07<02:04,  2.34s/it]
2026-02-13T18:30:09.7772573Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:09.7772954Z Loading safetensors checkpoint shards:  68% Completed | 111/163 [04:10<02:18,  2.67s/it]
2026-02-13T18:30:11.8798266Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:11.8798660Z Loading safetensors checkpoint shards:  69% Completed | 112/163 [04:12<02:07,  2.50s/it]
2026-02-13T18:30:14.7484142Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:14.7484512Z Loading safetensors checkpoint shards:  69% Completed | 113/163 [04:15<02:10,  2.61s/it]
2026-02-13T18:30:15.9538974Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:15.9539363Z Loading safetensors checkpoint shards:  70% Completed | 114/163 [04:16<01:47,  2.19s/it]
2026-02-13T18:30:17.4373663Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:17.4374047Z Loading safetensors checkpoint shards:  71% Completed | 115/163 [04:18<01:34,  1.98s/it]
2026-02-13T18:30:21.2611452Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:21.2611937Z Loading safetensors checkpoint shards:  71% Completed | 116/163 [04:22<01:58,  2.53s/it]
2026-02-13T18:30:22.5244876Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:22.5245781Z Loading safetensors checkpoint shards:  72% Completed | 117/163 [04:23<01:38,  2.15s/it]
2026-02-13T18:30:25.7597352Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:25.7597736Z Loading safetensors checkpoint shards:  72% Completed | 118/163 [04:26<01:51,  2.48s/it]
2026-02-13T18:30:26.8800722Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:26.8801090Z Loading safetensors checkpoint shards:  73% Completed | 119/163 [04:27<01:31,  2.07s/it]
2026-02-13T18:30:28.4214964Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:28.4215327Z Loading safetensors checkpoint shards:  74% Completed | 120/163 [04:29<01:22,  1.91s/it]
2026-02-13T18:30:29.8968688Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:29.8969056Z Loading safetensors checkpoint shards:  74% Completed | 121/163 [04:30<01:14,  1.78s/it]
2026-02-13T18:30:31.4436795Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:31.4437161Z Loading safetensors checkpoint shards:  75% Completed | 122/163 [04:32<01:10,  1.71s/it]
2026-02-13T18:30:34.9973721Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:34.9974101Z Loading safetensors checkpoint shards:  75% Completed | 123/163 [04:35<01:30,  2.26s/it]
2026-02-13T18:30:36.3486133Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:36.3486501Z Loading safetensors checkpoint shards:  76% Completed | 124/163 [04:37<01:17,  1.99s/it]
2026-02-13T18:30:39.6517129Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:39.6517520Z Loading safetensors checkpoint shards:  77% Completed | 125/163 [04:40<01:30,  2.38s/it]
2026-02-13T18:30:41.0880553Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:41.0880932Z Loading safetensors checkpoint shards:  77% Completed | 126/163 [04:41<01:17,  2.10s/it]
2026-02-13T18:30:42.4302425Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:42.4302825Z Loading safetensors checkpoint shards:  78% Completed | 127/163 [04:43<01:07,  1.87s/it]
2026-02-13T18:30:46.0390774Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:46.0391180Z Loading safetensors checkpoint shards:  79% Completed | 128/163 [04:46<01:23,  2.39s/it]
2026-02-13T18:30:48.6486740Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:48.6487141Z Loading safetensors checkpoint shards:  79% Completed | 129/163 [04:49<01:23,  2.46s/it]
2026-02-13T18:30:49.8833427Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:49.8833795Z Loading safetensors checkpoint shards:  80% Completed | 130/163 [04:50<01:08,  2.09s/it]
2026-02-13T18:30:51.1788950Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:51.1789323Z Loading safetensors checkpoint shards:  80% Completed | 131/163 [04:51<00:59,  1.85s/it]
2026-02-13T18:30:52.6926862Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:52.6927237Z Loading safetensors checkpoint shards:  81% Completed | 132/163 [04:53<00:54,  1.75s/it]
2026-02-13T18:30:56.4840351Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:56.4840765Z Loading safetensors checkpoint shards:  82% Completed | 133/163 [04:57<01:10,  2.36s/it]
2026-02-13T18:30:57.9855751Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:30:57.9856130Z Loading safetensors checkpoint shards:  82% Completed | 134/163 [04:58<01:01,  2.10s/it]
2026-02-13T18:31:02.0611236Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:02.0611614Z Loading safetensors checkpoint shards:  83% Completed | 135/163 [05:02<01:15,  2.70s/it]
2026-02-13T18:31:02.7107524Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:02.7107899Z Loading safetensors checkpoint shards:  83% Completed | 136/163 [05:03<00:56,  2.08s/it]
2026-02-13T18:31:03.8703903Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:03.8704265Z Loading safetensors checkpoint shards:  84% Completed | 137/163 [05:04<00:46,  1.81s/it]
2026-02-13T18:31:08.8149996Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:08.8150376Z Loading safetensors checkpoint shards:  85% Completed | 138/163 [05:09<01:08,  2.75s/it]
2026-02-13T18:31:11.2325480Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:11.2325882Z Loading safetensors checkpoint shards:  85% Completed | 139/163 [05:11<01:03,  2.65s/it]
2026-02-13T18:31:11.8660316Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:11.8660677Z Loading safetensors checkpoint shards:  86% Completed | 140/163 [05:12<00:47,  2.04s/it]
2026-02-13T18:31:13.3617187Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:13.3617559Z Loading safetensors checkpoint shards:  87% Completed | 141/163 [05:14<00:41,  1.88s/it]
2026-02-13T18:31:14.8099366Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:14.8099734Z Loading safetensors checkpoint shards:  87% Completed | 142/163 [05:15<00:36,  1.75s/it]
2026-02-13T18:31:18.3217441Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:18.3217886Z Loading safetensors checkpoint shards:  88% Completed | 143/163 [05:19<00:45,  2.28s/it]
2026-02-13T18:31:20.8586738Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:20.8587138Z Loading safetensors checkpoint shards:  88% Completed | 144/163 [05:21<00:44,  2.36s/it]
2026-02-13T18:31:23.3673653Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:23.3674036Z Loading safetensors checkpoint shards:  89% Completed | 145/163 [05:24<00:43,  2.40s/it]
2026-02-13T18:31:25.6621258Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:25.6621626Z Loading safetensors checkpoint shards:  90% Completed | 146/163 [05:26<00:40,  2.37s/it]
2026-02-13T18:31:27.2604908Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:27.2605282Z Loading safetensors checkpoint shards:  90% Completed | 147/163 [05:27<00:34,  2.14s/it]
2026-02-13T18:31:28.5701467Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:28.5701843Z Loading safetensors checkpoint shards:  91% Completed | 148/163 [05:29<00:28,  1.89s/it]
2026-02-13T18:31:29.9258972Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:29.9259345Z Loading safetensors checkpoint shards:  91% Completed | 149/163 [05:30<00:24,  1.73s/it]
2026-02-13T18:31:33.0724691Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:33.0725063Z Loading safetensors checkpoint shards:  93% Completed | 151/163 [05:33<00:19,  1.66s/it]
2026-02-13T18:31:34.4550871Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:34.4551252Z Loading safetensors checkpoint shards:  93% Completed | 152/163 [05:35<00:17,  1.59s/it]
2026-02-13T18:31:35.7642144Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:35.7642521Z Loading safetensors checkpoint shards:  94% Completed | 153/163 [05:36<00:15,  1.52s/it]
2026-02-13T18:31:38.0457294Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:38.0457664Z Loading safetensors checkpoint shards:  94% Completed | 154/163 [05:38<00:15,  1.72s/it]
2026-02-13T18:31:39.4216281Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:39.4216924Z Loading safetensors checkpoint shards:  95% Completed | 155/163 [05:40<00:13,  1.63s/it]
2026-02-13T18:31:40.5888725Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:40.5889122Z Loading safetensors checkpoint shards:  96% Completed | 156/163 [05:41<00:10,  1.50s/it]
2026-02-13T18:31:42.1996238Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:42.1996631Z Loading safetensors checkpoint shards:  96% Completed | 157/163 [05:42<00:09,  1.53s/it]
2026-02-13T18:31:46.6431417Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:46.6431792Z Loading safetensors checkpoint shards:  97% Completed | 158/163 [05:47<00:11,  2.38s/it]
2026-02-13T18:31:47.7933753Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:47.7934126Z Loading safetensors checkpoint shards:  98% Completed | 159/163 [05:48<00:08,  2.02s/it]
2026-02-13T18:31:49.4178247Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:49.4178626Z Loading safetensors checkpoint shards:  98% Completed | 160/163 [05:50<00:05,  1.90s/it]
2026-02-13T18:31:50.5595679Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:50.5596461Z Loading safetensors checkpoint shards:  99% Completed | 161/163 [05:51<00:03,  1.68s/it]
2026-02-13T18:31:54.3533941Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:54.3534328Z Loading safetensors checkpoint shards:  99% Completed | 162/163 [05:55<00:02,  2.31s/it]
2026-02-13T18:31:55.7501597Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:55.7501967Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [05:56<00:00,  2.04s/it]
2026-02-13T18:31:55.7525263Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:55.7525577Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [05:56<00:00,  2.19s/it]
2026-02-13T18:31:55.7535426Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:31:55.7662326Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m INFO 02-13 18:31:55 [default_loader.py:291] Loading weights took 356.51 seconds
2026-02-13T18:31:56.6865400Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m INFO 02-13 18:31:56 [default_loader.py:291] Loading weights took 357.35 seconds
2026-02-13T18:32:10.1056531Z INFO 02-13 18:32:10 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:32:10.1056970Z INFO 02-13 18:32:10 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:32:10.1058524Z INFO 02-13 18:32:10 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:32:10.1069699Z INFO 02-13 18:32:10 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:32:10.1080378Z INFO 02-13 18:32:10 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:32:10.1090522Z INFO 02-13 18:32:10 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:32:10.1100061Z INFO 02-13 18:32:10 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:32:10.1111810Z INFO 02-13 18:32:10 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:32:10.1120864Z INFO 02-13 18:32:10 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:32:10.1130840Z INFO 02-13 18:32:10 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:32:10.1142523Z INFO 02-13 18:32:10 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:32:10.1150731Z INFO 02-13 18:32:10 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:32:10.1162410Z INFO 02-13 18:32:10 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:32:10.1172323Z INFO 02-13 18:32:10 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:32:10.1182665Z INFO 02-13 18:32:10 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:32:10.1220399Z INFO 02-13 18:32:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:32:10.1230140Z INFO 02-13 18:32:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:32:10.1240558Z INFO 02-13 18:32:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:32:10.1250505Z INFO 02-13 18:32:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:32:10.1260447Z INFO 02-13 18:32:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:32:10.1269791Z INFO 02-13 18:32:10 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:32:10.1279676Z INFO 02-13 18:32:10 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:32:10.1289564Z INFO 02-13 18:32:10 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:32:10.1299803Z INFO 02-13 18:32:10 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:32:10.1309349Z INFO 02-13 18:32:10 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:32:10.1319257Z INFO 02-13 18:32:10 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:32:10.1330084Z INFO 02-13 18:32:10 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:32:10.1339407Z INFO 02-13 18:32:10 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:32:10.1349198Z INFO 02-13 18:32:10 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:32:10.1359849Z INFO 02-13 18:32:10 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:32:10.1370118Z INFO 02-13 18:32:10 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:32:10.1380759Z INFO 02-13 18:32:10 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:32:10.1390621Z INFO 02-13 18:32:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:32:10.1401087Z INFO 02-13 18:32:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:32:10.1410753Z INFO 02-13 18:32:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:32:10.1420701Z INFO 02-13 18:32:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:32:10.1741126Z INFO 02-13 18:32:10 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:32:10.1750424Z INFO 02-13 18:32:10 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:32:10.1759541Z INFO 02-13 18:32:10 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:32:10.1769625Z INFO 02-13 18:32:10 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:32:10.1783625Z INFO 02-13 18:32:10 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:32:10.1788203Z INFO 02-13 18:32:10 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:32:10.1798957Z INFO 02-13 18:32:10 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:32:10.1809779Z INFO 02-13 18:32:10 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:32:10.1822980Z INFO 02-13 18:32:10 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:32:10.1832859Z INFO 02-13 18:32:10 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:32:10.1842381Z INFO 02-13 18:32:10 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:32:10.1852307Z INFO 02-13 18:32:10 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:32:10.1862224Z INFO 02-13 18:32:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:32:10.1871424Z INFO 02-13 18:32:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:32:10.1881609Z INFO 02-13 18:32:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:32:10.1891387Z INFO 02-13 18:32:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:32:10.1901943Z INFO 02-13 18:32:10 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:32:10.1910915Z INFO 02-13 18:32:10 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:32:10.1923601Z INFO 02-13 18:32:10 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:32:10.1985096Z INFO 02-13 18:32:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:32:10.2606386Z INFO 02-13 18:32:10 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:32:10.2615158Z INFO 02-13 18:32:10 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:32:10.2624403Z INFO 02-13 18:32:10 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:32:10.2689358Z INFO 02-13 18:32:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:32:10.3027716Z INFO 02-13 18:32:10 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-13T18:32:10.3042468Z INFO 02-13 18:32:10 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-13T18:32:10.3047129Z INFO 02-13 18:32:10 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-13T18:32:10.3180478Z INFO 02-13 18:32:10 [__init__.py:217] Platform plugin ascend is activated
2026-02-13T18:32:33.2475789Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m WARNING 02-13 18:32:33 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-13T18:32:33.2484219Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m WARNING 02-13 18:32:33 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-13T18:32:33.2642672Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m WARNING 02-13 18:32:33 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-13T18:32:33.2764221Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m WARNING 02-13 18:32:33 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-13T18:32:33.2775576Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m WARNING 02-13 18:32:33 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-13T18:32:33.2800222Z [0;36m(Worker_DP1_TP1_EP9 pid=279)[0;0m WARNING 02-13 18:32:33 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-13T18:32:33.2829295Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m WARNING 02-13 18:32:33 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-13T18:32:33.2972401Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m WARNING 02-13 18:32:33 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-13T18:32:33.2978525Z [0;36m(Worker_DP1_TP5_EP13 pid=683)[0;0m WARNING 02-13 18:32:33 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-13T18:32:33.3025419Z [0;36m(Worker_DP1_TP4_EP12 pid=581)[0;0m WARNING 02-13 18:32:33 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-13T18:32:33.3035377Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-13 18:32:33 [model_runner_v1.py:2315] Loading drafter model...
2026-02-13T18:32:33.3052627Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m WARNING 02-13 18:32:33 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-13T18:32:33.3124659Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-13 18:32:33 [model_runner_v1.py:2315] Loading drafter model...
2026-02-13T18:32:33.3129148Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m WARNING 02-13 18:32:33 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-13T18:32:33.3138024Z [0;36m(Worker_DP1_TP7_EP15 pid=891)[0;0m WARNING 02-13 18:32:33 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-13T18:32:33.3188249Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m WARNING 02-13 18:32:33 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-13T18:32:33.3198132Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-13 18:32:33 [model_runner_v1.py:2315] Loading drafter model...
2026-02-13T18:32:33.3208263Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m WARNING 02-13 18:32:33 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-13T18:32:33.3317267Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m INFO 02-13 18:32:33 [model_runner_v1.py:2315] Loading drafter model...
2026-02-13T18:32:33.3360216Z [0;36m(Worker_DP1_TP1_EP9 pid=279)[0;0m INFO 02-13 18:32:33 [model_runner_v1.py:2315] Loading drafter model...
2026-02-13T18:32:33.3462429Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-13 18:32:33 [model_runner_v1.py:2315] Loading drafter model...
2026-02-13T18:32:33.3493702Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-13 18:32:33 [model_runner_v1.py:2315] Loading drafter model...
2026-02-13T18:32:33.3523673Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m WARNING 02-13 18:32:33 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-13T18:32:33.3597394Z [0;36m(Worker_DP1_TP4_EP12 pid=581)[0;0m INFO 02-13 18:32:33 [model_runner_v1.py:2315] Loading drafter model...
2026-02-13T18:32:33.3606806Z [0;36m(Worker_DP1_TP5_EP13 pid=683)[0;0m INFO 02-13 18:32:33 [model_runner_v1.py:2315] Loading drafter model...
2026-02-13T18:32:33.3618986Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-13 18:32:33 [model_runner_v1.py:2315] Loading drafter model...
2026-02-13T18:32:33.3687146Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-13 18:32:33 [model_runner_v1.py:2315] Loading drafter model...
2026-02-13T18:32:33.3728490Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-13 18:32:33 [model_runner_v1.py:2315] Loading drafter model...
2026-02-13T18:32:33.3761886Z [0;36m(Worker_DP1_TP7_EP15 pid=891)[0;0m INFO 02-13 18:32:33 [model_runner_v1.py:2315] Loading drafter model...
2026-02-13T18:32:33.3823554Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-13 18:32:33 [model_runner_v1.py:2315] Loading drafter model...
2026-02-13T18:32:33.4207329Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:33.4209187Z Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
2026-02-13T18:32:33.4385392Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-13 18:32:33 [model_runner_v1.py:2315] Loading drafter model...
2026-02-13T18:32:33.4759819Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m INFO 02-13 18:32:33 [model_runner_v1.py:2315] Loading drafter model...
2026-02-13T18:32:34.5456340Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:34.5456734Z Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:01<03:02,  1.12s/it]
2026-02-13T18:32:35.6645475Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:35.6645826Z Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:02<03:00,  1.12s/it]
2026-02-13T18:32:36.8624430Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:36.8624777Z Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:03<03:04,  1.16s/it]
2026-02-13T18:32:38.0314399Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:38.0314745Z Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:04<03:04,  1.16s/it]
2026-02-13T18:32:39.1725885Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:39.1726245Z Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:05<03:02,  1.15s/it]
2026-02-13T18:32:40.3067862Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:40.3068227Z Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:06<03:00,  1.15s/it]
2026-02-13T18:32:41.4644549Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:41.4644906Z Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:08<02:59,  1.15s/it]
2026-02-13T18:32:42.4559627Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:42.4560064Z Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:09<02:50,  1.10s/it]
2026-02-13T18:32:43.5311752Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:43.5312217Z Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:10<02:48,  1.09s/it]
2026-02-13T18:32:44.7007072Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:44.7007429Z Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:11<02:50,  1.12s/it]
2026-02-13T18:32:45.8627367Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:45.8627724Z Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:12<02:51,  1.13s/it]
2026-02-13T18:32:47.1315697Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:47.1316077Z Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:13<02:55,  1.16s/it]
2026-02-13T18:32:48.2929072Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:48.2929420Z Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:14<02:55,  1.17s/it]
2026-02-13T18:32:49.5627662Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:49.5628027Z Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:16<02:58,  1.20s/it]
2026-02-13T18:32:50.6385404Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:50.6385788Z Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:17<02:52,  1.16s/it]
2026-02-13T18:32:51.6195013Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:51.6195413Z Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:18<02:43,  1.11s/it]
2026-02-13T18:32:52.6006840Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:52.6007227Z Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:19<02:36,  1.07s/it]
2026-02-13T18:32:53.5146354Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:53.5146714Z Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:20<02:28,  1.02s/it]
2026-02-13T18:32:54.5634463Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:54.5634830Z Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:21<02:28,  1.03s/it]
2026-02-13T18:32:55.6911017Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:55.6911376Z Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:22<02:31,  1.06s/it]
2026-02-13T18:32:56.7803605Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:56.7803987Z Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:23<02:31,  1.07s/it]
2026-02-13T18:32:57.9059139Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:57.9059508Z Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:24<02:33,  1.09s/it]
2026-02-13T18:32:58.9721557Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:32:58.9721912Z Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:25<02:31,  1.08s/it]
2026-02-13T18:33:00.0778667Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:00.0779032Z Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:26<02:31,  1.09s/it]
2026-02-13T18:33:01.1251711Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:01.1252188Z Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:27<02:28,  1.08s/it]
2026-02-13T18:33:02.5346500Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:02.5346862Z Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:29<02:41,  1.18s/it]
2026-02-13T18:33:03.7349724Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:03.7350363Z Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:30<02:40,  1.18s/it]
2026-02-13T18:33:04.7866146Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:04.7866561Z Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:31<02:34,  1.14s/it]
2026-02-13T18:33:05.9364233Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:05.9364601Z Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:32<02:33,  1.15s/it]
2026-02-13T18:33:07.0352271Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:07.0352648Z Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:33<02:30,  1.13s/it]
2026-02-13T18:33:08.2201508Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:08.2201875Z Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:34<02:31,  1.15s/it]
2026-02-13T18:33:09.3306338Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:09.3306704Z Loading safetensors checkpoint shards:  20% Completed | 32/163 [00:35<02:28,  1.14s/it]
2026-02-13T18:33:10.4213444Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:10.4214106Z Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:36<02:25,  1.12s/it]
2026-02-13T18:33:11.5028897Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:11.5029282Z Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:38<02:23,  1.11s/it]
2026-02-13T18:33:12.6593794Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:12.6594171Z Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:39<02:23,  1.12s/it]
2026-02-13T18:33:13.6677931Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:13.6678395Z Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:40<02:18,  1.09s/it]
2026-02-13T18:33:14.7508167Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:14.7508617Z Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:41<02:17,  1.09s/it]
2026-02-13T18:33:15.8481174Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:15.8481527Z Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:42<02:16,  1.09s/it]
2026-02-13T18:33:17.0394168Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:17.0394564Z Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:43<02:19,  1.12s/it]
2026-02-13T18:33:18.0909570Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:18.0909952Z Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:44<02:15,  1.10s/it]
2026-02-13T18:33:19.3013647Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:19.3014007Z Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:45<02:18,  1.13s/it]
2026-02-13T18:33:20.5165703Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:20.5166076Z Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:47<02:20,  1.16s/it]
2026-02-13T18:33:21.6681899Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:21.6682384Z Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:48<02:18,  1.16s/it]
2026-02-13T18:33:22.7319175Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:22.7319580Z Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:49<02:14,  1.13s/it]
2026-02-13T18:33:23.9368281Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:23.9368642Z Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:50<02:15,  1.15s/it]
2026-02-13T18:33:25.1666399Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:25.1666773Z Loading safetensors checkpoint shards:  28% Completed | 46/163 [00:51<02:17,  1.17s/it]
2026-02-13T18:33:25.9275208Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:25.9275574Z Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:52<02:01,  1.05s/it]
2026-02-13T18:33:27.1197774Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:27.1198153Z Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:53<01:35,  1.19it/s]
2026-02-13T18:33:28.3137517Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:28.3137973Z Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:54<01:44,  1.08it/s]
2026-02-13T18:33:29.3433943Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:29.3434440Z Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:55<01:46,  1.05it/s]
2026-02-13T18:33:30.4897575Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:30.4898052Z Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:57<01:51,  1.01s/it]
2026-02-13T18:33:31.6446852Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:31.6447381Z Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:58<01:55,  1.05s/it]
2026-02-13T18:33:32.8144242Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:32.8144708Z Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:59<01:58,  1.08s/it]
2026-02-13T18:33:34.0297554Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:34.0297985Z Loading safetensors checkpoint shards:  34% Completed | 55/163 [01:00<02:01,  1.12s/it]
2026-02-13T18:33:35.2035209Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:35.2035921Z Loading safetensors checkpoint shards:  34% Completed | 56/163 [01:01<02:01,  1.14s/it]
2026-02-13T18:33:36.4178419Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:36.4178853Z Loading safetensors checkpoint shards:  35% Completed | 57/163 [01:02<02:02,  1.16s/it]
2026-02-13T18:33:37.6208606Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:37.6209033Z Loading safetensors checkpoint shards:  36% Completed | 58/163 [01:04<02:03,  1.17s/it]
2026-02-13T18:33:38.7782126Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:38.7782694Z Loading safetensors checkpoint shards:  36% Completed | 59/163 [01:05<02:01,  1.17s/it]
2026-02-13T18:33:40.0149279Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:40.0149756Z Loading safetensors checkpoint shards:  37% Completed | 60/163 [01:06<02:02,  1.19s/it]
2026-02-13T18:33:41.2609692Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:41.2610203Z Loading safetensors checkpoint shards:  37% Completed | 61/163 [01:07<02:02,  1.21s/it]
2026-02-13T18:33:42.4715832Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:42.4716270Z Loading safetensors checkpoint shards:  38% Completed | 62/163 [01:09<02:01,  1.21s/it]
2026-02-13T18:33:43.5174711Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:43.5175197Z Loading safetensors checkpoint shards:  39% Completed | 63/163 [01:10<01:55,  1.16s/it]
2026-02-13T18:33:44.7208533Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:44.7209048Z Loading safetensors checkpoint shards:  39% Completed | 64/163 [01:11<01:56,  1.17s/it]
2026-02-13T18:33:45.8652640Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:45.8653096Z Loading safetensors checkpoint shards:  40% Completed | 65/163 [01:12<01:54,  1.16s/it]
2026-02-13T18:33:46.9038303Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:46.9038820Z Loading safetensors checkpoint shards:  40% Completed | 66/163 [01:13<01:49,  1.13s/it]
2026-02-13T18:33:47.9344620Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:47.9345033Z Loading safetensors checkpoint shards:  41% Completed | 67/163 [01:14<01:45,  1.10s/it]
2026-02-13T18:33:49.0913261Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:49.0913710Z Loading safetensors checkpoint shards:  42% Completed | 68/163 [01:15<01:45,  1.12s/it]
2026-02-13T18:33:50.2712252Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:50.2712698Z Loading safetensors checkpoint shards:  42% Completed | 69/163 [01:16<01:46,  1.13s/it]
2026-02-13T18:33:51.5121661Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:51.5122221Z Loading safetensors checkpoint shards:  43% Completed | 70/163 [01:18<01:48,  1.17s/it]
2026-02-13T18:33:52.6258879Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:52.6259298Z Loading safetensors checkpoint shards:  44% Completed | 71/163 [01:19<01:45,  1.15s/it]
2026-02-13T18:33:53.6139286Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:53.6139714Z Loading safetensors checkpoint shards:  44% Completed | 72/163 [01:20<01:40,  1.10s/it]
2026-02-13T18:33:54.6321621Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:54.6322162Z Loading safetensors checkpoint shards:  45% Completed | 73/163 [01:21<01:36,  1.08s/it]
2026-02-13T18:33:55.6493377Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:55.6493798Z Loading safetensors checkpoint shards:  45% Completed | 74/163 [01:22<01:34,  1.06s/it]
2026-02-13T18:33:56.7753734Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:56.7754258Z Loading safetensors checkpoint shards:  46% Completed | 75/163 [01:23<01:34,  1.08s/it]
2026-02-13T18:33:57.9281348Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:57.9281836Z Loading safetensors checkpoint shards:  47% Completed | 76/163 [01:24<01:35,  1.10s/it]
2026-02-13T18:33:58.9526887Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:58.9527355Z Loading safetensors checkpoint shards:  47% Completed | 77/163 [01:25<01:32,  1.08s/it]
2026-02-13T18:33:59.9743896Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:33:59.9744411Z Loading safetensors checkpoint shards:  48% Completed | 78/163 [01:26<01:30,  1.06s/it]
2026-02-13T18:34:01.1278421Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:01.1278909Z Loading safetensors checkpoint shards:  48% Completed | 79/163 [01:27<01:31,  1.09s/it]
2026-02-13T18:34:02.2563796Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:02.2564321Z Loading safetensors checkpoint shards:  49% Completed | 80/163 [01:28<01:31,  1.10s/it]
2026-02-13T18:34:03.3174696Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:03.3175137Z Loading safetensors checkpoint shards:  50% Completed | 81/163 [01:29<01:29,  1.09s/it]
2026-02-13T18:34:04.3627581Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:04.3628010Z Loading safetensors checkpoint shards:  50% Completed | 82/163 [01:30<01:27,  1.08s/it]
2026-02-13T18:34:05.4630032Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:05.4630499Z Loading safetensors checkpoint shards:  51% Completed | 83/163 [01:32<01:26,  1.08s/it]
2026-02-13T18:34:06.5311493Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:06.5311967Z Loading safetensors checkpoint shards:  52% Completed | 84/163 [01:33<01:25,  1.08s/it]
2026-02-13T18:34:07.6389817Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:07.6390224Z Loading safetensors checkpoint shards:  52% Completed | 85/163 [01:34<01:24,  1.09s/it]
2026-02-13T18:34:08.7777767Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:08.7778273Z Loading safetensors checkpoint shards:  53% Completed | 86/163 [01:35<01:24,  1.10s/it]
2026-02-13T18:34:09.9066953Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:09.9067364Z Loading safetensors checkpoint shards:  53% Completed | 87/163 [01:36<01:24,  1.11s/it]
2026-02-13T18:34:10.8698998Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:10.8699544Z Loading safetensors checkpoint shards:  54% Completed | 88/163 [01:37<01:19,  1.07s/it]
2026-02-13T18:34:11.8670921Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:11.8671423Z Loading safetensors checkpoint shards:  55% Completed | 89/163 [01:38<01:17,  1.05s/it]
2026-02-13T18:34:12.8644367Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:12.8644801Z Loading safetensors checkpoint shards:  55% Completed | 90/163 [01:39<01:15,  1.03s/it]
2026-02-13T18:34:13.9243044Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:13.9243508Z Loading safetensors checkpoint shards:  56% Completed | 91/163 [01:40<01:14,  1.04s/it]
2026-02-13T18:34:14.9866261Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:14.9866716Z Loading safetensors checkpoint shards:  56% Completed | 92/163 [01:41<01:14,  1.05s/it]
2026-02-13T18:34:16.0422509Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:16.0423159Z Loading safetensors checkpoint shards:  57% Completed | 93/163 [01:42<01:13,  1.05s/it]
2026-02-13T18:34:17.1028099Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:17.1028607Z Loading safetensors checkpoint shards:  58% Completed | 94/163 [01:43<01:12,  1.05s/it]
2026-02-13T18:34:18.1685159Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:18.1685698Z Loading safetensors checkpoint shards:  58% Completed | 95/163 [01:44<01:11,  1.06s/it]
2026-02-13T18:34:19.2618201Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:19.2618648Z Loading safetensors checkpoint shards:  59% Completed | 96/163 [01:45<01:11,  1.07s/it]
2026-02-13T18:34:20.3211344Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:20.3211786Z Loading safetensors checkpoint shards:  60% Completed | 97/163 [01:46<01:10,  1.07s/it]
2026-02-13T18:34:21.4789610Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:21.4790025Z Loading safetensors checkpoint shards:  60% Completed | 98/163 [01:48<01:11,  1.09s/it]
2026-02-13T18:34:22.6066171Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:22.6066663Z Loading safetensors checkpoint shards:  61% Completed | 99/163 [01:49<01:10,  1.10s/it]
2026-02-13T18:34:23.7235484Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:23.7235990Z Loading safetensors checkpoint shards:  61% Completed | 100/163 [01:50<01:09,  1.11s/it]
2026-02-13T18:34:25.0154033Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:25.0154553Z Loading safetensors checkpoint shards:  62% Completed | 101/163 [01:51<01:12,  1.16s/it]
2026-02-13T18:34:26.0410220Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:26.0410635Z Loading safetensors checkpoint shards:  63% Completed | 102/163 [01:52<01:08,  1.12s/it]
2026-02-13T18:34:27.0669068Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:27.0669499Z Loading safetensors checkpoint shards:  63% Completed | 103/163 [01:53<01:05,  1.09s/it]
2026-02-13T18:34:28.0069683Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:28.0070106Z Loading safetensors checkpoint shards:  64% Completed | 104/163 [01:54<01:01,  1.05s/it]
2026-02-13T18:34:29.0133216Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:29.0133653Z Loading safetensors checkpoint shards:  64% Completed | 105/163 [01:55<01:00,  1.04s/it]
2026-02-13T18:34:29.2064906Z [0;36m(ApiServer_1 pid=186)[0;0m Process ApiServer_1:
2026-02-13T18:34:29.2126537Z [0;36m(ApiServer_3 pid=188)[0;0m Process ApiServer_3:
2026-02-13T18:34:29.2207238Z [0;36m(ApiServer_1 pid=186)[0;0m Traceback (most recent call last):
2026-02-13T18:34:29.2215124Z [0;36m(ApiServer_3 pid=188)[0;0m Traceback (most recent call last):
2026-02-13T18:34:29.2225778Z [0;36m(ApiServer_1 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-13T18:34:29.2234698Z [0;36m(ApiServer_1 pid=186)[0;0m     self.run()
2026-02-13T18:34:29.2246070Z [0;36m(ApiServer_1 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-13T18:34:29.2255535Z [0;36m(ApiServer_1 pid=186)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-13T18:34:29.2265588Z [0;36m(ApiServer_1 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-13T18:34:29.2274823Z [0;36m(ApiServer_1 pid=186)[0;0m     uvloop.run(
2026-02-13T18:34:29.2286257Z [0;36m(ApiServer_1 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-13T18:34:29.2295368Z [0;36m(ApiServer_3 pid=188)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-13T18:34:29.2304728Z [0;36m(ApiServer_1 pid=186)[0;0m     return runner.run(wrapper())
2026-02-13T18:34:29.2314806Z [0;36m(ApiServer_3 pid=188)[0;0m     self.run()
2026-02-13T18:34:29.2324862Z [0;36m(ApiServer_1 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.2335364Z [0;36m(ApiServer_1 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-13T18:34:29.2345441Z [0;36m(ApiServer_3 pid=188)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-13T18:34:29.2354928Z [0;36m(ApiServer_1 pid=186)[0;0m     return self._loop.run_until_complete(task)
2026-02-13T18:34:29.2364646Z [0;36m(ApiServer_1 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.2374557Z [0;36m(ApiServer_3 pid=188)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-13T18:34:29.2385002Z [0;36m(ApiServer_1 pid=186)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-13T18:34:29.2395789Z [0;36m(ApiServer_3 pid=188)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-13T18:34:29.2406702Z [0;36m(ApiServer_1 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-13T18:34:29.2416333Z [0;36m(ApiServer_3 pid=188)[0;0m     uvloop.run(
2026-02-13T18:34:29.2425748Z [0;36m(ApiServer_1 pid=186)[0;0m     return await main
2026-02-13T18:34:29.2436117Z [0;36m(ApiServer_3 pid=188)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-13T18:34:29.2446121Z [0;36m(ApiServer_1 pid=186)[0;0m            ^^^^^^^^^^
2026-02-13T18:34:29.2455558Z [0;36m(ApiServer_3 pid=188)[0;0m     return runner.run(wrapper())
2026-02-13T18:34:29.2465390Z [0;36m(ApiServer_1 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-13T18:34:29.2475399Z [0;36m(ApiServer_3 pid=188)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.2485458Z [0;36m(ApiServer_1 pid=186)[0;0m     async with build_async_engine_client(
2026-02-13T18:34:29.2495843Z [0;36m(ApiServer_3 pid=188)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-13T18:34:29.2505536Z [0;36m(ApiServer_1 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-13T18:34:29.2515388Z [0;36m(ApiServer_3 pid=188)[0;0m     return self._loop.run_until_complete(task)
2026-02-13T18:34:29.2525452Z [0;36m(ApiServer_1 pid=186)[0;0m     return await anext(self.gen)
2026-02-13T18:34:29.2534756Z [0;36m(ApiServer_3 pid=188)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.2543936Z [0;36m(ApiServer_1 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.2553849Z [0;36m(ApiServer_3 pid=188)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-13T18:34:29.2564179Z [0;36m(ApiServer_1 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-13T18:34:29.2573858Z [0;36m(ApiServer_3 pid=188)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-13T18:34:29.2583313Z [0;36m(ApiServer_1 pid=186)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-13T18:34:29.2592716Z [0;36m(ApiServer_3 pid=188)[0;0m     return await main
2026-02-13T18:34:29.2603012Z [0;36m(ApiServer_1 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-13T18:34:29.2612394Z [0;36m(ApiServer_3 pid=188)[0;0m            ^^^^^^^^^^
2026-02-13T18:34:29.2622258Z [0;36m(ApiServer_1 pid=186)[0;0m     return await anext(self.gen)
2026-02-13T18:34:29.2632492Z [0;36m(ApiServer_3 pid=188)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-13T18:34:29.2642411Z [0;36m(ApiServer_1 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.2652347Z [0;36m(ApiServer_3 pid=188)[0;0m     async with build_async_engine_client(
2026-02-13T18:34:29.2662358Z [0;36m(ApiServer_1 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-13T18:34:29.2671846Z [0;36m(ApiServer_3 pid=188)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-13T18:34:29.2681316Z [0;36m(ApiServer_1 pid=186)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-13T18:34:29.2690588Z [0;36m(ApiServer_3 pid=188)[0;0m     return await anext(self.gen)
2026-02-13T18:34:29.2700134Z [0;36m(ApiServer_1 pid=186)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.2709627Z [0;36m(ApiServer_3 pid=188)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.2720081Z [0;36m(ApiServer_3 pid=188)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-13T18:34:29.2730062Z [0;36m(ApiServer_1 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-13T18:34:29.2739163Z [0;36m(ApiServer_1 pid=186)[0;0m     return cls(
2026-02-13T18:34:29.2749061Z [0;36m(ApiServer_3 pid=188)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-13T18:34:29.2758138Z [0;36m(ApiServer_1 pid=186)[0;0m            ^^^^
2026-02-13T18:34:29.2768742Z [0;36m(ApiServer_3 pid=188)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-13T18:34:29.2777605Z [0;36m(ApiServer_1 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-13T18:34:29.2787058Z [0;36m(ApiServer_3 pid=188)[0;0m     return await anext(self.gen)
2026-02-13T18:34:29.2796580Z [0;36m(ApiServer_3 pid=188)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.2806656Z [0;36m(ApiServer_1 pid=186)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-13T18:34:29.2816752Z [0;36m(ApiServer_1 pid=186)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.2826351Z [0;36m(ApiServer_3 pid=188)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-13T18:34:29.2835711Z [0;36m(ApiServer_3 pid=188)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-13T18:34:29.2846593Z [0;36m(ApiServer_1 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-13T18:34:29.2855079Z [0;36m(ApiServer_3 pid=188)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.2864695Z [0;36m(ApiServer_1 pid=186)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-13T18:34:29.2874308Z [0;36m(ApiServer_1 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.2884364Z [0;36m(ApiServer_3 pid=188)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-13T18:34:29.2893848Z [0;36m(ApiServer_3 pid=188)[0;0m     return cls(
2026-02-13T18:34:29.2903543Z [0;36m(ApiServer_1 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-13T18:34:29.2913093Z [0;36m(ApiServer_1 pid=186)[0;0m     super().__init__(
2026-02-13T18:34:29.2922182Z [0;36m(ApiServer_3 pid=188)[0;0m            ^^^^
2026-02-13T18:34:29.2932449Z [0;36m(ApiServer_1 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-13T18:34:29.2941890Z [0;36m(ApiServer_3 pid=188)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-13T18:34:29.2951203Z [0;36m(ApiServer_1 pid=186)[0;0m     super().__init__(
2026-02-13T18:34:29.2961347Z [0;36m(ApiServer_3 pid=188)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-13T18:34:29.2970798Z [0;36m(ApiServer_3 pid=188)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.2980432Z [0;36m(ApiServer_1 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-13T18:34:29.2989340Z [0;36m(ApiServer_1 pid=186)[0;0m     super().__init__(
2026-02-13T18:34:29.2999155Z [0;36m(ApiServer_3 pid=188)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-13T18:34:29.3009005Z [0;36m(ApiServer_1 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-13T18:34:29.3017680Z [0;36m(ApiServer_3 pid=188)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-13T18:34:29.3027237Z [0;36m(ApiServer_1 pid=186)[0;0m     raise TimeoutError(
2026-02-13T18:34:29.3037776Z [0;36m(ApiServer_3 pid=188)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.3048113Z [0;36m(ApiServer_3 pid=188)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-13T18:34:29.3056761Z [0;36m(ApiServer_3 pid=188)[0;0m     super().__init__(
2026-02-13T18:34:29.3065994Z [0;36m(ApiServer_1 pid=186)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-13T18:34:29.3076092Z [0;36m(ApiServer_3 pid=188)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-13T18:34:29.3092593Z [0;36m(ApiServer_3 pid=188)[0;0m     super().__init__(
2026-02-13T18:34:29.3095543Z [0;36m(ApiServer_3 pid=188)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-13T18:34:29.3105042Z [0;36m(ApiServer_3 pid=188)[0;0m     super().__init__(
2026-02-13T18:34:29.3115292Z [0;36m(ApiServer_3 pid=188)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-13T18:34:29.3125290Z [0;36m(ApiServer_3 pid=188)[0;0m     raise TimeoutError(
2026-02-13T18:34:29.3134943Z [0;36m(ApiServer_3 pid=188)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-13T18:34:29.3143996Z [0;36m(ApiServer_0 pid=185)[0;0m Process ApiServer_0:
2026-02-13T18:34:29.3153452Z [0;36m(ApiServer_0 pid=185)[0;0m Traceback (most recent call last):
2026-02-13T18:34:29.3164034Z [0;36m(ApiServer_0 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-13T18:34:29.3172618Z [0;36m(ApiServer_0 pid=185)[0;0m     self.run()
2026-02-13T18:34:29.3181932Z [0;36m(ApiServer_0 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-13T18:34:29.3192135Z [0;36m(ApiServer_0 pid=185)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-13T18:34:29.3202163Z [0;36m(ApiServer_0 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-13T18:34:29.3211527Z [0;36m(ApiServer_0 pid=185)[0;0m     uvloop.run(
2026-02-13T18:34:29.3221886Z [0;36m(ApiServer_0 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-13T18:34:29.3230554Z [0;36m(ApiServer_0 pid=185)[0;0m     return runner.run(wrapper())
2026-02-13T18:34:29.3240412Z [0;36m(ApiServer_0 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.3250231Z [0;36m(ApiServer_0 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-13T18:34:29.3259681Z [0;36m(ApiServer_0 pid=185)[0;0m     return self._loop.run_until_complete(task)
2026-02-13T18:34:29.3269114Z [0;36m(ApiServer_0 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.3279249Z [0;36m(ApiServer_0 pid=185)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-13T18:34:29.3289068Z [0;36m(ApiServer_0 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-13T18:34:29.3298906Z [0;36m(ApiServer_0 pid=185)[0;0m     return await main
2026-02-13T18:34:29.3308431Z [0;36m(ApiServer_0 pid=185)[0;0m            ^^^^^^^^^^
2026-02-13T18:34:29.3318865Z [0;36m(ApiServer_0 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-13T18:34:29.3328942Z [0;36m(ApiServer_0 pid=185)[0;0m     async with build_async_engine_client(
2026-02-13T18:34:29.3339024Z [0;36m(ApiServer_0 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-13T18:34:29.3348213Z [0;36m(ApiServer_0 pid=185)[0;0m     return await anext(self.gen)
2026-02-13T18:34:29.3357938Z [0;36m(ApiServer_0 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.3368362Z [0;36m(ApiServer_0 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-13T18:34:29.3377356Z [0;36m(ApiServer_0 pid=185)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-13T18:34:29.3387249Z [0;36m(ApiServer_0 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-13T18:34:29.3396779Z [0;36m(ApiServer_0 pid=185)[0;0m     return await anext(self.gen)
2026-02-13T18:34:29.3406191Z [0;36m(ApiServer_0 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.3416385Z [0;36m(ApiServer_0 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-13T18:34:29.3425313Z [0;36m(ApiServer_0 pid=185)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-13T18:34:29.3434967Z [0;36m(ApiServer_0 pid=185)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.3445434Z [0;36m(ApiServer_0 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-13T18:34:29.3454402Z [0;36m(ApiServer_0 pid=185)[0;0m     return cls(
2026-02-13T18:34:29.3463317Z [0;36m(ApiServer_0 pid=185)[0;0m            ^^^^
2026-02-13T18:34:29.3473325Z [0;36m(ApiServer_0 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-13T18:34:29.3483122Z [0;36m(ApiServer_0 pid=185)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-13T18:34:29.3492650Z [0;36m(ApiServer_0 pid=185)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.3502494Z [0;36m(ApiServer_0 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-13T18:34:29.3511698Z [0;36m(ApiServer_0 pid=185)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-13T18:34:29.3521386Z [0;36m(ApiServer_0 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.3531383Z [0;36m(ApiServer_0 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-13T18:34:29.3541156Z [0;36m(ApiServer_0 pid=185)[0;0m     super().__init__(
2026-02-13T18:34:29.3549892Z [0;36m(ApiServer_0 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-13T18:34:29.3559551Z [0;36m(ApiServer_0 pid=185)[0;0m     super().__init__(
2026-02-13T18:34:29.3569242Z [0;36m(ApiServer_0 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-13T18:34:29.3578563Z [0;36m(ApiServer_0 pid=185)[0;0m     super().__init__(
2026-02-13T18:34:29.3587462Z [0;36m(ApiServer_0 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-13T18:34:29.3635953Z [0;36m(ApiServer_0 pid=185)[0;0m     raise TimeoutError(
2026-02-13T18:34:29.3636481Z [0;36m(ApiServer_0 pid=185)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-13T18:34:29.5648848Z [0;36m(ApiServer_2 pid=187)[0;0m Process ApiServer_2:
2026-02-13T18:34:29.5674467Z [0;36m(ApiServer_2 pid=187)[0;0m Traceback (most recent call last):
2026-02-13T18:34:29.5684021Z [0;36m(ApiServer_2 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-13T18:34:29.5693292Z [0;36m(ApiServer_2 pid=187)[0;0m     self.run()
2026-02-13T18:34:29.5704415Z [0;36m(ApiServer_2 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-13T18:34:29.5714357Z [0;36m(ApiServer_2 pid=187)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-13T18:34:29.5726859Z [0;36m(ApiServer_2 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-13T18:34:29.5736267Z [0;36m(ApiServer_2 pid=187)[0;0m     uvloop.run(
2026-02-13T18:34:29.5746222Z [0;36m(ApiServer_2 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-13T18:34:29.5755457Z [0;36m(ApiServer_2 pid=187)[0;0m     return runner.run(wrapper())
2026-02-13T18:34:29.5765777Z [0;36m(ApiServer_2 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.5775781Z [0;36m(ApiServer_2 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-13T18:34:29.5785190Z [0;36m(ApiServer_2 pid=187)[0;0m     return self._loop.run_until_complete(task)
2026-02-13T18:34:29.5794280Z [0;36m(ApiServer_2 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.5804766Z [0;36m(ApiServer_2 pid=187)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-13T18:34:29.5815025Z [0;36m(ApiServer_2 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-13T18:34:29.5823674Z [0;36m(ApiServer_2 pid=187)[0;0m     return await main
2026-02-13T18:34:29.5833070Z [0;36m(ApiServer_2 pid=187)[0;0m            ^^^^^^^^^^
2026-02-13T18:34:29.5843448Z [0;36m(ApiServer_2 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-13T18:34:29.5852457Z [0;36m(ApiServer_2 pid=187)[0;0m     async with build_async_engine_client(
2026-02-13T18:34:29.5862528Z [0;36m(ApiServer_2 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-13T18:34:29.5871783Z [0;36m(ApiServer_2 pid=187)[0;0m     return await anext(self.gen)
2026-02-13T18:34:29.5880502Z [0;36m(ApiServer_2 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.5890840Z [0;36m(ApiServer_2 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-13T18:34:29.5899661Z [0;36m(ApiServer_2 pid=187)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-13T18:34:29.5909097Z [0;36m(ApiServer_2 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-13T18:34:29.5918515Z [0;36m(ApiServer_2 pid=187)[0;0m     return await anext(self.gen)
2026-02-13T18:34:29.5927575Z [0;36m(ApiServer_2 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.5937358Z [0;36m(ApiServer_2 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-13T18:34:29.5947391Z [0;36m(ApiServer_2 pid=187)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-13T18:34:29.5956266Z [0;36m(ApiServer_2 pid=187)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.5967780Z [0;36m(ApiServer_2 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-13T18:34:29.5974796Z [0;36m(ApiServer_2 pid=187)[0;0m     return cls(
2026-02-13T18:34:29.5984194Z [0;36m(ApiServer_2 pid=187)[0;0m            ^^^^
2026-02-13T18:34:29.5993897Z [0;36m(ApiServer_2 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-13T18:34:29.6004899Z [0;36m(ApiServer_2 pid=187)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-13T18:34:29.6013942Z [0;36m(ApiServer_2 pid=187)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.6023537Z [0;36m(ApiServer_2 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-13T18:34:29.6033135Z [0;36m(ApiServer_2 pid=187)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-13T18:34:29.6074261Z [0;36m(ApiServer_2 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-13T18:34:29.6074847Z [0;36m(ApiServer_2 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-13T18:34:29.6075332Z [0;36m(ApiServer_2 pid=187)[0;0m     super().__init__(
2026-02-13T18:34:29.6075889Z [0;36m(ApiServer_2 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-13T18:34:29.6081085Z [0;36m(ApiServer_2 pid=187)[0;0m     super().__init__(
2026-02-13T18:34:29.6090807Z [0;36m(ApiServer_2 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-13T18:34:29.6100183Z [0;36m(ApiServer_2 pid=187)[0;0m     super().__init__(
2026-02-13T18:34:29.6109352Z [0;36m(ApiServer_2 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-13T18:34:29.6118735Z [0;36m(ApiServer_2 pid=187)[0;0m     raise TimeoutError(
2026-02-13T18:34:29.6128984Z [0;36m(ApiServer_2 pid=187)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-13T18:34:29.9191354Z [0;36m(ApiServer_1 pid=186)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-13T18:34:29.9926475Z [0;36m(ApiServer_3 pid=188)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-13T18:34:30.0051706Z [0;36m(ApiServer_0 pid=185)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-13T18:34:30.0116767Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:30.0117198Z Loading safetensors checkpoint shards:  65% Completed | 106/163 [01:56<00:58,  1.02s/it]
2026-02-13T18:34:30.1695610Z [0;36m(ApiServer_2 pid=187)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-13T18:34:32.8221230Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:32.8221684Z Loading safetensors checkpoint shards:  66% Completed | 107/163 [01:59<01:27,  1.56s/it]
2026-02-13T18:34:32.9304342Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:32.9304807Z Loading safetensors checkpoint shards:  67% Completed | 110/163 [01:59<00:37,  1.42it/s]
2026-02-13T18:34:33.0448434Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:33.0448971Z Loading safetensors checkpoint shards:  69% Completed | 113/163 [01:59<00:20,  2.46it/s]
2026-02-13T18:34:33.1578435Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:33.1578956Z Loading safetensors checkpoint shards:  71% Completed | 116/163 [01:59<00:12,  3.80it/s]
2026-02-13T18:34:33.2685306Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:33.2685735Z Loading safetensors checkpoint shards:  73% Completed | 119/163 [01:59<00:08,  5.48it/s]
2026-02-13T18:34:33.3780426Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:33.3780854Z Loading safetensors checkpoint shards:  75% Completed | 122/163 [01:59<00:05,  7.51it/s]
2026-02-13T18:34:33.4828720Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:33.4829117Z Loading safetensors checkpoint shards:  77% Completed | 125/163 [02:00<00:03,  9.88it/s]
2026-02-13T18:34:33.5880786Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:33.5881412Z Loading safetensors checkpoint shards:  79% Completed | 128/163 [02:00<00:02, 12.47it/s]
2026-02-13T18:34:33.7013041Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:33.7013559Z Loading safetensors checkpoint shards:  81% Completed | 132/163 [02:00<00:01, 16.59it/s]
2026-02-13T18:34:33.8040227Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:33.8040696Z Loading safetensors checkpoint shards:  83% Completed | 135/163 [02:00<00:01, 18.81it/s]
2026-02-13T18:34:33.9108630Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:33.9109129Z Loading safetensors checkpoint shards:  85% Completed | 138/163 [02:00<00:01, 20.81it/s]
2026-02-13T18:34:34.0176379Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:34.0176806Z Loading safetensors checkpoint shards:  87% Completed | 141/163 [02:00<00:00, 22.53it/s]
2026-02-13T18:34:34.1241383Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:34.1241828Z Loading safetensors checkpoint shards:  88% Completed | 144/163 [02:00<00:00, 23.95it/s]
2026-02-13T18:34:34.2325019Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:34.2325501Z Loading safetensors checkpoint shards:  90% Completed | 147/163 [02:00<00:00, 24.96it/s]
2026-02-13T18:34:38.3084433Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:38.3084939Z Loading safetensors checkpoint shards:  92% Completed | 150/163 [02:04<00:05,  2.31it/s]
2026-02-13T18:34:38.4167900Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:38.4168375Z Loading safetensors checkpoint shards:  94% Completed | 153/163 [02:04<00:03,  3.18it/s]
2026-02-13T18:34:38.5168336Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:38.5168792Z Loading safetensors checkpoint shards:  96% Completed | 156/163 [02:05<00:01,  4.34it/s]
2026-02-13T18:34:38.6214209Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:38.6214652Z Loading safetensors checkpoint shards:  98% Completed | 159/163 [02:05<00:00,  5.81it/s]
2026-02-13T18:34:38.7061570Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-13 18:34:38 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-13T18:34:38.7229750Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:38.7230240Z Loading safetensors checkpoint shards:  99% Completed | 162/163 [02:05<00:00,  7.66it/s]
2026-02-13T18:34:38.7443333Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-13 18:34:38 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-13T18:34:38.7500082Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-13 18:34:38 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-13T18:34:38.7575390Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:38.7575819Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [02:05<00:00,  1.30it/s]
2026-02-13T18:34:38.7584750Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:34:38.7609821Z [0;36m(Worker_DP1_TP4_EP12 pid=581)[0;0m INFO 02-13 18:34:38 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-13T18:34:38.8145568Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m INFO 02-13 18:34:38 [default_loader.py:291] Loading weights took 125.39 seconds
2026-02-13T18:34:38.8318335Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-13 18:34:38 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-13T18:34:38.8574594Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m INFO 02-13 18:34:38 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-13T18:34:38.8888100Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-13 18:34:38 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-13T18:34:38.9425868Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-13 18:34:38 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-13T18:34:39.2607204Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-13 18:34:39 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-13T18:34:39.2648429Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-13 18:34:39 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-13T18:34:39.3579089Z [0;36m(Worker_DP1_TP5_EP13 pid=683)[0;0m INFO 02-13 18:34:39 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-13T18:34:39.4251950Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-13 18:34:39 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-13T18:34:39.4371605Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m INFO 02-13 18:34:39 [default_loader.py:291] Loading weights took 125.88 seconds
2026-02-13T18:34:39.4559163Z [0;36m(Worker_DP1_TP7_EP15 pid=891)[0;0m INFO 02-13 18:34:39 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-13T18:34:39.4742589Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m INFO 02-13 18:34:39 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-13T18:34:39.4815702Z [0;36m(Worker_DP1_TP1_EP9 pid=279)[0;0m INFO 02-13 18:34:39 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-13T18:34:39.5012687Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-13 18:34:39 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-13T18:34:40.0620729Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-13 18:34:40 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-13T18:34:40.4803421Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m INFO 02-13 18:34:40 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-13T18:34:40.5870380Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-13 18:34:40 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-13T18:34:40.6560097Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-13 18:34:40 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-13T18:34:40.6748853Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-13 18:34:40 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-13T18:34:41.1620755Z [0;36m(Worker_DP1_TP4_EP12 pid=581)[0;0m INFO 02-13 18:34:41 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-13T18:34:41.4089079Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-13 18:34:41 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-13T18:34:41.4188892Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-13 18:34:41 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-13T18:34:41.4935479Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m INFO 02-13 18:34:41 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-13T18:34:41.8137070Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-13 18:34:41 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-13T18:34:41.8744609Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-13 18:34:41 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-13T18:34:41.8954328Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-13 18:34:41 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-13T18:34:41.9229609Z [0;36m(Worker_DP1_TP5_EP13 pid=683)[0;0m INFO 02-13 18:34:41 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-13T18:34:41.9323732Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-13 18:34:41 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-13T18:34:42.4484062Z [0;36m(Worker_DP1_TP7_EP15 pid=891)[0;0m INFO 02-13 18:34:42 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-13T18:34:42.5109787Z [0;36m(Worker_DP1_TP1_EP9 pid=279)[0;0m INFO 02-13 18:34:42 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-13T18:35:01.7275836Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m INFO 02-13 18:35:01 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/5f4e5deba5/rank_0_0/backbone for vLLM's torch.compile
2026-02-13T18:35:01.7412478Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m INFO 02-13 18:35:01 [backends.py:865] Dynamo bytecode transform time: 4.69 s
2026-02-13T18:35:01.8336706Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m INFO 02-13 18:35:01 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/5f4e5deba5/rank_0_1/backbone for vLLM's torch.compile
2026-02-13T18:35:01.8484217Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m INFO 02-13 18:35:01 [backends.py:865] Dynamo bytecode transform time: 4.80 s
2026-02-13T18:35:08.0957484Z [0;36m(Worker_DP1_TP1_EP9 pid=279)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-13T18:35:08.0973877Z [0;36m(Worker_DP1_TP1_EP9 pid=279)[0;0m   warnings.warn(
2026-02-13T18:35:08.0985917Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-13T18:35:08.0995792Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m   warnings.warn(
2026-02-13T18:35:08.1028049Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-13T18:35:08.1037577Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m   warnings.warn(
2026-02-13T18:35:08.1122875Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-13T18:35:08.1131740Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m   warnings.warn(
2026-02-13T18:35:08.1311075Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-13T18:35:08.1319403Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m   warnings.warn(
2026-02-13T18:35:08.1583220Z [0;36m(Worker_DP1_TP5_EP13 pid=683)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-13T18:35:08.1590724Z [0;36m(Worker_DP1_TP5_EP13 pid=683)[0;0m   warnings.warn(
2026-02-13T18:35:08.1602492Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-13T18:35:08.1612232Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m   warnings.warn(
2026-02-13T18:35:08.1675896Z [0;36m(Worker_DP1_TP4_EP12 pid=581)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-13T18:35:08.1685171Z [0;36m(Worker_DP1_TP4_EP12 pid=581)[0;0m   warnings.warn(
2026-02-13T18:35:08.1869309Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-13T18:35:08.1878455Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m   warnings.warn(
2026-02-13T18:35:08.1996290Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-13T18:35:08.2004721Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m   warnings.warn(
2026-02-13T18:35:08.2133232Z [0;36m(Worker_DP1_TP7_EP15 pid=891)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-13T18:35:08.2134344Z [0;36m(Worker_DP1_TP7_EP15 pid=891)[0;0m   warnings.warn(
2026-02-13T18:35:08.2237018Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-13T18:35:08.2244904Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m   warnings.warn(
2026-02-13T18:35:08.2281473Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-13T18:35:08.2290860Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m   warnings.warn(
2026-02-13T18:35:08.2481380Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-13T18:35:08.2490675Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m   warnings.warn(
2026-02-13T18:35:08.2972144Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-13T18:35:08.2981175Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m   warnings.warn(
2026-02-13T18:35:08.3018565Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-13T18:35:08.3027589Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m   warnings.warn(
2026-02-13T18:35:21.5553577Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m INFO 02-13 18:35:21 [backends.py:319] Compiling a graph for compile range (1, 4096) takes 13.43 s
2026-02-13T18:35:21.5578552Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m INFO 02-13 18:35:21 [monitor.py:34] torch.compile takes 18.22 s in total
2026-02-13T18:35:22.1623973Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m INFO 02-13 18:35:22 [backends.py:319] Compiling a graph for compile range (1, 4096) takes 13.98 s
2026-02-13T18:35:22.1648675Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m INFO 02-13 18:35:22 [monitor.py:34] torch.compile takes 18.67 s in total
2026-02-13T18:35:27.9657747Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-13 18:35:27 [worker.py:338] Available memory: 18254325248, total memory: 65787658240
2026-02-13T18:35:28.0058722Z [0;36m(Worker_DP1_TP1_EP9 pid=279)[0;0m INFO 02-13 18:35:28 [worker.py:338] Available memory: 18546254540, total memory: 65796046848
2026-02-13T18:35:28.0178112Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-13 18:35:28 [worker.py:338] Available memory: 18254029312, total memory: 65787658240
2026-02-13T18:35:28.0201015Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-13 18:35:28 [worker.py:338] Available memory: 18552797900, total memory: 65796046848
2026-02-13T18:35:28.0672440Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-13 18:35:28 [worker.py:338] Available memory: 18555226828, total memory: 65796046848
2026-02-13T18:35:28.0897086Z [0;36m(Worker_DP1_TP7_EP15 pid=891)[0;0m INFO 02-13 18:35:28 [worker.py:338] Available memory: 18572205772, total memory: 65796046848
2026-02-13T18:35:28.1355954Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m INFO 02-13 18:35:28 [worker.py:338] Available memory: 17875921408, total memory: 65787658240
2026-02-13T18:35:28.1625010Z [0;36m(Worker_DP1_TP4_EP12 pid=581)[0;0m INFO 02-13 18:35:28 [worker.py:338] Available memory: 18274808320, total memory: 65787658240
2026-02-13T18:35:28.2340850Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-13 18:35:28 [worker.py:338] Available memory: 18547286732, total memory: 65796046848
2026-02-13T18:35:28.5291807Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-13 18:35:28 [worker.py:338] Available memory: 18263905792, total memory: 65787658240
2026-02-13T18:35:28.6249587Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-13 18:35:28 [worker.py:338] Available memory: 18247369216, total memory: 65787658240
2026-02-13T18:35:28.7554383Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-13 18:35:28 [worker.py:338] Available memory: 18550086348, total memory: 65796046848
2026-02-13T18:35:29.3285212Z [0;36m(Worker_DP1_TP5_EP13 pid=683)[0;0m INFO 02-13 18:35:29 [worker.py:338] Available memory: 18542783180, total memory: 65796046848
2026-02-13T18:35:29.5042934Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-13 18:35:29 [worker.py:338] Available memory: 18549912268, total memory: 65796046848
2026-02-13T18:35:29.5071095Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m INFO 02-13 18:35:29 [worker.py:338] Available memory: 17874717184, total memory: 65787658240
2026-02-13T18:35:29.5095132Z [0;36m(EngineCore_DP1 pid=174)[0;0m INFO 02-13 18:35:29 [kv_cache_utils.py:1307] GPU KV cache size: 204,672 tokens
2026-02-13T18:35:29.5104704Z [0;36m(EngineCore_DP1 pid=174)[0;0m INFO 02-13 18:35:29 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 24.98x
2026-02-13T18:35:29.7117216Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-13 18:35:29 [worker.py:338] Available memory: 18269534720, total memory: 65787658240
2026-02-13T18:35:29.7144980Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-13 18:35:29 [kv_cache_utils.py:1307] GPU KV cache size: 204,672 tokens
2026-02-13T18:35:29.7154141Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-13 18:35:29 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 24.98x
2026-02-13T18:35:46.6439073Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m 
2026-02-13T18:35:46.6439833Z Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s][rank8]:[W213 18:35:46.494795431 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-13T18:35:46.6450892Z [rank14]:[W213 18:35:46.494806761 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-13T18:35:46.6460889Z [rank15]:[W213 18:35:46.494834832 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-13T18:35:46.6472116Z [rank10]:[W213 18:35:46.494845632 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-13T18:35:46.6483643Z [rank11]:[W213 18:35:46.494861812 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-13T18:35:46.6499515Z [rank12]:[W213 18:35:46.494889872 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-13T18:35:46.6510430Z [rank6]:[W213 18:35:46.494930962 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-13T18:35:46.6521584Z [rank3]:[W213 18:35:46.494933942 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-13T18:35:46.6532117Z [rank13]:[W213 18:35:46.495188945 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-13T18:35:46.6545301Z [rank9]:[W213 18:35:46.495248645 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-13T18:35:46.6553672Z [rank0]:[W213 18:35:46.495504898 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-13T18:35:46.6563648Z [rank5]:[W213 18:35:46.495600509 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-13T18:35:46.6573296Z [rank7]:[W213 18:35:46.495738140 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-13T18:35:46.6583233Z [rank4]:[W213 18:35:46.496442806 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-13T18:35:46.6593386Z [rank2]:[W213 18:35:46.497149613 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-13T18:35:46.6605244Z [rank1]:[W213 18:35:46.497739378 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-13T18:35:49.0197241Z [rank3]:[W213 18:35:49.870533009 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-13T18:35:49.0206084Z [rank0]:[W213 18:35:49.870637310 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-13T18:35:49.0215588Z [rank5]:[W213 18:35:49.870832742 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-13T18:35:49.0225542Z [rank4]:[W213 18:35:49.872061083 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-13T18:35:49.0235508Z [rank2]:[W213 18:35:49.872291215 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-13T18:35:49.0245661Z [rank7]:[W213 18:35:49.872382826 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-13T18:35:49.0256227Z [rank6]:[W213 18:35:49.873430166 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-13T18:35:49.0266235Z [rank1]:[W213 18:35:49.875040160 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-13T18:35:49.2227963Z [rank14]:[W213 18:35:49.074720220 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-13T18:35:49.2257119Z [rank11]:[W213 18:35:49.075301735 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-13T18:35:49.2269082Z [rank8]:[W213 18:35:49.075476917 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-13T18:35:49.2282423Z [rank13]:[W213 18:35:49.075857160 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-13T18:35:49.2293044Z [rank12]:[W213 18:35:49.075912881 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-13T18:35:49.2303172Z [rank10]:[W213 18:35:49.075939381 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-13T18:35:49.2313839Z [rank15]:[W213 18:35:49.076051122 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-13T18:35:49.2324239Z [rank9]:[W213 18:35:49.076153153 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-13T18:35:52.1268686Z 
2026-02-13T18:35:52.1269370Z Capturing CUDA graphs (decode, FULL):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:17<00:17, 17.30s/it]
2026-02-13T18:35:52.1270016Z Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:20<00:00,  8.80s/it]
2026-02-13T18:35:52.1270540Z Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:20<00:00, 10.08s/it]
2026-02-13T18:35:52.6331118Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m INFO 02-13 18:35:52 [gpu_model_runner.py:5051] Graph capturing finished in 21 secs, took 0.27 GiB
2026-02-13T18:35:52.6544456Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m INFO 02-13 18:35:52 [gpu_model_runner.py:5051] Graph capturing finished in 21 secs, took 0.27 GiB
2026-02-13T18:35:53.1232077Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-13 18:35:53 [core.py:272] init engine (profile, create kv cache, warmup model) took 71.22 seconds
2026-02-13T18:35:53.3685442Z [0;36m(EngineCore_DP1 pid=174)[0;0m INFO 02-13 18:35:53 [core.py:272] init engine (profile, create kv cache, warmup model) took 70.85 seconds
2026-02-13T18:35:54.4480171Z INFO 02-13 18:35:54 [coordinator.py:200] All engine subscriptions received by DP coordinator
2026-02-13T18:35:54.4563284Z [0;36m(EngineCore_DP1 pid=174)[0;0m INFO 02-13 18:35:54 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-13T18:35:54.4571869Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-13 18:35:54 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-13T18:35:54.4583379Z [0;36m(EngineCore_DP1 pid=174)[0;0m INFO 02-13 18:35:54 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:35:54.4591829Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-13 18:35:54 [ascend_config.py:412] Dynamic EPLB is False
2026-02-13T18:35:54.4602465Z [0;36m(EngineCore_DP1 pid=174)[0;0m INFO 02-13 18:35:54 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:35:54.4612345Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-13 18:35:54 [ascend_config.py:413] The number of redundant experts is 0
2026-02-13T18:35:54.4623362Z [0;36m(EngineCore_DP1 pid=174)[0;0m INFO 02-13 18:35:54 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:35:54.4633951Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-13 18:35:54 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-13T18:35:54.4644532Z [0;36m(EngineCore_DP1 pid=174)[0;0m INFO 02-13 18:35:54 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-13T18:35:54.4654255Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-13 18:35:54 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-13T18:35:54.4662687Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-13 18:35:54 [platform.py:335] [91m
2026-02-13T18:35:54.4673509Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             **********************************************************************************
2026-02-13T18:35:54.4683854Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-13T18:35:54.4694129Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-13T18:35:54.4704754Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-13T18:35:54.4714487Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-13T18:35:54.4725317Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-13T18:35:54.4735021Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             * batch size for graph capture.
2026-02-13T18:35:54.4744642Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             * For more details, please refer to:
2026-02-13T18:35:54.4754963Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-13T18:35:54.4765111Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             **********************************************************************************[0m
2026-02-13T18:35:54.4774638Z [0;36m(EngineCore_DP1 pid=174)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             
2026-02-13T18:35:54.4784398Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-13 18:35:54 [platform.py:335] [91m
2026-02-13T18:35:54.4795266Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             **********************************************************************************
2026-02-13T18:35:54.4805153Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-13T18:35:54.4815817Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-13T18:35:54.4825288Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-13T18:35:54.4834994Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-13T18:35:54.4845810Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-13T18:35:54.4855433Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             * batch size for graph capture.
2026-02-13T18:35:54.4865192Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             * For more details, please refer to:
2026-02-13T18:35:54.4875208Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-13T18:35:54.4885471Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             **********************************************************************************[0m
2026-02-13T18:35:54.4895037Z [0;36m(EngineCore_DP0 pid=155)[0;0m WARNING 02-13 18:35:54 [platform.py:335]             
2026-02-13T18:35:54.4904437Z INFO 02-13 18:35:54 [utils.py:249] Waiting for API servers to complete ...
2026-02-13T18:35:54.4914905Z [0;36m(EngineCore_DP0 pid=155)[0;0m INFO 02-13 18:35:54 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-13T18:35:54.4925008Z [0;36m(EngineCore_DP1 pid=174)[0;0m INFO 02-13 18:35:54 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-13T18:35:54.4934490Z ERROR 02-13 18:35:54 [utils.py:290] Exception occurred while running API servers: Process ApiServer_0 (PID: 185) died with exit code 1
2026-02-13T18:35:54.4944131Z ERROR 02-13 18:35:54 [utils.py:290] Traceback (most recent call last):
2026-02-13T18:35:54.4953336Z ERROR 02-13 18:35:54 [utils.py:290]   File "/vllm-workspace/vllm/vllm/v1/utils.py", line 277, in wait_for_completion_or_failure
2026-02-13T18:35:54.4963229Z ERROR 02-13 18:35:54 [utils.py:290]     raise RuntimeError(
2026-02-13T18:35:54.4973165Z ERROR 02-13 18:35:54 [utils.py:290] RuntimeError: Process ApiServer_0 (PID: 185) died with exit code 1
2026-02-13T18:35:54.4981906Z INFO 02-13 18:35:54 [utils.py:293] Terminating remaining processes ...
2026-02-13T18:35:54.7450458Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-13T18:35:54.7458604Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-13T18:35:54.7467805Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-13T18:35:54.7477432Z [0;36m(Worker_DP1_TP4_EP12 pid=581)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-13T18:35:54.7487666Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-13T18:35:54.7497029Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-13T18:35:54.7506613Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-13T18:35:54.7515644Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-13T18:35:54.7526275Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-13T18:35:54.7535453Z [0;36m(Worker_DP1_TP7_EP15 pid=891)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-13T18:35:54.7544611Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-13T18:35:54.7553984Z [0;36m(Worker_DP1_TP0_EP8 pid=215)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-13T18:35:54.7564353Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-13T18:35:54.7573469Z [0;36m(Worker_DP1_TP5_EP13 pid=683)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-13T18:35:54.7583232Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-13T18:35:54.7593243Z [0;36m(Worker_DP0_TP0_EP0 pid=214)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-13T18:35:54.7602786Z [0;36m(Worker_DP0_TP2_EP2 pid=370)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-13T18:35:54.7612754Z [0;36m(Worker_DP1_TP1_EP9 pid=279)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-13T18:35:54.7621738Z [0;36m(Worker_DP1_TP3_EP11 pid=477)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-13T18:35:54.7631435Z [0;36m(Worker_DP0_TP5_EP5 pid=682)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-13T18:35:54.7641977Z [0;36m(Worker_DP1_TP7_EP15 pid=891)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-13T18:35:54.7650806Z [0;36m(Worker_DP0_TP1_EP1 pid=276)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-13T18:35:54.7660152Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-13T18:35:54.7669353Z [0;36m(Worker_DP1_TP4_EP12 pid=581)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-13T18:35:54.7679291Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-13T18:35:54.7689100Z [0;36m(Worker_DP1_TP6_EP14 pid=788)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-13T18:35:54.7698620Z [0;36m(Worker_DP1_TP5_EP13 pid=683)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-13T18:35:54.7708077Z [0;36m(Worker_DP1_TP2_EP10 pid=373)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-13T18:35:54.7718360Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-13T18:35:54.7728170Z [0;36m(Worker_DP0_TP7_EP7 pid=890)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-13T18:35:54.7738090Z [0;36m(Worker_DP1_TP1_EP9 pid=279)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-13T18:35:54.7747788Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-13 18:35:54 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-13T18:35:59.7782414Z Traceback (most recent call last):
2026-02-13T18:35:59.7796484Z   File "/usr/local/python3.11.14/bin/vllm", line 7, in <module>
2026-02-13T18:35:59.7812839Z     sys.exit(main())
2026-02-13T18:35:59.7823694Z              ^^^^^^
2026-02-13T18:35:59.7832816Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/main.py", line 73, in main
2026-02-13T18:35:59.7842575Z     args.dispatch_function(args)
2026-02-13T18:35:59.7852952Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 108, in cmd
2026-02-13T18:35:59.7861407Z     run_multi_api_server(args)
2026-02-13T18:35:59.7871936Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 282, in run_multi_api_server
2026-02-13T18:35:59.7882128Z     wait_for_completion_or_failure(
2026-02-13T18:35:59.7892362Z   File "/vllm-workspace/vllm/vllm/v1/utils.py", line 277, in wait_for_completion_or_failure
2026-02-13T18:35:59.7901717Z     raise RuntimeError(
2026-02-13T18:35:59.7911390Z RuntimeError: Process ApiServer_0 (PID: 185) died with exit code 1
2026-02-13T18:35:59.8758791Z [ERROR] 2026-02-13-18:35:59 (PID:139, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
2026-02-13T18:36:00.1631504Z sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-13T18:36:02.0338880Z /usr/local/python3.11.14/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked shared_memory objects to clean up at shutdown
2026-02-13T18:36:02.0348060Z   warnings.warn('resource_tracker: There appear to be %d '
2026-02-13T18:36:07.7175286Z FAILED
2026-02-13T18:36:07.7185273Z 
2026-02-13T18:36:07.7196191Z =================================== FAILURES ===================================
2026-02-13T18:36:07.7206593Z _______________________________ test_multi_node ________________________________
2026-02-13T18:36:07.7259473Z 
2026-02-13T18:36:07.7259618Z     @pytest.mark.asyncio
2026-02-13T18:36:07.7260156Z     async def test_multi_node() -> None:
2026-02-13T18:36:07.7260471Z         config = MultiNodeConfigLoader.from_yaml()
2026-02-13T18:36:07.7260783Z     
2026-02-13T18:36:07.7263911Z         with ProxyLauncher(
2026-02-13T18:36:07.7272988Z                 nodes=config.nodes,
2026-02-13T18:36:07.7281933Z                 disagg_cfg=config.disagg_cfg,
2026-02-13T18:36:07.7291017Z                 envs=config.envs,
2026-02-13T18:36:07.7300145Z                 proxy_port=config.proxy_port,
2026-02-13T18:36:07.7309647Z                 cur_index=config.cur_index,
2026-02-13T18:36:07.7331829Z         ) as proxy:
2026-02-13T18:36:07.7332148Z     
2026-02-13T18:36:07.7337033Z >           with RemoteOpenAIServer(
2026-02-13T18:36:07.7347401Z                     model=config.model,
2026-02-13T18:36:07.7356869Z                     vllm_serve_args=config.server_cmd,
2026-02-13T18:36:07.7365947Z                     server_port=config.server_port,
2026-02-13T18:36:07.7374912Z                     server_host=config.master_ip,
2026-02-13T18:36:07.7383655Z                     env_dict=config.envs,
2026-02-13T18:36:07.7392918Z                     auto_port=False,
2026-02-13T18:36:07.7402282Z                     proxy_port=proxy.proxy_port,
2026-02-13T18:36:07.7412057Z                     disaggregated_prefill=config.disagg_cfg,
2026-02-13T18:36:07.7420548Z                     nodes_info=config.nodes,
2026-02-13T18:36:07.7430705Z                     max_wait_seconds=2800,
2026-02-13T18:36:07.7440819Z             ) as server:
2026-02-13T18:36:07.7450516Z 
2026-02-13T18:36:07.7459019Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py:21: 
2026-02-13T18:36:07.7467927Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-13T18:36:07.7477560Z tests/e2e/conftest.py:306: in __init__
2026-02-13T18:36:07.7487491Z     self._wait_for_multiple_servers(
2026-02-13T18:36:07.7496279Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-13T18:36:07.7506223Z 
2026-02-13T18:36:07.7519503Z self = <tests.e2e.conftest.RemoteOpenAIServer object at 0xffff27a4b7d0>
2026-02-13T18:36:07.7526851Z targets = [('10.0.0.74', 'http://10.0.0.74:8080/health')], timeout = 2800
2026-02-13T18:36:07.7535956Z log_interval = 30.0
2026-02-13T18:36:07.7544481Z 
2026-02-13T18:36:07.7553611Z     def _wait_for_multiple_servers(self,
2026-02-13T18:36:07.7563604Z                                    targets,
2026-02-13T18:36:07.7572841Z                                    timeout: float,
2026-02-13T18:36:07.7581733Z                                    log_interval: float = 30.0):
2026-02-13T18:36:07.7590718Z         """
2026-02-13T18:36:07.7600551Z         targets: List[(node_ip, url)]
2026-02-13T18:36:07.7609896Z         log_interval
2026-02-13T18:36:07.7618591Z         """
2026-02-13T18:36:07.7627895Z         start = time.time()
2026-02-13T18:36:07.7637355Z         client = requests
2026-02-13T18:36:07.7646886Z     
2026-02-13T18:36:07.7655653Z         ready = {node_ip: False for node_ip, _ in targets}
2026-02-13T18:36:07.7664904Z     
2026-02-13T18:36:07.7674263Z         last_log_time = 0.0
2026-02-13T18:36:07.7684435Z     
2026-02-13T18:36:07.7693703Z         while True:
2026-02-13T18:36:07.7703075Z             now = time.time()
2026-02-13T18:36:07.7712336Z             all_ready = True
2026-02-13T18:36:07.7721366Z             should_log = (now - last_log_time) >= log_interval
2026-02-13T18:36:07.7730707Z     
2026-02-13T18:36:07.7739449Z             for node_ip, url in targets:
2026-02-13T18:36:07.7748527Z                 if ready[node_ip]:
2026-02-13T18:36:07.7758332Z                     continue
2026-02-13T18:36:07.7767466Z     
2026-02-13T18:36:07.7777382Z                 try:
2026-02-13T18:36:07.7786316Z                     resp = client.get(url)
2026-02-13T18:36:07.7795595Z                     if resp.status_code == 200:
2026-02-13T18:36:07.7805209Z                         ready[node_ip] = True
2026-02-13T18:36:07.7814953Z                         logger.info(f"[READY] Node {node_ip} is ready.")
2026-02-13T18:36:07.7833604Z                 except RequestException:
2026-02-13T18:36:07.7833856Z                     all_ready = False
2026-02-13T18:36:07.7841394Z                     if should_log:
2026-02-13T18:36:07.7850823Z                         logger.debug(f"[WAIT] {url}: connection failed")
2026-02-13T18:36:07.7860053Z     
2026-02-13T18:36:07.7869211Z                     # check unexpected exit
2026-02-13T18:36:07.7878509Z                     result = self._poll()
2026-02-13T18:36:07.7888202Z                     if result is not None and result != 0:
2026-02-13T18:36:07.7897661Z >                       raise RuntimeError(
2026-02-13T18:36:07.7906475Z                             f"Server at {node_ip} exited unexpectedly."
2026-02-13T18:36:07.7915959Z                         ) from None
2026-02-13T18:36:07.7926178Z E                       RuntimeError: Server at 10.0.0.74 exited unexpectedly.
2026-02-13T18:36:07.7935065Z 
2026-02-13T18:36:07.7944546Z tests/e2e/conftest.py:399: RuntimeError
2026-02-13T18:36:07.7954572Z =============================== warnings summary ===============================
2026-02-13T18:36:07.7966518Z <frozen importlib._bootstrap>:241
2026-02-13T18:36:07.7975746Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute
2026-02-13T18:36:07.8043340Z 
2026-02-13T18:36:07.8043537Z <frozen importlib._bootstrap>:241
2026-02-13T18:36:07.8043997Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute
2026-02-13T18:36:07.8044331Z 
2026-02-13T18:36:07.8044705Z ../../usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647
2026-02-13T18:36:07.8045752Z   /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-13T18:36:07.8046661Z     warnings.warn(
2026-02-13T18:36:07.8055529Z 
2026-02-13T18:36:07.8065079Z ../../usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8
2026-02-13T18:36:07.8084812Z   /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
2026-02-13T18:36:07.8085512Z     import pkg_resources
2026-02-13T18:36:07.8094445Z 
2026-02-13T18:36:07.8103109Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2026-02-13T18:36:07.8112850Z =========================== short test summary info ============================
2026-02-13T18:36:07.8122397Z FAILED tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node
2026-02-13T18:36:07.8131745Z ================== 1 failed, 4 warnings in 740.95s (0:12:20) ===================
2026-02-13T18:36:09.4947098Z [0;31mFAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml âœ— ERROR: Some tests failed, please check the error stack above for details. If this is insufficient to pinpoint the error, please download and review the logs of all other nodes from the job's summary.[0m
2026-02-13T18:36:09.6562132Z Cleaning up background log streams...
2026-02-13T18:36:09.7378370Z ##[error]Error: failed to run script step: Error: command terminated with non-zero exit code: command terminated with exit code 1
2026-02-13T18:36:09.7413784Z ##[error]Process completed with exit code 1.
2026-02-13T18:36:09.7509464Z ##[error]Executing the custom container implementation failed. Please contact your self hosted runner administrator.
2026-02-13T18:36:09.7918707Z ##[group]Run actions/upload-artifact@v6
2026-02-13T18:36:09.7919056Z with:
2026-02-13T18:36:09.7919336Z   name: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs
2026-02-13T18:36:09.7919613Z   path: /tmp/vllm*_logs.txt
2026-02-13T18:36:09.7919871Z   retention-days: 7
2026-02-13T18:36:09.7920098Z   if-no-files-found: warn
2026-02-13T18:36:09.7920302Z   compression-level: 6
2026-02-13T18:36:09.7920549Z   overwrite: false
2026-02-13T18:36:09.7920782Z   include-hidden-files: false
2026-02-13T18:36:09.7920995Z env:
2026-02-13T18:36:09.7921312Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-13T18:36:09.7921581Z ##[endgroup]
2026-02-13T18:36:09.8054391Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-13T18:36:09.8055236Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-13T18:36:09.8055574Z ##[endgroup]
2026-02-13T18:36:10.1578512Z (node:915) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-13T18:36:10.1579295Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-13T18:36:11.1532331Z With the provided path, there will be 1 file uploaded
2026-02-13T18:36:11.1537972Z Artifact name is valid!
2026-02-13T18:36:11.1538639Z Root directory input is valid!
2026-02-13T18:36:12.1045716Z Beginning upload of artifact content to blob storage
2026-02-13T18:36:13.0128682Z Uploaded bytes 12574
2026-02-13T18:36:13.2373282Z Finished uploading artifact content to blob storage!
2026-02-13T18:36:13.2373833Z SHA256 digest of uploaded artifact zip is 8e5b45df31f41d9dafb2421433f722f6cc1ea85426921853a9fc9d3339ca5adf
2026-02-13T18:36:13.2374540Z Finalizing artifact upload
2026-02-13T18:36:14.1048967Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs.zip successfully finalized. Artifact ID 5504020059
2026-02-13T18:36:14.1049812Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs has been successfully uploaded! Final size is 12574 bytes. Artifact ID is 5504020059
2026-02-13T18:36:14.1052451Z Artifact download URL: https://github.com/vllm-project/vllm-ascend/actions/runs/21994466849/artifacts/5504020059
2026-02-13T18:36:15.7656570Z ##[group]Run kubectl get pods -n "$NAMESPACE" --ignore-not-found=true
2026-02-13T18:36:15.7657149Z [36;1mkubectl get pods -n "$NAMESPACE" --ignore-not-found=true[0m
2026-02-13T18:36:15.7657564Z [36;1mkubectl delete -f ./lws.yaml --ignore-not-found=true || true[0m
2026-02-13T18:36:15.7658005Z shell: bash -el {0}
2026-02-13T18:36:15.7658230Z env:
2026-02-13T18:36:15.7658470Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-13T18:36:15.7658793Z ##[endgroup]
2026-02-13T18:36:15.7739784Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-13T18:36:15.7740766Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-13T18:36:15.7741065Z ##[endgroup]
2026-02-13T18:36:16.1486296Z (node:1077) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-13T18:36:16.1487427Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-13T18:36:16.8378854Z NAME                                             READY   STATUS    RESTARTS     AGE
2026-02-13T18:36:16.8379337Z linux-aarch64-a3-0-n4cwm-runner-jbfld            1/1     Running   0            14m
2026-02-13T18:36:16.8379803Z linux-aarch64-a3-0-n4cwm-runner-jbfld-workflow   1/1     Running   0            13m
2026-02-13T18:36:16.8380257Z vllm-0                                           1/1     Running   1 (7s ago)   13m
2026-02-13T18:36:16.8380687Z vllm-0-1                                         1/1     Running   0            13m
2026-02-13T18:36:16.9100731Z leaderworkerset.leaderworkerset.x-k8s.io "vllm" deleted from vllm-project namespace
2026-02-13T18:36:16.9491512Z service "vllm-leader" deleted from vllm-project namespace
2026-02-13T18:36:17.4689356Z Post job cleanup.
2026-02-13T18:36:17.4713348Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-13T18:36:17.4714164Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-13T18:36:17.4714484Z ##[endgroup]
2026-02-13T18:36:17.8412599Z (node:1201) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-13T18:36:17.8413361Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-13T18:36:18.4991014Z [command]/usr/bin/git version
2026-02-13T18:36:18.5164997Z git version 2.34.1
2026-02-13T18:36:18.5195828Z Copying '/root/.gitconfig' to '/__w/_temp/e815aafc-54fa-4b48-9e08-07e3896fb467/.gitconfig'
2026-02-13T18:36:18.5207144Z Temporarily overriding HOME='/__w/_temp/e815aafc-54fa-4b48-9e08-07e3896fb467' before making global git config changes
2026-02-13T18:36:18.5207976Z Adding repository directory to the temporary git global config as a safe directory
2026-02-13T18:36:18.5211434Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-13T18:36:18.5249579Z Removing SSH command configuration
2026-02-13T18:36:18.5253933Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-13T18:36:18.5307169Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-13T18:36:18.5790238Z Removing HTTP extra header
2026-02-13T18:36:18.5793685Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-13T18:36:18.5819063Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-13T18:36:18.5993190Z Removing includeIf entries pointing to credentials config files
2026-02-13T18:36:18.5997606Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-13T18:36:18.6016429Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-13T18:36:18.6016842Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-13T18:36:18.6017191Z includeif.gitdir:/github/workspace/.git.path
2026-02-13T18:36:18.6017597Z includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-13T18:36:18.6023249Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-13T18:36:18.6044203Z /__w/_temp/git-credentials-0cd6c63f-61a2-4d77-8ad7-9a62b26dae91.config
2026-02-13T18:36:18.6051936Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-0cd6c63f-61a2-4d77-8ad7-9a62b26dae91.config
2026-02-13T18:36:18.6084211Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-13T18:36:18.6102476Z /__w/_temp/git-credentials-0cd6c63f-61a2-4d77-8ad7-9a62b26dae91.config
2026-02-13T18:36:18.6109178Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-0cd6c63f-61a2-4d77-8ad7-9a62b26dae91.config
2026-02-13T18:36:18.6134353Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git.path
2026-02-13T18:36:18.6151643Z /github/runner_temp/git-credentials-0cd6c63f-61a2-4d77-8ad7-9a62b26dae91.config
2026-02-13T18:36:18.6158862Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-0cd6c63f-61a2-4d77-8ad7-9a62b26dae91.config
2026-02-13T18:36:18.6188009Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-13T18:36:18.6205421Z /github/runner_temp/git-credentials-0cd6c63f-61a2-4d77-8ad7-9a62b26dae91.config
2026-02-13T18:36:18.6211404Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-0cd6c63f-61a2-4d77-8ad7-9a62b26dae91.config
2026-02-13T18:36:18.6243555Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-13T18:36:18.6416639Z Removing credentials config '/__w/_temp/git-credentials-0cd6c63f-61a2-4d77-8ad7-9a62b26dae91.config'
2026-02-13T18:36:37.3455386Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-13T18:36:37.3456396Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-13T18:36:37.3456766Z ##[endgroup]
2026-02-13T18:36:37.7340028Z Cleaning up orphan processes
