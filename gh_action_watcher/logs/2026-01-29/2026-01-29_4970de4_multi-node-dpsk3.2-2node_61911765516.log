# Run ID: 21486238570
# Commit: 4970de42420f6504dabf6d70b3cbfa7881021165
# Job: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
# Date: 2026-01-29
============================================================

ï»¿2026-01-29T18:47:09.4518677Z Current runner version: '2.330.0'
2026-01-29T18:47:09.4523666Z Runner name: 'linux-aarch64-a3-0-wgt9d-runner-kctb4'
2026-01-29T18:47:09.4524406Z Runner group name: 'Default'
2026-01-29T18:47:09.4525188Z Machine name: 'linux-aarch64-a3-0-wgt9d-runner-kctb4'
2026-01-29T18:47:09.4528583Z ##[group]GITHUB_TOKEN Permissions
2026-01-29T18:47:09.4530523Z Actions: write
2026-01-29T18:47:09.4530944Z ArtifactMetadata: write
2026-01-29T18:47:09.4531392Z Attestations: write
2026-01-29T18:47:09.4531762Z Checks: write
2026-01-29T18:47:09.4532439Z Contents: write
2026-01-29T18:47:09.4532847Z Deployments: write
2026-01-29T18:47:09.4533261Z Discussions: write
2026-01-29T18:47:09.4533637Z Issues: write
2026-01-29T18:47:09.4534003Z Metadata: read
2026-01-29T18:47:09.4534469Z Models: read
2026-01-29T18:47:09.4534821Z Packages: write
2026-01-29T18:47:09.4535200Z Pages: write
2026-01-29T18:47:09.4535576Z PullRequests: write
2026-01-29T18:47:09.4535958Z RepositoryProjects: write
2026-01-29T18:47:09.4536537Z SecurityEvents: write
2026-01-29T18:47:09.4536960Z Statuses: write
2026-01-29T18:47:09.4537368Z ##[endgroup]
2026-01-29T18:47:09.4539103Z Secret source: Actions
2026-01-29T18:47:09.4539604Z Prepare workflow directory
2026-01-29T18:47:09.5123931Z Prepare all required actions
2026-01-29T18:47:09.5155673Z Getting action download info
2026-01-29T18:47:10.7006786Z Download action repository 'actions/checkout@v6' (SHA:8e8c483db84b4bee98b60c0593521ed34d9990e8)
2026-01-29T18:47:15.8741828Z Download action repository 'actions/upload-artifact@v6' (SHA:b7c566a772e6b6bfb58ed0dc250532a479d7789f)
2026-01-29T18:47:23.3824511Z Uses: vllm-project/vllm-ascend/.github/workflows/_e2e_nightly_multi_node.yaml@refs/heads/main (4970de42420f6504dabf6d70b3cbfa7881021165)
2026-01-29T18:47:23.3827824Z ##[group] Inputs
2026-01-29T18:47:23.3828143Z   soc_version: a3
2026-01-29T18:47:23.3828420Z   runner: linux-aarch64-a3-0
2026-01-29T18:47:23.3828858Z   image: swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3
2026-01-29T18:47:23.3829312Z   config_file_path: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-01-29T18:47:23.3829627Z   replicas: 1
2026-01-29T18:47:23.3829893Z   size: 2
2026-01-29T18:47:23.3830128Z   vllm_version: v0.14.1
2026-01-29T18:47:23.3830507Z   vllm_ascend_remote_url: https://github.com/vllm-project/vllm-ascend.git
2026-01-29T18:47:23.3830821Z   vllm_ascend_ref: main
2026-01-29T18:47:23.3831087Z ##[endgroup]
2026-01-29T18:47:23.3831567Z Complete job name: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-01-29T18:47:23.4355426Z ##[group]Run '/home/runner/k8s/index.js'
2026-01-29T18:47:23.4357944Z shell: /home/runner/externals/node20/bin/node {0}
2026-01-29T18:47:23.4358443Z ##[endgroup]
2026-01-29T18:47:38.9556387Z (node:72) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-01-29T18:47:38.9557378Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-01-29T18:48:04.9578600Z ##[group]Run # Decode and save kubeconfig
2026-01-29T18:48:04.9579095Z [36;1m# Decode and save kubeconfig[0m
2026-01-29T18:48:04.9611622Z [36;1mecho "***" | base64 -d > $KUBECONFIG[0m
2026-01-29T18:48:04.9612375Z shell: bash -el {0}
2026-01-29T18:48:04.9612614Z ##[endgroup]
2026-01-29T18:48:04.9737902Z ##[group]Run '/home/runner/k8s/index.js'
2026-01-29T18:48:04.9738940Z shell: /home/runner/externals/node20/bin/node {0}
2026-01-29T18:48:04.9739252Z ##[endgroup]
2026-01-29T18:48:05.3406964Z (node:5671) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-01-29T18:48:05.3408345Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-01-29T18:48:40.6264437Z ##[group]Run actions/checkout@v6
2026-01-29T18:48:40.6264834Z with:
2026-01-29T18:48:40.6265174Z   repository: vllm-project/vllm-ascend
2026-01-29T18:48:40.6265861Z   token: ***
2026-01-29T18:48:40.6266096Z   ssh-strict: true
2026-01-29T18:48:40.6266445Z   ssh-user: git
2026-01-29T18:48:40.6266650Z   persist-credentials: true
2026-01-29T18:48:40.6266888Z   clean: true
2026-01-29T18:48:40.6267262Z   sparse-checkout-cone-mode: true
2026-01-29T18:48:40.6267513Z   fetch-depth: 1
2026-01-29T18:48:40.6267732Z   fetch-tags: false
2026-01-29T18:48:40.6267981Z   show-progress: true
2026-01-29T18:48:40.6268178Z   lfs: false
2026-01-29T18:48:40.6268383Z   submodules: false
2026-01-29T18:48:40.6268607Z   set-safe-directory: true
2026-01-29T18:48:40.6268856Z ##[endgroup]
2026-01-29T18:48:40.6325050Z ##[group]Run '/home/runner/k8s/index.js'
2026-01-29T18:48:40.6325861Z shell: /home/runner/externals/node20/bin/node {0}
2026-01-29T18:48:40.6326179Z ##[endgroup]
2026-01-29T18:48:40.9892763Z (node:6098) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-01-29T18:48:40.9893600Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-01-29T18:48:58.9630474Z Syncing repository: vllm-project/vllm-ascend
2026-01-29T18:48:58.9631566Z ##[group]Getting Git version info
2026-01-29T18:48:58.9631886Z Working directory is '/__w/vllm-ascend/vllm-ascend'
2026-01-29T18:48:58.9632629Z [command]/usr/bin/git version
2026-01-29T18:48:58.9632919Z git version 2.34.1
2026-01-29T18:48:58.9634356Z ##[endgroup]
2026-01-29T18:48:58.9637261Z Copying '/root/.gitconfig' to '/__w/_temp/66887d3f-28db-424f-88c4-b082ca9d584a/.gitconfig'
2026-01-29T18:48:58.9637902Z Temporarily overriding HOME='/__w/_temp/66887d3f-28db-424f-88c4-b082ca9d584a' before making global git config changes
2026-01-29T18:48:58.9638517Z Adding repository directory to the temporary git global config as a safe directory
2026-01-29T18:48:58.9641911Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-01-29T18:48:58.9670257Z Deleting the contents of '/__w/vllm-ascend/vllm-ascend'
2026-01-29T18:48:58.9673351Z ##[group]Initializing the repository
2026-01-29T18:48:58.9676531Z [command]/usr/bin/git init /__w/vllm-ascend/vllm-ascend
2026-01-29T18:48:58.9788640Z hint: Using 'master' as the name for the initial branch. This default branch name
2026-01-29T18:48:58.9789121Z hint: is subject to change. To configure the initial branch name to use in all
2026-01-29T18:48:58.9789548Z hint: of your new repositories, which will suppress this warning, call:
2026-01-29T18:48:58.9789997Z hint: 
2026-01-29T18:48:58.9790304Z hint: 	git config --global init.defaultBranch <name>
2026-01-29T18:48:58.9790595Z hint: 
2026-01-29T18:48:58.9790857Z hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and
2026-01-29T18:48:58.9791438Z hint: 'development'. The just-created branch can be renamed via this command:
2026-01-29T18:48:58.9791745Z hint: 
2026-01-29T18:48:58.9792100Z hint: 	git branch -m <name>
2026-01-29T18:48:58.9796781Z Initialized empty Git repository in /__w/vllm-ascend/vllm-ascend/.git/
2026-01-29T18:48:58.9805202Z [command]/usr/bin/git remote add origin https://github.com/vllm-project/vllm-ascend
2026-01-29T18:48:58.9848290Z ##[endgroup]
2026-01-29T18:48:58.9848673Z ##[group]Disabling automatic garbage collection
2026-01-29T18:48:58.9850944Z [command]/usr/bin/git config --local gc.auto 0
2026-01-29T18:48:58.9876311Z ##[endgroup]
2026-01-29T18:48:58.9876668Z ##[group]Setting up auth
2026-01-29T18:48:58.9877831Z Removing SSH command configuration
2026-01-29T18:48:58.9882922Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-01-29T18:48:58.9909554Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-01-29T18:48:59.0098156Z Removing HTTP extra header
2026-01-29T18:48:59.0098692Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-01-29T18:48:59.0122364Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-01-29T18:48:59.0294899Z Removing includeIf entries pointing to credentials config files
2026-01-29T18:48:59.0299739Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-01-29T18:48:59.0324503Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-01-29T18:48:59.0501792Z [command]/usr/bin/git config --file /__w/_temp/git-credentials-50f19b6e-f920-4ce1-983d-e6980535abcc.config http.https://github.com/.extraheader AUTHORIZATION: basic ***
2026-01-29T18:48:59.0539453Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-50f19b6e-f920-4ce1-983d-e6980535abcc.config
2026-01-29T18:48:59.0568657Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-50f19b6e-f920-4ce1-983d-e6980535abcc.config
2026-01-29T18:48:59.0593747Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-50f19b6e-f920-4ce1-983d-e6980535abcc.config
2026-01-29T18:48:59.0618851Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-50f19b6e-f920-4ce1-983d-e6980535abcc.config
2026-01-29T18:48:59.0641085Z ##[endgroup]
2026-01-29T18:48:59.0641544Z ##[group]Fetching the repository
2026-01-29T18:48:59.0648858Z [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --no-recurse-submodules --depth=1 origin +4970de42420f6504dabf6d70b3cbfa7881021165:refs/remotes/origin/main
2026-01-29T18:49:00.7113816Z From https://gh-proxy.test.osinfra.cn/https://github.com/vllm-project/vllm-ascend
2026-01-29T18:49:00.7114434Z  * [new ref]         4970de42420f6504dabf6d70b3cbfa7881021165 -> origin/main
2026-01-29T18:49:00.7128251Z ##[endgroup]
2026-01-29T18:49:00.7128764Z ##[group]Determining the checkout info
2026-01-29T18:49:00.7129319Z ##[endgroup]
2026-01-29T18:49:00.7133148Z [command]/usr/bin/git sparse-checkout disable
2026-01-29T18:49:00.7173403Z [command]/usr/bin/git config --local --unset-all extensions.worktreeConfig
2026-01-29T18:49:00.7196978Z ##[group]Checking out the ref
2026-01-29T18:49:00.7200640Z [command]/usr/bin/git checkout --progress --force -B main refs/remotes/origin/main
2026-01-29T18:49:00.8026165Z Switched to a new branch 'main'
2026-01-29T18:49:00.8026718Z Branch 'main' set up to track remote branch 'main' from 'origin'.
2026-01-29T18:49:00.8035592Z ##[endgroup]
2026-01-29T18:49:00.8073577Z [command]/usr/bin/git log -1 --format=%H
2026-01-29T18:49:00.8094610Z 4970de42420f6504dabf6d70b3cbfa7881021165
2026-01-29T18:49:18.5258980Z ##[group]Run # prepare for lws entrypoint scripts
2026-01-29T18:49:18.5259367Z [36;1m# prepare for lws entrypoint scripts[0m
2026-01-29T18:49:18.5259796Z [36;1minstall -D tests/e2e/nightly/multi_node/scripts/run.sh /root/.cache/tests/run.sh[0m
2026-01-29T18:49:18.5260344Z shell: bash -el {0}
2026-01-29T18:49:18.5260554Z ##[endgroup]
2026-01-29T18:49:18.5338184Z ##[group]Run '/home/runner/k8s/index.js'
2026-01-29T18:49:18.5339264Z shell: /home/runner/externals/node20/bin/node {0}
2026-01-29T18:49:18.5339588Z ##[endgroup]
2026-01-29T18:49:18.8924800Z (node:6689) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-01-29T18:49:18.8925595Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-01-29T18:49:54.9252558Z ##[group]Run set -euo pipefail
2026-01-29T18:49:54.9252833Z [36;1mset -euo pipefail[0m
2026-01-29T18:49:54.9253005Z [36;1m[0m
2026-01-29T18:49:54.9253151Z [36;1mCRD_NAME="${CRD_NAME:-vllm}"[0m
2026-01-29T18:49:54.9253521Z [36;1mTIMEOUT=${TIMEOUT:-120}[0m
2026-01-29T18:49:54.9253715Z [36;1mSLEEP_INTERVAL=2[0m
2026-01-29T18:49:54.9253871Z [36;1m[0m
2026-01-29T18:49:54.9254114Z [36;1mecho "Deleting leaderworkerset [$CRD_NAME] in namespace [$NAMESPACE]..."[0m
2026-01-29T18:49:54.9254523Z [36;1mkubectl delete leaderworkerset "$CRD_NAME" -n "$NAMESPACE" --ignore-not-found[0m
2026-01-29T18:49:54.9254820Z [36;1m[0m
2026-01-29T18:49:54.9255033Z [36;1mecho "Waiting for all pods starting with 'vllm' to be deleted..."[0m
2026-01-29T18:49:54.9255304Z [36;1mSTART_TIME=$(date +%s)[0m
2026-01-29T18:49:54.9255471Z [36;1m[0m
2026-01-29T18:49:54.9255606Z [36;1mwhile true; do[0m
2026-01-29T18:49:54.9255760Z [36;1m  NOW=$(date +%s)[0m
2026-01-29T18:49:54.9255976Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-01-29T18:49:54.9256152Z [36;1m[0m
2026-01-29T18:49:54.9256310Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-01-29T18:49:54.9256586Z [36;1m    echo "Timeout reached ($TIMEOUT seconds), some pods still exist:"[0m
2026-01-29T18:49:54.9256907Z [36;1m    kubectl get pods -n "$NAMESPACE" | grep '^vllm' || true[0m
2026-01-29T18:49:54.9257142Z [36;1m    exit 1[0m
2026-01-29T18:49:54.9257290Z [36;1m  fi[0m
2026-01-29T18:49:54.9257510Z [36;1m[0m
2026-01-29T18:49:54.9257888Z [36;1m  PODS_EXIST=$(kubectl get pods -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null | tr ' ' '\n' | grep '^vllm' || true)[0m
2026-01-29T18:49:54.9258300Z [36;1m[0m
2026-01-29T18:49:54.9258447Z [36;1m  if [[ -z "$PODS_EXIST" ]]; then[0m
2026-01-29T18:49:54.9258660Z [36;1m    echo "All vllm pods deleted."[0m
2026-01-29T18:49:54.9258845Z [36;1m    break[0m
2026-01-29T18:49:54.9258991Z [36;1m  else[0m
2026-01-29T18:49:54.9259197Z [36;1m    echo "Waiting for pods to be deleted: $PODS_EXIST"[0m
2026-01-29T18:49:54.9259432Z [36;1m    sleep $SLEEP_INTERVAL[0m
2026-01-29T18:49:54.9259612Z [36;1m  fi[0m
2026-01-29T18:49:54.9259748Z [36;1mdone[0m
2026-01-29T18:49:54.9260023Z shell: bash -el {0}
2026-01-29T18:49:54.9260182Z ##[endgroup]
2026-01-29T18:49:54.9329449Z ##[group]Run '/home/runner/k8s/index.js'
2026-01-29T18:49:54.9330130Z shell: /home/runner/externals/node20/bin/node {0}
2026-01-29T18:49:54.9330340Z ##[endgroup]
2026-01-29T18:49:55.2904293Z (node:7471) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-01-29T18:49:55.2904972Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-01-29T18:50:13.4228021Z Deleting leaderworkerset [vllm] in namespace [vllm-project]...
2026-01-29T18:50:13.7187858Z Waiting for all pods starting with 'vllm' to be deleted...
2026-01-29T18:50:13.8480576Z All vllm pods deleted.
2026-01-29T18:50:31.8053994Z ##[group]Run set -e
2026-01-29T18:50:31.8054309Z [36;1mset -e[0m
2026-01-29T18:50:31.8054545Z [36;1m[0m
2026-01-29T18:50:31.8054780Z [36;1msize="2"[0m
2026-01-29T18:50:31.8054996Z [36;1mreplicas="1"[0m
2026-01-29T18:50:31.8055431Z [36;1mimage="swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3"[0m
2026-01-29T18:50:31.8055947Z [36;1mconfig_file_path="DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-01-29T18:50:31.8056331Z [36;1mfail_tag=FAIL_TAG_"DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-01-29T18:50:31.8056705Z [36;1mecho "FAIL_TAG=${fail_tag}" >> $GITHUB_ENV[0m
2026-01-29T18:50:31.8057031Z [36;1m[0m
2026-01-29T18:50:31.8057305Z [36;1mrequired_params=("size" "replicas" "image" "config_file_path")[0m
2026-01-29T18:50:31.8057796Z [36;1mfor param in "${required_params[@]}"; do[0m
2026-01-29T18:50:31.8058066Z [36;1m  if [ -z "${!param}" ]; then[0m
2026-01-29T18:50:31.8058477Z [36;1m    echo "Error: Parameter '$param' is required but empty"[0m
2026-01-29T18:50:31.8058755Z [36;1m    exit 1[0m
2026-01-29T18:50:31.8058958Z [36;1m  fi[0m
2026-01-29T18:50:31.8059194Z [36;1mdone[0m
2026-01-29T18:50:31.8059372Z [36;1m[0m
2026-01-29T18:50:31.8059712Z [36;1mif [ "a3" = "a3" ]; then[0m
2026-01-29T18:50:31.8059993Z [36;1m  npu_per_node=16[0m
2026-01-29T18:50:31.8060371Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws.yaml.jinja2"[0m
2026-01-29T18:50:31.8060703Z [36;1melse[0m
2026-01-29T18:50:31.8060915Z [36;1m  npu_per_node=8[0m
2026-01-29T18:50:31.8061252Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws-a2.yaml.jinja2"[0m
2026-01-29T18:50:31.8061600Z [36;1mfi[0m
2026-01-29T18:50:31.8061792Z [36;1m[0m
2026-01-29T18:50:31.8062120Z [36;1mjinja2 $TEMPLATE_FILE \[0m
2026-01-29T18:50:31.8062388Z [36;1m  -D size="$size" \[0m
2026-01-29T18:50:31.8062639Z [36;1m  -D replicas="$replicas" \[0m
2026-01-29T18:50:31.8062906Z [36;1m  -D image="$image" \[0m
2026-01-29T18:50:31.8063188Z [36;1m  -D config_file_path="$config_file_path" \[0m
2026-01-29T18:50:31.8063482Z [36;1m  -D npu_per_node="$npu_per_node" \[0m
2026-01-29T18:50:31.8063828Z [36;1m  -D fail_tag="$fail_tag" \[0m
2026-01-29T18:50:31.8064079Z [36;1m  --outfile lws.yaml[0m
2026-01-29T18:50:31.8064309Z [36;1m[0m
2026-01-29T18:50:31.8064522Z [36;1mkubectl apply -f ./lws.yaml[0m
2026-01-29T18:50:31.8064917Z shell: bash -el {0}
2026-01-29T18:50:31.8065133Z ##[endgroup]
2026-01-29T18:50:31.8158246Z ##[group]Run '/home/runner/k8s/index.js'
2026-01-29T18:50:31.8159187Z shell: /home/runner/externals/node20/bin/node {0}
2026-01-29T18:50:31.8159530Z ##[endgroup]
2026-01-29T18:50:32.1832387Z (node:8437) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-01-29T18:50:32.1833103Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-01-29T18:50:50.9896857Z leaderworkerset.leaderworkerset.x-k8s.io/vllm created
2026-01-29T18:50:51.0224862Z service/vllm-leader created
2026-01-29T18:51:09.0763725Z ##[group]Run POD_PREFIX="${POD_PREFIX:-vllm-0}"
2026-01-29T18:51:09.0764060Z [36;1mPOD_PREFIX="${POD_PREFIX:-vllm-0}"[0m
2026-01-29T18:51:09.0764275Z [36;1mSIZE="2"[0m
2026-01-29T18:51:09.0764456Z [36;1mTIMEOUT=1200  # default timeout 20 minutes[0m
2026-01-29T18:51:09.0764672Z [36;1m[0m
2026-01-29T18:51:09.0764979Z [36;1mecho "Waiting for Pods in namespace [$NAMESPACE] to become Running and Ready (timeout ${TIMEOUT}s)..."[0m
2026-01-29T18:51:09.0765327Z [36;1m[0m
2026-01-29T18:51:09.0765471Z [36;1mSTART_TIME=$(date +%s)[0m
2026-01-29T18:51:09.0765639Z [36;1m[0m
2026-01-29T18:51:09.0765778Z [36;1mwhile true; do[0m
2026-01-29T18:51:09.0765938Z [36;1m  NOW=$(date +%s)[0m
2026-01-29T18:51:09.0766126Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-01-29T18:51:09.0766344Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-01-29T18:51:09.0766574Z [36;1m    echo "Timeout reached after ${ELAPSED}s"[0m
2026-01-29T18:51:09.0766847Z [36;1m    echo "Dumping pod status for debugging:"[0m
2026-01-29T18:51:09.0767074Z [36;1m    kubectl get pods -n "$NAMESPACE"[0m
2026-01-29T18:51:09.0767328Z [36;1m    kubectl describe pod "$LEADER_POD" -n "$NAMESPACE"[0m
2026-01-29T18:51:09.0767563Z [36;1m    exit 1[0m
2026-01-29T18:51:09.0767801Z [36;1m  fi[0m
2026-01-29T18:51:09.0767943Z [36;1m[0m
2026-01-29T18:51:09.0768089Z [36;1m  # 1) check follower pods[0m
2026-01-29T18:51:09.0768278Z [36;1m  ALL_FOLLOWERS_READY=true[0m
2026-01-29T18:51:09.0768472Z [36;1m  for ((i=1; i<SIZE; i++)); do[0m
2026-01-29T18:51:09.0768665Z [36;1m    POD="${POD_PREFIX}-${i}"[0m
2026-01-29T18:51:09.0769039Z [36;1m    PHASE=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-01-29T18:51:09.0769590Z [36;1m    READY=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-01-29T18:51:09.0769952Z [36;1m[0m
2026-01-29T18:51:09.0770137Z [36;1m    echo "Follower [$POD] phase=$PHASE ready=$READY"[0m
2026-01-29T18:51:09.0770363Z [36;1m[0m
2026-01-29T18:51:09.0770542Z [36;1m    if [[ "$PHASE" != "Running" || "$READY" != "true" ]]; then[0m
2026-01-29T18:51:09.0770955Z [36;1m      echo "Follower [$POD] not Ready yet..."[0m
2026-01-29T18:51:09.0771181Z [36;1m      ALL_FOLLOWERS_READY=false[0m
2026-01-29T18:51:09.0771365Z [36;1m      break[0m
2026-01-29T18:51:09.0771517Z [36;1m    fi[0m
2026-01-29T18:51:09.0771656Z [36;1m  done[0m
2026-01-29T18:51:09.0771784Z [36;1m[0m
2026-01-29T18:51:09.0771928Z [36;1m  # 2) check leader pod[0m
2026-01-29T18:51:09.0772478Z [36;1m  LEADER_PHASE=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-01-29T18:51:09.0773081Z [36;1m  LEADER_READY=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-01-29T18:51:09.0773478Z [36;1m[0m
2026-01-29T18:51:09.0773705Z [36;1m  echo "Leader [$LEADER_POD] phase=$LEADER_PHASE ready=$LEADER_READY"[0m
2026-01-29T18:51:09.0773964Z [36;1m[0m
2026-01-29T18:51:09.0774176Z [36;1m  if [[ "$LEADER_PHASE" != "Running" || "$LEADER_READY" != "true" ]]; then[0m
2026-01-29T18:51:09.0774467Z [36;1m    echo "Leader not Ready yet..."[0m
2026-01-29T18:51:09.0774671Z [36;1m    ALL_FOLLOWERS_READY=false[0m
2026-01-29T18:51:09.0774859Z [36;1m  fi[0m
2026-01-29T18:51:09.0774988Z [36;1m[0m
2026-01-29T18:51:09.0775157Z [36;1m  if [[ "$ALL_FOLLOWERS_READY" == "true" ]]; then[0m
2026-01-29T18:51:09.0775490Z [36;1m    echo "All follower pods and leader pod are Running and Ready â€” continuing."[0m
2026-01-29T18:51:09.0775774Z [36;1m    break[0m
2026-01-29T18:51:09.0775922Z [36;1m  fi[0m
2026-01-29T18:51:09.0776057Z [36;1m[0m
2026-01-29T18:51:09.0776181Z [36;1m  sleep 2[0m
2026-01-29T18:51:09.0776326Z [36;1mdone[0m
2026-01-29T18:51:09.0776593Z shell: bash -el {0}
2026-01-29T18:51:09.0776737Z env:
2026-01-29T18:51:09.0777090Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-01-29T18:51:09.0777406Z ##[endgroup]
2026-01-29T18:51:09.0866348Z ##[group]Run '/home/runner/k8s/index.js'
2026-01-29T18:51:09.0867109Z shell: /home/runner/externals/node20/bin/node {0}
2026-01-29T18:51:09.0867391Z ##[endgroup]
2026-01-29T18:51:09.4448351Z (node:9582) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-01-29T18:51:09.4449050Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-01-29T18:51:27.7095441Z Waiting for Pods in namespace [vllm-project] to become Running and Ready (timeout 1200s)...
2026-01-29T18:51:27.8337037Z Follower [vllm-0-1] phase=Pending ready=false
2026-01-29T18:51:27.8337288Z Follower [vllm-0-1] not Ready yet...
2026-01-29T18:51:27.9623194Z Leader [vllm-0] phase=Running ready=true
2026-01-29T18:51:30.0876405Z Follower [vllm-0-1] phase=Pending ready=false
2026-01-29T18:51:30.0876711Z Follower [vllm-0-1] not Ready yet...
2026-01-29T18:51:30.2148012Z Leader [vllm-0] phase=Running ready=true
2026-01-29T18:51:32.3376248Z Follower [vllm-0-1] phase=Pending ready=false
2026-01-29T18:51:32.3376577Z Follower [vllm-0-1] not Ready yet...
2026-01-29T18:51:32.4499569Z Leader [vllm-0] phase=Running ready=true
2026-01-29T18:51:34.5835184Z Follower [vllm-0-1] phase=Pending ready=false
2026-01-29T18:51:34.5835472Z Follower [vllm-0-1] not Ready yet...
2026-01-29T18:51:34.6933893Z Leader [vllm-0] phase=Running ready=true
2026-01-29T18:51:36.8243754Z Follower [vllm-0-1] phase=Pending ready=false
2026-01-29T18:51:36.8244066Z Follower [vllm-0-1] not Ready yet...
2026-01-29T18:51:36.9524116Z Leader [vllm-0] phase=Running ready=true
2026-01-29T18:51:39.0762085Z Follower [vllm-0-1] phase=Pending ready=false
2026-01-29T18:51:39.0762415Z Follower [vllm-0-1] not Ready yet...
2026-01-29T18:51:39.2033159Z Leader [vllm-0] phase=Running ready=true
2026-01-29T18:51:41.3382771Z Follower [vllm-0-1] phase=Pending ready=false
2026-01-29T18:51:41.3383078Z Follower [vllm-0-1] not Ready yet...
2026-01-29T18:51:41.4671419Z Leader [vllm-0] phase=Running ready=true
2026-01-29T18:51:43.5840884Z Follower [vllm-0-1] phase=Pending ready=false
2026-01-29T18:51:43.5841338Z Follower [vllm-0-1] not Ready yet...
2026-01-29T18:51:43.7026990Z Leader [vllm-0] phase=Running ready=true
2026-01-29T18:51:45.8276660Z Follower [vllm-0-1] phase=Pending ready=false
2026-01-29T18:51:45.8276943Z Follower [vllm-0-1] not Ready yet...
2026-01-29T18:51:45.9404478Z Leader [vllm-0] phase=Running ready=true
2026-01-29T18:51:48.0746562Z Follower [vllm-0-1] phase=Pending ready=false
2026-01-29T18:51:48.0746855Z Follower [vllm-0-1] not Ready yet...
2026-01-29T18:51:48.2040215Z Leader [vllm-0] phase=Running ready=true
2026-01-29T18:51:50.3356043Z Follower [vllm-0-1] phase=Pending ready=false
2026-01-29T18:51:50.3356346Z Follower [vllm-0-1] not Ready yet...
2026-01-29T18:51:50.4641540Z Leader [vllm-0] phase=Running ready=true
2026-01-29T18:51:52.5813935Z Follower [vllm-0-1] phase=Pending ready=false
2026-01-29T18:51:52.5814237Z Follower [vllm-0-1] not Ready yet...
2026-01-29T18:51:52.7015225Z Leader [vllm-0] phase=Running ready=true
2026-01-29T18:51:54.8310872Z Follower [vllm-0-1] phase=Pending ready=false
2026-01-29T18:51:54.8311162Z Follower [vllm-0-1] not Ready yet...
2026-01-29T18:51:54.9550724Z Leader [vllm-0] phase=Running ready=true
2026-01-29T18:51:57.0665093Z Follower [vllm-0-1] phase=Running ready=true
2026-01-29T18:51:57.1838011Z Leader [vllm-0] phase=Running ready=true
2026-01-29T18:51:57.1838659Z All follower pods and leader pod are Running and Ready â€” continuing.
2026-01-29T18:52:15.5098415Z ##[group]Run set -euo pipefail
2026-01-29T18:52:15.5098682Z [36;1mset -euo pipefail[0m
2026-01-29T18:52:15.5098846Z [36;1m[0m
2026-01-29T18:52:15.5098981Z [36;1msize="2"[0m
2026-01-29T18:52:15.5099121Z [36;1mpids=()[0m
2026-01-29T18:52:15.5099291Z [36;1m[0m
2026-01-29T18:52:15.5099427Z [36;1mcleanup() {[0m
2026-01-29T18:52:15.5099623Z [36;1m  echo "Cleaning up background log streams..."[0m
2026-01-29T18:52:15.5099868Z [36;1m  for pid in "${pids[@]}"; do[0m
2026-01-29T18:52:15.5100077Z [36;1m    kill "$pid" 2>/dev/null || true[0m
2026-01-29T18:52:15.5100276Z [36;1m  done[0m
2026-01-29T18:52:15.5100417Z [36;1m}[0m
2026-01-29T18:52:15.5100553Z [36;1mtrap cleanup EXIT[0m
2026-01-29T18:52:15.5100717Z [36;1m[0m
2026-01-29T18:52:15.5100870Z [36;1mfor i in $(seq 1 $((size - 1))); do[0m
2026-01-29T18:52:15.5101068Z [36;1m  POD="vllm-0-${i}"[0m
2026-01-29T18:52:15.5101233Z [36;1m[0m
2026-01-29T18:52:15.5101455Z [36;1m  echo "==== Collecting logs from worker pod: $POD ===="[0m
2026-01-29T18:52:15.5101732Z [36;1m  kubectl logs -f "$POD" -n "$NAMESPACE" \[0m
2026-01-29T18:52:15.5101960Z [36;1m    > "/tmp/${POD}_logs.txt" 2>&1 &[0m
2026-01-29T18:52:15.5102286Z [36;1m[0m
2026-01-29T18:52:15.5102422Z [36;1m  pids+=($!)[0m
2026-01-29T18:52:15.5102582Z [36;1mdone[0m
2026-01-29T18:52:15.5102722Z [36;1m[0m
2026-01-29T18:52:15.5102916Z [36;1mecho "==== Streaming logs from leader pod: $LEADER_POD ===="[0m
2026-01-29T18:52:15.5103211Z [36;1mecho "Looking for logs containing: $FAIL_TAG"[0m
2026-01-29T18:52:15.5103434Z [36;1m[0m
2026-01-29T18:52:15.5103666Z [36;1mkubectl logs -f "$LEADER_POD" -n "$NAMESPACE" | while IFS= read -r line; do[0m
2026-01-29T18:52:15.5103957Z [36;1m  echo "$line"[0m
2026-01-29T18:52:15.5104157Z [36;1m  if echo "$line" | grep -q "$FAIL_TAG"; then[0m
2026-01-29T18:52:15.5104366Z [36;1m    exit 1[0m
2026-01-29T18:52:15.5104514Z [36;1m  fi[0m
2026-01-29T18:52:15.5104642Z [36;1mdone[0m
2026-01-29T18:52:15.5104975Z shell: bash -el {0}
2026-01-29T18:52:15.5105127Z env:
2026-01-29T18:52:15.5105317Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-01-29T18:52:15.5105548Z ##[endgroup]
2026-01-29T18:52:15.5194816Z ##[group]Run '/home/runner/k8s/index.js'
2026-01-29T18:52:15.5195494Z shell: /home/runner/externals/node20/bin/node {0}
2026-01-29T18:52:15.5195703Z ##[endgroup]
2026-01-29T18:52:15.8938379Z (node:10910) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-01-29T18:52:15.8939397Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-01-29T18:52:34.4363903Z ==== Collecting logs from worker pod: vllm-0-1 ====
2026-01-29T18:52:34.4364751Z ==== Streaming logs from leader pod: vllm-0 ====
2026-01-29T18:52:34.4365137Z Looking for logs containing: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-01-29T18:52:34.5243099Z ====> Check NPU info
2026-01-29T18:52:34.5253804Z +------------------------------------------------------------------------------------------------+
2026-01-29T18:52:34.5266104Z | npu-smi 25.5.0                   Version: 25.5.0                                               |
2026-01-29T18:52:34.5275659Z +---------------------------+---------------+----------------------------------------------------+
2026-01-29T18:52:34.5286509Z | NPU   Name                | Health        | Power(W)    Temp(C)           Hugepages-Usage(page)|
2026-01-29T18:52:34.5297137Z | Chip  Phy-ID              | Bus-Id        | AICore(%)   Memory-Usage(MB)  HBM-Usage(MB)        |
2026-01-29T18:52:34.5307129Z +===========================+===============+====================================================+
2026-01-29T18:52:34.5316562Z | 0     Ascend910           | OK            | 163.5       36                0    / 0             |
2026-01-29T18:52:34.5327112Z | 0     0                   | 0000:9D:00.0  | 0           0    / 0          3151 / 65536         |
2026-01-29T18:52:34.5337048Z +------------------------------------------------------------------------------------------------+
2026-01-29T18:52:34.5346989Z | 0     Ascend910           | OK            | -           37                0    / 0             |
2026-01-29T18:52:34.5356738Z | 1     1                   | 0000:9F:00.0  | 0           0    / 0          2895 / 65536         |
2026-01-29T18:52:34.5366414Z +===========================+===============+====================================================+
2026-01-29T18:52:34.5375685Z | 1     Ascend910           | OK            | 163.2       36                0    / 0             |
2026-01-29T18:52:34.5384637Z | 0     2                   | 0000:99:00.0  | 0           0    / 0          3160 / 65536         |
2026-01-29T18:52:34.5393965Z +------------------------------------------------------------------------------------------------+
2026-01-29T18:52:34.5404830Z | 1     Ascend910           | OK            | -           37                0    / 0             |
2026-01-29T18:52:34.5413770Z | 1     3                   | 0000:9B:00.0  | 0           0    / 0          2883 / 65536         |
2026-01-29T18:52:34.5423383Z +===========================+===============+====================================================+
2026-01-29T18:52:34.5433109Z | 2     Ascend910           | OK            | 164.4       37                0    / 0             |
2026-01-29T18:52:34.5443259Z | 0     4                   | 0000:95:00.0  | 0           0    / 0          3155 / 65536         |
2026-01-29T18:52:34.5454137Z +------------------------------------------------------------------------------------------------+
2026-01-29T18:52:34.5462322Z | 2     Ascend910           | OK            | -           37                0    / 0             |
2026-01-29T18:52:34.5471884Z | 1     5                   | 0000:97:00.0  | 0           0    / 0          2880 / 65536         |
2026-01-29T18:52:34.5481290Z +===========================+===============+====================================================+
2026-01-29T18:52:34.5491337Z | 3     Ascend910           | OK            | 172.7       36                0    / 0             |
2026-01-29T18:52:34.5500397Z | 0     6                   | 0000:91:00.0  | 0           0    / 0          3144 / 65536         |
2026-01-29T18:52:34.5510650Z +------------------------------------------------------------------------------------------------+
2026-01-29T18:52:34.5520401Z | 3     Ascend910           | OK            | -           37                0    / 0             |
2026-01-29T18:52:34.5529861Z | 1     7                   | 0000:93:00.0  | 0           0    / 0          2895 / 65536         |
2026-01-29T18:52:34.5539335Z +===========================+===============+====================================================+
2026-01-29T18:52:34.5548611Z | 4     Ascend910           | OK            | 163.5       36                0    / 0             |
2026-01-29T18:52:34.5558308Z | 0     8                   | 0000:8D:00.0  | 0           0    / 0          3150 / 65536         |
2026-01-29T18:52:34.5567978Z +------------------------------------------------------------------------------------------------+
2026-01-29T18:52:34.5580892Z | 4     Ascend910           | OK            | -           35                0    / 0             |
2026-01-29T18:52:34.5587465Z | 1     9                   | 0000:8F:00.0  | 0           0    / 0          2893 / 65536         |
2026-01-29T18:52:34.5597520Z +===========================+===============+====================================================+
2026-01-29T18:52:34.5606418Z | 5     Ascend910           | OK            | 165.5       35                0    / 0             |
2026-01-29T18:52:34.5616107Z | 0     10                  | 0000:89:00.0  | 0           0    / 0          3145 / 65536         |
2026-01-29T18:52:34.5625530Z +------------------------------------------------------------------------------------------------+
2026-01-29T18:52:34.5634736Z | 5     Ascend910           | OK            | -           36                0    / 0             |
2026-01-29T18:52:34.5644686Z | 1     11                  | 0000:8B:00.0  | 0           0    / 0          2891 / 65536         |
2026-01-29T18:52:34.5654197Z +===========================+===============+====================================================+
2026-01-29T18:52:34.5663186Z | 6     Ascend910           | OK            | 166.2       36                0    / 0             |
2026-01-29T18:52:34.5672720Z | 0     12                  | 0000:85:00.0  | 0           0    / 0          3142 / 65536         |
2026-01-29T18:52:34.5682574Z +------------------------------------------------------------------------------------------------+
2026-01-29T18:52:34.5692273Z | 6     Ascend910           | OK            | -           35                0    / 0             |
2026-01-29T18:52:34.5701389Z | 1     13                  | 0000:87:00.0  | 0           0    / 0          2894 / 65536         |
2026-01-29T18:52:34.5710412Z +===========================+===============+====================================================+
2026-01-29T18:52:34.5720332Z | 7     Ascend910           | OK            | 163.7       36                0    / 0             |
2026-01-29T18:52:34.5729771Z | 0     14                  | 0000:81:00.0  | 0           0    / 0          3155 / 65536         |
2026-01-29T18:52:34.5739878Z +------------------------------------------------------------------------------------------------+
2026-01-29T18:52:34.5749427Z | 7     Ascend910           | OK            | -           36                0    / 0             |
2026-01-29T18:52:34.5759847Z | 1     15                  | 0000:83:00.0  | 0           0    / 0          2882 / 65536         |
2026-01-29T18:52:34.5772824Z +===========================+===============+====================================================+
2026-01-29T18:52:34.5783585Z +---------------------------+---------------+----------------------------------------------------+
2026-01-29T18:52:34.5793043Z | NPU     Chip              | Process id    | Process name             | Process memory(MB)      |
2026-01-29T18:52:34.5802989Z +===========================+===============+====================================================+
2026-01-29T18:52:34.5814912Z | No running processes found in NPU 0                                                            |
2026-01-29T18:52:34.5826468Z +===========================+===============+====================================================+
2026-01-29T18:52:34.5835345Z | No running processes found in NPU 1                                                            |
2026-01-29T18:52:34.5846300Z +===========================+===============+====================================================+
2026-01-29T18:52:34.5856652Z | No running processes found in NPU 2                                                            |
2026-01-29T18:52:34.5867384Z +===========================+===============+====================================================+
2026-01-29T18:52:34.5876681Z | No running processes found in NPU 3                                                            |
2026-01-29T18:52:34.5886460Z +===========================+===============+====================================================+
2026-01-29T18:52:34.5896331Z | No running processes found in NPU 4                                                            |
2026-01-29T18:52:34.5908018Z +===========================+===============+====================================================+
2026-01-29T18:52:34.5915270Z | No running processes found in NPU 5                                                            |
2026-01-29T18:52:34.5924985Z +===========================+===============+====================================================+
2026-01-29T18:52:34.5934096Z | No running processes found in NPU 6                                                            |
2026-01-29T18:52:34.5945165Z +===========================+===============+====================================================+
2026-01-29T18:52:34.5954843Z | No running processes found in NPU 7                                                            |
2026-01-29T18:52:34.5966196Z +===========================+===============+====================================================+
2026-01-29T18:52:34.5975935Z package_name=Ascend-cann-toolkit
2026-01-29T18:52:34.5985745Z version=8.5.0
2026-01-29T18:52:34.5995992Z innerversion=V100R001C25SPC001B232
2026-01-29T18:52:34.6006331Z compatible_version=[V100R001C15],[V100R001C18],[V100R001C19],[V100R001C20],[V100R001C21],[V100R001C23]
2026-01-29T18:52:34.6014822Z arch=aarch64
2026-01-29T18:52:34.6024042Z os=linux
2026-01-29T18:52:34.6034147Z path=/usr/local/Ascend/cann-8.5.0
2026-01-29T18:52:34.6042783Z ====> Configure mirrors and git proxy
2026-01-29T18:52:34.6107893Z Writing to /root/.config/pip/pip.conf
2026-01-29T18:52:34.6108140Z Installed vLLM-related Python packages:
2026-01-29T18:52:34.6108449Z ais_bench_benchmark               3.0.20250930               /vllm-workspace/vllm-ascend/benchmark
2026-01-29T18:52:34.6108789Z vllm                              0.14.1+empty               /vllm-workspace/vllm
2026-01-29T18:52:34.6109117Z vllm_ascend                       0.14.0rc2.dev43+ge35f30441 /vllm-workspace/vllm-ascend
2026-01-29T18:52:34.6110728Z 
2026-01-29T18:52:34.6110812Z ============================
2026-01-29T18:52:34.6117973Z vLLM Git information
2026-01-29T18:52:34.6126780Z ============================
2026-01-29T18:52:34.6136146Z Branch:      HEAD
2026-01-29T18:52:34.6145329Z Commit hash: d7de043d55d1dd629554467e23874097e1c48993
2026-01-29T18:52:34.6153884Z Author:      Shengqi Chen <harry-chen@outlook.com>
2026-01-29T18:52:34.6163912Z Date:        2026-01-23 14:21:49 -0800
2026-01-29T18:52:34.6173770Z Message:     [CI] fix version comparsion and exclusion patterns in upload-release-wheels.sh (#32971)
2026-01-29T18:52:34.6182923Z Tags:        v0.14.1
2026-01-29T18:52:34.6192126Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm.git (fetch)
2026-01-29T18:52:34.6201222Z 
2026-01-29T18:52:34.6210525Z 
2026-01-29T18:52:34.6220472Z ============================
2026-01-29T18:52:34.6251769Z vLLM-Ascend Git information
2026-01-29T18:52:34.6252124Z ============================
2026-01-29T18:52:34.6252363Z Branch:      main
2026-01-29T18:52:34.6259270Z Commit hash: e35f304419825fbf92acf772bf8d7bbe4e43d167
2026-01-29T18:52:34.6268444Z Author:      Li Wang <wangli858794774@gmail.com>
2026-01-29T18:52:34.6278038Z Date:        2026-01-29 20:28:10 +0800
2026-01-29T18:52:34.6287327Z Message:     [CI] Auto partition for test cases (#6379)
2026-01-29T18:52:34.6296689Z Tags:        
2026-01-29T18:52:34.6306591Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm-ascend (fetch)
2026-01-29T18:52:34.6314769Z 
2026-01-29T18:52:34.6324629Z ====> Check triton ascend info
2026-01-29T18:52:34.6333369Z Ubuntu clang version 15.0.7
2026-01-29T18:52:34.6342783Z Target: aarch64-unknown-linux-gnu
2026-01-29T18:52:34.6354777Z Thread model: posix
2026-01-29T18:52:34.6361368Z InstalledDir: /usr/bin
2026-01-29T18:52:34.6370524Z Found candidate GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-01-29T18:52:34.6379903Z Selected GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-01-29T18:52:34.6388720Z Candidate multilib: .;@m64
2026-01-29T18:52:34.6397965Z Selected multilib: .;@m64
2026-01-29T18:52:34.6407140Z /usr/local/Ascend/ascend-toolkit/latest/bin/bishengir-compile
2026-01-29T18:52:34.6416506Z Name: triton-ascend
2026-01-29T18:52:34.6425524Z Version: 3.2.0
2026-01-29T18:52:34.6434756Z Summary: A language and compiler for custom Deep Learning operations on Ascend hardwares
2026-01-29T18:52:34.6445667Z Home-page: https://gitcode.com/Ascend/triton-ascend/
2026-01-29T18:52:34.6453639Z Author: 
2026-01-29T18:52:34.6462585Z Author-email: 
2026-01-29T18:52:34.6471177Z License: 
2026-01-29T18:52:34.6481014Z Location: /usr/local/python3.11.14/lib/python3.11/site-packages
2026-01-29T18:52:34.6490569Z Requires: 
2026-01-29T18:52:34.6499449Z Required-by: vllm_ascend
2026-01-29T18:52:34.6508821Z INFO 01-29 18:51:29 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:52:34.6518949Z INFO 01-29 18:51:29 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:52:34.6528534Z INFO 01-29 18:51:29 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:52:34.6537150Z INFO 01-29 18:51:29 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:52:34.6546326Z ============================= test session starts ==============================
2026-01-29T18:52:34.6556423Z platform linux -- Python 3.11.14, pytest-8.4.2, pluggy-1.6.0 -- /usr/local/python3.11.14/bin/python3
2026-01-29T18:52:34.6565581Z cachedir: .pytest_cache
2026-01-29T18:52:34.6574554Z rootdir: /vllm-workspace/vllm-ascend
2026-01-29T18:52:34.6583842Z configfile: pyproject.toml
2026-01-29T18:52:34.6593215Z plugins: cov-7.0.0, asyncio-1.3.0, mock-3.15.1, anyio-4.12.1
2026-01-29T18:52:34.6602953Z asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
2026-01-29T18:52:34.6612399Z collecting ... collected 1 item
2026-01-29T18:52:34.6621024Z 
2026-01-29T18:52:34.6630740Z [2026-01-29 18:51:36] INFO multi_node_config.py:294: Loading config yaml: tests/e2e/nightly/multi_node/config/DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-01-29T18:52:34.6640353Z [2026-01-29 18:51:36] INFO multi_node_config.py:351: Resolving cluster IPs via DNS...
2026-01-29T18:52:34.6653969Z [2026-01-29 18:51:56] INFO multi_node_config.py:212: Node 0 envs: {'HCCL_OP_EXPANSION_MODE': 'AIV', 'VLLM_USE_MODELSCOPE': 'True', 'HCCL_BUFFSIZE': '1024', 'SERVER_PORT': '8080', 'OMP_PROC_BIND': 'False', 'OMP_NUM_THREADS': '1', 'PYTORCH_NPU_ALLOC_CONF': 'expandable_segments:True', 'VLLM_ASCEND_ENABLE_FLASHCOMM1': '1', 'ASCEND_A3_EBA_ENABLE': '1', 'HCCL_IF_IP': '10.0.0.77', 'HCCL_SOCKET_IFNAME': 'eth0', 'GLOO_SOCKET_IFNAME': 'eth0', 'TP_SOCKET_IFNAME': 'eth0', 'LOCAL_IP': '10.0.0.77', 'NIC_NAME': 'eth0', 'MASTER_IP': '10.0.0.77'}
2026-01-29T18:52:34.6660410Z [2026-01-29 18:51:56] INFO multi_node_config.py:137: Not launching proxy on non-master node
2026-01-29T18:52:34.6673855Z [2026-01-29 18:51:56] INFO conftest.py:231: Starting server with command: vllm serve vllm-ascend/DeepSeek-V3.2-W8A8 --host 0.0.0.0 --port 8080 --data-parallel-size 4 --data-parallel-size-local 2 --data-parallel-address 10.0.0.77 --data-parallel-rpc-port 13399 --tensor-parallel-size 8 --quantization ascend --seed 1024 --enable-expert-parallel --max-num-seqs 16 --max-model-len 8192 --max-num-batched-tokens 4096 --no-enable-prefix-caching --gpu-memory-utilization 0.85 --trust-remote-code --speculative-config {"num_speculative_tokens": 2, "method":"deepseek_mtp"} --compilation-config {"cudagraph_capture_sizes": [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], "cudagraph_mode": "FULL_DECODE_ONLY"} --additional-config {"layer_sharding": ["q_b_proj", "o_proj"]} --tokenizer-mode deepseek_v32 --reasoning-parser deepseek_v3
2026-01-29T18:52:34.6686055Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node INFO 01-29 18:52:01 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:52:34.6690780Z INFO 01-29 18:52:01 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:52:34.6700502Z INFO 01-29 18:52:01 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:52:34.6709645Z INFO 01-29 18:52:01 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:52:34.6719589Z 2026-01-29 18:52:07,442 - 66 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.14.1+empty: default 1
2026-01-29T18:52:34.6729246Z INFO 01-29 18:52:07 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-01-29T18:52:34.6738524Z [0;36m(APIServer pid=66)[0;0m INFO 01-29 18:52:07 [api_server.py:1272] vLLM API server version 0.14.1
2026-01-29T18:52:34.6757620Z [0;36m(APIServer pid=66)[0;0m INFO 01-29 18:52:07 [utils.py:263] non-default args: {'model_tag': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'host': '0.0.0.0', 'port': 8080, 'model': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'tokenizer_mode': 'deepseek_v32', 'trust_remote_code': True, 'seed': 1024, 'max_model_len': 8192, 'quantization': 'ascend', 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 8, 'data_parallel_size': 4, 'data_parallel_size_local': 2, 'data_parallel_address': '10.0.0.77', 'data_parallel_rpc_port': 13399, 'enable_expert_parallel': True, 'gpu_memory_utilization': 0.85, 'enable_prefix_caching': False, 'max_num_batched_tokens': 4096, 'max_num_seqs': 16, 'speculative_config': {'num_speculative_tokens': 2, 'method': 'deepseek_mtp'}, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}, 'additional_config': {'layer_sharding': ['q_b_proj', 'o_proj']}}
2026-01-29T18:52:34.6762718Z [0;36m(APIServer pid=66)[0;0m 2026-01-29 18:52:07,755 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-01-29T18:52:34.6774339Z [0;36m(APIServer pid=66)[0;0m INFO 01-29 18:52:07 [arg_utils.py:603] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-01-29T18:52:34.6783765Z [0;36m(APIServer pid=66)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-01-29T18:52:34.6794088Z [0;36m(APIServer pid=66)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-01-29T18:52:34.6804904Z [0;36m(APIServer pid=66)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-01-29T18:52:34.6814729Z [0;36m(APIServer pid=66)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-01-29T18:52:34.6824310Z [0;36m(APIServer pid=66)[0;0m INFO 01-29 18:52:17 [model.py:530] Resolved architecture: DeepseekV32ForCausalLM
2026-01-29T18:52:34.6833846Z [0;36m(APIServer pid=66)[0;0m INFO 01-29 18:52:17 [model.py:1545] Using max model len 8192
2026-01-29T18:52:34.6844447Z [0;36m(APIServer pid=66)[0;0m WARNING 01-29 18:52:17 [speculative.py:254] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-01-29T18:52:34.6854345Z [0;36m(APIServer pid=66)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-01-29T18:52:34.6864038Z [0;36m(APIServer pid=66)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-01-29T18:52:34.6874897Z [0;36m(APIServer pid=66)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-01-29T18:52:34.6884657Z [0;36m(APIServer pid=66)[0;0m INFO 01-29 18:52:17 [model.py:530] Resolved architecture: DeepSeekMTPModel
2026-01-29T18:52:34.6894444Z [0;36m(APIServer pid=66)[0;0m INFO 01-29 18:52:17 [model.py:1545] Using max model len 163840
2026-01-29T18:52:34.6904720Z [0;36m(APIServer pid=66)[0;0m WARNING 01-29 18:52:17 [speculative.py:372] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-01-29T18:52:34.6914568Z [0;36m(APIServer pid=66)[0;0m INFO 01-29 18:52:17 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-01-29T18:52:34.6924007Z [0;36m(APIServer pid=66)[0;0m INFO 01-29 18:52:17 [vllm.py:630] Asynchronous scheduling is enabled.
2026-01-29T18:52:34.6938730Z [0;36m(APIServer pid=66)[0;0m INFO 01-29 18:52:17 [vllm.py:637] Disabling NCCL for DP synchronization when using async scheduling.
2026-01-29T18:52:34.6949603Z [0;36m(APIServer pid=66)[0;0m WARNING 01-29 18:52:17 [platform.py:716] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-01-29T18:52:34.6961202Z [0;36m(APIServer pid=66)[0;0m INFO 01-29 18:52:17 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-01-29T18:52:34.6970108Z [0;36m(APIServer pid=66)[0;0m WARNING 01-29 18:52:17 [vllm.py:1054] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-01-29T18:52:34.6979678Z [0;36m(APIServer pid=66)[0;0m INFO 01-29 18:52:17 [platform.py:307] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-01-29T18:52:34.6989244Z [0;36m(APIServer pid=66)[0;0m WARNING 01-29 18:52:17 [platform.py:324] [91m
2026-01-29T18:52:34.6999922Z [0;36m(APIServer pid=66)[0;0m WARNING 01-29 18:52:17 [platform.py:324]             **********************************************************************************
2026-01-29T18:52:34.7009573Z [0;36m(APIServer pid=66)[0;0m WARNING 01-29 18:52:17 [platform.py:324]             * WARNING: You have enabled the *full graph* feature.
2026-01-29T18:52:34.7019721Z [0;36m(APIServer pid=66)[0;0m WARNING 01-29 18:52:17 [platform.py:324]             * This is an early experimental stage and may involve various unknown issues.
2026-01-29T18:52:34.7029631Z [0;36m(APIServer pid=66)[0;0m WARNING 01-29 18:52:17 [platform.py:324]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-01-29T18:52:34.7040417Z [0;36m(APIServer pid=66)[0;0m WARNING 01-29 18:52:17 [platform.py:324]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-01-29T18:52:34.7050619Z [0;36m(APIServer pid=66)[0;0m WARNING 01-29 18:52:17 [platform.py:324]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-01-29T18:52:34.7059943Z [0;36m(APIServer pid=66)[0;0m WARNING 01-29 18:52:17 [platform.py:324]             * batch size for graph capture.
2026-01-29T18:52:34.7069632Z [0;36m(APIServer pid=66)[0;0m WARNING 01-29 18:52:17 [platform.py:324]             * For more details, please refer to:
2026-01-29T18:52:34.7080443Z [0;36m(APIServer pid=66)[0;0m WARNING 01-29 18:52:17 [platform.py:324]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-01-29T18:52:34.7091775Z [0;36m(APIServer pid=66)[0;0m WARNING 01-29 18:52:17 [platform.py:324]             **********************************************************************************[0m
2026-01-29T18:52:34.7101262Z [0;36m(APIServer pid=66)[0;0m WARNING 01-29 18:52:17 [platform.py:324]             
2026-01-29T18:52:34.7111212Z [0;36m(APIServer pid=66)[0;0m INFO 01-29 18:52:17 [platform.py:417] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-01-29T18:52:34.7120558Z [0;36m(APIServer pid=66)[0;0m INFO 01-29 18:52:18 [utils.py:839] Started DP Coordinator process (PID: 86)
2026-01-29T18:52:34.7130485Z INFO 01-29 18:52:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:52:34.7139485Z INFO 01-29 18:52:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:52:34.7148540Z INFO 01-29 18:52:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:52:34.7158033Z INFO 01-29 18:52:23 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:52:34.7167889Z INFO 01-29 18:52:23 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:52:34.7176930Z INFO 01-29 18:52:23 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:52:34.7186700Z INFO 01-29 18:52:23 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:52:34.7196119Z INFO 01-29 18:52:23 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:52:34.7206740Z INFO 01-29 18:52:33 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:52:34.7215814Z INFO 01-29 18:52:33 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:52:34.7225210Z INFO 01-29 18:52:33 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:52:34.7234219Z INFO 01-29 18:52:33 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:52:46.5882370Z [0;36m(EngineCore_DP0 pid=89)[0;0m 2026-01-29 18:52:46,586 - 89 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.14.1+empty: default 1
2026-01-29T18:52:46.5904987Z [0;36m(EngineCore_DP1 pid=108)[0;0m 2026-01-29 18:52:46,588 - 108 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.14.1+empty: default 1
2026-01-29T18:52:46.5926007Z [0;36m(EngineCore_DP0 pid=89)[0;0m INFO 01-29 18:52:46 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-01-29T18:52:46.5952623Z [0;36m(EngineCore_DP0 pid=89)[0;0m INFO 01-29 18:52:46 [core.py:97] Initializing a V1 LLM engine (v0.14.1) with config: model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', speculative_config=SpeculativeConfig(method='mtp', model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', num_spec_tokens=2), tokenizer='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', skip_tokenizer_init=False, tokenizer_mode=deepseek_v32, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=4, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=1024, served_model_name=/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [24, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 48, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
2026-01-29T18:52:46.5960573Z [0;36m(EngineCore_DP1 pid=108)[0;0m INFO 01-29 18:52:46 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-01-29T18:52:51.4733705Z INFO 01-29 18:52:51 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:52:51.4743120Z INFO 01-29 18:52:51 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:52:51.4759889Z INFO 01-29 18:52:51 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:52:51.4807003Z INFO 01-29 18:52:51 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:52:51.5618011Z INFO 01-29 18:52:51 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:52:51.5626885Z INFO 01-29 18:52:51 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:52:51.5637832Z INFO 01-29 18:52:51 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:52:51.5696685Z INFO 01-29 18:52:51 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:52:57.1317124Z 2026-01-29 18:52:57,129 - 133 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.14.1+empty: default 1
2026-01-29T18:52:57.1341072Z 2026-01-29 18:52:57,130 - 132 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.14.1+empty: default 1
2026-01-29T18:52:57.1365086Z INFO 01-29 18:52:57 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-01-29T18:52:57.1374935Z INFO 01-29 18:52:57 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-01-29T18:53:01.8135184Z INFO 01-29 18:53:01 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-01-29T18:53:01.8138973Z INFO 01-29 18:53:01 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-01-29T18:53:01.8916486Z INFO 01-29 18:53:01 [parallel_state.py:1214] world_size=32 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.77:39541 backend=hccl
2026-01-29T18:53:01.8937907Z INFO 01-29 18:53:01 [parallel_state.py:1214] world_size=32 rank=8 local_rank=0 distributed_init_method=tcp://10.0.0.77:39541 backend=hccl
2026-01-29T18:53:01.9330160Z INFO 01-29 18:53:01 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:53:01.9339704Z INFO 01-29 18:53:01 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:53:01.9350110Z INFO 01-29 18:53:01 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:53:01.9406839Z INFO 01-29 18:53:01 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:53:01.9695550Z INFO 01-29 18:53:01 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:53:01.9705634Z INFO 01-29 18:53:01 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:53:01.9715946Z INFO 01-29 18:53:01 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:53:01.9772873Z INFO 01-29 18:53:01 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:53:07.6608980Z 2026-01-29 18:53:07,659 - 150 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.14.1+empty: default 1
2026-01-29T18:53:07.6641514Z INFO 01-29 18:53:07 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-01-29T18:53:07.6826606Z 2026-01-29 18:53:07,681 - 151 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.14.1+empty: default 1
2026-01-29T18:53:07.6864062Z INFO 01-29 18:53:07 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-01-29T18:53:10.1763836Z INFO 01-29 18:53:10 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-01-29T18:53:10.2046014Z INFO 01-29 18:53:10 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-01-29T18:53:10.6085640Z INFO 01-29 18:53:10 [parallel_state.py:1214] world_size=32 rank=1 local_rank=1 distributed_init_method=tcp://10.0.0.77:39541 backend=hccl
2026-01-29T18:53:10.6310208Z INFO 01-29 18:53:10 [parallel_state.py:1214] world_size=32 rank=9 local_rank=1 distributed_init_method=tcp://10.0.0.77:39541 backend=hccl
2026-01-29T18:53:12.3967957Z INFO 01-29 18:53:12 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:53:12.3976961Z INFO 01-29 18:53:12 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:53:12.3987736Z INFO 01-29 18:53:12 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:53:12.4044483Z INFO 01-29 18:53:12 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:53:12.4895585Z INFO 01-29 18:53:12 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:53:12.4904171Z INFO 01-29 18:53:12 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:53:12.4914055Z INFO 01-29 18:53:12 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:53:12.4969597Z INFO 01-29 18:53:12 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:53:17.9749777Z 2026-01-29 18:53:17,973 - 244 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.14.1+empty: default 1
2026-01-29T18:53:17.9782122Z INFO 01-29 18:53:17 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-01-29T18:53:18.0465170Z 2026-01-29 18:53:18,045 - 245 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.14.1+empty: default 1
2026-01-29T18:53:18.0500444Z INFO 01-29 18:53:18 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-01-29T18:53:20.4844481Z INFO 01-29 18:53:20 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-01-29T18:53:20.5246279Z INFO 01-29 18:53:20 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-01-29T18:53:20.9074976Z INFO 01-29 18:53:20 [parallel_state.py:1214] world_size=32 rank=10 local_rank=2 distributed_init_method=tcp://10.0.0.77:39541 backend=hccl
2026-01-29T18:53:20.9619336Z INFO 01-29 18:53:20 [parallel_state.py:1214] world_size=32 rank=2 local_rank=2 distributed_init_method=tcp://10.0.0.77:39541 backend=hccl
2026-01-29T18:53:22.6388513Z INFO 01-29 18:53:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:53:22.6397139Z INFO 01-29 18:53:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:53:22.6406724Z INFO 01-29 18:53:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:53:22.6460964Z INFO 01-29 18:53:22 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:53:22.6782813Z INFO 01-29 18:53:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:53:22.6793431Z INFO 01-29 18:53:22 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:53:22.6803354Z INFO 01-29 18:53:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:53:22.6853042Z INFO 01-29 18:53:22 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:53:28.2697946Z 2026-01-29 18:53:28,267 - 348 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.14.1+empty: default 1
2026-01-29T18:53:28.2730461Z INFO 01-29 18:53:28 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-01-29T18:53:28.3450733Z 2026-01-29 18:53:28,343 - 349 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.14.1+empty: default 1
2026-01-29T18:53:28.3486013Z INFO 01-29 18:53:28 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-01-29T18:53:30.6837346Z INFO 01-29 18:53:30 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-01-29T18:53:30.8222163Z INFO 01-29 18:53:30 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-01-29T18:53:31.1029430Z INFO 01-29 18:53:31 [parallel_state.py:1214] world_size=32 rank=11 local_rank=3 distributed_init_method=tcp://10.0.0.77:39541 backend=hccl
2026-01-29T18:53:31.2497446Z INFO 01-29 18:53:31 [parallel_state.py:1214] world_size=32 rank=3 local_rank=3 distributed_init_method=tcp://10.0.0.77:39541 backend=hccl
2026-01-29T18:53:32.8796532Z INFO 01-29 18:53:32 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:53:32.8803888Z INFO 01-29 18:53:32 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:53:32.8813539Z INFO 01-29 18:53:32 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:53:32.8848225Z INFO 01-29 18:53:32 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:53:32.9467476Z INFO 01-29 18:53:32 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:53:32.9476463Z INFO 01-29 18:53:32 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:53:32.9486510Z INFO 01-29 18:53:32 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:53:32.9544288Z INFO 01-29 18:53:32 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:53:38.6225112Z 2026-01-29 18:53:38,620 - 452 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.14.1+empty: default 1
2026-01-29T18:53:38.6256720Z INFO 01-29 18:53:38 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-01-29T18:53:38.6466011Z 2026-01-29 18:53:38,645 - 453 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.14.1+empty: default 1
2026-01-29T18:53:38.6504401Z INFO 01-29 18:53:38 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-01-29T18:53:41.0788569Z INFO 01-29 18:53:41 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-01-29T18:53:41.1208360Z INFO 01-29 18:53:41 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-01-29T18:53:41.5102351Z INFO 01-29 18:53:41 [parallel_state.py:1214] world_size=32 rank=12 local_rank=4 distributed_init_method=tcp://10.0.0.77:39541 backend=hccl
2026-01-29T18:53:41.5434118Z INFO 01-29 18:53:41 [parallel_state.py:1214] world_size=32 rank=4 local_rank=4 distributed_init_method=tcp://10.0.0.77:39541 backend=hccl
2026-01-29T18:53:43.3163798Z INFO 01-29 18:53:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:53:43.3173338Z INFO 01-29 18:53:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:53:43.3184095Z INFO 01-29 18:53:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:53:43.3237451Z INFO 01-29 18:53:43 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:53:43.4410004Z INFO 01-29 18:53:43 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:53:43.4418108Z INFO 01-29 18:53:43 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:53:43.4428136Z INFO 01-29 18:53:43 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:53:43.4484258Z INFO 01-29 18:53:43 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:53:48.9114643Z 2026-01-29 18:53:48,909 - 557 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.14.1+empty: default 1
2026-01-29T18:53:48.9145335Z INFO 01-29 18:53:48 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-01-29T18:53:48.9276095Z 2026-01-29 18:53:48,926 - 556 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.14.1+empty: default 1
2026-01-29T18:53:48.9314056Z INFO 01-29 18:53:48 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-01-29T18:53:51.4007151Z INFO 01-29 18:53:51 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-01-29T18:53:51.4234956Z INFO 01-29 18:53:51 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-01-29T18:53:51.8202610Z INFO 01-29 18:53:51 [parallel_state.py:1214] world_size=32 rank=5 local_rank=5 distributed_init_method=tcp://10.0.0.77:39541 backend=hccl
2026-01-29T18:53:51.8606728Z INFO 01-29 18:53:51 [parallel_state.py:1214] world_size=32 rank=13 local_rank=5 distributed_init_method=tcp://10.0.0.77:39541 backend=hccl
2026-01-29T18:53:53.6281569Z INFO 01-29 18:53:53 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:53:53.6292175Z INFO 01-29 18:53:53 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:53:53.6305419Z INFO 01-29 18:53:53 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:53:53.6352560Z INFO 01-29 18:53:53 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:53:53.6377088Z INFO 01-29 18:53:53 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:53:53.6387027Z INFO 01-29 18:53:53 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:53:53.6396805Z INFO 01-29 18:53:53 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:53:53.6453193Z INFO 01-29 18:53:53 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:53:59.2995901Z 2026-01-29 18:53:59,297 - 660 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.14.1+empty: default 1
2026-01-29T18:53:59.3027772Z INFO 01-29 18:53:59 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-01-29T18:53:59.3361460Z 2026-01-29 18:53:59,334 - 661 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.14.1+empty: default 1
2026-01-29T18:53:59.3398139Z INFO 01-29 18:53:59 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-01-29T18:54:01.7656905Z INFO 01-29 18:54:01 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-01-29T18:54:01.8183753Z INFO 01-29 18:54:01 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-01-29T18:54:02.2110766Z INFO 01-29 18:54:02 [parallel_state.py:1214] world_size=32 rank=14 local_rank=6 distributed_init_method=tcp://10.0.0.77:39541 backend=hccl
2026-01-29T18:54:02.2668252Z INFO 01-29 18:54:02 [parallel_state.py:1214] world_size=32 rank=6 local_rank=6 distributed_init_method=tcp://10.0.0.77:39541 backend=hccl
2026-01-29T18:54:03.9734260Z INFO 01-29 18:54:03 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:54:03.9741851Z INFO 01-29 18:54:03 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:54:03.9751003Z INFO 01-29 18:54:03 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:54:03.9810457Z INFO 01-29 18:54:03 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:54:04.0361385Z INFO 01-29 18:54:04 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-01-29T18:54:04.0369892Z INFO 01-29 18:54:04 [__init__.py:45] - ascend -> vllm_ascend:register
2026-01-29T18:54:04.0379446Z INFO 01-29 18:54:04 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-01-29T18:54:04.0436369Z INFO 01-29 18:54:04 [__init__.py:217] Platform plugin ascend is activated
2026-01-29T18:54:09.6600288Z 2026-01-29 18:54:09,657 - 764 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.14.1+empty: default 1
2026-01-29T18:54:09.6631007Z INFO 01-29 18:54:09 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-01-29T18:54:09.6863649Z 2026-01-29 18:54:09,684 - 765 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.14.1+empty: default 1
2026-01-29T18:54:09.6902605Z INFO 01-29 18:54:09 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-01-29T18:54:12.0545610Z INFO 01-29 18:54:12 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-01-29T18:54:12.3061276Z INFO 01-29 18:54:12 [ascend_config.py:55] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-01-29T18:54:12.4657773Z INFO 01-29 18:54:12 [parallel_state.py:1214] world_size=32 rank=15 local_rank=7 distributed_init_method=tcp://10.0.0.77:39541 backend=hccl
2026-01-29T18:54:12.7351276Z INFO 01-29 18:54:12 [parallel_state.py:1214] world_size=32 rank=7 local_rank=7 distributed_init_method=tcp://10.0.0.77:39541 backend=hccl
2026-01-29T18:54:12.7783625Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:12.7807108Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:12.7818261Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:12.7831735Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:12.8368147Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:12.8376846Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:12.8386484Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:12.8639219Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:12.8649275Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:12.8660442Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:12.8670062Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:12.8680459Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:12.8690102Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:12.8699493Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:12.8709437Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:12.8718948Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:12.9118172Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-01-29T18:54:12.9127496Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-01-29T18:54:12.9138732Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-01-29T18:54:12.9147656Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-01-29T18:54:12.9156785Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-01-29T18:54:12.9166719Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-01-29T18:54:12.9176079Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-01-29T18:54:12.9185256Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-01-29T18:54:12.9194488Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-01-29T18:54:12.9204390Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-01-29T18:54:12.9212883Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-01-29T18:54:12.9222145Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-01-29T18:54:12.9231469Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-01-29T18:54:12.9241153Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-01-29T18:54:12.9251122Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-01-29T18:54:12.9260255Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-01-29T18:54:12.9269521Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9280441Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9289327Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9298163Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9307567Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9319051Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9328951Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9337798Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9346644Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9356657Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9368569Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9376451Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9385595Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9395346Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9405454Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9414716Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9423738Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9433988Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9444168Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9453928Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9463326Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9473049Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9482881Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9492569Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9926185Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9945459Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9955262Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9966623Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9976617Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9986544Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:12.9996390Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:13.0006281Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:13.0015300Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:13.0025102Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:13.0036354Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:13.0044824Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:13.0055296Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:13.0064261Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:13.0074267Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:13.0083873Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:13.0093313Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:13.0105200Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:13.0112372Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:13.0121875Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:13.0131875Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:13.0144920Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:13.0151369Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:13.0162221Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-01-29T18:54:13.0173416Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.0183969Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.0194054Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.0204621Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.0214703Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.0224571Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.0233646Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.0244436Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.0254171Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.0263219Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.0272918Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.0282878Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.0292424Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.0301569Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.0310444Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.0320522Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.0459312Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.0480073Z INFO 01-29 18:54:13 [parallel_state.py:1425] rank 0 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
2026-01-29T18:54:13.1161043Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.1180606Z INFO 01-29 18:54:13 [parallel_state.py:1425] rank 1 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
2026-01-29T18:54:13.1240978Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.1262736Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.1271663Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.1281868Z INFO 01-29 18:54:13 [parallel_state.py:1425] rank 2 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
2026-01-29T18:54:13.1292360Z INFO 01-29 18:54:13 [parallel_state.py:1425] rank 5 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 5, EP rank 5
2026-01-29T18:54:13.1302913Z INFO 01-29 18:54:13 [parallel_state.py:1425] rank 6 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 6, EP rank 6
2026-01-29T18:54:13.1310358Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.1319221Z INFO 01-29 18:54:13 [parallel_state.py:1425] rank 7 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 7, EP rank 7
2026-01-29T18:54:13.1328884Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.1338324Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.1348170Z INFO 01-29 18:54:13 [parallel_state.py:1425] rank 11 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 3, EP rank 11
2026-01-29T18:54:13.1357732Z INFO 01-29 18:54:13 [parallel_state.py:1425] rank 14 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 6, EP rank 14
2026-01-29T18:54:13.1773818Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.1793004Z INFO 01-29 18:54:13 [parallel_state.py:1425] rank 12 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 4, EP rank 12
2026-01-29T18:54:13.1812929Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.1833816Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.1843627Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.1853550Z INFO 01-29 18:54:13 [parallel_state.py:1425] rank 4 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 4, EP rank 4
2026-01-29T18:54:13.1862818Z INFO 01-29 18:54:13 [parallel_state.py:1425] rank 9 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 1, EP rank 9
2026-01-29T18:54:13.1872384Z INFO 01-29 18:54:13 [parallel_state.py:1425] rank 10 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 2, EP rank 10
2026-01-29T18:54:13.1889502Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.1911384Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.1921688Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.1931278Z INFO 01-29 18:54:13 [parallel_state.py:1425] rank 3 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
2026-01-29T18:54:13.1940618Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.1950410Z INFO 01-29 18:54:13 [parallel_state.py:1425] rank 8 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 0, EP rank 8
2026-01-29T18:54:13.1960460Z INFO 01-29 18:54:13 [parallel_state.py:1425] rank 13 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 5, EP rank 13
2026-01-29T18:54:13.1970009Z INFO 01-29 18:54:13 [parallel_state.py:1425] rank 15 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 7, EP rank 15
2026-01-29T18:54:13.2209619Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.2232448Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.2243084Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.2250762Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.2260386Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.2269882Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.2279629Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.2288941Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.2298352Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.2309918Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.2316975Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.2330636Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.2336123Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.2344899Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.2354466Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.2365513Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-01-29T18:54:13.2373801Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.2383240Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.2392892Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.2402926Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.2412161Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.2421270Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.2430363Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.2440216Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.2449869Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.2459353Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.2468288Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.2477435Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.2487740Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.2496889Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.2507755Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.2516744Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-01-29T18:54:13.2527253Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.2536744Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.2545237Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.2555072Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.2564958Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.2574686Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.2584297Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.2594151Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.2604222Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.2614138Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.2624120Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.2633470Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.2643218Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.2653428Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.2662911Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.2672568Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.3588251Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.3609049Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.3620105Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.3629555Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.3640306Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.3650322Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.3660727Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.3671419Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.3681001Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.3691807Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.3701930Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.3712924Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.3723930Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.3733668Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.3743552Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.3753670Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.4085630Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.4137902Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.4140532Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.4141742Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.4142970Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.4152901Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.4163231Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.4173332Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.4185651Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.4193237Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.4204653Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.4214907Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.4224961Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.4235434Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.4247061Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.4257813Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.4269091Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.4277567Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.4288706Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.4298773Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.4309239Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.4323586Z [0;36m(Worker_DP1_TP0_EP8 pid=133)[0;0m INFO 01-29 18:54:13 [model_runner_v1.py:2365] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-01-29T18:54:13.4333948Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.4343511Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m INFO 01-29 18:54:13 [model_runner_v1.py:2365] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-01-29T18:54:13.4354369Z [0;36m(Worker_DP1_TP7_EP15 pid=764)[0;0m INFO 01-29 18:54:13 [model_runner_v1.py:2365] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-01-29T18:54:13.4365231Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m INFO 01-29 18:54:13 [model_runner_v1.py:2365] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-01-29T18:54:13.4374843Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.4385512Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.4395118Z [0;36m(Worker_DP0_TP6_EP6 pid=661)[0;0m INFO 01-29 18:54:13 [model_runner_v1.py:2365] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-01-29T18:54:13.4406048Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.4416543Z WARNING 01-29 18:54:13 [eagle.py:105] Currently the eagle proposer only supports cudagraph_mode PIECEWISE, if you want the drafter to use cuda graphs, please set compilation_config.cudagraph_mode to PIECEWISE or FULL_AND_PIECEWISE
2026-01-29T18:54:13.4426768Z [0;36m(Worker_DP0_TP7_EP7 pid=765)[0;0m INFO 01-29 18:54:13 [model_runner_v1.py:2365] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-01-29T18:54:13.4436800Z [0;36m(Worker_DP0_TP5_EP5 pid=556)[0;0m INFO 01-29 18:54:13 [model_runner_v1.py:2365] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-01-29T18:54:13.4446440Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.4456378Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.4466504Z [0;36m(Worker_DP0_TP4_EP4 pid=453)[0;0m INFO 01-29 18:54:13 [model_runner_v1.py:2365] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-01-29T18:54:13.4476001Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.4486949Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.4496828Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.4507257Z WARNING 01-29 18:54:13 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-01-29T18:54:13.4519013Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m INFO 01-29 18:54:13 [model_runner_v1.py:2365] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-01-29T18:54:13.4528684Z [0;36m(Worker_DP0_TP1_EP1 pid=151)[0;0m INFO 01-29 18:54:13 [model_runner_v1.py:2365] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-01-29T18:54:13.4538018Z [0;36m(Worker_DP1_TP5_EP13 pid=557)[0;0m INFO 01-29 18:54:13 [model_runner_v1.py:2365] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-01-29T18:54:13.4547845Z [0;36m(Worker_DP1_TP2_EP10 pid=244)[0;0m INFO 01-29 18:54:13 [model_runner_v1.py:2365] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-01-29T18:54:13.4559144Z [0;36m(Worker_DP0_TP3_EP3 pid=349)[0;0m INFO 01-29 18:54:13 [model_runner_v1.py:2365] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-01-29T18:54:13.4569565Z [0;36m(Worker_DP1_TP1_EP9 pid=150)[0;0m INFO 01-29 18:54:13 [model_runner_v1.py:2365] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-01-29T18:54:13.4579220Z [0;36m(Worker_DP1_TP3_EP11 pid=348)[0;0m INFO 01-29 18:54:13 [model_runner_v1.py:2365] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-01-29T18:54:13.5020077Z [0;36m(Worker_DP0_TP0_EP0 pid=132)[0;0m INFO 01-29 18:54:13 [model_runner_v1.py:2365] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-01-29T18:54:13.8601768Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m [2026-01-29 18:54:13] INFO modelslim_config.py:291: Using the vLLM Ascend modelslim Quantization now!
2026-01-29T18:54:13.8626986Z [0;36m(Worker_DP1_TP0_EP8 pid=133)[0;0m [2026-01-29 18:54:13] INFO modelslim_config.py:291: Using the vLLM Ascend modelslim Quantization now!
2026-01-29T18:54:13.8700327Z [0;36m(Worker_DP0_TP4_EP4 pid=453)[0;0m [2026-01-29 18:54:13] INFO modelslim_config.py:291: Using the vLLM Ascend modelslim Quantization now!
2026-01-29T18:54:13.8730895Z [0;36m(Worker_DP1_TP7_EP15 pid=764)[0;0m [2026-01-29 18:54:13] INFO modelslim_config.py:291: Using the vLLM Ascend modelslim Quantization now!
2026-01-29T18:54:13.8933449Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m [2026-01-29 18:54:13] INFO modelslim_config.py:291: Using the vLLM Ascend modelslim Quantization now!
2026-01-29T18:54:13.9069426Z [0;36m(Worker_DP0_TP6_EP6 pid=661)[0;0m [2026-01-29 18:54:13] INFO modelslim_config.py:291: Using the vLLM Ascend modelslim Quantization now!
2026-01-29T18:54:13.9224942Z [0;36m(Worker_DP0_TP5_EP5 pid=556)[0;0m [2026-01-29 18:54:13] INFO modelslim_config.py:291: Using the vLLM Ascend modelslim Quantization now!
2026-01-29T18:54:13.9477824Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m [2026-01-29 18:54:13] INFO modelslim_config.py:291: Using the vLLM Ascend modelslim Quantization now!
2026-01-29T18:54:13.9553709Z [0;36m(Worker_DP0_TP7_EP7 pid=765)[0;0m [2026-01-29 18:54:13] INFO modelslim_config.py:291: Using the vLLM Ascend modelslim Quantization now!
2026-01-29T18:54:13.9988695Z [0;36m(Worker_DP0_TP0_EP0 pid=132)[0;0m [2026-01-29 18:54:13] INFO modelslim_config.py:291: Using the vLLM Ascend modelslim Quantization now!
2026-01-29T18:54:14.0518390Z [0;36m(Worker_DP1_TP7_EP15 pid=764)[0;0m INFO 01-29 18:54:14 [layer.py:503] [EP Rank 15/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-01-29T18:54:14.0523149Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m INFO 01-29 18:54:14 [layer.py:503] [EP Rank 14/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-01-29T18:54:14.0532989Z [0;36m(Worker_DP0_TP4_EP4 pid=453)[0;0m INFO 01-29 18:54:14 [layer.py:503] [EP Rank 4/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-01-29T18:54:14.0546108Z [0;36m(Worker_DP0_TP6_EP6 pid=661)[0;0m INFO 01-29 18:54:14 [layer.py:503] [EP Rank 6/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-01-29T18:54:14.0554556Z [0;36m(Worker_DP1_TP0_EP8 pid=133)[0;0m INFO 01-29 18:54:14 [layer.py:503] [EP Rank 8/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-01-29T18:54:14.0561832Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m INFO 01-29 18:54:14 [layer.py:503] [EP Rank 2/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-01-29T18:54:14.0571787Z [0;36m(Worker_DP0_TP5_EP5 pid=556)[0;0m INFO 01-29 18:54:14 [layer.py:503] [EP Rank 5/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-01-29T18:54:14.0583025Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m INFO 01-29 18:54:14 [layer.py:503] [EP Rank 12/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-01-29T18:54:14.0650996Z [0;36m(Worker_DP0_TP7_EP7 pid=765)[0;0m INFO 01-29 18:54:14 [layer.py:503] [EP Rank 7/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-01-29T18:54:14.0938893Z [0;36m(Worker_DP1_TP3_EP11 pid=348)[0;0m [2026-01-29 18:54:14] INFO modelslim_config.py:291: Using the vLLM Ascend modelslim Quantization now!
2026-01-29T18:54:14.1085206Z [0;36m(Worker_DP0_TP0_EP0 pid=132)[0;0m INFO 01-29 18:54:14 [layer.py:503] [EP Rank 0/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-01-29T18:54:14.1551921Z [0;36m(Worker_DP1_TP1_EP9 pid=150)[0;0m [2026-01-29 18:54:14] INFO modelslim_config.py:291: Using the vLLM Ascend modelslim Quantization now!
2026-01-29T18:54:14.1626650Z [0;36m(Worker_DP1_TP5_EP13 pid=557)[0;0m [2026-01-29 18:54:14] INFO modelslim_config.py:291: Using the vLLM Ascend modelslim Quantization now!
2026-01-29T18:54:14.1987530Z [0;36m(Worker_DP1_TP3_EP11 pid=348)[0;0m INFO 01-29 18:54:14 [layer.py:503] [EP Rank 11/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-01-29T18:54:14.2469332Z [0;36m(Worker_DP1_TP2_EP10 pid=244)[0;0m [2026-01-29 18:54:14] INFO modelslim_config.py:291: Using the vLLM Ascend modelslim Quantization now!
2026-01-29T18:54:14.2744966Z [0;36m(Worker_DP1_TP1_EP9 pid=150)[0;0m INFO 01-29 18:54:14 [layer.py:503] [EP Rank 9/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-01-29T18:54:14.2916895Z [0;36m(Worker_DP1_TP5_EP13 pid=557)[0;0m INFO 01-29 18:54:14 [layer.py:503] [EP Rank 13/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-01-29T18:54:14.3650225Z [0;36m(Worker_DP1_TP2_EP10 pid=244)[0;0m INFO 01-29 18:54:14 [layer.py:503] [EP Rank 10/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-01-29T18:54:14.4218946Z [0;36m(Worker_DP0_TP1_EP1 pid=151)[0;0m [2026-01-29 18:54:14] INFO modelslim_config.py:291: Using the vLLM Ascend modelslim Quantization now!
2026-01-29T18:54:14.4771548Z [0;36m(Worker_DP0_TP3_EP3 pid=349)[0;0m [2026-01-29 18:54:14] INFO modelslim_config.py:291: Using the vLLM Ascend modelslim Quantization now!
2026-01-29T18:54:14.5301792Z [0;36m(Worker_DP0_TP1_EP1 pid=151)[0;0m INFO 01-29 18:54:14 [layer.py:503] [EP Rank 1/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-01-29T18:54:14.5904369Z [0;36m(Worker_DP0_TP3_EP3 pid=349)[0;0m INFO 01-29 18:54:14 [layer.py:503] [EP Rank 3/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-01-29T18:54:15.2759999Z [0;36m(Worker_DP0_TP6_EP6 pid=661)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-01-29T18:54:15.2767380Z [0;36m(Worker_DP0_TP6_EP6 pid=661)[0;0m   return func(*args, **kwargs)
2026-01-29T18:54:15.2785510Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-01-29T18:54:15.2794897Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m   return func(*args, **kwargs)
2026-01-29T18:54:15.2806035Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-01-29T18:54:15.2813902Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m   return func(*args, **kwargs)
2026-01-29T18:54:15.2867403Z [0;36m(Worker_DP0_TP5_EP5 pid=556)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-01-29T18:54:15.2876324Z [0;36m(Worker_DP0_TP5_EP5 pid=556)[0;0m   return func(*args, **kwargs)
2026-01-29T18:54:15.2920778Z [0;36m(Worker_DP1_TP0_EP8 pid=133)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-01-29T18:54:15.2929454Z [0;36m(Worker_DP1_TP0_EP8 pid=133)[0;0m   return func(*args, **kwargs)
2026-01-29T18:54:15.2956665Z [0;36m(Worker_DP1_TP7_EP15 pid=764)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-01-29T18:54:15.2964831Z [0;36m(Worker_DP1_TP7_EP15 pid=764)[0;0m   return func(*args, **kwargs)
2026-01-29T18:54:15.3259314Z [0;36m(Worker_DP0_TP7_EP7 pid=765)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-01-29T18:54:15.3265817Z [0;36m(Worker_DP0_TP7_EP7 pid=765)[0;0m   return func(*args, **kwargs)
2026-01-29T18:54:15.3281646Z [0;36m(Worker_DP0_TP0_EP0 pid=132)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-01-29T18:54:15.3291305Z [0;36m(Worker_DP0_TP0_EP0 pid=132)[0;0m   return func(*args, **kwargs)
2026-01-29T18:54:15.3338261Z [0;36m(Worker_DP1_TP2_EP10 pid=244)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-01-29T18:54:15.3341244Z [0;36m(Worker_DP1_TP2_EP10 pid=244)[0;0m   return func(*args, **kwargs)
2026-01-29T18:54:15.3358062Z [0;36m(Worker_DP0_TP3_EP3 pid=349)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-01-29T18:54:15.3381466Z [0;36m(Worker_DP0_TP3_EP3 pid=349)[0;0m   return func(*args, **kwargs)
2026-01-29T18:54:15.3474207Z [0;36m(Worker_DP0_TP4_EP4 pid=453)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-01-29T18:54:15.3482749Z [0;36m(Worker_DP0_TP4_EP4 pid=453)[0;0m   return func(*args, **kwargs)
2026-01-29T18:54:15.3532373Z [0;36m(Worker_DP1_TP0_EP8 pid=133)[0;0m INFO 01-29 18:54:15 [fused_moe.py:211] [EP Rank 8/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-01-29T18:54:15.3561219Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m INFO 01-29 18:54:15 [fused_moe.py:211] [EP Rank 12/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-01-29T18:54:15.3570842Z [0;36m(Worker_DP0_TP6_EP6 pid=661)[0;0m INFO 01-29 18:54:15 [fused_moe.py:211] [EP Rank 6/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-01-29T18:54:15.3580890Z [0;36m(Worker_DP0_TP5_EP5 pid=556)[0;0m INFO 01-29 18:54:15 [fused_moe.py:211] [EP Rank 5/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-01-29T18:54:15.3590988Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m INFO 01-29 18:54:15 [fused_moe.py:211] [EP Rank 2/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-01-29T18:54:15.3675966Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-01-29T18:54:15.3684714Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m   return func(*args, **kwargs)
2026-01-29T18:54:15.3697816Z [0;36m(Worker_DP1_TP7_EP15 pid=764)[0;0m INFO 01-29 18:54:15 [fused_moe.py:211] [EP Rank 15/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-01-29T18:54:15.3847712Z [0;36m(Worker_DP0_TP7_EP7 pid=765)[0;0m INFO 01-29 18:54:15 [fused_moe.py:211] [EP Rank 7/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-01-29T18:54:15.3868942Z [0;36m(Worker_DP0_TP0_EP0 pid=132)[0;0m INFO 01-29 18:54:15 [fused_moe.py:211] [EP Rank 0/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-01-29T18:54:15.3893736Z [0;36m(Worker_DP1_TP3_EP11 pid=348)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-01-29T18:54:15.3902382Z [0;36m(Worker_DP1_TP3_EP11 pid=348)[0;0m   return func(*args, **kwargs)
2026-01-29T18:54:15.3967019Z [0;36m(Worker_DP1_TP2_EP10 pid=244)[0;0m INFO 01-29 18:54:15 [fused_moe.py:211] [EP Rank 10/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-01-29T18:54:15.4012820Z [0;36m(Worker_DP0_TP3_EP3 pid=349)[0;0m INFO 01-29 18:54:15 [fused_moe.py:211] [EP Rank 3/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-01-29T18:54:15.4034687Z [0;36m(Worker_DP0_TP4_EP4 pid=453)[0;0m INFO 01-29 18:54:15 [fused_moe.py:211] [EP Rank 4/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-01-29T18:54:15.4246056Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m INFO 01-29 18:54:15 [fused_moe.py:211] [EP Rank 14/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-01-29T18:54:15.4482308Z [0;36m(Worker_DP1_TP3_EP11 pid=348)[0;0m INFO 01-29 18:54:15 [fused_moe.py:211] [EP Rank 11/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-01-29T18:54:15.4529878Z [0;36m(Worker_DP1_TP5_EP13 pid=557)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-01-29T18:54:15.4537991Z [0;36m(Worker_DP1_TP5_EP13 pid=557)[0;0m   return func(*args, **kwargs)
2026-01-29T18:54:15.4588964Z [0;36m(Worker_DP1_TP1_EP9 pid=150)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-01-29T18:54:15.4596783Z [0;36m(Worker_DP1_TP1_EP9 pid=150)[0;0m   return func(*args, **kwargs)
2026-01-29T18:54:15.4738217Z [0;36m(Worker_DP0_TP1_EP1 pid=151)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-01-29T18:54:15.4746397Z [0;36m(Worker_DP0_TP1_EP1 pid=151)[0;0m   return func(*args, **kwargs)
2026-01-29T18:54:15.5126920Z [0;36m(Worker_DP1_TP5_EP13 pid=557)[0;0m INFO 01-29 18:54:15 [fused_moe.py:211] [EP Rank 13/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-01-29T18:54:15.5211475Z [0;36m(Worker_DP1_TP1_EP9 pid=150)[0;0m INFO 01-29 18:54:15 [fused_moe.py:211] [EP Rank 9/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-01-29T18:54:15.5384558Z [0;36m(Worker_DP0_TP1_EP1 pid=151)[0;0m INFO 01-29 18:54:15 [fused_moe.py:211] [EP Rank 1/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-01-29T18:56:16.3086917Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749] WorkerProc failed to start.
2026-01-29T18:56:16.3244042Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749] Traceback (most recent call last):
2026-01-29T18:56:16.3257031Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 720, in worker_main
2026-01-29T18:56:16.3265691Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     worker = WorkerProc(*args, **kwargs)
2026-01-29T18:56:16.3276427Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.3288403Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 560, in __init__
2026-01-29T18:56:16.3297572Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.worker.load_model()
2026-01-29T18:56:16.3307078Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-01-29T18:56:16.3316758Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.model_runner.load_model()
2026-01-29T18:56:16.3326172Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2368, in load_model
2026-01-29T18:56:16.3335783Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.model = get_model(vllm_config=self.vllm_config)
2026-01-29T18:56:16.3345127Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.3355763Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 132, in get_model
2026-01-29T18:56:16.3365637Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     return loader.load_model(vllm_config=vllm_config, model_config=model_config)
2026-01-29T18:56:16.3375316Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.3386252Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-01-29T18:56:16.3395516Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     model = initialize_model(
2026-01-29T18:56:16.3405620Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]             ^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.3415362Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-01-29T18:56:16.3424289Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-01-29T18:56:16.3434640Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.3445120Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1422, in __init__
2026-01-29T18:56:16.3454653Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.model = self.model_cls(
2026-01-29T18:56:16.3464990Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                  ^^^^^^^^^^^^^^^
2026-01-29T18:56:16.3474832Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 305, in __init__
2026-01-29T18:56:16.3485717Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     old_init(self, **kwargs)
2026-01-29T18:56:16.3494950Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1279, in __init__
2026-01-29T18:56:16.3505425Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-01-29T18:56:16.3514973Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                                                     ^^^^^^^^^^^^
2026-01-29T18:56:16.3528567Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 605, in make_layers
2026-01-29T18:56:16.3548037Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     + [
2026-01-29T18:56:16.3548817Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]       ^
2026-01-29T18:56:16.3555140Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 606, in <listcomp>
2026-01-29T18:56:16.3565571Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-01-29T18:56:16.3575278Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.3589348Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1281, in <lambda>
2026-01-29T18:56:16.3598492Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     lambda prefix: DeepseekV2DecoderLayer(
2026-01-29T18:56:16.3605396Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.3615489Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1175, in __init__
2026-01-29T18:56:16.3624671Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.mlp = DeepseekV2MoE(
2026-01-29T18:56:16.3634451Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                ^^^^^^^^^^^^^^
2026-01-29T18:56:16.3645221Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 308, in __init__
2026-01-29T18:56:16.3654933Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.experts = SharedFusedMoE(
2026-01-29T18:56:16.3664726Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                    ^^^^^^^^^^^^^^^
2026-01-29T18:56:16.3675339Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-01-29T18:56:16.3685576Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     AscendFusedMoE.__init__(self, **kwargs)
2026-01-29T18:56:16.3696271Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-01-29T18:56:16.3706436Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     setup_moe_comm_method(self.moe_config)
2026-01-29T18:56:16.3715887Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-01-29T18:56:16.3726889Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-01-29T18:56:16.3736729Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.3747034Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-01-29T18:56:16.3758441Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749] WorkerProc failed to start.
2026-01-29T18:56:16.3771978Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749] Traceback (most recent call last):
2026-01-29T18:56:16.3790136Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 720, in worker_main
2026-01-29T18:56:16.3802180Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     worker = WorkerProc(*args, **kwargs)
2026-01-29T18:56:16.3815196Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.3824726Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 560, in __init__
2026-01-29T18:56:16.3834947Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.worker.load_model()
2026-01-29T18:56:16.3846050Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-01-29T18:56:16.3855617Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.model_runner.load_model()
2026-01-29T18:56:16.3865798Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2368, in load_model
2026-01-29T18:56:16.3876631Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.model = get_model(vllm_config=self.vllm_config)
2026-01-29T18:56:16.3886642Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.3897425Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 132, in get_model
2026-01-29T18:56:16.3907772Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     return loader.load_model(vllm_config=vllm_config, model_config=model_config)
2026-01-29T18:56:16.3918317Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.3929060Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-01-29T18:56:16.3938552Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     model = initialize_model(
2026-01-29T18:56:16.3948416Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]             ^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.3958441Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-01-29T18:56:16.3968210Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-01-29T18:56:16.3977408Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.3987481Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1422, in __init__
2026-01-29T18:56:16.3997393Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.model = self.model_cls(
2026-01-29T18:56:16.4007617Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                  ^^^^^^^^^^^^^^^
2026-01-29T18:56:16.4017677Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 305, in __init__
2026-01-29T18:56:16.4026530Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     old_init(self, **kwargs)
2026-01-29T18:56:16.4037292Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1279, in __init__
2026-01-29T18:56:16.4046904Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-01-29T18:56:16.4056652Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                                                     ^^^^^^^^^^^^
2026-01-29T18:56:16.4066481Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 605, in make_layers
2026-01-29T18:56:16.4075714Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     + [
2026-01-29T18:56:16.4085869Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]       ^
2026-01-29T18:56:16.4095653Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 606, in <listcomp>
2026-01-29T18:56:16.4105104Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-01-29T18:56:16.4114960Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.4137204Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1281, in <lambda>
2026-01-29T18:56:16.4139029Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     lambda prefix: DeepseekV2DecoderLayer(
2026-01-29T18:56:16.4144106Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.4153529Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1175, in __init__
2026-01-29T18:56:16.4163140Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.mlp = DeepseekV2MoE(
2026-01-29T18:56:16.4172991Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                ^^^^^^^^^^^^^^
2026-01-29T18:56:16.4182896Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 308, in __init__
2026-01-29T18:56:16.4192305Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.experts = SharedFusedMoE(
2026-01-29T18:56:16.4202296Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                    ^^^^^^^^^^^^^^^
2026-01-29T18:56:16.4212281Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-01-29T18:56:16.4221276Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     AscendFusedMoE.__init__(self, **kwargs)
2026-01-29T18:56:16.4231268Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-01-29T18:56:16.4241018Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     setup_moe_comm_method(self.moe_config)
2026-01-29T18:56:16.4251769Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-01-29T18:56:16.4261272Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-01-29T18:56:16.4270610Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.4288510Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-01-29T18:56:16.4298458Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.token_dispatcher = self._get_token_dispatcher()
2026-01-29T18:56:16.4299337Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.4309399Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-01-29T18:56:16.4319016Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     return TokenDispatcherWithAll2AllV(
2026-01-29T18:56:16.4339436Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.4340309Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-01-29T18:56:16.4348212Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-01-29T18:56:16.4358370Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.4368674Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-01-29T18:56:16.4378114Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749] [ERROR] 2026-01-29-18:56:15 (PID:452, Device:4, RankID:-1) ERR02200 DIST call hccl api failed.
2026-01-29T18:56:16.4388746Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749] [PID: 452] 2026-01-29-18:56:15.391.994 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-01-29T18:56:16.4399626Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-01-29T18:56:16.4408147Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]         TraceBack (most recent call last):
2026-01-29T18:56:16.4418605Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-01-29T18:56:16.4427120Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749] 
2026-01-29T18:56:16.4437869Z ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.token_dispatcher = self._get_token_dispatcher()
2026-01-29T18:56:16.4447649Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.4457550Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-01-29T18:56:16.4466586Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     return TokenDispatcherWithAll2AllV(
2026-01-29T18:56:16.4476741Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.4486865Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-01-29T18:56:16.4496840Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-01-29T18:56:16.4505505Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.4515895Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-01-29T18:56:16.4526110Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749] [ERROR] 2026-01-29-18:56:15 (PID:660, Device:6, RankID:-1) ERR02200 DIST call hccl api failed.
2026-01-29T18:56:16.4536777Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749] [PID: 660] 2026-01-29-18:56:15.391.776 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-01-29T18:56:16.4548057Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-01-29T18:56:16.4556618Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]         TraceBack (most recent call last):
2026-01-29T18:56:16.4566905Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-01-29T18:56:16.4575531Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749] 
2026-01-29T18:56:16.4585182Z [0;36m(Worker_DP1_TP6_EP14 pid=660)[0;0m INFO 01-29 18:56:16 [multiproc_executor.py:707] Parent process exited, terminating worker
2026-01-29T18:56:16.4596549Z [0;36m(Worker_DP1_TP4_EP12 pid=452)[0;0m INFO 01-29 18:56:16 [multiproc_executor.py:707] Parent process exited, terminating worker
2026-01-29T18:56:16.4605050Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749] WorkerProc failed to start.
2026-01-29T18:56:16.4613923Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749] Traceback (most recent call last):
2026-01-29T18:56:16.4623797Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 720, in worker_main
2026-01-29T18:56:16.4634589Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     worker = WorkerProc(*args, **kwargs)
2026-01-29T18:56:16.4643891Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.4653257Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 560, in __init__
2026-01-29T18:56:16.4662544Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.worker.load_model()
2026-01-29T18:56:16.4671841Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/worker.py", line 416, in load_model
2026-01-29T18:56:16.4681579Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.model_runner.load_model()
2026-01-29T18:56:16.4691220Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/worker/model_runner_v1.py", line 2368, in load_model
2026-01-29T18:56:16.4700944Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.model = get_model(vllm_config=self.vllm_config)
2026-01-29T18:56:16.4711845Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.4720217Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/__init__.py", line 132, in get_model
2026-01-29T18:56:16.4729952Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     return loader.load_model(vllm_config=vllm_config, model_config=model_config)
2026-01-29T18:56:16.4739786Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.4750093Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
2026-01-29T18:56:16.4760005Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     model = initialize_model(
2026-01-29T18:56:16.4769730Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]             ^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.4780837Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
2026-01-29T18:56:16.4790408Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     return model_class(vllm_config=vllm_config, prefix=prefix)
2026-01-29T18:56:16.4800339Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.4810968Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1422, in __init__
2026-01-29T18:56:16.4819827Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.model = self.model_cls(
2026-01-29T18:56:16.4829258Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                  ^^^^^^^^^^^^^^^
2026-01-29T18:56:16.4855608Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/compilation/decorators.py", line 305, in __init__
2026-01-29T18:56:16.4856366Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     old_init(self, **kwargs)
2026-01-29T18:56:16.4862698Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1279, in __init__
2026-01-29T18:56:16.4868359Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.start_layer, self.end_layer, self.layers = make_layers(
2026-01-29T18:56:16.4878899Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                                                     ^^^^^^^^^^^^
2026-01-29T18:56:16.4887607Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 605, in make_layers
2026-01-29T18:56:16.4897084Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     + [
2026-01-29T18:56:16.4906966Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]       ^
2026-01-29T18:56:16.4917342Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/utils.py", line 606, in <listcomp>
2026-01-29T18:56:16.4927448Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
2026-01-29T18:56:16.4935961Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.4945505Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1281, in <lambda>
2026-01-29T18:56:16.4954903Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     lambda prefix: DeepseekV2DecoderLayer(
2026-01-29T18:56:16.4965126Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                    ^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.4974469Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 1175, in __init__
2026-01-29T18:56:16.4983743Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.mlp = DeepseekV2MoE(
2026-01-29T18:56:16.4993611Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                ^^^^^^^^^^^^^^
2026-01-29T18:56:16.5004145Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm/vllm/model_executor/models/deepseek_v2.py", line 308, in __init__
2026-01-29T18:56:16.5013122Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.experts = SharedFusedMoE(
2026-01-29T18:56:16.5021927Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                    ^^^^^^^^^^^^^^^
2026-01-29T18:56:16.5031766Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 412, in __init__
2026-01-29T18:56:16.5041666Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     AscendFusedMoE.__init__(self, **kwargs)
2026-01-29T18:56:16.5052193Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/fused_moe.py", line 241, in __init__
2026-01-29T18:56:16.5061114Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     setup_moe_comm_method(self.moe_config)
2026-01-29T18:56:16.5071101Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 45, in setup_moe_comm_method
2026-01-29T18:56:16.5081458Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     _MoECommMethods[MoECommType.ALLTOALL] = AlltoAllCommImpl(moe_config)
2026-01-29T18:56:16.5091894Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.5101894Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 76, in __init__
2026-01-29T18:56:16.5111704Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.token_dispatcher = self._get_token_dispatcher()
2026-01-29T18:56:16.5121295Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.5130882Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/moe_comm_method.py", line 253, in _get_token_dispatcher
2026-01-29T18:56:16.5140891Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     return TokenDispatcherWithAll2AllV(
2026-01-29T18:56:16.5169697Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.5174007Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]   File "/vllm-workspace/vllm-ascend/vllm_ascend/ops/fused_moe/token_dispatcher.py", line 417, in __init__
2026-01-29T18:56:16.5186660Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]     self.moe_all_to_all_group_name = backend.get_hccl_comm_name(local_rank)
2026-01-29T18:56:16.5206051Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:16.5220253Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749] RuntimeError: create_config:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:130 HCCL function error: hcclCommInitRootInfoConfig(numRanks, &rootInfo, rank, config, &(comm->hcclComm_)), error code is 1
2026-01-29T18:56:16.5221754Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749] [ERROR] 2026-01-29-18:56:15 (PID:245, Device:2, RankID:-1) ERR02200 DIST call hccl api failed.
2026-01-29T18:56:16.5233099Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749] [PID: 245] 2026-01-29-18:56:15.392.211 Ranktable_Detect_Failed(EI0015): Failed to collect cluster information of the communicator based on rootInfo detection. Reason: Within the timeout period, all ranks in the communication domain failed to connect to the server..
2026-01-29T18:56:16.5245148Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]         Solution: 1. Check whether all ranks in the communicator have delivered the communicator creation interface. 2. Check the connectivity between the host networks of all nodes and the server node. 3. Check whether the HCCL_SOCKET_IFNAME environment variable of all nodes is correctly configured. 4. Increase the timeout by configuring the HCCL_CONNECT_TIMEOUT environment variable.
2026-01-29T18:56:16.5253307Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]         TraceBack (most recent call last):
2026-01-29T18:56:16.5265742Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749]         Failed to collect cluster information of the communicator based on rootInfo detection. Reason: rank num[32]is different with rank list size[16] in total topo rank info..
2026-01-29T18:56:16.5272812Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m ERROR 01-29 18:56:16 [multiproc_executor.py:749] 
2026-01-29T18:56:16.5282679Z [0;36m(Worker_DP0_TP2_EP2 pid=245)[0;0m INFO 01-29 18:56:16 [multiproc_executor.py:707] Parent process exited, terminating worker
2026-01-29T18:56:16.5291481Z [0;36m(Worker_DP0_TP6_EP6 pid=661)[0;0m INFO 01-29 18:56:16 [multiproc_executor.py:707] Parent process exited, terminating worker
2026-01-29T18:56:16.5302544Z [0;36m(Worker_DP0_TP4_EP4 pid=453)[0;0m INFO 01-29 18:56:16 [multiproc_executor.py:707] Parent process exited, terminating worker
2026-01-29T18:56:16.5311827Z [0;36m(Worker_DP1_TP0_EP8 pid=133)[0;0m INFO 01-29 18:56:16 [multiproc_executor.py:707] Parent process exited, terminating worker
2026-01-29T18:56:16.5320386Z [0;36m(Worker_DP1_TP7_EP15 pid=764)[0;0m INFO 01-29 18:56:16 [multiproc_executor.py:707] Parent process exited, terminating worker
2026-01-29T18:56:16.5330430Z [0;36m(Worker_DP1_TP3_EP11 pid=348)[0;0m INFO 01-29 18:56:16 [multiproc_executor.py:707] Parent process exited, terminating worker
2026-01-29T18:56:16.5340117Z [0;36m(Worker_DP1_TP5_EP13 pid=557)[0;0m INFO 01-29 18:56:16 [multiproc_executor.py:707] Parent process exited, terminating worker
2026-01-29T18:56:16.5349529Z [0;36m(Worker_DP0_TP7_EP7 pid=765)[0;0m INFO 01-29 18:56:16 [multiproc_executor.py:707] Parent process exited, terminating worker
2026-01-29T18:56:16.5359504Z [0;36m(Worker_DP0_TP5_EP5 pid=556)[0;0m INFO 01-29 18:56:16 [multiproc_executor.py:707] Parent process exited, terminating worker
2026-01-29T18:56:16.5368832Z [0;36m(Worker_DP0_TP3_EP3 pid=349)[0;0m INFO 01-29 18:56:16 [multiproc_executor.py:707] Parent process exited, terminating worker
2026-01-29T18:56:16.5378085Z [0;36m(Worker_DP1_TP1_EP9 pid=150)[0;0m INFO 01-29 18:56:16 [multiproc_executor.py:707] Parent process exited, terminating worker
2026-01-29T18:56:16.5415584Z [0;36m(Worker_DP1_TP2_EP10 pid=244)[0;0m INFO 01-29 18:56:16 [multiproc_executor.py:707] Parent process exited, terminating worker
2026-01-29T18:56:16.5906692Z [0;36m(Worker_DP0_TP0_EP0 pid=132)[0;0m INFO 01-29 18:56:16 [multiproc_executor.py:707] Parent process exited, terminating worker
2026-01-29T18:56:16.7668548Z [0;36m(Worker_DP0_TP1_EP1 pid=151)[0;0m INFO 01-29 18:56:16 [multiproc_executor.py:707] Parent process exited, terminating worker
2026-01-29T18:56:20.3345023Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936] EngineCore failed to start.
2026-01-29T18:56:20.3356778Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936] Traceback (most recent call last):
2026-01-29T18:56:20.3369181Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 919, in run_engine_core
2026-01-29T18:56:20.3379304Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936]     engine_core = DPEngineCoreProc(*args, **kwargs)
2026-01-29T18:56:20.3391408Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:20.3405920Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 1269, in __init__
2026-01-29T18:56:20.3416803Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936]     super().__init__(
2026-01-29T18:56:20.3425556Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 692, in __init__
2026-01-29T18:56:20.3436013Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936]     super().__init__(
2026-01-29T18:56:20.3445616Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 106, in __init__
2026-01-29T18:56:20.3455903Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936]     self.model_executor = executor_class(vllm_config)
2026-01-29T18:56:20.3465769Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:20.3476708Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
2026-01-29T18:56:20.3486670Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936]     super().__init__(vllm_config)
2026-01-29T18:56:20.3496535Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936]   File "/vllm-workspace/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
2026-01-29T18:56:20.3506374Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936]     self._init_executor()
2026-01-29T18:56:20.3517041Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 172, in _init_executor
2026-01-29T18:56:20.3527212Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936]     self.workers = WorkerProc.wait_for_ready(unready_workers)
2026-01-29T18:56:20.3536868Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:20.3547007Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 658, in wait_for_ready
2026-01-29T18:56:20.3557265Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936]     raise e from None
2026-01-29T18:56:20.3568444Z [0;36m(EngineCore_DP1 pid=108)[0;0m ERROR 01-29 18:56:20 [core.py:936] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
2026-01-29T18:56:20.3584160Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936] EngineCore failed to start.
2026-01-29T18:56:20.3589169Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936] Traceback (most recent call last):
2026-01-29T18:56:20.3599290Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 919, in run_engine_core
2026-01-29T18:56:20.3609262Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936]     engine_core = DPEngineCoreProc(*args, **kwargs)
2026-01-29T18:56:20.3619099Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:20.3628465Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 1269, in __init__
2026-01-29T18:56:20.3638534Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936]     super().__init__(
2026-01-29T18:56:20.3648316Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 692, in __init__
2026-01-29T18:56:20.3658042Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936]     super().__init__(
2026-01-29T18:56:20.3668261Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936]   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 106, in __init__
2026-01-29T18:56:20.3678789Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936]     self.model_executor = executor_class(vllm_config)
2026-01-29T18:56:20.3688767Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:20.3698676Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
2026-01-29T18:56:20.3707575Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936]     super().__init__(vllm_config)
2026-01-29T18:56:20.3717878Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936]   File "/vllm-workspace/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
2026-01-29T18:56:20.3727504Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936]     self._init_executor()
2026-01-29T18:56:20.3737241Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 172, in _init_executor
2026-01-29T18:56:20.3746737Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936]     self.workers = WorkerProc.wait_for_ready(unready_workers)
2026-01-29T18:56:20.3756624Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:20.3767407Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936]   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 658, in wait_for_ready
2026-01-29T18:56:20.3777795Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936]     raise e from None
2026-01-29T18:56:20.3789019Z [0;36m(EngineCore_DP0 pid=89)[0;0m ERROR 01-29 18:56:20 [core.py:936] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
2026-01-29T18:56:21.3183189Z [0;36m(EngineCore_DP0 pid=89)[0;0m Process EngineCore_DP0:
2026-01-29T18:56:21.3213952Z [0;36m(EngineCore_DP0 pid=89)[0;0m Traceback (most recent call last):
2026-01-29T18:56:21.3242997Z [0;36m(EngineCore_DP0 pid=89)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-01-29T18:56:21.3252989Z [0;36m(EngineCore_DP0 pid=89)[0;0m     self.run()
2026-01-29T18:56:21.3263617Z [0;36m(EngineCore_DP0 pid=89)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-01-29T18:56:21.3273796Z [0;36m(EngineCore_DP0 pid=89)[0;0m     self._target(*self._args, **self._kwargs)
2026-01-29T18:56:21.3284371Z [0;36m(EngineCore_DP0 pid=89)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 940, in run_engine_core
2026-01-29T18:56:21.3293662Z [0;36m(EngineCore_DP0 pid=89)[0;0m     raise e
2026-01-29T18:56:21.3303778Z [0;36m(EngineCore_DP0 pid=89)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 919, in run_engine_core
2026-01-29T18:56:21.3312998Z [0;36m(EngineCore_DP0 pid=89)[0;0m     engine_core = DPEngineCoreProc(*args, **kwargs)
2026-01-29T18:56:21.3323663Z [0;36m(EngineCore_DP0 pid=89)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:21.3333294Z [0;36m(EngineCore_DP0 pid=89)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 1269, in __init__
2026-01-29T18:56:21.3343140Z [0;36m(EngineCore_DP0 pid=89)[0;0m     super().__init__(
2026-01-29T18:56:21.3352927Z [0;36m(EngineCore_DP0 pid=89)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 692, in __init__
2026-01-29T18:56:21.3361765Z [0;36m(EngineCore_DP0 pid=89)[0;0m     super().__init__(
2026-01-29T18:56:21.3371786Z [0;36m(EngineCore_DP0 pid=89)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 106, in __init__
2026-01-29T18:56:21.3381472Z [0;36m(EngineCore_DP0 pid=89)[0;0m     self.model_executor = executor_class(vllm_config)
2026-01-29T18:56:21.3390849Z [0;36m(EngineCore_DP0 pid=89)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:21.3401351Z [0;36m(EngineCore_DP0 pid=89)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
2026-01-29T18:56:21.3410946Z [0;36m(EngineCore_DP0 pid=89)[0;0m     super().__init__(vllm_config)
2026-01-29T18:56:21.3419956Z [0;36m(EngineCore_DP0 pid=89)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
2026-01-29T18:56:21.3430145Z [0;36m(EngineCore_DP0 pid=89)[0;0m     self._init_executor()
2026-01-29T18:56:21.3440346Z [0;36m(EngineCore_DP0 pid=89)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 172, in _init_executor
2026-01-29T18:56:21.3449880Z [0;36m(EngineCore_DP0 pid=89)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
2026-01-29T18:56:21.3459640Z [0;36m(EngineCore_DP0 pid=89)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:21.3469295Z [0;36m(EngineCore_DP0 pid=89)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 658, in wait_for_ready
2026-01-29T18:56:21.3478837Z [0;36m(EngineCore_DP0 pid=89)[0;0m     raise e from None
2026-01-29T18:56:21.3489380Z [0;36m(EngineCore_DP0 pid=89)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
2026-01-29T18:56:21.3498401Z [0;36m(EngineCore_DP1 pid=108)[0;0m Process EngineCore_DP1:
2026-01-29T18:56:21.3516730Z [0;36m(EngineCore_DP1 pid=108)[0;0m Traceback (most recent call last):
2026-01-29T18:56:21.3526721Z [0;36m(EngineCore_DP1 pid=108)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-01-29T18:56:21.3535764Z [0;36m(EngineCore_DP1 pid=108)[0;0m     self.run()
2026-01-29T18:56:21.3545432Z [0;36m(EngineCore_DP1 pid=108)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-01-29T18:56:21.3554431Z [0;36m(EngineCore_DP1 pid=108)[0;0m     self._target(*self._args, **self._kwargs)
2026-01-29T18:56:21.3564624Z [0;36m(EngineCore_DP1 pid=108)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 940, in run_engine_core
2026-01-29T18:56:21.3573878Z [0;36m(EngineCore_DP1 pid=108)[0;0m     raise e
2026-01-29T18:56:21.3583850Z [0;36m(EngineCore_DP1 pid=108)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 919, in run_engine_core
2026-01-29T18:56:21.3593598Z [0;36m(EngineCore_DP1 pid=108)[0;0m     engine_core = DPEngineCoreProc(*args, **kwargs)
2026-01-29T18:56:21.3603673Z [0;36m(EngineCore_DP1 pid=108)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:21.3613673Z [0;36m(EngineCore_DP1 pid=108)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 1269, in __init__
2026-01-29T18:56:21.3622638Z [0;36m(EngineCore_DP1 pid=108)[0;0m     super().__init__(
2026-01-29T18:56:21.3632080Z [0;36m(EngineCore_DP1 pid=108)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 692, in __init__
2026-01-29T18:56:21.3641050Z [0;36m(EngineCore_DP1 pid=108)[0;0m     super().__init__(
2026-01-29T18:56:21.3651223Z [0;36m(EngineCore_DP1 pid=108)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core.py", line 106, in __init__
2026-01-29T18:56:21.3660275Z [0;36m(EngineCore_DP1 pid=108)[0;0m     self.model_executor = executor_class(vllm_config)
2026-01-29T18:56:21.3671113Z [0;36m(EngineCore_DP1 pid=108)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:21.3680461Z [0;36m(EngineCore_DP1 pid=108)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
2026-01-29T18:56:21.3689925Z [0;36m(EngineCore_DP1 pid=108)[0;0m     super().__init__(vllm_config)
2026-01-29T18:56:21.3702194Z [0;36m(EngineCore_DP1 pid=108)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
2026-01-29T18:56:21.3708250Z [0;36m(EngineCore_DP1 pid=108)[0;0m     self._init_executor()
2026-01-29T18:56:21.3718291Z [0;36m(EngineCore_DP1 pid=108)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 172, in _init_executor
2026-01-29T18:56:21.3727976Z [0;36m(EngineCore_DP1 pid=108)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
2026-01-29T18:56:21.3737164Z [0;36m(EngineCore_DP1 pid=108)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:21.3747371Z [0;36m(EngineCore_DP1 pid=108)[0;0m   File "/vllm-workspace/vllm/vllm/v1/executor/multiproc_executor.py", line 658, in wait_for_ready
2026-01-29T18:56:21.3757297Z [0;36m(EngineCore_DP1 pid=108)[0;0m     raise e from None
2026-01-29T18:56:21.3769063Z [0;36m(EngineCore_DP1 pid=108)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
2026-01-29T18:56:23.7294015Z [0;36m(APIServer pid=66)[0;0m Traceback (most recent call last):
2026-01-29T18:56:23.7294699Z [0;36m(APIServer pid=66)[0;0m   File "/usr/local/python3.11.14/bin/vllm", line 7, in <module>
2026-01-29T18:56:23.7295187Z [0;36m(APIServer pid=66)[0;0m     sys.exit(main())
2026-01-29T18:56:23.7306688Z [0;36m(APIServer pid=66)[0;0m              ^^^^^^
2026-01-29T18:56:23.7317540Z [0;36m(APIServer pid=66)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/main.py", line 73, in main
2026-01-29T18:56:23.7327621Z [0;36m(APIServer pid=66)[0;0m     args.dispatch_function(args)
2026-01-29T18:56:23.7337903Z [0;36m(APIServer pid=66)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 60, in cmd
2026-01-29T18:56:23.7347621Z [0;36m(APIServer pid=66)[0;0m     uvloop.run(run_server(args))
2026-01-29T18:56:23.7358657Z [0;36m(APIServer pid=66)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-01-29T18:56:23.7368284Z [0;36m(APIServer pid=66)[0;0m     return runner.run(wrapper())
2026-01-29T18:56:23.7378105Z [0;36m(APIServer pid=66)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:23.7388190Z [0;36m(APIServer pid=66)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-01-29T18:56:23.7397357Z [0;36m(APIServer pid=66)[0;0m     return self._loop.run_until_complete(task)
2026-01-29T18:56:23.7406737Z [0;36m(APIServer pid=66)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:23.7416525Z [0;36m(APIServer pid=66)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-01-29T18:56:23.7426580Z [0;36m(APIServer pid=66)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-01-29T18:56:23.7436682Z [0;36m(APIServer pid=66)[0;0m     return await main
2026-01-29T18:56:23.7446743Z [0;36m(APIServer pid=66)[0;0m            ^^^^^^^^^^
2026-01-29T18:56:23.7455824Z [0;36m(APIServer pid=66)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 1319, in run_server
2026-01-29T18:56:23.7466260Z [0;36m(APIServer pid=66)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
2026-01-29T18:56:23.7475998Z [0;36m(APIServer pid=66)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 1338, in run_server_worker
2026-01-29T18:56:23.7488085Z [0;36m(APIServer pid=66)[0;0m     async with build_async_engine_client(
2026-01-29T18:56:23.7495801Z [0;36m(APIServer pid=66)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-01-29T18:56:23.7505134Z [0;36m(APIServer pid=66)[0;0m     return await anext(self.gen)
2026-01-29T18:56:23.7515743Z [0;36m(APIServer pid=66)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:23.7525988Z [0;36m(APIServer pid=66)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 173, in build_async_engine_client
2026-01-29T18:56:23.7536229Z [0;36m(APIServer pid=66)[0;0m     async with build_async_engine_client_from_engine_args(
2026-01-29T18:56:23.7545690Z [0;36m(APIServer pid=66)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-01-29T18:56:23.7554591Z [0;36m(APIServer pid=66)[0;0m     return await anext(self.gen)
2026-01-29T18:56:23.7564770Z [0;36m(APIServer pid=66)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:23.7574779Z [0;36m(APIServer pid=66)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 214, in build_async_engine_client_from_engine_args
2026-01-29T18:56:23.7583882Z [0;36m(APIServer pid=66)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-01-29T18:56:23.7593398Z [0;36m(APIServer pid=66)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:23.7603965Z [0;36m(APIServer pid=66)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 205, in from_vllm_config
2026-01-29T18:56:23.7613041Z [0;36m(APIServer pid=66)[0;0m     return cls(
2026-01-29T18:56:23.7621858Z [0;36m(APIServer pid=66)[0;0m            ^^^^
2026-01-29T18:56:23.7632713Z [0;36m(APIServer pid=66)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 132, in __init__
2026-01-29T18:56:23.7643734Z [0;36m(APIServer pid=66)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-01-29T18:56:23.7652311Z [0;36m(APIServer pid=66)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:23.7661956Z [0;36m(APIServer pid=66)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-01-29T18:56:23.7671686Z [0;36m(APIServer pid=66)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-01-29T18:56:23.7681604Z [0;36m(APIServer pid=66)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-01-29T18:56:23.7691279Z [0;36m(APIServer pid=66)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1196, in __init__
2026-01-29T18:56:23.7700249Z [0;36m(APIServer pid=66)[0;0m     super().__init__(
2026-01-29T18:56:23.7710152Z [0;36m(APIServer pid=66)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1037, in __init__
2026-01-29T18:56:23.7719713Z [0;36m(APIServer pid=66)[0;0m     super().__init__(
2026-01-29T18:56:23.7729978Z [0;36m(APIServer pid=66)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 824, in __init__
2026-01-29T18:56:23.7738833Z [0;36m(APIServer pid=66)[0;0m     super().__init__(
2026-01-29T18:56:23.7748721Z [0;36m(APIServer pid=66)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 479, in __init__
2026-01-29T18:56:23.7760296Z [0;36m(APIServer pid=66)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
2026-01-29T18:56:23.7771918Z [0;36m(APIServer pid=66)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 144, in __exit__
2026-01-29T18:56:23.7782551Z [0;36m(APIServer pid=66)[0;0m     next(self.gen)
2026-01-29T18:56:23.7792636Z [0;36m(APIServer pid=66)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/utils.py", line 921, in launch_core_engines
2026-01-29T18:56:23.7803591Z [0;36m(APIServer pid=66)[0;0m     wait_for_engine_startup(
2026-01-29T18:56:23.7813015Z [0;36m(APIServer pid=66)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/utils.py", line 980, in wait_for_engine_startup
2026-01-29T18:56:23.7822224Z [0;36m(APIServer pid=66)[0;0m     raise RuntimeError(
2026-01-29T18:56:23.7832195Z [0;36m(APIServer pid=66)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
2026-01-29T18:56:23.8021372Z [0;36m(APIServer pid=66)[0;0m [ERROR] 2026-01-29-18:56:23 (PID:66, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
2026-01-29T18:56:24.1378755Z [0;36m(APIServer pid=66)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-01-29T18:56:25.6982568Z /usr/local/python3.11.14/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 18 leaked shared_memory objects to clean up at shutdown
2026-01-29T18:56:25.6989509Z   warnings.warn('resource_tracker: There appear to be %d '
2026-01-29T18:56:28.4102591Z FAILED
2026-01-29T18:56:28.4111302Z 
2026-01-29T18:56:28.4122139Z =================================== FAILURES ===================================
2026-01-29T18:56:28.4132669Z _______________________________ test_multi_node ________________________________
2026-01-29T18:56:28.4142746Z 
2026-01-29T18:56:28.4153023Z     @pytest.mark.asyncio
2026-01-29T18:56:28.4163404Z     async def test_multi_node() -> None:
2026-01-29T18:56:28.4173194Z         config = MultiNodeConfigLoader.from_yaml()
2026-01-29T18:56:28.4182257Z     
2026-01-29T18:56:28.4192281Z         with ProxyLauncher(
2026-01-29T18:56:28.4202237Z                 nodes=config.nodes,
2026-01-29T18:56:28.4210754Z                 disagg_cfg=config.disagg_cfg,
2026-01-29T18:56:28.4219545Z                 envs=config.envs,
2026-01-29T18:56:28.4229060Z                 proxy_port=config.proxy_port,
2026-01-29T18:56:28.4238780Z                 cur_index=config.cur_index,
2026-01-29T18:56:28.4248116Z         ) as proxy:
2026-01-29T18:56:28.4258340Z     
2026-01-29T18:56:28.4267598Z >           with RemoteOpenAIServer(
2026-01-29T18:56:28.4277382Z                     model=config.model,
2026-01-29T18:56:28.4286765Z                     vllm_serve_args=config.server_cmd,
2026-01-29T18:56:28.4295036Z                     server_port=config.server_port,
2026-01-29T18:56:28.4307474Z                     server_host=config.master_ip,
2026-01-29T18:56:28.4314573Z                     env_dict=config.envs,
2026-01-29T18:56:28.4323927Z                     auto_port=False,
2026-01-29T18:56:28.4333749Z                     proxy_port=proxy.proxy_port,
2026-01-29T18:56:28.4343781Z                     disaggregated_prefill=config.disagg_cfg,
2026-01-29T18:56:28.4352981Z                     nodes_info=config.nodes,
2026-01-29T18:56:28.4363518Z                     max_wait_seconds=2800,
2026-01-29T18:56:28.4372332Z             ) as server:
2026-01-29T18:56:28.4380898Z 
2026-01-29T18:56:28.4390210Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py:21: 
2026-01-29T18:56:28.4399553Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-01-29T18:56:28.4408790Z tests/e2e/conftest.py:296: in __init__
2026-01-29T18:56:28.4418609Z     self._wait_for_multiple_servers(
2026-01-29T18:56:28.4428624Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-01-29T18:56:28.4438262Z 
2026-01-29T18:56:28.4447173Z self = <tests.e2e.conftest.RemoteOpenAIServer object at 0xfffefce6b390>
2026-01-29T18:56:28.4456871Z targets = [('10.0.0.77', 'http://10.0.0.77:8080/health')], timeout = 2800
2026-01-29T18:56:28.4466177Z log_interval = 30.0
2026-01-29T18:56:28.4474909Z 
2026-01-29T18:56:28.4484939Z     def _wait_for_multiple_servers(self,
2026-01-29T18:56:28.4494085Z                                    targets,
2026-01-29T18:56:28.4502766Z                                    timeout: float,
2026-01-29T18:56:28.4512341Z                                    log_interval: float = 30.0):
2026-01-29T18:56:28.4522945Z         """
2026-01-29T18:56:28.4531607Z         targets: List[(node_ip, url)]
2026-01-29T18:56:28.4540945Z         log_interval
2026-01-29T18:56:28.4550335Z         """
2026-01-29T18:56:28.4560409Z         start = time.time()
2026-01-29T18:56:28.4569319Z         client = requests
2026-01-29T18:56:28.4578128Z     
2026-01-29T18:56:28.4587661Z         ready = {node_ip: False for node_ip, _ in targets}
2026-01-29T18:56:28.4597758Z     
2026-01-29T18:56:28.4608103Z         last_log_time = 0.0
2026-01-29T18:56:28.4616255Z     
2026-01-29T18:56:28.4624933Z         while True:
2026-01-29T18:56:28.4634506Z             now = time.time()
2026-01-29T18:56:28.4644183Z             all_ready = True
2026-01-29T18:56:28.4654204Z             should_log = (now - last_log_time) >= log_interval
2026-01-29T18:56:28.4662764Z     
2026-01-29T18:56:28.4672169Z             for node_ip, url in targets:
2026-01-29T18:56:28.4682293Z                 if ready[node_ip]:
2026-01-29T18:56:28.4690697Z                     continue
2026-01-29T18:56:28.4699865Z     
2026-01-29T18:56:28.4709120Z                 try:
2026-01-29T18:56:28.4718135Z                     resp = client.get(url)
2026-01-29T18:56:28.4727509Z                     if resp.status_code == 200:
2026-01-29T18:56:28.4737003Z                         ready[node_ip] = True
2026-01-29T18:56:28.4746496Z                         logger.info(f"[READY] Node {node_ip} is ready.")
2026-01-29T18:56:28.4755044Z                 except RequestException:
2026-01-29T18:56:28.4765115Z                     all_ready = False
2026-01-29T18:56:28.4776018Z                     if should_log:
2026-01-29T18:56:28.4787400Z                         logger.debug(f"[WAIT] {url}: connection failed")
2026-01-29T18:56:28.4795613Z     
2026-01-29T18:56:28.4806120Z                     # check unexpected exit
2026-01-29T18:56:28.4815521Z                     result = self._poll()
2026-01-29T18:56:28.4825150Z                     if result is not None and result != 0:
2026-01-29T18:56:28.4834871Z >                       raise RuntimeError(
2026-01-29T18:56:28.4844199Z                             f"Server at {node_ip} exited unexpectedly."
2026-01-29T18:56:28.4852571Z                         ) from None
2026-01-29T18:56:28.4862230Z E                       RuntimeError: Server at 10.0.0.77 exited unexpectedly.
2026-01-29T18:56:28.4871013Z 
2026-01-29T18:56:28.4880843Z tests/e2e/conftest.py:389: RuntimeError
2026-01-29T18:56:28.4890820Z =============================== warnings summary ===============================
2026-01-29T18:56:28.4899994Z <frozen importlib._bootstrap>:241
2026-01-29T18:56:28.4909660Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute
2026-01-29T18:56:28.4918434Z 
2026-01-29T18:56:28.4927239Z <frozen importlib._bootstrap>:241
2026-01-29T18:56:28.4937382Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute
2026-01-29T18:56:28.4946240Z 
2026-01-29T18:56:28.4955662Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2026-01-29T18:56:28.4965452Z =========================== short test summary info ============================
2026-01-29T18:56:28.4975246Z FAILED tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node
2026-01-29T18:56:28.4984314Z ================== 1 failed, 2 warnings in 291.61s (0:04:51) ===================
2026-01-29T18:56:30.0760098Z [0;31mFAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml âœ— ERROR: Some tests failed, please check the error stack above for details. If this is insufficient to pinpoint the error, please download and review the logs of all other nodes from the job's summary.[0m
2026-01-29T18:56:30.2187588Z Cleaning up background log streams...
2026-01-29T18:56:30.2892477Z ##[error]Error: failed to run script step: Error: command terminated with non-zero exit code: command terminated with exit code 1
2026-01-29T18:56:30.2931405Z ##[error]Process completed with exit code 1.
2026-01-29T18:56:30.3028903Z ##[error]Executing the custom container implementation failed. Please contact your self hosted runner administrator.
2026-01-29T18:56:30.3483230Z ##[group]Run actions/upload-artifact@v6
2026-01-29T18:56:30.3483522Z with:
2026-01-29T18:56:30.3483770Z   name: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs
2026-01-29T18:56:30.3484083Z   path: /tmp/vllm*_logs.txt
2026-01-29T18:56:30.3484314Z   retention-days: 7
2026-01-29T18:56:30.3484614Z   if-no-files-found: warn
2026-01-29T18:56:30.3484936Z   compression-level: 6
2026-01-29T18:56:30.3485209Z   overwrite: false
2026-01-29T18:56:30.3485546Z   include-hidden-files: false
2026-01-29T18:56:30.3485788Z env:
2026-01-29T18:56:30.3486049Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-01-29T18:56:30.3486488Z ##[endgroup]
2026-01-29T18:56:30.3514763Z ##[group]Run '/home/runner/k8s/index.js'
2026-01-29T18:56:30.3515700Z shell: /home/runner/externals/node20/bin/node {0}
2026-01-29T18:56:30.3516000Z ##[endgroup]
2026-01-29T18:56:30.7114205Z (node:11711) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-01-29T18:56:30.7114994Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-01-29T18:56:50.2117268Z With the provided path, there will be 1 file uploaded
2026-01-29T18:56:50.2122244Z Artifact name is valid!
2026-01-29T18:56:50.2122564Z Root directory input is valid!
2026-01-29T18:56:51.3766674Z Beginning upload of artifact content to blob storage
2026-01-29T18:56:53.1943548Z Uploaded bytes 10849
2026-01-29T18:56:53.4711362Z Finished uploading artifact content to blob storage!
2026-01-29T18:56:53.4712159Z SHA256 digest of uploaded artifact zip is 1620916b914cb41465fe0e19968613a1be129d01997f0f57f94ae3add78c675f
2026-01-29T18:56:53.4712691Z Finalizing artifact upload
2026-01-29T18:56:54.2978208Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs.zip successfully finalized. Artifact ID 5308521797
2026-01-29T18:56:54.2979120Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs has been successfully uploaded! Final size is 10849 bytes. Artifact ID is 5308521797
2026-01-29T18:56:54.2983696Z Artifact download URL: https://github.com/vllm-project/vllm-ascend/actions/runs/21486238570/artifacts/5308521797
2026-01-29T18:57:12.8375027Z ##[group]Run kubectl get pods -n $NAMESPACE --ignore-not-found=true
2026-01-29T18:57:12.8375538Z [36;1mkubectl get pods -n $NAMESPACE --ignore-not-found=true[0m
2026-01-29T18:57:12.8375968Z [36;1mkubectl delete -f ./lws.yaml --ignore-not-found=true || true[0m
2026-01-29T18:57:12.8376363Z shell: bash -el {0}
2026-01-29T18:57:12.8376610Z env:
2026-01-29T18:57:12.8377091Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-01-29T18:57:12.8377480Z ##[endgroup]
2026-01-29T18:57:12.8448125Z ##[group]Run '/home/runner/k8s/index.js'
2026-01-29T18:57:12.8448862Z shell: /home/runner/externals/node20/bin/node {0}
2026-01-29T18:57:12.8449161Z ##[endgroup]
2026-01-29T18:57:13.1988070Z (node:13364) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-01-29T18:57:13.1989300Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-01-29T18:57:32.0169009Z NAME                                             READY   STATUS    RESTARTS      AGE
2026-01-29T18:57:32.0169550Z linux-aarch64-a3-0-wgt9d-runner-kctb4            1/1     Running   0             10m
2026-01-29T18:57:32.0170073Z linux-aarch64-a3-0-wgt9d-runner-kctb4-workflow   1/1     Running   0             10m
2026-01-29T18:57:32.0170476Z vllm-0                                           1/1     Running   1 (62s ago)   6m41s
2026-01-29T18:57:32.0171277Z vllm-0-1                                         1/1     Running   0             6m41s
2026-01-29T18:57:32.0830388Z leaderworkerset.leaderworkerset.x-k8s.io "vllm" deleted from vllm-project namespace
2026-01-29T18:57:32.1020365Z service "vllm-leader" deleted from vllm-project namespace
2026-01-29T18:57:50.6498779Z Post job cleanup.
2026-01-29T18:57:50.6523403Z ##[group]Run '/home/runner/k8s/index.js'
2026-01-29T18:57:50.6524234Z shell: /home/runner/externals/node20/bin/node {0}
2026-01-29T18:57:50.6524651Z ##[endgroup]
2026-01-29T18:57:51.0074918Z (node:15198) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-01-29T18:57:51.0075756Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-01-29T18:58:09.7828028Z [command]/usr/bin/git version
2026-01-29T18:58:09.8017971Z git version 2.34.1
2026-01-29T18:58:09.8050096Z Copying '/root/.gitconfig' to '/__w/_temp/d82b6c3b-c719-4f32-91ef-bcc6680bcc20/.gitconfig'
2026-01-29T18:58:09.8057395Z Temporarily overriding HOME='/__w/_temp/d82b6c3b-c719-4f32-91ef-bcc6680bcc20' before making global git config changes
2026-01-29T18:58:09.8058129Z Adding repository directory to the temporary git global config as a safe directory
2026-01-29T18:58:09.8062807Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-01-29T18:58:09.8103348Z Removing SSH command configuration
2026-01-29T18:58:09.8108702Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-01-29T18:58:09.8166626Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-01-29T18:58:09.8624716Z Removing HTTP extra header
2026-01-29T18:58:09.8628167Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-01-29T18:58:09.8654822Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-01-29T18:58:09.8842435Z Removing includeIf entries pointing to credentials config files
2026-01-29T18:58:09.8845948Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-01-29T18:58:09.8865211Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-01-29T18:58:09.8865628Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-01-29T18:58:09.8866011Z includeif.gitdir:/github/workspace/.git.path
2026-01-29T18:58:09.8866375Z includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-01-29T18:58:09.8872746Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-01-29T18:58:09.8891582Z /__w/_temp/git-credentials-50f19b6e-f920-4ce1-983d-e6980535abcc.config
2026-01-29T18:58:09.8900183Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-50f19b6e-f920-4ce1-983d-e6980535abcc.config
2026-01-29T18:58:09.8933420Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-01-29T18:58:09.8952705Z /__w/_temp/git-credentials-50f19b6e-f920-4ce1-983d-e6980535abcc.config
2026-01-29T18:58:09.8960008Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-50f19b6e-f920-4ce1-983d-e6980535abcc.config
2026-01-29T18:58:09.8991038Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git.path
2026-01-29T18:58:09.9009650Z /github/runner_temp/git-credentials-50f19b6e-f920-4ce1-983d-e6980535abcc.config
2026-01-29T18:58:09.9017594Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-50f19b6e-f920-4ce1-983d-e6980535abcc.config
2026-01-29T18:58:09.9043763Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-01-29T18:58:09.9061537Z /github/runner_temp/git-credentials-50f19b6e-f920-4ce1-983d-e6980535abcc.config
2026-01-29T18:58:09.9068307Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-50f19b6e-f920-4ce1-983d-e6980535abcc.config
2026-01-29T18:58:09.9100597Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-01-29T18:58:09.9275316Z Removing credentials config '/__w/_temp/git-credentials-50f19b6e-f920-4ce1-983d-e6980535abcc.config'
2026-01-29T18:58:28.4506722Z ##[group]Run '/home/runner/k8s/index.js'
2026-01-29T18:58:28.4507770Z shell: /home/runner/externals/node20/bin/node {0}
2026-01-29T18:58:28.4508108Z ##[endgroup]
2026-01-29T18:58:28.8695024Z Cleaning up orphan processes
