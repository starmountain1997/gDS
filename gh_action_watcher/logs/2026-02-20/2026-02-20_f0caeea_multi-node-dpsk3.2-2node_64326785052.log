# Run ID: 22231958921
# Commit: f0caeeadcb37261beebd4a6e32934fa9f460db98
# Job: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
# Date: 2026-02-20
============================================================

ï»¿2026-02-20T18:16:33.7244053Z Current runner version: '2.330.0'
2026-02-20T18:16:33.7248674Z Runner name: 'linux-aarch64-a3-0-n4cwm-runner-qpgps'
2026-02-20T18:16:33.7249415Z Runner group name: 'Default'
2026-02-20T18:16:33.7250120Z Machine name: 'linux-aarch64-a3-0-n4cwm-runner-qpgps'
2026-02-20T18:16:33.7253573Z ##[group]GITHUB_TOKEN Permissions
2026-02-20T18:16:33.7255578Z Actions: write
2026-02-20T18:16:33.7256052Z ArtifactMetadata: write
2026-02-20T18:16:33.7256468Z Attestations: write
2026-02-20T18:16:33.7256834Z Checks: write
2026-02-20T18:16:33.7257216Z Contents: write
2026-02-20T18:16:33.7257681Z Deployments: write
2026-02-20T18:16:33.7258040Z Discussions: write
2026-02-20T18:16:33.7258423Z Issues: write
2026-02-20T18:16:33.7258755Z Metadata: read
2026-02-20T18:16:33.7259130Z Models: read
2026-02-20T18:16:33.7259498Z Packages: write
2026-02-20T18:16:33.7259845Z Pages: write
2026-02-20T18:16:33.7260214Z PullRequests: write
2026-02-20T18:16:33.7260615Z RepositoryProjects: write
2026-02-20T18:16:33.7261147Z SecurityEvents: write
2026-02-20T18:16:33.7261591Z Statuses: write
2026-02-20T18:16:33.7262190Z ##[endgroup]
2026-02-20T18:16:33.7263939Z Secret source: Actions
2026-02-20T18:16:33.7264563Z Prepare workflow directory
2026-02-20T18:16:33.7777837Z Prepare all required actions
2026-02-20T18:16:33.7808853Z Getting action download info
2026-02-20T18:16:34.8875602Z Download action repository 'actions/checkout@v6' (SHA:de0fac2e4500dabe0009e67214ff5f5447ce83dd)
2026-02-20T18:16:39.5708086Z Download action repository 'actions/upload-artifact@v6' (SHA:b7c566a772e6b6bfb58ed0dc250532a479d7789f)
2026-02-20T18:16:46.9646606Z Uses: vllm-project/vllm-ascend/.github/workflows/_e2e_nightly_multi_node.yaml@refs/heads/main (f0caeeadcb37261beebd4a6e32934fa9f460db98)
2026-02-20T18:16:46.9650106Z ##[group] Inputs
2026-02-20T18:16:46.9650430Z   soc_version: a3
2026-02-20T18:16:46.9650683Z   runner: linux-aarch64-a3-0
2026-02-20T18:16:46.9651071Z   image: swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3
2026-02-20T18:16:46.9651542Z   config_file_path: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-20T18:16:46.9651851Z   replicas: 1
2026-02-20T18:16:46.9652194Z   size: 2
2026-02-20T18:16:46.9652433Z   vllm_version: v0.15.0
2026-02-20T18:16:46.9652787Z   vllm_ascend_remote_url: https://github.com/vllm-project/vllm-ascend.git
2026-02-20T18:16:46.9653121Z   vllm_ascend_ref: main
2026-02-20T18:16:46.9653373Z ##[endgroup]
2026-02-20T18:16:46.9653909Z Complete job name: multi-node (multi-node-dpsk3.2-2node, DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml, 2) / DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-20T18:16:47.0154042Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-20T18:16:47.0156412Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-20T18:16:47.0156993Z ##[endgroup]
2026-02-20T18:17:02.5136475Z (node:69) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-20T18:17:02.5137291Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-20T18:17:04.8441623Z ##[group]Run # Decode and save kubeconfig
2026-02-20T18:17:04.8442376Z [36;1m# Decode and save kubeconfig[0m
2026-02-20T18:17:04.8474718Z [36;1mecho "***" | base64 -d > "$KUBECONFIG"[0m
2026-02-20T18:17:04.8475280Z shell: bash -el {0}
2026-02-20T18:17:04.8475530Z ##[endgroup]
2026-02-20T18:17:04.8654492Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-20T18:17:04.8655398Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-20T18:17:04.8655709Z ##[endgroup]
2026-02-20T18:17:05.2158891Z (node:402) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-20T18:17:05.2159771Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-20T18:17:06.0972952Z ##[group]Run actions/checkout@v6
2026-02-20T18:17:06.0973292Z with:
2026-02-20T18:17:06.0973576Z   repository: vllm-project/vllm-ascend
2026-02-20T18:17:06.0974222Z   token: ***
2026-02-20T18:17:06.0974429Z   ssh-strict: true
2026-02-20T18:17:06.0974663Z   ssh-user: git
2026-02-20T18:17:06.0974890Z   persist-credentials: true
2026-02-20T18:17:06.0975120Z   clean: true
2026-02-20T18:17:06.0975417Z   sparse-checkout-cone-mode: true
2026-02-20T18:17:06.0975722Z   fetch-depth: 1
2026-02-20T18:17:06.0975927Z   fetch-tags: false
2026-02-20T18:17:06.0976154Z   show-progress: true
2026-02-20T18:17:06.0976334Z   lfs: false
2026-02-20T18:17:06.0976568Z   submodules: false
2026-02-20T18:17:06.0976791Z   set-safe-directory: true
2026-02-20T18:17:06.0976993Z ##[endgroup]
2026-02-20T18:17:06.1018340Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-20T18:17:06.1019229Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-20T18:17:06.1019623Z ##[endgroup]
2026-02-20T18:17:06.4557117Z (node:433) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-20T18:17:06.4557940Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-20T18:17:07.0141259Z Syncing repository: vllm-project/vllm-ascend
2026-02-20T18:17:07.0142732Z ##[group]Getting Git version info
2026-02-20T18:17:07.0143105Z Working directory is '/__w/vllm-ascend/vllm-ascend'
2026-02-20T18:17:07.0143818Z [command]/usr/bin/git version
2026-02-20T18:17:07.0144043Z git version 2.34.1
2026-02-20T18:17:07.0145435Z ##[endgroup]
2026-02-20T18:17:07.0148459Z Copying '/root/.gitconfig' to '/__w/_temp/8a816c08-2e2d-49fd-bcd5-938cdc63effb/.gitconfig'
2026-02-20T18:17:07.0151208Z Temporarily overriding HOME='/__w/_temp/8a816c08-2e2d-49fd-bcd5-938cdc63effb' before making global git config changes
2026-02-20T18:17:07.0151873Z Adding repository directory to the temporary git global config as a safe directory
2026-02-20T18:17:07.0154739Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-20T18:17:07.0183274Z Deleting the contents of '/__w/vllm-ascend/vllm-ascend'
2026-02-20T18:17:07.0185786Z ##[group]Initializing the repository
2026-02-20T18:17:07.0189461Z [command]/usr/bin/git init /__w/vllm-ascend/vllm-ascend
2026-02-20T18:17:07.0304864Z hint: Using 'master' as the name for the initial branch. This default branch name
2026-02-20T18:17:07.0305370Z hint: is subject to change. To configure the initial branch name to use in all
2026-02-20T18:17:07.0305833Z hint: of your new repositories, which will suppress this warning, call:
2026-02-20T18:17:07.0306174Z hint: 
2026-02-20T18:17:07.0306443Z hint: 	git config --global init.defaultBranch <name>
2026-02-20T18:17:07.0306747Z hint: 
2026-02-20T18:17:07.0307039Z hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and
2026-02-20T18:17:07.0307504Z hint: 'development'. The just-created branch can be renamed via this command:
2026-02-20T18:17:07.0307946Z hint: 
2026-02-20T18:17:07.0308139Z hint: 	git branch -m <name>
2026-02-20T18:17:07.0311097Z Initialized empty Git repository in /__w/vllm-ascend/vllm-ascend/.git/
2026-02-20T18:17:07.0319769Z [command]/usr/bin/git remote add origin https://github.com/vllm-project/vllm-ascend
2026-02-20T18:17:07.0364333Z ##[endgroup]
2026-02-20T18:17:07.0364746Z ##[group]Disabling automatic garbage collection
2026-02-20T18:17:07.0366588Z [command]/usr/bin/git config --local gc.auto 0
2026-02-20T18:17:07.0390829Z ##[endgroup]
2026-02-20T18:17:07.0391146Z ##[group]Setting up auth
2026-02-20T18:17:07.0419176Z Removing SSH command configuration
2026-02-20T18:17:07.0420002Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-20T18:17:07.0561220Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-20T18:17:07.0752876Z Removing HTTP extra header
2026-02-20T18:17:07.0756208Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-20T18:17:07.0783441Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-20T18:17:07.0966372Z Removing includeIf entries pointing to credentials config files
2026-02-20T18:17:07.0970867Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-20T18:17:07.0996488Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-20T18:17:07.1186915Z [command]/usr/bin/git config --file /__w/_temp/git-credentials-0ad1268f-cd13-4f46-b1f8-db2541803ef4.config http.https://github.com/.extraheader AUTHORIZATION: basic ***
2026-02-20T18:17:07.1220624Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-0ad1268f-cd13-4f46-b1f8-db2541803ef4.config
2026-02-20T18:17:07.1251855Z [command]/usr/bin/git config --local includeIf.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-0ad1268f-cd13-4f46-b1f8-db2541803ef4.config
2026-02-20T18:17:07.1283371Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-0ad1268f-cd13-4f46-b1f8-db2541803ef4.config
2026-02-20T18:17:07.1314762Z [command]/usr/bin/git config --local includeIf.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-0ad1268f-cd13-4f46-b1f8-db2541803ef4.config
2026-02-20T18:17:07.1341067Z ##[endgroup]
2026-02-20T18:17:07.1341466Z ##[group]Fetching the repository
2026-02-20T18:17:07.1348599Z [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --no-recurse-submodules --depth=1 origin +f0caeeadcb37261beebd4a6e32934fa9f460db98:refs/remotes/origin/main
2026-02-20T18:17:08.9300860Z From https://gh-proxy.test.osinfra.cn/https://github.com/vllm-project/vllm-ascend
2026-02-20T18:17:08.9301585Z  * [new ref]         f0caeeadcb37261beebd4a6e32934fa9f460db98 -> origin/main
2026-02-20T18:17:08.9323231Z [command]/usr/bin/git branch --list --remote origin/main
2026-02-20T18:17:08.9348286Z   origin/main
2026-02-20T18:17:08.9355764Z [command]/usr/bin/git rev-parse refs/remotes/origin/main
2026-02-20T18:17:08.9373756Z f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-20T18:17:08.9377731Z ##[endgroup]
2026-02-20T18:17:08.9378123Z ##[group]Determining the checkout info
2026-02-20T18:17:08.9378514Z ##[endgroup]
2026-02-20T18:17:08.9382316Z [command]/usr/bin/git sparse-checkout disable
2026-02-20T18:17:08.9424086Z [command]/usr/bin/git config --local --unset-all extensions.worktreeConfig
2026-02-20T18:17:08.9454071Z ##[group]Checking out the ref
2026-02-20T18:17:08.9456813Z [command]/usr/bin/git checkout --progress --force -B main refs/remotes/origin/main
2026-02-20T18:17:09.0317307Z Switched to a new branch 'main'
2026-02-20T18:17:09.0317714Z Branch 'main' set up to track remote branch 'main' from 'origin'.
2026-02-20T18:17:09.0327969Z ##[endgroup]
2026-02-20T18:17:09.0368094Z [command]/usr/bin/git log -1 --format=%H
2026-02-20T18:17:09.0387251Z f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-20T18:17:09.4671203Z ##[group]Run # prepare for lws entrypoint scripts
2026-02-20T18:17:09.4671592Z [36;1m# prepare for lws entrypoint scripts[0m
2026-02-20T18:17:09.4672176Z [36;1minstall -D tests/e2e/nightly/multi_node/scripts/run.sh /root/.cache/tests/run.sh[0m
2026-02-20T18:17:09.4672684Z shell: bash -el {0}
2026-02-20T18:17:09.4672911Z ##[endgroup]
2026-02-20T18:17:09.4755429Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-20T18:17:09.4756280Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-20T18:17:09.4756571Z ##[endgroup]
2026-02-20T18:17:09.8268393Z (node:474) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-20T18:17:09.8269276Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-20T18:17:10.7497780Z ##[group]Run set -euo pipefail
2026-02-20T18:17:10.7498025Z [36;1mset -euo pipefail[0m
2026-02-20T18:17:10.7498197Z [36;1m[0m
2026-02-20T18:17:10.7498346Z [36;1mCRD_NAME="${CRD_NAME:-vllm}"[0m
2026-02-20T18:17:10.7498546Z [36;1mTIMEOUT=${TIMEOUT:-120}[0m
2026-02-20T18:17:10.7498732Z [36;1mSLEEP_INTERVAL=2[0m
2026-02-20T18:17:10.7498893Z [36;1m[0m
2026-02-20T18:17:10.7499124Z [36;1mecho "Deleting leaderworkerset [$CRD_NAME] in namespace [$NAMESPACE]..."[0m
2026-02-20T18:17:10.7499543Z [36;1mkubectl delete leaderworkerset "$CRD_NAME" -n "$NAMESPACE" --ignore-not-found[0m
2026-02-20T18:17:10.7499844Z [36;1m[0m
2026-02-20T18:17:10.7500054Z [36;1mecho "Waiting for all pods starting with 'vllm' to be deleted..."[0m
2026-02-20T18:17:10.7500328Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-20T18:17:10.7500497Z [36;1m[0m
2026-02-20T18:17:10.7500632Z [36;1mwhile true; do[0m
2026-02-20T18:17:10.7500793Z [36;1m  NOW=$(date +%s)[0m
2026-02-20T18:17:10.7501015Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-20T18:17:10.7501198Z [36;1m[0m
2026-02-20T18:17:10.7501358Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-20T18:17:10.7501652Z [36;1m    echo "Timeout reached ($TIMEOUT seconds), some pods still exist:"[0m
2026-02-20T18:17:10.7501975Z [36;1m    kubectl get pods -n "$NAMESPACE" | grep '^vllm' || true[0m
2026-02-20T18:17:10.7502377Z [36;1m    exit 1[0m
2026-02-20T18:17:10.7502522Z [36;1m  fi[0m
2026-02-20T18:17:10.7502661Z [36;1m[0m
2026-02-20T18:17:10.7503076Z [36;1m  PODS_EXIST=$(kubectl get pods -n "$NAMESPACE" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null | tr ' ' '\n' | grep '^vllm' || true)[0m
2026-02-20T18:17:10.7503476Z [36;1m[0m
2026-02-20T18:17:10.7503629Z [36;1m  if [[ -z "$PODS_EXIST" ]]; then[0m
2026-02-20T18:17:10.7503841Z [36;1m    echo "All vllm pods deleted."[0m
2026-02-20T18:17:10.7504022Z [36;1m    break[0m
2026-02-20T18:17:10.7504227Z [36;1m  else[0m
2026-02-20T18:17:10.7504438Z [36;1m    echo "Waiting for pods to be deleted: $PODS_EXIST"[0m
2026-02-20T18:17:10.7504678Z [36;1m    sleep $SLEEP_INTERVAL[0m
2026-02-20T18:17:10.7504858Z [36;1m  fi[0m
2026-02-20T18:17:10.7504995Z [36;1mdone[0m
2026-02-20T18:17:10.7505270Z shell: bash -el {0}
2026-02-20T18:17:10.7505425Z ##[endgroup]
2026-02-20T18:17:10.7589971Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-20T18:17:10.7590743Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-20T18:17:10.7591011Z ##[endgroup]
2026-02-20T18:17:11.1111861Z (node:528) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-20T18:17:11.1113040Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-20T18:17:11.6296696Z Deleting leaderworkerset [vllm] in namespace [vllm-project]...
2026-02-20T18:17:11.7049022Z Waiting for all pods starting with 'vllm' to be deleted...
2026-02-20T18:17:11.7708479Z All vllm pods deleted.
2026-02-20T18:17:12.2072280Z ##[group]Run set -e
2026-02-20T18:17:12.2072521Z [36;1mset -e[0m
2026-02-20T18:17:12.2072671Z [36;1m[0m
2026-02-20T18:17:12.2072810Z [36;1msize="2"[0m
2026-02-20T18:17:12.2072954Z [36;1mreplicas="1"[0m
2026-02-20T18:17:12.2073287Z [36;1mimage="swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/vllm-ascend:nightly-a3"[0m
2026-02-20T18:17:12.2073710Z [36;1mconfig_file_path="DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-20T18:17:12.2074023Z [36;1mfail_tag=FAIL_TAG_"DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml"[0m
2026-02-20T18:17:12.2074307Z [36;1mecho "FAIL_TAG=${fail_tag}" >> "$GITHUB_ENV"[0m
2026-02-20T18:17:12.2074512Z [36;1m[0m
2026-02-20T18:17:12.2074807Z [36;1mrequired_params=("size" "replicas" "image" "config_file_path")[0m
2026-02-20T18:17:12.2075103Z [36;1mfor param in "${required_params[@]}"; do[0m
2026-02-20T18:17:12.2075318Z [36;1m  if [ -z "${!param}" ]; then[0m
2026-02-20T18:17:12.2075779Z [36;1m    echo "Error: Parameter '$param' is required but empty"[0m
2026-02-20T18:17:12.2076014Z [36;1m    exit 1[0m
2026-02-20T18:17:12.2076168Z [36;1m  fi[0m
2026-02-20T18:17:12.2076310Z [36;1mdone[0m
2026-02-20T18:17:12.2076441Z [36;1m[0m
2026-02-20T18:17:12.2076583Z [36;1mif [ "a3" = "a3" ]; then[0m
2026-02-20T18:17:12.2076772Z [36;1m  npu_per_node=16[0m
2026-02-20T18:17:12.2077035Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws.yaml.jinja2"[0m
2026-02-20T18:17:12.2077321Z [36;1melse[0m
2026-02-20T18:17:12.2077460Z [36;1m  npu_per_node=8[0m
2026-02-20T18:17:12.2077730Z [36;1m  TEMPLATE_FILE="tests/e2e/nightly/multi_node/scripts/lws-a2.yaml.jinja2"[0m
2026-02-20T18:17:12.2078016Z [36;1mfi[0m
2026-02-20T18:17:12.2078144Z [36;1m[0m
2026-02-20T18:17:12.2078294Z [36;1mjinja2 $TEMPLATE_FILE \[0m
2026-02-20T18:17:12.2078485Z [36;1m  -D size="$size" \[0m
2026-02-20T18:17:12.2078663Z [36;1m  -D replicas="$replicas" \[0m
2026-02-20T18:17:12.2078857Z [36;1m  -D image="$image" \[0m
2026-02-20T18:17:12.2079074Z [36;1m  -D config_file_path="$config_file_path" \[0m
2026-02-20T18:17:12.2079306Z [36;1m  -D npu_per_node="$npu_per_node" \[0m
2026-02-20T18:17:12.2079514Z [36;1m  -D fail_tag="$fail_tag" \[0m
2026-02-20T18:17:12.2079700Z [36;1m  --outfile lws.yaml[0m
2026-02-20T18:17:12.2079867Z [36;1m[0m
2026-02-20T18:17:12.2080012Z [36;1mkubectl apply -f ./lws.yaml[0m
2026-02-20T18:17:12.2080328Z shell: bash -el {0}
2026-02-20T18:17:12.2080484Z ##[endgroup]
2026-02-20T18:17:12.2160221Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-20T18:17:12.2160866Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-20T18:17:12.2161083Z ##[endgroup]
2026-02-20T18:17:12.5640097Z (node:594) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-20T18:17:12.5640806Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-20T18:17:13.5068981Z leaderworkerset.leaderworkerset.x-k8s.io/vllm created
2026-02-20T18:17:13.5245941Z service/vllm-leader created
2026-02-20T18:17:13.9693398Z ##[group]Run POD_PREFIX="${POD_PREFIX:-vllm-0}"
2026-02-20T18:17:13.9693837Z [36;1mPOD_PREFIX="${POD_PREFIX:-vllm-0}"[0m
2026-02-20T18:17:13.9694083Z [36;1mSIZE="2"[0m
2026-02-20T18:17:13.9694353Z [36;1mTIMEOUT=1200  # default timeout 20 minutes[0m
2026-02-20T18:17:13.9694648Z [36;1m[0m
2026-02-20T18:17:13.9695131Z [36;1mecho "Waiting for Pods in namespace [$NAMESPACE] to become Running and Ready (timeout ${TIMEOUT}s)..."[0m
2026-02-20T18:17:13.9695579Z [36;1m[0m
2026-02-20T18:17:13.9695814Z [36;1mSTART_TIME=$(date +%s)[0m
2026-02-20T18:17:13.9696036Z [36;1m[0m
2026-02-20T18:17:13.9696257Z [36;1mwhile true; do[0m
2026-02-20T18:17:13.9696591Z [36;1m  NOW=$(date +%s)[0m
2026-02-20T18:17:13.9696825Z [36;1m  ELAPSED=$((NOW - START_TIME))[0m
2026-02-20T18:17:13.9697140Z [36;1m  if [[ $ELAPSED -ge $TIMEOUT ]]; then[0m
2026-02-20T18:17:13.9697478Z [36;1m    echo "Timeout reached after ${ELAPSED}s"[0m
2026-02-20T18:17:13.9697869Z [36;1m    echo "Dumping pod status for debugging:"[0m
2026-02-20T18:17:13.9698179Z [36;1m    kubectl get pods -n "$NAMESPACE"[0m
2026-02-20T18:17:13.9698497Z [36;1m    kubectl describe pod "$LEADER_POD" -n "$NAMESPACE"[0m
2026-02-20T18:17:13.9698826Z [36;1m    exit 1[0m
2026-02-20T18:17:13.9699050Z [36;1m  fi[0m
2026-02-20T18:17:13.9699255Z [36;1m[0m
2026-02-20T18:17:13.9699484Z [36;1m  # 1) check follower pods[0m
2026-02-20T18:17:13.9699802Z [36;1m  ALL_FOLLOWERS_READY=true[0m
2026-02-20T18:17:13.9700105Z [36;1m  for ((i=1; i<SIZE; i++)); do[0m
2026-02-20T18:17:13.9700388Z [36;1m    POD="${POD_PREFIX}-${i}"[0m
2026-02-20T18:17:13.9700787Z [36;1m    PHASE=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-20T18:17:13.9701444Z [36;1m    READY=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-20T18:17:13.9702119Z [36;1m[0m
2026-02-20T18:17:13.9702418Z [36;1m    echo "Follower [$POD] phase=$PHASE ready=$READY"[0m
2026-02-20T18:17:13.9702712Z [36;1m[0m
2026-02-20T18:17:13.9702983Z [36;1m    if [[ "$PHASE" != "Running" || "$READY" != "true" ]]; then[0m
2026-02-20T18:17:13.9703344Z [36;1m      echo "Follower [$POD] not Ready yet..."[0m
2026-02-20T18:17:13.9703636Z [36;1m      ALL_FOLLOWERS_READY=false[0m
2026-02-20T18:17:13.9703971Z [36;1m      break[0m
2026-02-20T18:17:13.9704168Z [36;1m    fi[0m
2026-02-20T18:17:13.9704415Z [36;1m  done[0m
2026-02-20T18:17:13.9704634Z [36;1m[0m
2026-02-20T18:17:13.9704820Z [36;1m  # 2) check leader pod[0m
2026-02-20T18:17:13.9705417Z [36;1m  LEADER_PHASE=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")[0m
2026-02-20T18:17:13.9706097Z [36;1m  LEADER_READY=$(kubectl get pod "$LEADER_POD" -n "$NAMESPACE" -o jsonpath='{.status.containerStatuses[*].ready}' 2>/dev/null)[0m
2026-02-20T18:17:13.9706551Z [36;1m[0m
2026-02-20T18:17:13.9706776Z [36;1m  echo "Leader [$LEADER_POD] phase=$LEADER_PHASE ready=$LEADER_READY"[0m
2026-02-20T18:17:13.9707039Z [36;1m[0m
2026-02-20T18:17:13.9707252Z [36;1m  if [[ "$LEADER_PHASE" != "Running" || "$LEADER_READY" != "true" ]]; then[0m
2026-02-20T18:17:13.9707540Z [36;1m    echo "Leader not Ready yet..."[0m
2026-02-20T18:17:13.9707743Z [36;1m    ALL_FOLLOWERS_READY=false[0m
2026-02-20T18:17:13.9707930Z [36;1m  fi[0m
2026-02-20T18:17:13.9708062Z [36;1m[0m
2026-02-20T18:17:13.9708232Z [36;1m  if [[ "$ALL_FOLLOWERS_READY" == "true" ]]; then[0m
2026-02-20T18:17:13.9708565Z [36;1m    echo "All follower pods and leader pod are Running and Ready â€” continuing."[0m
2026-02-20T18:17:13.9708849Z [36;1m    break[0m
2026-02-20T18:17:13.9709000Z [36;1m  fi[0m
2026-02-20T18:17:13.9709136Z [36;1m[0m
2026-02-20T18:17:13.9709266Z [36;1m  sleep 2[0m
2026-02-20T18:17:13.9709413Z [36;1mdone[0m
2026-02-20T18:17:13.9709687Z shell: bash -el {0}
2026-02-20T18:17:13.9709833Z env:
2026-02-20T18:17:13.9710158Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-20T18:17:13.9710386Z ##[endgroup]
2026-02-20T18:17:13.9792972Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-20T18:17:13.9793828Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-20T18:17:13.9794097Z ##[endgroup]
2026-02-20T18:17:14.3283304Z (node:672) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-20T18:17:14.3284026Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-20T18:17:14.8546964Z Waiting for Pods in namespace [vllm-project] to become Running and Ready (timeout 1200s)...
2026-02-20T18:17:14.9681905Z Follower [vllm-0-1] phase=Pending ready=
2026-02-20T18:17:14.9682248Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:15.0818891Z Leader [vllm-0] phase=Pending ready=
2026-02-20T18:17:15.0819140Z Leader not Ready yet...
2026-02-20T18:17:17.2319175Z Follower [vllm-0-1] phase=Pending ready=
2026-02-20T18:17:17.2319458Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:17.3511907Z Leader [vllm-0] phase=Pending ready=
2026-02-20T18:17:17.3512246Z Leader not Ready yet...
2026-02-20T18:17:19.4731140Z Follower [vllm-0-1] phase=Pending ready=
2026-02-20T18:17:19.4731426Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:19.5932544Z Leader [vllm-0] phase=Pending ready=
2026-02-20T18:17:19.5932806Z Leader not Ready yet...
2026-02-20T18:17:21.7077403Z Follower [vllm-0-1] phase=Pending ready=
2026-02-20T18:17:21.7077687Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:21.8198159Z Leader [vllm-0] phase=Pending ready=
2026-02-20T18:17:21.8198419Z Leader not Ready yet...
2026-02-20T18:17:23.9338911Z Follower [vllm-0-1] phase=Pending ready=
2026-02-20T18:17:23.9339222Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:24.0440814Z Leader [vllm-0] phase=Pending ready=
2026-02-20T18:17:24.0441405Z Leader not Ready yet...
2026-02-20T18:17:26.1530283Z Follower [vllm-0-1] phase=Pending ready=
2026-02-20T18:17:26.1530597Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:26.2704797Z Leader [vllm-0] phase=Pending ready=
2026-02-20T18:17:26.2705031Z Leader not Ready yet...
2026-02-20T18:17:28.3851316Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:17:28.3851610Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:28.4987814Z Leader [vllm-0] phase=Pending ready=false
2026-02-20T18:17:28.4988061Z Leader not Ready yet...
2026-02-20T18:17:30.6207991Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:17:30.6208277Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:30.7422178Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:17:32.8619532Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:17:32.8619809Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:32.9743006Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:17:35.1070118Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:17:35.1070403Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:35.2306966Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:17:37.3603641Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:17:37.3603918Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:37.4778863Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:17:39.5905377Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:17:39.5905661Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:39.7038385Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:17:41.8166556Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:17:41.8166829Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:41.9379204Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:17:44.0717522Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:17:44.0717894Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:44.1948341Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:17:46.3208668Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:17:46.3209268Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:46.4324529Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:17:48.5556854Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:17:48.5557125Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:48.6698273Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:17:50.7875857Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:17:50.7876151Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:50.9002790Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:17:53.0251069Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:17:53.0251355Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:53.1460599Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:17:55.2728828Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:17:55.2729269Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:55.3909655Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:17:57.5057361Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:17:57.5057669Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:57.6180533Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:17:59.7401260Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:17:59.7401548Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:17:59.8507965Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:18:01.9765287Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:18:01.9765558Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:18:02.0901683Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:18:04.2082587Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:18:04.2082936Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:18:04.3259883Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:18:06.4420377Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:18:06.4420661Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:18:06.5546689Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:18:08.6754532Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:18:08.6754863Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:18:08.7908449Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:18:10.9104827Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:18:10.9105131Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:18:11.0248553Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:18:13.1406872Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:18:13.1407147Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:18:13.2598567Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:18:15.3785405Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:18:15.3785687Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:18:15.4920969Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:18:17.6070611Z Follower [vllm-0-1] phase=Pending ready=false
2026-02-20T18:18:17.6070894Z Follower [vllm-0-1] not Ready yet...
2026-02-20T18:18:17.7224946Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:18:19.8368586Z Follower [vllm-0-1] phase=Running ready=true
2026-02-20T18:18:19.9500978Z Leader [vllm-0] phase=Running ready=true
2026-02-20T18:18:19.9501560Z All follower pods and leader pod are Running and Ready â€” continuing.
2026-02-20T18:18:20.3781496Z ##[group]Run set -euo pipefail
2026-02-20T18:18:20.3781749Z [36;1mset -euo pipefail[0m
2026-02-20T18:18:20.3781920Z [36;1m[0m
2026-02-20T18:18:20.3782172Z [36;1msize="2"[0m
2026-02-20T18:18:20.3782332Z [36;1mpids=()[0m
2026-02-20T18:18:20.3782470Z [36;1m[0m
2026-02-20T18:18:20.3782614Z [36;1mcleanup() {[0m
2026-02-20T18:18:20.3782814Z [36;1m  echo "Cleaning up background log streams..."[0m
2026-02-20T18:18:20.3783160Z [36;1m  for pid in "${pids[@]}"; do[0m
2026-02-20T18:18:20.3783381Z [36;1m    kill "$pid" 2>/dev/null || true[0m
2026-02-20T18:18:20.3783569Z [36;1m  done[0m
2026-02-20T18:18:20.3783713Z [36;1m}[0m
2026-02-20T18:18:20.3783874Z [36;1mtrap cleanup EXIT[0m
2026-02-20T18:18:20.3784035Z [36;1m[0m
2026-02-20T18:18:20.3784191Z [36;1mfor i in $(seq 1 $((size - 1))); do[0m
2026-02-20T18:18:20.3784396Z [36;1m  POD="vllm-0-${i}"[0m
2026-02-20T18:18:20.3784554Z [36;1m[0m
2026-02-20T18:18:20.3784788Z [36;1m  echo "==== Collecting logs from worker pod: $POD ===="[0m
2026-02-20T18:18:20.3785054Z [36;1m  kubectl logs -f "$POD" -n "$NAMESPACE" \[0m
2026-02-20T18:18:20.3785291Z [36;1m    > "/tmp/${POD}_logs.txt" 2>&1 &[0m
2026-02-20T18:18:20.3785475Z [36;1m[0m
2026-02-20T18:18:20.3785610Z [36;1m  pids+=($!)[0m
2026-02-20T18:18:20.3785865Z [36;1mdone[0m
2026-02-20T18:18:20.3786129Z [36;1m[0m
2026-02-20T18:18:20.3786326Z [36;1mecho "==== Streaming logs from leader pod: $LEADER_POD ===="[0m
2026-02-20T18:18:20.3786611Z [36;1mecho "Looking for logs containing: $FAIL_TAG"[0m
2026-02-20T18:18:20.3786818Z [36;1m[0m
2026-02-20T18:18:20.3787053Z [36;1mkubectl logs -f "$LEADER_POD" -n "$NAMESPACE" | while IFS= read -r line; do[0m
2026-02-20T18:18:20.3787349Z [36;1m  echo "$line"[0m
2026-02-20T18:18:20.3787539Z [36;1m  if echo "$line" | grep -q "$FAIL_TAG"; then[0m
2026-02-20T18:18:20.3787752Z [36;1m    exit 1[0m
2026-02-20T18:18:20.3787891Z [36;1m  fi[0m
2026-02-20T18:18:20.3788026Z [36;1mdone[0m
2026-02-20T18:18:20.3788334Z shell: bash -el {0}
2026-02-20T18:18:20.3788473Z env:
2026-02-20T18:18:20.3788667Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-20T18:18:20.3788895Z ##[endgroup]
2026-02-20T18:18:20.3876765Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-20T18:18:20.3877396Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-20T18:18:20.3877623Z ##[endgroup]
2026-02-20T18:18:20.7476941Z (node:778) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-20T18:18:20.7477761Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-20T18:18:21.2796929Z ==== Collecting logs from worker pod: vllm-0-1 ====
2026-02-20T18:18:21.2797586Z ==== Streaming logs from leader pod: vllm-0 ====
2026-02-20T18:18:21.2797892Z Looking for logs containing: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-20T18:18:21.3532617Z /usr/local/Ascend/ascend-toolkit/set_env.sh: line 31: CMAKE_PREFIX_PATH: unbound variable
2026-02-20T18:18:21.3543131Z ====> Check NPU info
2026-02-20T18:18:21.3553156Z +------------------------------------------------------------------------------------------------+
2026-02-20T18:18:21.3563714Z | npu-smi 25.5.0                   Version: 25.5.0                                               |
2026-02-20T18:18:21.3572963Z +---------------------------+---------------+----------------------------------------------------+
2026-02-20T18:18:21.3582986Z | NPU   Name                | Health        | Power(W)    Temp(C)           Hugepages-Usage(page)|
2026-02-20T18:18:21.3593163Z | Chip  Phy-ID              | Bus-Id        | AICore(%)   Memory-Usage(MB)  HBM-Usage(MB)        |
2026-02-20T18:18:21.3603481Z +===========================+===============+====================================================+
2026-02-20T18:18:21.3613191Z | 0     Ascend910           | OK            | 163.2       35                0    / 0             |
2026-02-20T18:18:21.3622964Z | 0     0                   | 0000:9D:00.0  | 0           0    / 0          3151 / 65536         |
2026-02-20T18:18:21.3632794Z +------------------------------------------------------------------------------------------------+
2026-02-20T18:18:21.3643166Z | 0     Ascend910           | OK            | -           36                0    / 0             |
2026-02-20T18:18:21.3652753Z | 1     1                   | 0000:9F:00.0  | 0           0    / 0          2895 / 65536         |
2026-02-20T18:18:21.3662525Z +===========================+===============+====================================================+
2026-02-20T18:18:21.3672521Z | 1     Ascend910           | OK            | 162.3       35                0    / 0             |
2026-02-20T18:18:21.3683850Z | 0     2                   | 0000:99:00.0  | 0           0    / 0          3162 / 65536         |
2026-02-20T18:18:21.3693357Z +------------------------------------------------------------------------------------------------+
2026-02-20T18:18:21.3701322Z | 1     Ascend910           | OK            | -           36                0    / 0             |
2026-02-20T18:18:21.3710551Z | 1     3                   | 0000:9B:00.0  | 0           0    / 0          2886 / 65536         |
2026-02-20T18:18:21.3720690Z +===========================+===============+====================================================+
2026-02-20T18:18:21.3730433Z | 2     Ascend910           | OK            | 164.3       36                0    / 0             |
2026-02-20T18:18:21.3739876Z | 0     4                   | 0000:95:00.0  | 0           0    / 0          3163 / 65536         |
2026-02-20T18:18:21.3749340Z +------------------------------------------------------------------------------------------------+
2026-02-20T18:18:21.3759634Z | 2     Ascend910           | OK            | -           37                0    / 0             |
2026-02-20T18:18:21.3770314Z | 1     5                   | 0000:97:00.0  | 0           0    / 0          2885 / 65536         |
2026-02-20T18:18:21.3780979Z +===========================+===============+====================================================+
2026-02-20T18:18:21.3790928Z | 3     Ascend910           | OK            | 171.8       36                0    / 0             |
2026-02-20T18:18:21.3801481Z | 0     6                   | 0000:91:00.0  | 0           0    / 0          3153 / 65536         |
2026-02-20T18:18:21.3811483Z +------------------------------------------------------------------------------------------------+
2026-02-20T18:18:21.3820762Z | 3     Ascend910           | OK            | -           36                0    / 0             |
2026-02-20T18:18:21.3830370Z | 1     7                   | 0000:93:00.0  | 0           0    / 0          2895 / 65536         |
2026-02-20T18:18:21.3840317Z +===========================+===============+====================================================+
2026-02-20T18:18:21.3849737Z | 4     Ascend910           | OK            | 163.5       36                0    / 0             |
2026-02-20T18:18:21.3858973Z | 0     8                   | 0000:8D:00.0  | 0           0    / 0          3153 / 65536         |
2026-02-20T18:18:21.3868158Z +------------------------------------------------------------------------------------------------+
2026-02-20T18:18:21.3878289Z | 4     Ascend910           | OK            | -           35                0    / 0             |
2026-02-20T18:18:21.3887762Z | 1     9                   | 0000:8F:00.0  | 0           0    / 0          2897 / 65536         |
2026-02-20T18:18:21.3897443Z +===========================+===============+====================================================+
2026-02-20T18:18:21.3907307Z | 5     Ascend910           | OK            | 165.0       34                0    / 0             |
2026-02-20T18:18:21.3917011Z | 0     10                  | 0000:89:00.0  | 0           0    / 0          3150 / 65536         |
2026-02-20T18:18:21.3926698Z +------------------------------------------------------------------------------------------------+
2026-02-20T18:18:21.3935948Z | 5     Ascend910           | OK            | -           36                0    / 0             |
2026-02-20T18:18:21.3944814Z | 1     11                  | 0000:8B:00.0  | 0           0    / 0          2898 / 65536         |
2026-02-20T18:18:21.3954351Z +===========================+===============+====================================================+
2026-02-20T18:18:21.3963638Z | 6     Ascend910           | OK            | 165.5       36                0    / 0             |
2026-02-20T18:18:21.3973841Z | 0     12                  | 0000:85:00.0  | 0           0    / 0          3150 / 65536         |
2026-02-20T18:18:21.3982659Z +------------------------------------------------------------------------------------------------+
2026-02-20T18:18:21.4012602Z | 6     Ascend910           | OK            | -           34                0    / 0             |
2026-02-20T18:18:21.4012921Z | 1     13                  | 0000:87:00.0  | 0           0    / 0          2896 / 65536         |
2026-02-20T18:18:21.4013205Z +===========================+===============+====================================================+
2026-02-20T18:18:21.4022199Z | 7     Ascend910           | OK            | 163.8       36                0    / 0             |
2026-02-20T18:18:21.4030954Z | 0     14                  | 0000:81:00.0  | 0           0    / 0          3159 / 65536         |
2026-02-20T18:18:21.4040767Z +------------------------------------------------------------------------------------------------+
2026-02-20T18:18:21.4050406Z | 7     Ascend910           | OK            | -           36                0    / 0             |
2026-02-20T18:18:21.4059806Z | 1     15                  | 0000:83:00.0  | 0           0    / 0          2883 / 65536         |
2026-02-20T18:18:21.4069517Z +===========================+===============+====================================================+
2026-02-20T18:18:21.4078686Z +---------------------------+---------------+----------------------------------------------------+
2026-02-20T18:18:21.4088171Z | NPU     Chip              | Process id    | Process name             | Process memory(MB)      |
2026-02-20T18:18:21.4097745Z +===========================+===============+====================================================+
2026-02-20T18:18:21.4107102Z | No running processes found in NPU 0                                                            |
2026-02-20T18:18:21.4116980Z +===========================+===============+====================================================+
2026-02-20T18:18:21.4126771Z | No running processes found in NPU 1                                                            |
2026-02-20T18:18:21.4136335Z +===========================+===============+====================================================+
2026-02-20T18:18:21.4146372Z | No running processes found in NPU 2                                                            |
2026-02-20T18:18:21.4156426Z +===========================+===============+====================================================+
2026-02-20T18:18:21.4166396Z | No running processes found in NPU 3                                                            |
2026-02-20T18:18:21.4175748Z +===========================+===============+====================================================+
2026-02-20T18:18:21.4185473Z | No running processes found in NPU 4                                                            |
2026-02-20T18:18:21.4196134Z +===========================+===============+====================================================+
2026-02-20T18:18:21.4205513Z | No running processes found in NPU 5                                                            |
2026-02-20T18:18:21.4214800Z +===========================+===============+====================================================+
2026-02-20T18:18:21.4224254Z | No running processes found in NPU 6                                                            |
2026-02-20T18:18:21.4233671Z +===========================+===============+====================================================+
2026-02-20T18:18:21.4243770Z | No running processes found in NPU 7                                                            |
2026-02-20T18:18:21.4253246Z +===========================+===============+====================================================+
2026-02-20T18:18:21.4262274Z package_name=Ascend-cann-toolkit
2026-02-20T18:18:21.4271316Z version=8.5.0
2026-02-20T18:18:21.4280766Z innerversion=V100R001C25SPC001B232
2026-02-20T18:18:21.4290127Z compatible_version=[V100R001C15],[V100R001C18],[V100R001C19],[V100R001C20],[V100R001C21],[V100R001C23]
2026-02-20T18:18:21.4299044Z arch=aarch64
2026-02-20T18:18:21.4308111Z os=linux
2026-02-20T18:18:21.4317360Z path=/usr/local/Ascend/cann-8.5.0
2026-02-20T18:18:21.4326832Z ====> Configure mirrors and git proxy
2026-02-20T18:18:21.4335984Z Writing to /root/.config/pip/pip.conf
2026-02-20T18:18:21.4345214Z Installed vLLM-related Python packages:
2026-02-20T18:18:21.4354945Z ais_bench_benchmark               3.0.20250930                /vllm-workspace/vllm-ascend/benchmark
2026-02-20T18:18:21.4364368Z vllm                              0.15.0+empty                /vllm-workspace/vllm
2026-02-20T18:18:21.4373666Z vllm_ascend                       0.14.0rc2.dev171+gf0caeeadc /vllm-workspace/vllm-ascend
2026-02-20T18:18:21.4382606Z 
2026-02-20T18:18:21.4391751Z ============================
2026-02-20T18:18:21.4401390Z vLLM Git information
2026-02-20T18:18:21.4410174Z ============================
2026-02-20T18:18:21.4419298Z Branch:      HEAD
2026-02-20T18:18:21.4428688Z Commit hash: f176443446f659dbab5315e056e605d8984fd976
2026-02-20T18:18:21.4438081Z Author:      TJian <tunjian.tan@embeddedllm.com>
2026-02-20T18:18:21.4446930Z Date:        2026-01-29 14:45:42 +0800
2026-02-20T18:18:21.4456403Z Message:     [Release] [CI] Optim release pipeline (#33156)
2026-02-20T18:18:21.4465077Z Tags:        v0.15.0
2026-02-20T18:18:21.4475897Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm.git (fetch)
2026-02-20T18:18:21.4484761Z 
2026-02-20T18:18:21.4493428Z 
2026-02-20T18:18:21.4502973Z ============================
2026-02-20T18:18:21.4511844Z vLLM-Ascend Git information
2026-02-20T18:18:21.4521667Z ============================
2026-02-20T18:18:21.4530611Z Branch:      main
2026-02-20T18:18:21.4540260Z Commit hash: f0caeeadcb37261beebd4a6e32934fa9f460db98
2026-02-20T18:18:21.4549155Z Author:      Nengjun Ma <nengjunma@outlook.com>
2026-02-20T18:18:21.4558804Z Date:        2026-02-14 18:54:04 +0800
2026-02-20T18:18:21.4568099Z Message:     [CI] unlock when load model (#6771)
2026-02-20T18:18:21.4577071Z Tags:        
2026-02-20T18:18:21.4586633Z Remote:      origin	https://ghfast.top/https://github.com/vllm-project/vllm-ascend (fetch)
2026-02-20T18:18:21.4595605Z 
2026-02-20T18:18:21.4605066Z ====> Check triton ascend info
2026-02-20T18:18:21.4614228Z Ubuntu clang version 15.0.7
2026-02-20T18:18:21.4623520Z Target: aarch64-unknown-linux-gnu
2026-02-20T18:18:21.4632585Z Thread model: posix
2026-02-20T18:18:21.4643261Z InstalledDir: /usr/bin
2026-02-20T18:18:21.4653245Z Found candidate GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-20T18:18:21.4662269Z Selected GCC installation: /usr/bin/../lib/gcc/aarch64-linux-gnu/11
2026-02-20T18:18:21.4671005Z Candidate multilib: .;@m64
2026-02-20T18:18:21.4683604Z Selected multilib: .;@m64
2026-02-20T18:18:21.4693801Z /usr/local/Ascend/cann-8.5.0/tools/bishengir/bin/bishengir-compile
2026-02-20T18:18:21.4702829Z Name: triton-ascend
2026-02-20T18:18:21.4711832Z Version: 3.2.0
2026-02-20T18:18:21.4721811Z Summary: A language and compiler for custom Deep Learning operations on Ascend hardwares
2026-02-20T18:18:21.4731127Z Home-page: https://gitcode.com/Ascend/triton-ascend/
2026-02-20T18:18:21.4739904Z Author: 
2026-02-20T18:18:21.4748942Z Author-email: 
2026-02-20T18:18:21.4758098Z License: 
2026-02-20T18:18:21.4768931Z Location: /usr/local/python3.11.14/lib/python3.11/site-packages
2026-02-20T18:18:21.4778409Z Requires: 
2026-02-20T18:18:21.4788105Z Required-by: vllm_ascend
2026-02-20T18:18:21.4798602Z INFO 02-20 18:17:51 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:18:21.4807833Z INFO 02-20 18:17:51 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:18:21.4817529Z INFO 02-20 18:17:51 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:18:21.4826636Z INFO 02-20 18:17:51 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:18:21.4836690Z ============================= test session starts ==============================
2026-02-20T18:18:21.4846746Z platform linux -- Python 3.11.14, pytest-8.4.2, pluggy-1.6.0 -- /usr/local/python3.11.14/bin/python3
2026-02-20T18:18:21.4855553Z cachedir: .pytest_cache
2026-02-20T18:18:21.4864976Z rootdir: /vllm-workspace/vllm-ascend
2026-02-20T18:18:21.4874354Z configfile: pyproject.toml
2026-02-20T18:18:21.4883561Z plugins: cov-7.0.0, asyncio-1.3.0, mock-3.15.1, anyio-4.12.1
2026-02-20T18:18:21.4893516Z asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
2026-02-20T18:18:21.4902490Z collecting ... collected 1 item
2026-02-20T18:18:21.4911426Z 
2026-02-20T18:18:21.4921805Z [2026-02-20 18:17:58] INFO multi_node_config.py:294: Loading config yaml: tests/e2e/nightly/multi_node/config/DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-20T18:18:21.4930961Z [2026-02-20 18:17:58] INFO multi_node_config.py:351: Resolving cluster IPs via DNS...
2026-02-20T18:18:24.0327344Z [2026-02-20 18:18:24] INFO multi_node_config.py:212: Node 0 envs: {'HCCL_OP_EXPANSION_MODE': 'AIV', 'VLLM_USE_MODELSCOPE': 'True', 'HCCL_BUFFSIZE': '1024', 'SERVER_PORT': '8080', 'OMP_PROC_BIND': 'False', 'OMP_NUM_THREADS': '1', 'PYTORCH_NPU_ALLOC_CONF': 'expandable_segments:True', 'VLLM_ASCEND_ENABLE_FLASHCOMM1': '1', 'ASCEND_A3_EBA_ENABLE': '1', 'HCCL_IF_IP': '10.0.0.78', 'HCCL_SOCKET_IFNAME': 'eth0', 'GLOO_SOCKET_IFNAME': 'eth0', 'TP_SOCKET_IFNAME': 'eth0', 'LOCAL_IP': '10.0.0.78', 'NIC_NAME': 'eth0', 'MASTER_IP': '10.0.0.78'}
2026-02-20T18:18:24.0348552Z [2026-02-20 18:18:24] INFO multi_node_config.py:137: Not launching proxy on non-master node
2026-02-20T18:18:24.0369649Z [2026-02-20 18:18:24] INFO conftest.py:241: Starting server with command: vllm serve vllm-ascend/DeepSeek-V3.2-W8A8 --host 0.0.0.0 --port 8080 --data-parallel-size 4 --data-parallel-size-local 2 --data-parallel-address 10.0.0.78 --data-parallel-rpc-port 13399 --tensor-parallel-size 8 --quantization ascend --seed 1024 --enable-expert-parallel --max-num-seqs 16 --max-model-len 8192 --max-num-batched-tokens 4096 --no-enable-prefix-caching --gpu-memory-utilization 0.85 --trust-remote-code --speculative-config {"num_speculative_tokens": 2, "method":"deepseek_mtp"} --compilation-config {"cudagraph_capture_sizes": [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], "cudagraph_mode": "FULL_DECODE_ONLY"} --additional-config {"layer_sharding": ["q_b_proj", "o_proj"]} --tokenizer-mode deepseek_v32 --reasoning-parser deepseek_v3
2026-02-20T18:18:28.2975269Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node INFO 02-20 18:18:28 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:18:28.2980280Z INFO 02-20 18:18:28 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:18:28.2990028Z INFO 02-20 18:18:28 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:18:28.3037505Z INFO 02-20 18:18:28 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:18:34.5590826Z 2026-02-20 18:18:34,557 - 138 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:18:34.5906805Z INFO 02-20 18:18:34 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:18:34.7325808Z INFO 02-20 18:18:34 [serve.py:100] Defaulting api_server_count to data_parallel_size (4).
2026-02-20T18:18:34.7347434Z INFO 02-20 18:18:34 [utils.py:325] 
2026-02-20T18:18:34.7358259Z INFO 02-20 18:18:34 [utils.py:325]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
2026-02-20T18:18:34.7369053Z INFO 02-20 18:18:34 [utils.py:325]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0
2026-02-20T18:18:34.7382385Z INFO 02-20 18:18:34 [utils.py:325]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   vllm-ascend/DeepSeek-V3.2-W8A8
2026-02-20T18:18:34.7392341Z INFO 02-20 18:18:34 [utils.py:325]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
2026-02-20T18:18:34.7402921Z INFO 02-20 18:18:34 [utils.py:325] 
2026-02-20T18:18:34.7422093Z INFO 02-20 18:18:34 [utils.py:261] non-default args: {'model_tag': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'api_server_count': 4, 'host': '0.0.0.0', 'port': 8080, 'model': 'vllm-ascend/DeepSeek-V3.2-W8A8', 'tokenizer_mode': 'deepseek_v32', 'trust_remote_code': True, 'seed': 1024, 'max_model_len': 8192, 'quantization': 'ascend', 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 8, 'data_parallel_size': 4, 'data_parallel_size_local': 2, 'data_parallel_address': '10.0.0.78', 'data_parallel_rpc_port': 13399, 'enable_expert_parallel': True, 'gpu_memory_utilization': 0.85, 'enable_prefix_caching': False, 'max_num_batched_tokens': 4096, 'max_num_seqs': 16, 'speculative_config': {'num_speculative_tokens': 2, 'method': 'deepseek_mtp'}, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}, 'additional_config': {'layer_sharding': ['q_b_proj', 'o_proj']}}
2026-02-20T18:18:34.7890512Z 2026-02-20 18:18:34,787 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-20T18:18:34.7983711Z INFO 02-20 18:18:34 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-20T18:18:34.8019248Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:18:34.8038274Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:18:34.8059691Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:18:34.8070118Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-20T18:18:34.8305687Z INFO 02-20 18:18:34 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-20T18:18:34.8315100Z INFO 02-20 18:18:34 [model.py:1561] Using max model len 8192
2026-02-20T18:18:35.0926967Z WARNING 02-20 18:18:35 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-20T18:18:35.0950503Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:18:35.0961824Z The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:18:35.0972524Z You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-20T18:18:35.1059690Z INFO 02-20 18:18:35 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-20T18:18:35.1069633Z INFO 02-20 18:18:35 [model.py:1561] Using max model len 163840
2026-02-20T18:18:35.1081269Z WARNING 02-20 18:18:35 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-20T18:18:35.1090911Z INFO 02-20 18:18:35 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-20T18:18:35.7112469Z INFO 02-20 18:18:35 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-20T18:18:35.7120703Z INFO 02-20 18:18:35 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-20T18:18:35.7140195Z WARNING 02-20 18:18:35 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-20T18:18:35.7150015Z WARNING 02-20 18:18:35 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-20T18:18:35.7159542Z INFO 02-20 18:18:35 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:18:35.7170706Z INFO 02-20 18:18:35 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:18:35.7181252Z INFO 02-20 18:18:35 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:18:35.7192856Z WARNING 02-20 18:18:35 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-20T18:18:35.7202494Z INFO 02-20 18:18:35 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-20T18:18:35.7212234Z WARNING 02-20 18:18:35 [platform.py:335] [91m
2026-02-20T18:18:35.7222656Z WARNING 02-20 18:18:35 [platform.py:335]             **********************************************************************************
2026-02-20T18:18:35.7232920Z WARNING 02-20 18:18:35 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-20T18:18:35.7243704Z WARNING 02-20 18:18:35 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-20T18:18:35.7253792Z WARNING 02-20 18:18:35 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-20T18:18:35.7263503Z WARNING 02-20 18:18:35 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-20T18:18:35.7274015Z WARNING 02-20 18:18:35 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-20T18:18:35.7284525Z WARNING 02-20 18:18:35 [platform.py:335]             * batch size for graph capture.
2026-02-20T18:18:35.7293791Z WARNING 02-20 18:18:35 [platform.py:335]             * For more details, please refer to:
2026-02-20T18:18:35.7304558Z WARNING 02-20 18:18:35 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-20T18:18:35.7314551Z WARNING 02-20 18:18:35 [platform.py:335]             **********************************************************************************[0m
2026-02-20T18:18:35.7324662Z WARNING 02-20 18:18:35 [platform.py:335]             
2026-02-20T18:18:35.7335320Z INFO 02-20 18:18:35 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-20T18:18:35.7345359Z INFO 02-20 18:18:35 [utils.py:851] Started DP Coordinator process (PID: 151)
2026-02-20T18:18:40.3958864Z INFO 02-20 18:18:40 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:18:40.3970017Z INFO 02-20 18:18:40 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:18:40.3982495Z INFO 02-20 18:18:40 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:18:40.4027185Z INFO 02-20 18:18:40 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:18:40.4086300Z INFO 02-20 18:18:40 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:18:40.4096167Z INFO 02-20 18:18:40 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:18:40.4105093Z INFO 02-20 18:18:40 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:18:40.4163069Z INFO 02-20 18:18:40 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:18:50.2327896Z INFO 02-20 18:18:50 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:18:50.2339323Z INFO 02-20 18:18:50 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:18:50.2349749Z INFO 02-20 18:18:50 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:18:50.2395581Z INFO 02-20 18:18:50 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:18:55.3913020Z INFO 02-20 18:18:55 [utils.py:218] Started 4 API server processes
2026-02-20T18:19:00.1064561Z INFO 02-20 18:19:00 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:19:00.1074681Z INFO 02-20 18:19:00 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:19:00.1085912Z INFO 02-20 18:19:00 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:19:00.1096298Z INFO 02-20 18:19:00 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:19:00.1135517Z INFO 02-20 18:19:00 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:19:00.1137680Z INFO 02-20 18:19:00 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:19:00.1140928Z INFO 02-20 18:19:00 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:19:00.1152762Z INFO 02-20 18:19:00 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:19:00.1717492Z INFO 02-20 18:19:00 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:19:00.1726116Z INFO 02-20 18:19:00 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:19:00.1735245Z INFO 02-20 18:19:00 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:19:00.1795569Z INFO 02-20 18:19:00 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:19:00.2794865Z INFO 02-20 18:19:00 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:19:00.2803977Z INFO 02-20 18:19:00 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:19:00.2816062Z INFO 02-20 18:19:00 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:19:00.2875603Z INFO 02-20 18:19:00 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:19:06.2868391Z [0;36m(ApiServer_0 pid=184)[0;0m 2026-02-20 18:19:06,284 - 184 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:19:06.3054839Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-20 18:19:06 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:19:06.3090922Z [0;36m(ApiServer_3 pid=187)[0;0m 2026-02-20 18:19:06,307 - 187 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:19:06.3239325Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-20 18:19:06 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:19:06.3319875Z [0;36m(ApiServer_0 pid=184)[0;0m 2026-02-20 18:19:06,330 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-20T18:19:06.3372953Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-20 18:19:06 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-20T18:19:06.3937748Z [0;36m(ApiServer_3 pid=187)[0;0m 2026-02-20 18:19:06,392 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-20T18:19:06.4083828Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-20 18:19:06 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-20T18:19:06.4352487Z [0;36m(ApiServer_0 pid=184)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:19:06.4375764Z [0;36m(ApiServer_0 pid=184)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:19:06.4404108Z [0;36m(ApiServer_0 pid=184)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:19:06.4414111Z [0;36m(ApiServer_0 pid=184)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-20T18:19:06.4483200Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-20 18:19:06 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-20T18:19:06.4507682Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-20 18:19:06 [model.py:1561] Using max model len 8192
2026-02-20T18:19:06.4836464Z [0;36m(ApiServer_1 pid=185)[0;0m 2026-02-20 18:19:06,482 - 185 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:19:06.4983383Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-20 18:19:06 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:19:06.5004850Z [0;36m(ApiServer_3 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:19:06.5015266Z [0;36m(ApiServer_3 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:19:06.5025497Z [0;36m(ApiServer_3 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:19:06.5035981Z [0;36m(ApiServer_3 pid=187)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-20T18:19:06.5080152Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-20 18:19:06 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-20T18:19:06.5102559Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-20 18:19:06 [model.py:1561] Using max model len 8192
2026-02-20T18:19:06.5147873Z [0;36m(ApiServer_1 pid=185)[0;0m 2026-02-20 18:19:06,513 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-20T18:19:06.5221474Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-20 18:19:06 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-20T18:19:06.5548363Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-20 18:19:06 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-20T18:19:06.5570660Z [0;36m(ApiServer_0 pid=184)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:19:06.5579692Z [0;36m(ApiServer_0 pid=184)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:19:06.5590986Z [0;36m(ApiServer_0 pid=184)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-20T18:19:06.5633933Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-20 18:19:06 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-20T18:19:06.5657392Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-20 18:19:06 [model.py:1561] Using max model len 163840
2026-02-20T18:19:06.5667168Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-20 18:19:06 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-20T18:19:06.5676557Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-20 18:19:06 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-20T18:19:06.6130479Z [0;36m(ApiServer_2 pid=186)[0;0m 2026-02-20 18:19:06,609 - 186 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:19:06.6144024Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-20 18:19:06 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-20T18:19:06.6165167Z [0;36m(ApiServer_3 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:19:06.6174914Z [0;36m(ApiServer_3 pid=187)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:19:06.6192433Z [0;36m(ApiServer_3 pid=187)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-20T18:19:06.6210472Z [0;36m(ApiServer_1 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:19:06.6239338Z [0;36m(ApiServer_1 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:19:06.6248926Z [0;36m(ApiServer_1 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:19:06.6258324Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-20 18:19:06 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-20T18:19:06.6268532Z [0;36m(ApiServer_1 pid=185)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-20T18:19:06.6277926Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-20 18:19:06 [model.py:1561] Using max model len 163840
2026-02-20T18:19:06.6288437Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-20 18:19:06 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-20T18:19:06.6298333Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-20 18:19:06 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-20T18:19:06.6307744Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-20 18:19:06 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:19:06.6318068Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-20 18:19:06 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-20T18:19:06.6328155Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-20 18:19:06 [model.py:1561] Using max model len 8192
2026-02-20T18:19:06.6423014Z [0;36m(ApiServer_2 pid=186)[0;0m 2026-02-20 18:19:06,641 - modelscope - WARNING - We can not confirm the cached file is for revision: master
2026-02-20T18:19:06.6491514Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-20 18:19:06 [arg_utils.py:602] HF_HUB_OFFLINE is True, replace model_id [vllm-ascend/DeepSeek-V3.2-W8A8] to model_path [/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8]
2026-02-20T18:19:06.7396968Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-20 18:19:06 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-20T18:19:06.7421173Z [0;36m(ApiServer_1 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:19:06.7430984Z [0;36m(ApiServer_1 pid=185)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:19:06.7440600Z [0;36m(ApiServer_1 pid=185)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-20T18:19:06.7483282Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-20 18:19:06 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-20T18:19:06.7505543Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-20 18:19:06 [model.py:1561] Using max model len 163840
2026-02-20T18:19:06.7515706Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-20 18:19:06 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-20T18:19:06.7524952Z [0;36m(ApiServer_2 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:19:06.7534729Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-20 18:19:06 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-20T18:19:06.7544863Z [0;36m(ApiServer_2 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:19:06.7554643Z [0;36m(ApiServer_2 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:19:06.7564948Z [0;36m(ApiServer_2 pid=186)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-20T18:19:06.7588697Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-20 18:19:06 [model.py:541] Resolved architecture: DeepseekV32ForCausalLM
2026-02-20T18:19:06.7622444Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-20 18:19:06 [model.py:1561] Using max model len 8192
2026-02-20T18:19:06.8740929Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-20 18:19:06 [speculative.py:270] method `deepseek_mtp` is deprecated and replaced with mtp.
2026-02-20T18:19:06.8765147Z [0;36m(ApiServer_2 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:19:06.8775311Z [0;36m(ApiServer_2 pid=186)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
2026-02-20T18:19:06.8786287Z [0;36m(ApiServer_2 pid=186)[0;0m You are using a model of type deepseek_v32 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.
2026-02-20T18:19:06.8824020Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-20 18:19:06 [model.py:541] Resolved architecture: DeepSeekMTPModel
2026-02-20T18:19:06.8848963Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-20 18:19:06 [model.py:1561] Using max model len 163840
2026-02-20T18:19:06.8859592Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-20 18:19:06 [speculative.py:388] Enabling num_speculative_tokens > 1 will runmultiple times of forward on same MTP layer,which may result in lower acceptance rate
2026-02-20T18:19:06.8868349Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-20 18:19:06 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=4096.
2026-02-20T18:19:07.0540422Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-20 18:19:07 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-20T18:19:07.0549032Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-20 18:19:07 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-20T18:19:07.0592247Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-20 18:19:07 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-20T18:19:07.0601574Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-20 18:19:07 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-20T18:19:07.0610258Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-20 18:19:07 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:19:07.0619776Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-20 18:19:07 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:19:07.0630209Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-20 18:19:07 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:19:07.0640241Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-20 18:19:07 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-20T18:19:07.0650160Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-20 18:19:07 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-20T18:19:07.0659289Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-20 18:19:07 [platform.py:335] [91m
2026-02-20T18:19:07.0669256Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             **********************************************************************************
2026-02-20T18:19:07.0678321Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-20T18:19:07.0688771Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-20T18:19:07.0698404Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-20T18:19:07.0709210Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-20T18:19:07.0718387Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-20T18:19:07.0727699Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * batch size for graph capture.
2026-02-20T18:19:07.0737549Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * For more details, please refer to:
2026-02-20T18:19:07.0747855Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-20T18:19:07.0758603Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             **********************************************************************************[0m
2026-02-20T18:19:07.0769065Z [0;36m(ApiServer_0 pid=184)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             
2026-02-20T18:19:07.0779506Z [0;36m(ApiServer_0 pid=184)[0;0m INFO 02-20 18:19:07 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-20T18:19:07.0893184Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-20 18:19:07 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-20T18:19:07.0903371Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-20 18:19:07 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-20T18:19:07.0918924Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-20 18:19:07 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-20T18:19:07.0928937Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-20 18:19:07 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-20T18:19:07.0938326Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-20 18:19:07 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:19:07.0947412Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-20 18:19:07 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:19:07.0958561Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-20 18:19:07 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:19:07.0968338Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-20 18:19:07 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-20T18:19:07.0978113Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-20 18:19:07 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-20T18:19:07.0987073Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-20 18:19:07 [platform.py:335] [91m
2026-02-20T18:19:07.0997134Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             **********************************************************************************
2026-02-20T18:19:07.1006872Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-20T18:19:07.1016729Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-20T18:19:07.1026480Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-20T18:19:07.1036652Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-20T18:19:07.1046507Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-20T18:19:07.1055942Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * batch size for graph capture.
2026-02-20T18:19:07.1065678Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * For more details, please refer to:
2026-02-20T18:19:07.1076547Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-20T18:19:07.1085815Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             **********************************************************************************[0m
2026-02-20T18:19:07.1095401Z [0;36m(ApiServer_3 pid=187)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             
2026-02-20T18:19:07.1104556Z [0;36m(ApiServer_3 pid=187)[0;0m INFO 02-20 18:19:07 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-20T18:19:07.1786316Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-20 18:19:07 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-20T18:19:07.1797692Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-20 18:19:07 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-20T18:19:07.1809954Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-20 18:19:07 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-20T18:19:07.1819392Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-20 18:19:07 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-20T18:19:07.1829689Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-20 18:19:07 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:19:07.1842744Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-20 18:19:07 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:19:07.1853183Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-20 18:19:07 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:19:07.1863036Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-20 18:19:07 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-20T18:19:07.1872663Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-20 18:19:07 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-20T18:19:07.1881796Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-20 18:19:07 [platform.py:335] [91m
2026-02-20T18:19:07.1891285Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             **********************************************************************************
2026-02-20T18:19:07.1900982Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-20T18:19:07.1911372Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-20T18:19:07.1922576Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-20T18:19:07.1932640Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-20T18:19:07.1942647Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-20T18:19:07.1952424Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * batch size for graph capture.
2026-02-20T18:19:07.1963184Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * For more details, please refer to:
2026-02-20T18:19:07.1973341Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-20T18:19:07.1982754Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             **********************************************************************************[0m
2026-02-20T18:19:07.1992305Z [0;36m(ApiServer_1 pid=185)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             
2026-02-20T18:19:07.2003172Z [0;36m(ApiServer_1 pid=185)[0;0m INFO 02-20 18:19:07 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-20T18:19:07.3123434Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-20 18:19:07 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-20T18:19:07.3132791Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-20 18:19:07 [vllm.py:634] Disabling NCCL for DP synchronization when using async scheduling.
2026-02-20T18:19:07.3143293Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-20 18:19:07 [platform.py:730] Speculative quantization is set but Ascend automatically uses the main model's quantization method. Resetting to None.
2026-02-20T18:19:07.3153080Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-20 18:19:07 [platform.py:771] Ignored parameter 'use_trtllm_ragged_deepseek_prefill'. This is a GPU-specific feature not supported on Ascend. Resetting to False.
2026-02-20T18:19:07.3163600Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-20 18:19:07 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:19:07.3172473Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-20 18:19:07 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:19:07.3183165Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-20 18:19:07 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:19:07.3192456Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-20 18:19:07 [vllm.py:1062] Batch sizes [3, 6, 9, 12, 15, 18, 21, 27, 30, 33, 36, 39, 42, 45] are removed because they are not multiple of tp_size 8 when sequence parallelism is enabled
2026-02-20T18:19:07.3201705Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-20 18:19:07 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-20T18:19:07.3211169Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-20 18:19:07 [platform.py:335] [91m
2026-02-20T18:19:07.3221174Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             **********************************************************************************
2026-02-20T18:19:07.3230396Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-20T18:19:07.3240418Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-20T18:19:07.3250389Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-20T18:19:07.3260014Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-20T18:19:07.3269990Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-20T18:19:07.3280149Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * batch size for graph capture.
2026-02-20T18:19:07.3289350Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * For more details, please refer to:
2026-02-20T18:19:07.3299760Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-20T18:19:07.3309156Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             **********************************************************************************[0m
2026-02-20T18:19:07.3318833Z [0;36m(ApiServer_2 pid=186)[0;0m WARNING 02-20 18:19:07 [platform.py:335]             
2026-02-20T18:19:07.3328387Z [0;36m(ApiServer_2 pid=186)[0;0m INFO 02-20 18:19:07 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-20T18:19:11.3372133Z [0;36m(EngineCore_DP0 pid=154)[0;0m 2026-02-20 18:19:11,335 - 154 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:19:11.3391234Z [0;36m(EngineCore_DP1 pid=173)[0;0m 2026-02-20 18:19:11,335 - 173 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:19:11.3420305Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-20 18:19:11 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:19:11.3429488Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-20 18:19:11 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:19:11.3453458Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-20 18:19:11 [core.py:96] Initializing a V1 LLM engine (v0.15.0) with config: model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', speculative_config=SpeculativeConfig(method='mtp', model='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', num_spec_tokens=2), tokenizer='/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8', skip_tokenizer_init=False, tokenizer_mode=deepseek_v32, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=4, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=bfloat16, device_config=npu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=1024, served_model_name=/root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'vllm_ascend.compilation.compiler_interface.AscendCompiler', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_DECODE_ONLY: (2, 0)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [24, 48], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 48, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
2026-02-20T18:19:15.9129413Z INFO 02-20 18:19:15 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:19:15.9140036Z INFO 02-20 18:19:15 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:19:15.9155233Z INFO 02-20 18:19:15 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:19:15.9260164Z INFO 02-20 18:19:15 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:19:15.9805115Z INFO 02-20 18:19:15 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:19:15.9815165Z INFO 02-20 18:19:15 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:19:15.9825745Z INFO 02-20 18:19:15 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:19:15.9879347Z INFO 02-20 18:19:15 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:19:21.0235720Z 2026-02-20 18:19:21,021 - 257 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:19:21.0293076Z INFO 02-20 18:19:21 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:19:21.0434723Z 2026-02-20 18:19:21,042 - 258 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:19:21.0494980Z INFO 02-20 18:19:21 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:19:23.0158793Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-20T18:19:23.0167335Z   warnings.warn(
2026-02-20T18:19:23.0178565Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-20T18:19:23.0188621Z   warnings.warn(
2026-02-20T18:19:25.6193437Z INFO 02-20 18:19:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:19:25.6204020Z INFO 02-20 18:19:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:19:25.6216420Z INFO 02-20 18:19:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:19:25.6263194Z INFO 02-20 18:19:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:19:25.7951773Z INFO 02-20 18:19:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:19:25.7960972Z INFO 02-20 18:19:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:19:25.7973256Z INFO 02-20 18:19:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:19:25.8030820Z INFO 02-20 18:19:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:19:26.5476941Z INFO 02-20 18:19:26 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:19:26.5484621Z INFO 02-20 18:19:26 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:19:26.5495293Z INFO 02-20 18:19:26 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:19:26.5503633Z INFO 02-20 18:19:26 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:19:26.5513468Z INFO 02-20 18:19:26 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:19:26.5524250Z INFO 02-20 18:19:26 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:19:26.5961432Z INFO 02-20 18:19:26 [parallel_state.py:1212] world_size=32 rank=8 local_rank=0 distributed_init_method=tcp://10.0.0.78:39341 backend=hccl
2026-02-20T18:19:26.5978650Z INFO 02-20 18:19:26 [parallel_state.py:1212] world_size=32 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.78:39341 backend=hccl
2026-02-20T18:19:30.6325634Z 2026-02-20 18:19:30,630 - 275 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:19:30.6395596Z INFO 02-20 18:19:30 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:19:30.8327211Z 2026-02-20 18:19:30,830 - 276 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:19:30.8360264Z INFO 02-20 18:19:30 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:19:32.0285444Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-20T18:19:32.0291038Z   warnings.warn(
2026-02-20T18:19:32.1126844Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-20T18:19:32.1133597Z   warnings.warn(
2026-02-20T18:19:34.1153434Z INFO 02-20 18:19:34 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:19:34.1162786Z INFO 02-20 18:19:34 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:19:34.1173645Z INFO 02-20 18:19:34 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:19:34.2272665Z INFO 02-20 18:19:34 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:19:34.2280987Z INFO 02-20 18:19:34 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:19:34.2292224Z INFO 02-20 18:19:34 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:19:34.5455630Z INFO 02-20 18:19:34 [parallel_state.py:1212] world_size=32 rank=1 local_rank=1 distributed_init_method=tcp://10.0.0.78:39341 backend=hccl
2026-02-20T18:19:34.6741538Z INFO 02-20 18:19:34 [parallel_state.py:1212] world_size=32 rank=9 local_rank=1 distributed_init_method=tcp://10.0.0.78:39341 backend=hccl
2026-02-20T18:19:35.4062411Z INFO 02-20 18:19:35 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:19:35.4069587Z INFO 02-20 18:19:35 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:19:35.4078896Z INFO 02-20 18:19:35 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:19:35.4180749Z INFO 02-20 18:19:35 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:19:35.5588285Z INFO 02-20 18:19:35 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:19:35.5597405Z INFO 02-20 18:19:35 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:19:35.5607715Z INFO 02-20 18:19:35 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:19:35.5666245Z INFO 02-20 18:19:35 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:19:40.6623565Z 2026-02-20 18:19:40,660 - 372 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:19:40.6679233Z INFO 02-20 18:19:40 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:19:40.6730951Z 2026-02-20 18:19:40,671 - 369 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:19:40.6793739Z INFO 02-20 18:19:40 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:19:42.0029742Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-20T18:19:42.0039476Z   warnings.warn(
2026-02-20T18:19:42.0055132Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-20T18:19:42.0065926Z   warnings.warn(
2026-02-20T18:19:44.1051349Z INFO 02-20 18:19:44 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:19:44.1061898Z INFO 02-20 18:19:44 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:19:44.1074340Z INFO 02-20 18:19:44 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:19:44.1382542Z INFO 02-20 18:19:44 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:19:44.1391610Z INFO 02-20 18:19:44 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:19:44.1402801Z INFO 02-20 18:19:44 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:19:44.5255252Z INFO 02-20 18:19:44 [parallel_state.py:1212] world_size=32 rank=2 local_rank=2 distributed_init_method=tcp://10.0.0.78:39341 backend=hccl
2026-02-20T18:19:44.5961118Z INFO 02-20 18:19:44 [parallel_state.py:1212] world_size=32 rank=10 local_rank=2 distributed_init_method=tcp://10.0.0.78:39341 backend=hccl
2026-02-20T18:19:45.1372070Z INFO 02-20 18:19:45 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:19:45.1372474Z INFO 02-20 18:19:45 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:19:45.1372909Z INFO 02-20 18:19:45 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:19:45.1387685Z INFO 02-20 18:19:45 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:19:45.2348160Z INFO 02-20 18:19:45 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:19:45.2353749Z INFO 02-20 18:19:45 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:19:45.2363457Z INFO 02-20 18:19:45 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:19:45.2373410Z INFO 02-20 18:19:45 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:19:50.1620994Z 2026-02-20 18:19:50,159 - 474 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:19:50.1643691Z 2026-02-20 18:19:50,159 - 473 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:19:50.1724822Z INFO 02-20 18:19:50 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:19:50.1733505Z INFO 02-20 18:19:50 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:19:51.4586893Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-20T18:19:51.4591974Z   warnings.warn(
2026-02-20T18:19:51.4628527Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-20T18:19:51.4637350Z   warnings.warn(
2026-02-20T18:19:53.6334522Z INFO 02-20 18:19:53 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:19:53.6344242Z INFO 02-20 18:19:53 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:19:53.6355099Z INFO 02-20 18:19:53 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:19:53.6366685Z INFO 02-20 18:19:53 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:19:53.6376743Z INFO 02-20 18:19:53 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:19:53.6387858Z INFO 02-20 18:19:53 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:19:54.0609987Z INFO 02-20 18:19:54 [parallel_state.py:1212] world_size=32 rank=11 local_rank=3 distributed_init_method=tcp://10.0.0.78:39341 backend=hccl
2026-02-20T18:19:54.0743348Z INFO 02-20 18:19:54 [parallel_state.py:1212] world_size=32 rank=3 local_rank=3 distributed_init_method=tcp://10.0.0.78:39341 backend=hccl
2026-02-20T18:19:54.5562432Z INFO 02-20 18:19:54 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:19:54.5569759Z INFO 02-20 18:19:54 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:19:54.5578949Z INFO 02-20 18:19:54 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:19:54.5633269Z INFO 02-20 18:19:54 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:19:54.9048752Z INFO 02-20 18:19:54 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:19:54.9057239Z INFO 02-20 18:19:54 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:19:54.9067290Z INFO 02-20 18:19:54 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:19:54.9121783Z INFO 02-20 18:19:54 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:19:59.7013959Z 2026-02-20 18:19:59,699 - 578 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:19:59.7070195Z INFO 02-20 18:19:59 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:19:59.9706015Z 2026-02-20 18:19:59,968 - 577 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:19:59.9765407Z INFO 02-20 18:19:59 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:20:01.0585552Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-20T18:20:01.0590446Z   warnings.warn(
2026-02-20T18:20:01.2814959Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-20T18:20:01.2821852Z   warnings.warn(
2026-02-20T18:20:03.5275801Z INFO 02-20 18:20:03 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:20:03.5406109Z INFO 02-20 18:20:03 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:20:03.5417297Z INFO 02-20 18:20:03 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:20:03.5427775Z INFO 02-20 18:20:03 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:20:03.5438403Z INFO 02-20 18:20:03 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:20:03.5449546Z INFO 02-20 18:20:03 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:20:03.9678655Z INFO 02-20 18:20:03 [parallel_state.py:1212] world_size=32 rank=12 local_rank=4 distributed_init_method=tcp://10.0.0.78:39341 backend=hccl
2026-02-20T18:20:03.9704393Z INFO 02-20 18:20:03 [parallel_state.py:1212] world_size=32 rank=4 local_rank=4 distributed_init_method=tcp://10.0.0.78:39341 backend=hccl
2026-02-20T18:20:06.4808535Z INFO 02-20 18:20:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:20:06.4816614Z INFO 02-20 18:20:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:20:06.4826892Z INFO 02-20 18:20:06 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:20:06.4837493Z INFO 02-20 18:20:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:20:06.4847788Z INFO 02-20 18:20:06 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:20:06.4858194Z INFO 02-20 18:20:06 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:20:06.4950067Z INFO 02-20 18:20:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:20:06.4961140Z INFO 02-20 18:20:06 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:20:14.5658852Z 2026-02-20 18:20:14,563 - 681 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:20:14.5669144Z 2026-02-20 18:20:14,563 - 684 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:20:14.5852603Z INFO 02-20 18:20:14 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:20:14.5863750Z INFO 02-20 18:20:14 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:20:16.5652763Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-20T18:20:16.5659780Z   warnings.warn(
2026-02-20T18:20:16.5671120Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-20T18:20:16.5679391Z   warnings.warn(
2026-02-20T18:20:19.1786493Z INFO 02-20 18:20:19 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:20:19.1795335Z INFO 02-20 18:20:19 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:20:19.1807040Z INFO 02-20 18:20:19 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:20:19.1816333Z INFO 02-20 18:20:19 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:20:19.1826055Z INFO 02-20 18:20:19 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:20:19.1837243Z INFO 02-20 18:20:19 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:20:19.6065867Z INFO 02-20 18:20:19 [parallel_state.py:1212] world_size=32 rank=13 local_rank=5 distributed_init_method=tcp://10.0.0.78:39341 backend=hccl
2026-02-20T18:20:19.6212617Z INFO 02-20 18:20:19 [parallel_state.py:1212] world_size=32 rank=5 local_rank=5 distributed_init_method=tcp://10.0.0.78:39341 backend=hccl
2026-02-20T18:20:20.4723824Z INFO 02-20 18:20:20 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:20:20.4732050Z INFO 02-20 18:20:20 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:20:20.4741244Z INFO 02-20 18:20:20 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:20:20.4849373Z INFO 02-20 18:20:20 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:20:20.4875071Z INFO 02-20 18:20:20 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:20:20.4884739Z INFO 02-20 18:20:20 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:20:20.4894195Z INFO 02-20 18:20:20 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:20:20.4928835Z INFO 02-20 18:20:20 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:20:25.7059747Z 2026-02-20 18:20:25,703 - 786 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:20:25.7114987Z INFO 02-20 18:20:25 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:20:25.7846517Z 2026-02-20 18:20:25,783 - 785 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:20:25.7886263Z INFO 02-20 18:20:25 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:20:27.0807684Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-20T18:20:27.0815778Z   warnings.warn(
2026-02-20T18:20:27.1266326Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-20T18:20:27.1273512Z   warnings.warn(
2026-02-20T18:20:29.2444842Z INFO 02-20 18:20:29 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:20:29.2455274Z INFO 02-20 18:20:29 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:20:29.2466705Z INFO 02-20 18:20:29 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:20:29.2514272Z INFO 02-20 18:20:29 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:20:29.2526040Z INFO 02-20 18:20:29 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:20:29.2539020Z INFO 02-20 18:20:29 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:20:29.6814748Z INFO 02-20 18:20:29 [parallel_state.py:1212] world_size=32 rank=14 local_rank=6 distributed_init_method=tcp://10.0.0.78:39341 backend=hccl
2026-02-20T18:20:29.7156518Z INFO 02-20 18:20:29 [parallel_state.py:1212] world_size=32 rank=6 local_rank=6 distributed_init_method=tcp://10.0.0.78:39341 backend=hccl
2026-02-20T18:20:30.2672985Z INFO 02-20 18:20:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:20:30.2677378Z INFO 02-20 18:20:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:20:30.2688753Z INFO 02-20 18:20:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:20:30.2759033Z INFO 02-20 18:20:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:20:30.2971841Z INFO 02-20 18:20:30 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:20:30.3063961Z INFO 02-20 18:20:30 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:20:30.3066147Z INFO 02-20 18:20:30 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:20:30.3070257Z INFO 02-20 18:20:30 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:20:35.3441692Z 2026-02-20 18:20:35,342 - 890 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:20:35.3495361Z INFO 02-20 18:20:35 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:20:35.4432440Z 2026-02-20 18:20:35,441 - 889 - vllmProfiler - INFO - VLLM_USE_V1 not set, auto-detected via vLLM 0.15.0+empty: default 1
2026-02-20T18:20:35.4600957Z INFO 02-20 18:20:35 [__init__.py:108] Registered model loader `<class 'vllm_ascend.model_loader.netloader.netloader.ModelNetLoaderElastic'>` with load format `netloader`
2026-02-20T18:20:36.6610369Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-20T18:20:36.6616995Z   warnings.warn(
2026-02-20T18:20:36.7895893Z /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-20T18:20:36.7903342Z   warnings.warn(
2026-02-20T18:20:38.7854027Z INFO 02-20 18:20:38 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:20:38.7860379Z INFO 02-20 18:20:38 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:20:38.7873568Z INFO 02-20 18:20:38 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:20:38.8944184Z INFO 02-20 18:20:38 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:20:38.8952871Z INFO 02-20 18:20:38 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:20:38.8964861Z INFO 02-20 18:20:38 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:20:39.2236363Z INFO 02-20 18:20:39 [parallel_state.py:1212] world_size=32 rank=15 local_rank=7 distributed_init_method=tcp://10.0.0.78:39341 backend=hccl
2026-02-20T18:20:39.3341937Z INFO 02-20 18:20:39 [parallel_state.py:1212] world_size=32 rank=7 local_rank=7 distributed_init_method=tcp://10.0.0.78:39341 backend=hccl
2026-02-20T18:20:39.9262708Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:39.9282071Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:39.9928484Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:39.9949435Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.0247880Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.0279113Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.0287654Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.0296560Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.0307752Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.0316854Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.0325970Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.0335739Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.0345564Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.0355844Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.0365803Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.0376580Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.0794771Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-20T18:20:40.0804715Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-20T18:20:40.0823537Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-20T18:20:40.0832811Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-20T18:20:40.0843053Z [Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-20T18:20:40.0852813Z [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-20T18:20:40.0862798Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-20T18:20:40.0872342Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-20T18:20:40.0883106Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-20T18:20:40.0893276Z [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-20T18:20:40.0902334Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-20T18:20:40.0913700Z [Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-20T18:20:40.0923182Z [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-20T18:20:40.0933307Z [Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-20T18:20:40.0943736Z [Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-20T18:20:40.0953271Z [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
2026-02-20T18:20:40.0963771Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.0973146Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.0983311Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.0993092Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1003240Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1012941Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1022875Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1032384Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1047293Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1052894Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1062373Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1072400Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1082500Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1091534Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1101816Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1111707Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1121976Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1130734Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1140428Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1150095Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1160054Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1169771Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1183073Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1188766Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1677167Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1697722Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1706691Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1715646Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1724727Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1733741Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1743071Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1752891Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1763056Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1773927Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1784697Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1794991Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1804822Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1813783Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1823253Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1833015Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1844064Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1853069Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1862875Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1872901Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1883044Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1892558Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1902057Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1911273Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2026-02-20T18:20:40.1921786Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.1931364Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.1941831Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.1951281Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.1961753Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.1971175Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.1980952Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.1989841Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.1999761Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.2009846Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.2023066Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.2028929Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.2038685Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.2048243Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.2057677Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.2067293Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.2151653Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.2168945Z INFO 02-20 18:20:40 [parallel_state.py:1423] rank 0 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
2026-02-20T18:20:40.3003537Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3022636Z INFO 02-20 18:20:40 [parallel_state.py:1423] rank 1 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
2026-02-20T18:20:40.3047954Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3065401Z INFO 02-20 18:20:40 [parallel_state.py:1423] rank 2 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
2026-02-20T18:20:40.3075657Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3084827Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3095155Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3103808Z INFO 02-20 18:20:40 [parallel_state.py:1423] rank 6 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 6, EP rank 6
2026-02-20T18:20:40.3113264Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3123730Z INFO 02-20 18:20:40 [parallel_state.py:1423] rank 7 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 7, EP rank 7
2026-02-20T18:20:40.3133235Z INFO 02-20 18:20:40 [parallel_state.py:1423] rank 9 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 1, EP rank 9
2026-02-20T18:20:40.3142935Z INFO 02-20 18:20:40 [parallel_state.py:1423] rank 8 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 0, EP rank 8
2026-02-20T18:20:40.3152121Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3162409Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3171847Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3181688Z INFO 02-20 18:20:40 [parallel_state.py:1423] rank 13 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 5, EP rank 13
2026-02-20T18:20:40.3191474Z INFO 02-20 18:20:40 [parallel_state.py:1423] rank 12 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 4, EP rank 12
2026-02-20T18:20:40.3201253Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3211055Z INFO 02-20 18:20:40 [parallel_state.py:1423] rank 14 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 6, EP rank 14
2026-02-20T18:20:40.3220614Z INFO 02-20 18:20:40 [parallel_state.py:1423] rank 15 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 7, EP rank 15
2026-02-20T18:20:40.3230401Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3240551Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3249854Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3259522Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3268669Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3279223Z INFO 02-20 18:20:40 [parallel_state.py:1423] rank 4 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 4, EP rank 4
2026-02-20T18:20:40.3288926Z INFO 02-20 18:20:40 [parallel_state.py:1423] rank 10 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 2, EP rank 10
2026-02-20T18:20:40.3298438Z INFO 02-20 18:20:40 [parallel_state.py:1423] rank 11 in world size 32 is assigned as DP rank 1, PP rank 0, PCP rank 0, TP rank 3, EP rank 11
2026-02-20T18:20:40.3307492Z INFO 02-20 18:20:40 [parallel_state.py:1423] rank 5 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 5, EP rank 5
2026-02-20T18:20:40.3317801Z INFO 02-20 18:20:40 [parallel_state.py:1423] rank 3 in world size 32 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
2026-02-20T18:20:40.3507928Z [Gloo] Rank 0 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3563635Z [Gloo] Rank 2 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3573907Z [Gloo] Rank 1 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3583432Z [Gloo] Rank 3 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3591788Z [Gloo] Rank 4 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3601437Z [Gloo] Rank 5 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3610890Z [Gloo] Rank 6 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3620381Z [Gloo] Rank 7 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3629250Z [Gloo] Rank 9 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3638919Z [Gloo] Rank 8 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3649311Z [Gloo] Rank 10 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3658610Z [Gloo] Rank 11 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3667394Z [Gloo] Rank 13 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3677051Z [Gloo] Rank 12 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3686855Z [Gloo] Rank 14 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3696058Z [Gloo] Rank 15 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
2026-02-20T18:20:40.3705573Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.3715052Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.3725003Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.3734228Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.3743970Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.3753471Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.3768074Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.3780195Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.3790704Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.3802387Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.3812134Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.3821730Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.3832188Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.3840781Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.3851212Z [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.3860053Z [Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2026-02-20T18:20:40.4786019Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.4816619Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.4825909Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.4841201Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.4900331Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.4900985Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.4901733Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.4902522Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.4909104Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.4919472Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.4928450Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.4937415Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.4947081Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.4956808Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.4966537Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.5007752Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.5343571Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.5352769Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.5369399Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.5378962Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.5388672Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.5398566Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.5408698Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.5418018Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.5428088Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.5437617Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.5448171Z [0;36m(Worker_DP1_TP1_EP9 pid=276)[0;0m INFO 02-20 18:20:40 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-20T18:20:40.5457735Z [0;36m(Worker_DP0_TP2_EP2 pid=369)[0;0m INFO 02-20 18:20:40 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-20T18:20:40.5467429Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-20 18:20:40 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-20T18:20:40.5478126Z [0;36m(Worker_DP1_TP2_EP10 pid=372)[0;0m INFO 02-20 18:20:40 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-20T18:20:40.5488783Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.5498268Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.5508351Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.5518669Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-20 18:20:40 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-20T18:20:40.5527772Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.5537371Z [0;36m(Worker_DP1_TP7_EP15 pid=890)[0;0m INFO 02-20 18:20:40 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-20T18:20:40.5547083Z [0;36m(Worker_DP0_TP1_EP1 pid=275)[0;0m INFO 02-20 18:20:40 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-20T18:20:40.5556709Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.5566971Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-20 18:20:40 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-20T18:20:40.5576539Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-20 18:20:40 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-20T18:20:40.5585464Z WARNING 02-20 18:20:40 [__init__.py:204] min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
2026-02-20T18:20:40.5595651Z [0;36m(Worker_DP1_TP6_EP14 pid=785)[0;0m INFO 02-20 18:20:40 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-20T18:20:40.5605596Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-20 18:20:40 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-20T18:20:40.5614710Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m INFO 02-20 18:20:40 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-20T18:20:40.5624468Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-20 18:20:40 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-20T18:20:40.5634135Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-20 18:20:40 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-20T18:20:40.5644562Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-20 18:20:40 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-20T18:20:40.6142242Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m INFO 02-20 18:20:40 [model_runner_v1.py:2308] Starting to load model /root/.cache/modelscope/hub/models/vllm-ascend/DeepSeek-V3___2-W8A8...
2026-02-20T18:20:40.9412877Z [0;36m(Worker_DP1_TP2_EP10 pid=372)[0;0m [2026-02-20 18:20:40] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-20T18:20:40.9437972Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m [2026-02-20 18:20:40] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-20T18:20:41.0011497Z [0;36m(Worker_DP1_TP1_EP9 pid=276)[0;0m [2026-02-20 18:20:40] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-20T18:20:41.0063565Z [0;36m(Worker_DP0_TP2_EP2 pid=369)[0;0m [2026-02-20 18:20:41] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-20T18:20:41.0136443Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m [2026-02-20 18:20:41] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-20T18:20:41.0181837Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m [2026-02-20 18:20:41] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-20T18:20:41.0273407Z [0;36m(Worker_DP1_TP7_EP15 pid=890)[0;0m [2026-02-20 18:20:41] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-20T18:20:41.0341823Z [0;36m(Worker_DP1_TP6_EP14 pid=785)[0;0m [2026-02-20 18:20:41] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-20T18:20:41.1211887Z [0;36m(Worker_DP0_TP1_EP1 pid=275)[0;0m [2026-02-20 18:20:41] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-20T18:20:41.1447177Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-20 18:20:41 [layer.py:475] [EP Rank 4/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-20T18:20:41.1456740Z [0;36m(Worker_DP1_TP2_EP10 pid=372)[0;0m INFO 02-20 18:20:41 [layer.py:475] [EP Rank 10/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-20T18:20:41.1466738Z [0;36m(Worker_DP1_TP1_EP9 pid=276)[0;0m INFO 02-20 18:20:41 [layer.py:475] [EP Rank 9/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-20T18:20:41.1476679Z [0;36m(Worker_DP1_TP7_EP15 pid=890)[0;0m INFO 02-20 18:20:41 [layer.py:475] [EP Rank 15/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-20T18:20:41.1486919Z [0;36m(Worker_DP0_TP2_EP2 pid=369)[0;0m INFO 02-20 18:20:41 [layer.py:475] [EP Rank 2/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-20T18:20:41.1497348Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-20 18:20:41 [layer.py:475] [EP Rank 11/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-20T18:20:41.1507135Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-20 18:20:41 [layer.py:475] [EP Rank 12/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-20T18:20:41.1517493Z [0;36m(Worker_DP1_TP6_EP14 pid=785)[0;0m INFO 02-20 18:20:41 [layer.py:475] [EP Rank 14/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-20T18:20:41.2277788Z [0;36m(Worker_DP0_TP1_EP1 pid=275)[0;0m INFO 02-20 18:20:41 [layer.py:475] [EP Rank 1/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-20T18:20:41.2418640Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m [2026-02-20 18:20:41] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-20T18:20:41.2682251Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m [2026-02-20 18:20:41] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-20T18:20:41.3338320Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m [2026-02-20 18:20:41] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-20T18:20:41.3520696Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-20 18:20:41 [layer.py:475] [EP Rank 7/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-20T18:20:41.3988404Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m [2026-02-20 18:20:41] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-20T18:20:41.4303098Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m INFO 02-20 18:20:41 [layer.py:475] [EP Rank 8/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-20T18:20:41.4499663Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-20 18:20:41 [layer.py:475] [EP Rank 13/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-20T18:20:41.5096460Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-20 18:20:41 [layer.py:475] [EP Rank 6/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-20T18:20:41.5567244Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m [2026-02-20 18:20:41] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-20T18:20:41.5796548Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m [2026-02-20 18:20:41] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-20T18:20:41.6656182Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-20 18:20:41 [layer.py:475] [EP Rank 5/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-20T18:20:41.6778245Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m [2026-02-20 18:20:41] INFO modelslim_config.py:286: Using the vLLM Ascend modelslim Quantization now!
2026-02-20T18:20:41.6974064Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-20 18:20:41 [layer.py:475] [EP Rank 3/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-20T18:20:41.8343364Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m INFO 02-20 18:20:41 [layer.py:475] [EP Rank 0/32] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-20T18:20:42.3520613Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-20T18:20:42.3529964Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m   return func(*args, **kwargs)
2026-02-20T18:20:42.4256731Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m INFO 02-20 18:20:42 [fused_moe.py:207] [EP Rank 0/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7.
2026-02-20T18:20:42.5672561Z [0;36m(Worker_DP1_TP6_EP14 pid=785)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-20T18:20:42.5679177Z [0;36m(Worker_DP1_TP6_EP14 pid=785)[0;0m   return func(*args, **kwargs)
2026-02-20T18:20:42.5690178Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-20T18:20:42.5698824Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m   return func(*args, **kwargs)
2026-02-20T18:20:42.5725087Z [0;36m(Worker_DP0_TP1_EP1 pid=275)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-20T18:20:42.5733975Z [0;36m(Worker_DP0_TP1_EP1 pid=275)[0;0m   return func(*args, **kwargs)
2026-02-20T18:20:42.6024831Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-20T18:20:42.6044030Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m   return func(*args, **kwargs)
2026-02-20T18:20:42.6066921Z [0;36m(Worker_DP0_TP2_EP2 pid=369)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-20T18:20:42.6076026Z [0;36m(Worker_DP0_TP2_EP2 pid=369)[0;0m   return func(*args, **kwargs)
2026-02-20T18:20:42.6089715Z [0;36m(Worker_DP1_TP2_EP10 pid=372)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-20T18:20:42.6098649Z [0;36m(Worker_DP1_TP2_EP10 pid=372)[0;0m   return func(*args, **kwargs)
2026-02-20T18:20:42.6113439Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-20T18:20:42.6122507Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m   return func(*args, **kwargs)
2026-02-20T18:20:42.6133986Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-20T18:20:42.6143263Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m   return func(*args, **kwargs)
2026-02-20T18:20:42.6156775Z [0;36m(Worker_DP1_TP7_EP15 pid=890)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-20T18:20:42.6188200Z [0;36m(Worker_DP1_TP7_EP15 pid=890)[0;0m   return func(*args, **kwargs)
2026-02-20T18:20:42.6256836Z [0;36m(Worker_DP1_TP1_EP9 pid=276)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-20T18:20:42.6265783Z [0;36m(Worker_DP1_TP1_EP9 pid=276)[0;0m   return func(*args, **kwargs)
2026-02-20T18:20:42.6277971Z [0;36m(Worker_DP1_TP6_EP14 pid=785)[0;0m INFO 02-20 18:20:42 [fused_moe.py:207] [EP Rank 14/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119.
2026-02-20T18:20:42.6288387Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m INFO 02-20 18:20:42 [fused_moe.py:207] [EP Rank 8/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71.
2026-02-20T18:20:42.6330257Z [0;36m(Worker_DP0_TP1_EP1 pid=275)[0;0m INFO 02-20 18:20:42 [fused_moe.py:207] [EP Rank 1/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->8, 1->9, 2->10, 3->11, 4->12, 5->13, 6->14, 7->15.
2026-02-20T18:20:42.6536764Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-20T18:20:42.6545746Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m   return func(*args, **kwargs)
2026-02-20T18:20:42.6620741Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-20T18:20:42.6629482Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m   return func(*args, **kwargs)
2026-02-20T18:20:42.6644440Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-20T18:20:42.6653214Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m   return func(*args, **kwargs)
2026-02-20T18:20:42.6664624Z [0;36m(Worker_DP0_TP2_EP2 pid=369)[0;0m INFO 02-20 18:20:42 [fused_moe.py:207] [EP Rank 2/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23.
2026-02-20T18:20:42.6674285Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-20 18:20:42 [fused_moe.py:207] [EP Rank 13/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->104, 1->105, 2->106, 3->107, 4->108, 5->109, 6->110, 7->111.
2026-02-20T18:20:42.6685428Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-20T18:20:42.6694045Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m   return func(*args, **kwargs)
2026-02-20T18:20:42.6704418Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch/utils/_device.py:103: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)
2026-02-20T18:20:42.6713595Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m   return func(*args, **kwargs)
2026-02-20T18:20:42.6725433Z [0;36m(Worker_DP1_TP2_EP10 pid=372)[0;0m INFO 02-20 18:20:42 [fused_moe.py:207] [EP Rank 10/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87.
2026-02-20T18:20:42.6735618Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-20 18:20:42 [fused_moe.py:207] [EP Rank 4/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39.
2026-02-20T18:20:42.6745342Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-20 18:20:42 [fused_moe.py:207] [EP Rank 7/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->56, 1->57, 2->58, 3->59, 4->60, 5->61, 6->62, 7->63.
2026-02-20T18:20:42.6783757Z [0;36m(Worker_DP1_TP7_EP15 pid=890)[0;0m INFO 02-20 18:20:42 [fused_moe.py:207] [EP Rank 15/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->120, 1->121, 2->122, 3->123, 4->124, 5->125, 6->126, 7->127.
2026-02-20T18:20:42.6849456Z [0;36m(Worker_DP1_TP1_EP9 pid=276)[0;0m INFO 02-20 18:20:42 [fused_moe.py:207] [EP Rank 9/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->72, 1->73, 2->74, 3->75, 4->76, 5->77, 6->78, 7->79.
2026-02-20T18:20:42.7144069Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-20 18:20:42 [fused_moe.py:207] [EP Rank 12/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103.
2026-02-20T18:20:42.7250628Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-20 18:20:42 [fused_moe.py:207] [EP Rank 3/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->24, 1->25, 2->26, 3->27, 4->28, 5->29, 6->30, 7->31.
2026-02-20T18:20:42.7283477Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-20 18:20:42 [fused_moe.py:207] [EP Rank 11/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->88, 1->89, 2->90, 3->91, 4->92, 5->93, 6->94, 7->95.
2026-02-20T18:20:42.7305403Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-20 18:20:42 [fused_moe.py:207] [EP Rank 5/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->40, 1->41, 2->42, 3->43, 4->44, 5->45, 6->46, 7->47.
2026-02-20T18:20:42.7328615Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-20 18:20:42 [fused_moe.py:207] [EP Rank 6/32] Expert parallelism is enabled. Local/global number of experts: 8/256. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55.
2026-02-20T18:20:42.9366345Z [0;36m(Worker_DP1_TP6_EP14 pid=785)[0;0m INFO 02-20 18:20:42 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-20T18:20:42.9583618Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-20 18:20:42 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-20T18:20:42.9677629Z [0;36m(Worker_DP0_TP2_EP2 pid=369)[0;0m INFO 02-20 18:20:42 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-20T18:20:42.9678512Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-20 18:20:42 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-20T18:20:42.9701480Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-20 18:20:42 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-20T18:20:42.9725899Z [0;36m(Worker_DP1_TP2_EP10 pid=372)[0;0m INFO 02-20 18:20:42 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-20T18:20:42.9845026Z [0;36m(Worker_DP1_TP1_EP9 pid=276)[0;0m INFO 02-20 18:20:42 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-20T18:20:43.0330187Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-20 18:20:43 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-20T18:20:43.0349779Z [0;36m(Worker_DP1_TP7_EP15 pid=890)[0;0m INFO 02-20 18:20:43 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-20T18:20:43.0377099Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-20 18:20:43 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-20T18:20:43.0425960Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-20 18:20:43 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-20T18:20:43.0447869Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-20 18:20:43 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-20T18:20:43.0482874Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-20 18:20:43 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-20T18:20:43.0668130Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m INFO 02-20 18:20:43 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-20T18:20:43.1431386Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m INFO 02-20 18:20:43 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-20T18:20:43.1483729Z [0;36m(Worker_DP0_TP1_EP1 pid=275)[0;0m INFO 02-20 18:20:43 [fused_moe.py:414] Sequence parallelism is enabled, shared experts are replicated for best performance.
2026-02-20T18:20:46.2733559Z [0;36m(Worker_DP0_TP2_EP2 pid=369)[0;0m INFO 02-20 18:20:46 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-20T18:20:46.2875205Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-20 18:20:46 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-20T18:20:46.3976503Z [0;36m(Worker_DP1_TP1_EP9 pid=276)[0;0m INFO 02-20 18:20:46 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-20T18:20:46.4243969Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-20 18:20:46 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-20T18:20:46.4497445Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-20 18:20:46 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-20T18:20:46.4522544Z [0;36m(Worker_DP1_TP6_EP14 pid=785)[0;0m INFO 02-20 18:20:46 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-20T18:20:46.4987288Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-20 18:20:46 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-20T18:20:46.5261552Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-20 18:20:46 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-20T18:20:46.5367655Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-20 18:20:46 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-20T18:20:46.5429449Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-20 18:20:46 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-20T18:20:46.5521073Z [0;36m(Worker_DP1_TP7_EP15 pid=890)[0;0m INFO 02-20 18:20:46 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-20T18:20:46.5724179Z [0;36m(Worker_DP1_TP2_EP10 pid=372)[0;0m INFO 02-20 18:20:46 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-20T18:20:46.5820407Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-20 18:20:46 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-20T18:20:46.5903292Z [0;36m(Worker_DP0_TP1_EP1 pid=275)[0;0m INFO 02-20 18:20:46 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-20T18:20:46.5947008Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m INFO 02-20 18:20:46 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-20T18:20:46.6623944Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:20:46.6624367Z Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
2026-02-20T18:20:46.7190973Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m INFO 02-20 18:20:46 [compilation.py:863] Using OOT custom backend for compilation.
2026-02-20T18:20:47.7383404Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:20:47.7383923Z Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:01<02:54,  1.08s/it]
2026-02-20T18:20:50.9085394Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:20:50.9086392Z Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:04<06:11,  2.31s/it]
2026-02-20T18:20:52.2208806Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:20:52.2209332Z Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:05<04:56,  1.85s/it]
2026-02-20T18:20:53.6431264Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:20:53.6431679Z Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:06<04:27,  1.68s/it]
2026-02-20T18:20:56.8155402Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:20:56.8155929Z Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:10<05:50,  2.22s/it]
2026-02-20T18:20:59.6684257Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:20:59.6684722Z Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:13<06:22,  2.44s/it]
2026-02-20T18:21:02.3722817Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:02.3723333Z Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:15<06:33,  2.52s/it]
2026-02-20T18:21:04.9675834Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:04.9676267Z Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:18<06:34,  2.55s/it]
2026-02-20T18:21:06.3293223Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:06.3293999Z Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:19<05:35,  2.18s/it]
2026-02-20T18:21:07.6981753Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:07.6982275Z Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:21<04:54,  1.93s/it]
2026-02-20T18:21:09.0734624Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:09.0735220Z Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:22<04:27,  1.76s/it]
2026-02-20T18:21:10.4371407Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:10.4371929Z Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:23<04:07,  1.64s/it]
2026-02-20T18:21:11.8746759Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:11.8747322Z Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:25<03:56,  1.58s/it]
2026-02-20T18:21:15.6124472Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:15.6125072Z Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:28<05:32,  2.23s/it]
2026-02-20T18:21:18.1361951Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:18.1362503Z Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:31<05:43,  2.32s/it]
2026-02-20T18:21:20.7375345Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:20.7375791Z Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:34<05:53,  2.40s/it]
2026-02-20T18:21:23.4671235Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:23.4671727Z Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:36<06:05,  2.50s/it]
2026-02-20T18:21:26.0755285Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:26.0755723Z Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:39<06:07,  2.53s/it]
2026-02-20T18:21:27.3939166Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:27.3939653Z Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:40<05:12,  2.17s/it]
2026-02-20T18:21:28.7523111Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:28.7523594Z Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:42<04:35,  1.93s/it]
2026-02-20T18:21:32.0985642Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:32.0986181Z Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:45<05:33,  2.35s/it]
2026-02-20T18:21:33.1108599Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:33.1109112Z Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:46<04:34,  1.95s/it]
2026-02-20T18:21:34.3975494Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:34.3976107Z Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:47<04:05,  1.75s/it]
2026-02-20T18:21:35.7957890Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:35.7958420Z Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:49<03:48,  1.65s/it]
2026-02-20T18:21:39.7783418Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:39.7783916Z Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:53<05:23,  2.35s/it]
2026-02-20T18:21:40.7885198Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:40.7885644Z Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:54<04:26,  1.95s/it]
2026-02-20T18:21:42.1128727Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:42.1129203Z Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:55<03:59,  1.76s/it]
2026-02-20T18:21:44.9785444Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:44.9785927Z Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:58<04:42,  2.09s/it]
2026-02-20T18:21:46.6434811Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:46.6435311Z Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:59<04:23,  1.96s/it]
2026-02-20T18:21:48.0175373Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:48.0175916Z Loading safetensors checkpoint shards:  18% Completed | 30/163 [01:01<03:57,  1.79s/it]
2026-02-20T18:21:49.4317524Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:49.4317991Z Loading safetensors checkpoint shards:  19% Completed | 31/163 [01:02<03:41,  1.67s/it]
2026-02-20T18:21:50.8622526Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:50.8623104Z Loading safetensors checkpoint shards:  20% Completed | 32/163 [01:04<03:29,  1.60s/it]
2026-02-20T18:21:52.5322583Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:52.5323105Z Loading safetensors checkpoint shards:  20% Completed | 33/163 [01:05<03:30,  1.62s/it]
2026-02-20T18:21:53.9204303Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:53.9204852Z Loading safetensors checkpoint shards:  21% Completed | 34/163 [01:07<03:20,  1.55s/it]
2026-02-20T18:21:55.2213205Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:55.2213731Z Loading safetensors checkpoint shards:  21% Completed | 35/163 [01:08<03:09,  1.48s/it]
2026-02-20T18:21:59.2104056Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:21:59.2104540Z Loading safetensors checkpoint shards:  22% Completed | 36/163 [01:12<04:43,  2.23s/it]
2026-02-20T18:22:00.3375568Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:00.3376040Z Loading safetensors checkpoint shards:  23% Completed | 37/163 [01:13<03:59,  1.90s/it]
2026-02-20T18:22:01.8195499Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:01.8195942Z Loading safetensors checkpoint shards:  23% Completed | 38/163 [01:15<03:41,  1.77s/it]
2026-02-20T18:22:03.3685997Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:03.3686514Z Loading safetensors checkpoint shards:  24% Completed | 39/163 [01:16<03:31,  1.71s/it]
2026-02-20T18:22:07.3251752Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:07.3252437Z Loading safetensors checkpoint shards:  25% Completed | 40/163 [01:20<04:52,  2.38s/it]
2026-02-20T18:22:08.7463436Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:08.7463872Z Loading safetensors checkpoint shards:  25% Completed | 41/163 [01:22<04:15,  2.09s/it]
2026-02-20T18:22:10.1718805Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:10.1719322Z Loading safetensors checkpoint shards:  26% Completed | 42/163 [01:23<03:49,  1.89s/it]
2026-02-20T18:22:13.8301648Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:13.8302352Z Loading safetensors checkpoint shards:  26% Completed | 43/163 [01:27<04:50,  2.42s/it]
2026-02-20T18:22:15.0655967Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:15.0656416Z Loading safetensors checkpoint shards:  27% Completed | 44/163 [01:28<04:05,  2.07s/it]
2026-02-20T18:22:16.3762609Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:16.3763135Z Loading safetensors checkpoint shards:  28% Completed | 45/163 [01:29<03:37,  1.84s/it]
2026-02-20T18:22:17.6610814Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:17.6611256Z Loading safetensors checkpoint shards:  28% Completed | 46/163 [01:30<03:15,  1.67s/it]
2026-02-20T18:22:24.3385649Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:24.3386106Z Loading safetensors checkpoint shards:  29% Completed | 47/163 [01:37<06:08,  3.17s/it]
2026-02-20T18:22:26.6346506Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:26.6346972Z Loading safetensors checkpoint shards:  29% Completed | 48/163 [01:39<05:34,  2.91s/it]
2026-02-20T18:22:26.9013522Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:26.9014000Z Loading safetensors checkpoint shards:  30% Completed | 49/163 [01:40<04:01,  2.12s/it]
2026-02-20T18:22:29.1260514Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:29.1261010Z Loading safetensors checkpoint shards:  31% Completed | 50/163 [01:42<04:02,  2.15s/it]
2026-02-20T18:22:31.3365580Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:31.3366031Z Loading safetensors checkpoint shards:  31% Completed | 51/163 [01:44<04:02,  2.17s/it]
2026-02-20T18:22:32.2481881Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:32.2482821Z Loading safetensors checkpoint shards:  32% Completed | 52/163 [01:45<03:18,  1.79s/it]
2026-02-20T18:22:35.5475514Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:35.5476053Z Loading safetensors checkpoint shards:  33% Completed | 53/163 [01:48<04:06,  2.24s/it]
2026-02-20T18:22:36.8016205Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:36.8016763Z Loading safetensors checkpoint shards:  33% Completed | 54/163 [01:50<03:32,  1.95s/it]
2026-02-20T18:22:38.0801001Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:38.0801466Z Loading safetensors checkpoint shards:  34% Completed | 55/163 [01:51<03:08,  1.75s/it]
2026-02-20T18:22:39.3281352Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:39.3281839Z Loading safetensors checkpoint shards:  34% Completed | 56/163 [01:52<02:50,  1.60s/it]
2026-02-20T18:22:40.6369994Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:40.6370608Z Loading safetensors checkpoint shards:  35% Completed | 57/163 [01:53<02:40,  1.51s/it]
2026-02-20T18:22:41.8829463Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:41.8829976Z Loading safetensors checkpoint shards:  36% Completed | 58/163 [01:55<02:30,  1.43s/it]
2026-02-20T18:22:46.1247201Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:46.1247662Z Loading safetensors checkpoint shards:  36% Completed | 59/163 [01:59<03:56,  2.27s/it]
2026-02-20T18:22:47.1118790Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:47.1119249Z Loading safetensors checkpoint shards:  37% Completed | 60/163 [02:00<03:14,  1.89s/it]
2026-02-20T18:22:48.5089316Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:48.5089873Z Loading safetensors checkpoint shards:  37% Completed | 61/163 [02:01<02:57,  1.74s/it]
2026-02-20T18:22:52.0305031Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:52.0305648Z Loading safetensors checkpoint shards:  38% Completed | 62/163 [02:05<03:49,  2.28s/it]
2026-02-20T18:22:53.2731964Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:53.2732607Z Loading safetensors checkpoint shards:  39% Completed | 63/163 [02:06<03:16,  1.97s/it]
2026-02-20T18:22:54.5167249Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:54.5167827Z Loading safetensors checkpoint shards:  39% Completed | 64/163 [02:07<02:53,  1.75s/it]
2026-02-20T18:22:57.9264777Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:22:57.9265183Z Loading safetensors checkpoint shards:  40% Completed | 65/163 [02:11<03:40,  2.25s/it]
2026-02-20T18:23:00.3860936Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:00.3861348Z Loading safetensors checkpoint shards:  40% Completed | 66/163 [02:13<03:44,  2.31s/it]
2026-02-20T18:23:01.6471733Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:01.6472637Z Loading safetensors checkpoint shards:  41% Completed | 67/163 [02:14<03:11,  2.00s/it]
2026-02-20T18:23:04.7223469Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:04.7223925Z Loading safetensors checkpoint shards:  42% Completed | 68/163 [02:18<03:40,  2.32s/it]
2026-02-20T18:23:05.9248278Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:05.9248723Z Loading safetensors checkpoint shards:  42% Completed | 69/163 [02:19<03:06,  1.98s/it]
2026-02-20T18:23:07.2797391Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:07.2797828Z Loading safetensors checkpoint shards:  43% Completed | 70/163 [02:20<02:46,  1.80s/it]
2026-02-20T18:23:10.2653658Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:10.2654118Z Loading safetensors checkpoint shards:  44% Completed | 71/163 [02:23<03:18,  2.15s/it]
2026-02-20T18:23:13.0125925Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:13.0126424Z Loading safetensors checkpoint shards:  44% Completed | 72/163 [02:26<03:32,  2.33s/it]
2026-02-20T18:23:14.3835646Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:14.3836200Z Loading safetensors checkpoint shards:  45% Completed | 73/163 [02:27<03:03,  2.04s/it]
2026-02-20T18:23:15.7184993Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:15.7185816Z Loading safetensors checkpoint shards:  45% Completed | 74/163 [02:29<02:42,  1.83s/it]
2026-02-20T18:23:17.0890369Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:17.0890784Z Loading safetensors checkpoint shards:  46% Completed | 75/163 [02:30<02:28,  1.69s/it]
2026-02-20T18:23:18.5237357Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:18.5237824Z Loading safetensors checkpoint shards:  47% Completed | 76/163 [02:31<02:20,  1.62s/it]
2026-02-20T18:23:22.6631290Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:22.6631827Z Loading safetensors checkpoint shards:  47% Completed | 77/163 [02:36<03:24,  2.37s/it]
2026-02-20T18:23:23.4080071Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:23.4080534Z Loading safetensors checkpoint shards:  48% Completed | 78/163 [02:36<02:40,  1.88s/it]
2026-02-20T18:23:24.7576274Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:24.7576778Z Loading safetensors checkpoint shards:  48% Completed | 79/163 [02:38<02:24,  1.72s/it]
2026-02-20T18:23:27.9560547Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:27.9560982Z Loading safetensors checkpoint shards:  49% Completed | 80/163 [02:41<02:59,  2.17s/it]
2026-02-20T18:23:30.6595716Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:30.6596137Z Loading safetensors checkpoint shards:  50% Completed | 81/163 [02:43<03:10,  2.33s/it]
2026-02-20T18:23:31.9887036Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:31.9887555Z Loading safetensors checkpoint shards:  50% Completed | 82/163 [02:45<02:44,  2.03s/it]
2026-02-20T18:23:34.9868130Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:34.9868656Z Loading safetensors checkpoint shards:  51% Completed | 83/163 [02:48<03:05,  2.32s/it]
2026-02-20T18:23:36.3088277Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:36.3088799Z Loading safetensors checkpoint shards:  52% Completed | 84/163 [02:49<02:39,  2.02s/it]
2026-02-20T18:23:39.3682173Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:39.3682770Z Loading safetensors checkpoint shards:  52% Completed | 85/163 [02:52<03:01,  2.33s/it]
2026-02-20T18:23:40.5698632Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:40.5699161Z Loading safetensors checkpoint shards:  53% Completed | 86/163 [02:53<02:33,  1.99s/it]
2026-02-20T18:23:42.7711975Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:42.7712637Z Loading safetensors checkpoint shards:  53% Completed | 87/163 [02:56<02:36,  2.06s/it]
2026-02-20T18:23:45.8174454Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:45.8174946Z Loading safetensors checkpoint shards:  54% Completed | 88/163 [02:59<02:56,  2.35s/it]
2026-02-20T18:23:47.0090078Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:47.0090536Z Loading safetensors checkpoint shards:  55% Completed | 89/163 [03:00<02:28,  2.00s/it]
2026-02-20T18:23:50.1485846Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:50.1486347Z Loading safetensors checkpoint shards:  55% Completed | 90/163 [03:03<02:51,  2.34s/it]
2026-02-20T18:23:51.3476682Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:51.3477129Z Loading safetensors checkpoint shards:  56% Completed | 91/163 [03:04<02:24,  2.00s/it]
2026-02-20T18:23:54.4047796Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:54.4048266Z Loading safetensors checkpoint shards:  56% Completed | 92/163 [03:07<02:44,  2.32s/it]
2026-02-20T18:23:55.7054233Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:55.7054668Z Loading safetensors checkpoint shards:  57% Completed | 93/163 [03:09<02:20,  2.01s/it]
2026-02-20T18:23:57.0932668Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:57.0933240Z Loading safetensors checkpoint shards:  58% Completed | 94/163 [03:10<02:05,  1.83s/it]
2026-02-20T18:23:58.7001874Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:23:58.7002563Z Loading safetensors checkpoint shards:  58% Completed | 95/163 [03:12<01:59,  1.76s/it]
2026-02-20T18:24:02.1614533Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:02.1615082Z Loading safetensors checkpoint shards:  59% Completed | 96/163 [03:15<02:32,  2.27s/it]
2026-02-20T18:24:03.2506997Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:03.2507525Z Loading safetensors checkpoint shards:  60% Completed | 97/163 [03:16<02:06,  1.92s/it]
2026-02-20T18:24:04.5704734Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:04.5705270Z Loading safetensors checkpoint shards:  60% Completed | 98/163 [03:17<01:52,  1.74s/it]
2026-02-20T18:24:05.8734413Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:05.8734923Z Loading safetensors checkpoint shards:  61% Completed | 99/163 [03:19<01:42,  1.61s/it]
2026-02-20T18:24:07.4555199Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:07.4555657Z Loading safetensors checkpoint shards:  61% Completed | 100/163 [03:20<01:40,  1.60s/it]
2026-02-20T18:24:08.6757923Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:08.6758471Z Loading safetensors checkpoint shards:  62% Completed | 101/163 [03:22<01:32,  1.49s/it]
2026-02-20T18:24:11.5867781Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:11.5868326Z Loading safetensors checkpoint shards:  63% Completed | 102/163 [03:24<01:56,  1.91s/it]
2026-02-20T18:24:14.1356983Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:14.1357461Z Loading safetensors checkpoint shards:  63% Completed | 103/163 [03:27<02:06,  2.10s/it]
2026-02-20T18:24:16.7724587Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:16.7725019Z Loading safetensors checkpoint shards:  64% Completed | 104/163 [03:30<02:13,  2.26s/it]
2026-02-20T18:24:17.8418141Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:17.8418665Z Loading safetensors checkpoint shards:  64% Completed | 105/163 [03:31<01:50,  1.91s/it]
2026-02-20T18:24:20.8558147Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:20.8558621Z Loading safetensors checkpoint shards:  65% Completed | 106/163 [03:34<02:07,  2.24s/it]
2026-02-20T18:24:22.0185310Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:22.0185870Z Loading safetensors checkpoint shards:  66% Completed | 107/163 [03:35<01:47,  1.92s/it]
2026-02-20T18:24:24.7046824Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:24.7047388Z Loading safetensors checkpoint shards:  66% Completed | 108/163 [03:38<01:58,  2.15s/it]
2026-02-20T18:24:25.9645368Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:25.9645905Z Loading safetensors checkpoint shards:  67% Completed | 109/163 [03:39<01:41,  1.88s/it]
2026-02-20T18:24:28.7313485Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:28.7313926Z Loading safetensors checkpoint shards:  67% Completed | 110/163 [03:42<01:53,  2.15s/it]
2026-02-20T18:24:31.5432207Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:31.5432698Z Loading safetensors checkpoint shards:  68% Completed | 111/163 [03:44<02:01,  2.35s/it]
2026-02-20T18:24:34.0358599Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:34.0359089Z Loading safetensors checkpoint shards:  69% Completed | 112/163 [03:47<02:01,  2.39s/it]
2026-02-20T18:24:36.7515586Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:36.7516043Z Loading safetensors checkpoint shards:  69% Completed | 113/163 [03:50<02:04,  2.49s/it]
2026-02-20T18:24:37.8538676Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:37.8539169Z Loading safetensors checkpoint shards:  70% Completed | 114/163 [03:51<01:41,  2.07s/it]
2026-02-20T18:24:39.1706134Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:39.1706677Z Loading safetensors checkpoint shards:  71% Completed | 115/163 [03:52<01:28,  1.85s/it]
2026-02-20T18:24:42.4424078Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:42.4424641Z Loading safetensors checkpoint shards:  71% Completed | 116/163 [03:55<01:46,  2.27s/it]
2026-02-20T18:24:43.6957115Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:43.6957847Z Loading safetensors checkpoint shards:  72% Completed | 117/163 [03:57<01:30,  1.97s/it]
2026-02-20T18:24:46.4865814Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:46.4866272Z Loading safetensors checkpoint shards:  72% Completed | 118/163 [03:59<01:39,  2.21s/it]
2026-02-20T18:24:47.7795747Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:47.7796280Z Loading safetensors checkpoint shards:  73% Completed | 119/163 [04:01<01:25,  1.94s/it]
2026-02-20T18:24:49.1188970Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:49.1189416Z Loading safetensors checkpoint shards:  74% Completed | 120/163 [04:02<01:15,  1.76s/it]
2026-02-20T18:24:50.4074134Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:50.4074638Z Loading safetensors checkpoint shards:  74% Completed | 121/163 [04:03<01:07,  1.62s/it]
2026-02-20T18:24:51.7770547Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:51.7770994Z Loading safetensors checkpoint shards:  75% Completed | 122/163 [04:05<01:03,  1.54s/it]
2026-02-20T18:24:55.4057850Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:55.4058297Z Loading safetensors checkpoint shards:  75% Completed | 123/163 [04:08<01:26,  2.17s/it]
2026-02-20T18:24:56.6254041Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:56.6254485Z Loading safetensors checkpoint shards:  76% Completed | 124/163 [04:09<01:13,  1.88s/it]
2026-02-20T18:24:59.5980393Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:24:59.5980850Z Loading safetensors checkpoint shards:  77% Completed | 125/163 [04:12<01:24,  2.21s/it]
2026-02-20T18:25:00.9186961Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:00.9187430Z Loading safetensors checkpoint shards:  77% Completed | 126/163 [04:14<01:11,  1.94s/it]
2026-02-20T18:25:02.2498916Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:02.2499477Z Loading safetensors checkpoint shards:  78% Completed | 127/163 [04:15<01:03,  1.76s/it]
2026-02-20T18:25:05.6442812Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:05.6443452Z Loading safetensors checkpoint shards:  79% Completed | 128/163 [04:18<01:18,  2.25s/it]
2026-02-20T18:25:08.5111017Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:08.5111566Z Loading safetensors checkpoint shards:  79% Completed | 129/163 [04:21<01:22,  2.44s/it]
2026-02-20T18:25:09.6129985Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:09.6130546Z Loading safetensors checkpoint shards:  80% Completed | 130/163 [04:22<01:07,  2.04s/it]
2026-02-20T18:25:10.9112349Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:10.9112872Z Loading safetensors checkpoint shards:  80% Completed | 131/163 [04:24<00:58,  1.81s/it]
2026-02-20T18:25:12.3466935Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:12.3467673Z Loading safetensors checkpoint shards:  81% Completed | 132/163 [04:25<00:52,  1.70s/it]
2026-02-20T18:25:16.1234962Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:16.1235482Z Loading safetensors checkpoint shards:  82% Completed | 133/163 [04:29<01:09,  2.32s/it]
2026-02-20T18:25:17.4072479Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:17.4072974Z Loading safetensors checkpoint shards:  82% Completed | 134/163 [04:30<00:58,  2.01s/it]
2026-02-20T18:25:20.4835682Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:20.4836157Z Loading safetensors checkpoint shards:  83% Completed | 135/163 [04:33<01:05,  2.33s/it]
2026-02-20T18:25:21.6538979Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:21.6539430Z Loading safetensors checkpoint shards:  83% Completed | 136/163 [04:34<00:53,  1.98s/it]
2026-02-20T18:25:22.9358670Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:22.9359140Z Loading safetensors checkpoint shards:  84% Completed | 137/163 [04:36<00:46,  1.77s/it]
2026-02-20T18:25:26.1107849Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:26.1108381Z Loading safetensors checkpoint shards:  85% Completed | 138/163 [04:39<00:54,  2.19s/it]
2026-02-20T18:25:28.6167794Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:28.6168606Z Loading safetensors checkpoint shards:  85% Completed | 139/163 [04:41<00:54,  2.29s/it]
2026-02-20T18:25:29.8810498Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:29.8811014Z Loading safetensors checkpoint shards:  86% Completed | 140/163 [04:43<00:45,  1.98s/it]
2026-02-20T18:25:31.2857367Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:31.2857912Z Loading safetensors checkpoint shards:  87% Completed | 141/163 [04:44<00:39,  1.81s/it]
2026-02-20T18:25:32.7102079Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:32.7102595Z Loading safetensors checkpoint shards:  87% Completed | 142/163 [04:46<00:35,  1.69s/it]
2026-02-20T18:25:36.3095499Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:36.3096073Z Loading safetensors checkpoint shards:  88% Completed | 143/163 [04:49<00:45,  2.26s/it]
2026-02-20T18:25:38.5748981Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:38.5749480Z Loading safetensors checkpoint shards:  88% Completed | 144/163 [04:51<00:43,  2.26s/it]
2026-02-20T18:25:40.3268996Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:40.3269444Z Loading safetensors checkpoint shards:  89% Completed | 145/163 [04:53<00:37,  2.11s/it]
2026-02-20T18:25:42.6929203Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:42.6929659Z Loading safetensors checkpoint shards:  90% Completed | 146/163 [04:56<00:37,  2.19s/it]
2026-02-20T18:25:44.5908540Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:44.5909033Z Loading safetensors checkpoint shards:  90% Completed | 147/163 [04:57<00:33,  2.10s/it]
2026-02-20T18:25:46.0066727Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:46.0067173Z Loading safetensors checkpoint shards:  91% Completed | 148/163 [04:59<00:28,  1.90s/it]
2026-02-20T18:25:47.3165191Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:47.3165728Z Loading safetensors checkpoint shards:  91% Completed | 149/163 [05:00<00:24,  1.72s/it]
2026-02-20T18:25:50.2564362Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:50.2564967Z Loading safetensors checkpoint shards:  93% Completed | 151/163 [05:03<00:19,  1.60s/it]
2026-02-20T18:25:51.3032517Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:51.3033031Z Loading safetensors checkpoint shards:  93% Completed | 152/163 [05:04<00:16,  1.47s/it]
2026-02-20T18:25:52.5396083Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:52.5396520Z Loading safetensors checkpoint shards:  94% Completed | 153/163 [05:05<00:14,  1.41s/it]
2026-02-20T18:25:54.6936824Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:54.6937344Z Loading safetensors checkpoint shards:  94% Completed | 154/163 [05:08<00:14,  1.61s/it]
2026-02-20T18:25:55.8920694Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:55.8921114Z Loading safetensors checkpoint shards:  95% Completed | 155/163 [05:09<00:11,  1.49s/it]
2026-02-20T18:25:57.2170990Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:57.2171448Z Loading safetensors checkpoint shards:  96% Completed | 156/163 [05:10<00:10,  1.45s/it]
2026-02-20T18:25:58.5199584Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:25:58.5200161Z Loading safetensors checkpoint shards:  96% Completed | 157/163 [05:11<00:08,  1.40s/it]
2026-02-20T18:26:02.2905741Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:26:02.2906226Z Loading safetensors checkpoint shards:  97% Completed | 158/163 [05:15<00:10,  2.10s/it]
2026-02-20T18:26:03.6409165Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:26:03.6409611Z Loading safetensors checkpoint shards:  98% Completed | 159/163 [05:16<00:07,  1.88s/it]
2026-02-20T18:26:04.9581623Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:26:04.9582201Z Loading safetensors checkpoint shards:  98% Completed | 160/163 [05:18<00:05,  1.71s/it]
2026-02-20T18:26:06.4043096Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:26:06.4044120Z Loading safetensors checkpoint shards:  99% Completed | 161/163 [05:19<00:03,  1.63s/it]
2026-02-20T18:26:09.7363175Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:26:09.7363838Z Loading safetensors checkpoint shards:  99% Completed | 162/163 [05:23<00:02,  2.14s/it]
2026-02-20T18:26:11.1804712Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:26:11.1805308Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [05:24<00:00,  1.93s/it]
2026-02-20T18:26:11.1822644Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:26:11.1823058Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [05:24<00:00,  1.99s/it]
2026-02-20T18:26:11.1834498Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:26:11.1976195Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m INFO 02-20 18:26:11 [default_loader.py:291] Loading weights took 324.53 seconds
2026-02-20T18:26:12.1713194Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m INFO 02-20 18:26:12 [default_loader.py:291] Loading weights took 325.39 seconds
2026-02-20T18:26:25.1483659Z INFO 02-20 18:26:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:26:25.1492075Z INFO 02-20 18:26:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:26:25.1502762Z INFO 02-20 18:26:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:26:25.1524456Z INFO 02-20 18:26:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:26:25.1536906Z INFO 02-20 18:26:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:26:25.1545166Z INFO 02-20 18:26:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:26:25.1622368Z INFO 02-20 18:26:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:26:25.1631969Z INFO 02-20 18:26:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:26:25.1823691Z INFO 02-20 18:26:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:26:25.1832851Z INFO 02-20 18:26:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:26:25.1842632Z INFO 02-20 18:26:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:26:25.1853110Z INFO 02-20 18:26:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:26:25.1862553Z INFO 02-20 18:26:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:26:25.1873237Z INFO 02-20 18:26:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:26:25.1883017Z INFO 02-20 18:26:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:26:25.1892383Z INFO 02-20 18:26:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:26:25.1901664Z INFO 02-20 18:26:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:26:25.1911811Z INFO 02-20 18:26:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:26:25.1921961Z INFO 02-20 18:26:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:26:25.1932110Z INFO 02-20 18:26:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:26:25.1941662Z INFO 02-20 18:26:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:26:25.1950924Z INFO 02-20 18:26:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:26:25.1961144Z INFO 02-20 18:26:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:26:25.1970411Z INFO 02-20 18:26:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:26:25.1979693Z INFO 02-20 18:26:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:26:25.1989521Z INFO 02-20 18:26:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:26:25.1999497Z INFO 02-20 18:26:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:26:25.2009158Z INFO 02-20 18:26:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:26:25.2019264Z INFO 02-20 18:26:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:26:25.2029137Z INFO 02-20 18:26:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:26:25.2039761Z INFO 02-20 18:26:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:26:25.2048963Z INFO 02-20 18:26:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:26:25.2058505Z INFO 02-20 18:26:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:26:25.2067632Z INFO 02-20 18:26:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:26:25.2077842Z INFO 02-20 18:26:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:26:25.2087338Z INFO 02-20 18:26:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:26:25.2097138Z INFO 02-20 18:26:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:26:25.2112339Z INFO 02-20 18:26:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:26:25.2127242Z INFO 02-20 18:26:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:26:25.2127753Z INFO 02-20 18:26:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:26:25.2136630Z INFO 02-20 18:26:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:26:25.2167123Z INFO 02-20 18:26:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:26:25.2167470Z INFO 02-20 18:26:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:26:25.2167964Z INFO 02-20 18:26:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:26:25.2176441Z INFO 02-20 18:26:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:26:25.2185799Z INFO 02-20 18:26:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:26:25.2195152Z INFO 02-20 18:26:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:26:25.2205071Z INFO 02-20 18:26:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:26:25.2214160Z INFO 02-20 18:26:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:26:25.2223609Z INFO 02-20 18:26:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:26:25.2233157Z INFO 02-20 18:26:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:26:25.2243478Z INFO 02-20 18:26:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:26:25.2339343Z INFO 02-20 18:26:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:26:25.2348436Z INFO 02-20 18:26:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:26:25.2358265Z INFO 02-20 18:26:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:26:25.2423268Z INFO 02-20 18:26:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:26:25.3142603Z INFO 02-20 18:26:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:26:25.3153289Z INFO 02-20 18:26:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:26:25.3164393Z INFO 02-20 18:26:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:26:25.3174476Z INFO 02-20 18:26:25 [__init__.py:43] Available plugins for group vllm.platform_plugins:
2026-02-20T18:26:25.3185826Z INFO 02-20 18:26:25 [__init__.py:45] - ascend -> vllm_ascend:register
2026-02-20T18:26:25.3195840Z INFO 02-20 18:26:25 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2026-02-20T18:26:25.3249760Z INFO 02-20 18:26:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:26:25.3278017Z INFO 02-20 18:26:25 [__init__.py:217] Platform plugin ascend is activated
2026-02-20T18:26:52.4501747Z [0;36m(Worker_DP1_TP6_EP14 pid=785)[0;0m WARNING 02-20 18:26:52 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-20T18:26:52.4512273Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m WARNING 02-20 18:26:52 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-20T18:26:52.4574551Z [0;36m(Worker_DP0_TP1_EP1 pid=275)[0;0m WARNING 02-20 18:26:52 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-20T18:26:52.4626467Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m WARNING 02-20 18:26:52 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-20T18:26:52.4648141Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m WARNING 02-20 18:26:52 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-20T18:26:52.4667660Z [0;36m(Worker_DP0_TP2_EP2 pid=369)[0;0m WARNING 02-20 18:26:52 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-20T18:26:52.4687900Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m WARNING 02-20 18:26:52 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-20T18:26:52.4698105Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m WARNING 02-20 18:26:52 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-20T18:26:52.4867915Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m WARNING 02-20 18:26:52 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-20T18:26:52.4919920Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m WARNING 02-20 18:26:52 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-20T18:26:52.4988999Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m WARNING 02-20 18:26:52 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-20T18:26:52.5105176Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m WARNING 02-20 18:26:52 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-20T18:26:52.5132214Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m INFO 02-20 18:26:52 [model_runner_v1.py:2315] Loading drafter model...
2026-02-20T18:26:52.5142294Z [0;36m(Worker_DP1_TP6_EP14 pid=785)[0;0m INFO 02-20 18:26:52 [model_runner_v1.py:2315] Loading drafter model...
2026-02-20T18:26:52.5188683Z [0;36m(Worker_DP0_TP1_EP1 pid=275)[0;0m INFO 02-20 18:26:52 [model_runner_v1.py:2315] Loading drafter model...
2026-02-20T18:26:52.5222512Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-20 18:26:52 [model_runner_v1.py:2315] Loading drafter model...
2026-02-20T18:26:52.5242913Z [0;36m(Worker_DP1_TP7_EP15 pid=890)[0;0m WARNING 02-20 18:26:52 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-20T18:26:52.5292903Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-20 18:26:52 [model_runner_v1.py:2315] Loading drafter model...
2026-02-20T18:26:52.5318799Z [0;36m(Worker_DP0_TP2_EP2 pid=369)[0;0m INFO 02-20 18:26:52 [model_runner_v1.py:2315] Loading drafter model...
2026-02-20T18:26:52.5329746Z [0;36m(Worker_DP1_TP2_EP10 pid=372)[0;0m WARNING 02-20 18:26:52 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-20T18:26:52.5340039Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-20 18:26:52 [model_runner_v1.py:2315] Loading drafter model...
2026-02-20T18:26:52.5412479Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-20 18:26:52 [model_runner_v1.py:2315] Loading drafter model...
2026-02-20T18:26:52.5534956Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-20 18:26:52 [model_runner_v1.py:2315] Loading drafter model...
2026-02-20T18:26:52.5612525Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-20 18:26:52 [model_runner_v1.py:2315] Loading drafter model...
2026-02-20T18:26:52.5666957Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-20 18:26:52 [model_runner_v1.py:2315] Loading drafter model...
2026-02-20T18:26:52.5715991Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m INFO 02-20 18:26:52 [model_runner_v1.py:2315] Loading drafter model...
2026-02-20T18:26:52.5766501Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m WARNING 02-20 18:26:52 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-20T18:26:52.5937036Z [0;36m(Worker_DP1_TP2_EP10 pid=372)[0;0m INFO 02-20 18:26:52 [model_runner_v1.py:2315] Loading drafter model...
2026-02-20T18:26:52.6415643Z [0;36m(Worker_DP1_TP1_EP9 pid=276)[0;0m WARNING 02-20 18:26:52 [sfa_v1.py:523] Currently mlapo does not support SFA with CP,thus mlapo is disabled for these layers.
2026-02-20T18:26:52.6442952Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-20 18:26:52 [model_runner_v1.py:2315] Loading drafter model...
2026-02-20T18:26:52.6486771Z [0;36m(Worker_DP1_TP7_EP15 pid=890)[0;0m INFO 02-20 18:26:52 [model_runner_v1.py:2315] Loading drafter model...
2026-02-20T18:26:52.6519310Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:26:52.6519666Z Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
2026-02-20T18:26:52.7049508Z [0;36m(Worker_DP1_TP1_EP9 pid=276)[0;0m INFO 02-20 18:26:52 [model_runner_v1.py:2315] Loading drafter model...
2026-02-20T18:26:53.6029656Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:26:53.6030168Z Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<02:34,  1.05it/s]
2026-02-20T18:26:54.4060895Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:26:54.4061371Z Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:01<02:19,  1.16it/s]
2026-02-20T18:26:55.2554749Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:26:55.2555181Z Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:02<02:17,  1.17it/s]
2026-02-20T18:26:56.1192293Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:26:56.1192878Z Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:03<02:16,  1.16it/s]
2026-02-20T18:26:56.9030187Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:26:56.9030878Z Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:04<02:11,  1.20it/s]
2026-02-20T18:26:57.8009854Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:26:57.8010289Z Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:05<02:14,  1.17it/s]
2026-02-20T18:26:58.5313875Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:26:58.5314397Z Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:05<02:06,  1.23it/s]
2026-02-20T18:26:59.1029371Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:26:59.1029884Z Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:06<01:54,  1.36it/s]
2026-02-20T18:26:59.6950517Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:26:59.6951018Z Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:07<01:46,  1.45it/s]
2026-02-20T18:27:00.2907032Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:00.2907520Z Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:07<01:41,  1.51it/s]
2026-02-20T18:27:00.8680858Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:00.8681395Z Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:08<01:36,  1.58it/s]
2026-02-20T18:27:02.1275775Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:02.1276516Z Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:09<02:04,  1.21it/s]
2026-02-20T18:27:03.5090611Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:03.5091124Z Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:10<02:29,  1.01it/s]
2026-02-20T18:27:04.8701523Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:04.8701933Z Loading safetensors checkpoint shards:   9% Completed | 14/163 [00:12<02:44,  1.11s/it]
2026-02-20T18:27:06.0470987Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:06.0471422Z Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:13<02:46,  1.13s/it]
2026-02-20T18:27:07.1562922Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:07.1563327Z Loading safetensors checkpoint shards:  10% Completed | 16/163 [00:14<02:44,  1.12s/it]
2026-02-20T18:27:08.2091287Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:08.2091775Z Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:15<02:40,  1.10s/it]
2026-02-20T18:27:09.2568538Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:09.2569013Z Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:16<02:37,  1.09s/it]
2026-02-20T18:27:10.4113941Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:10.4114422Z Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:17<02:39,  1.11s/it]
2026-02-20T18:27:11.6568905Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:11.6569450Z Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:19<02:44,  1.15s/it]
2026-02-20T18:27:12.8088564Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:12.8089098Z Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:20<02:43,  1.15s/it]
2026-02-20T18:27:13.9582945Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:13.9583472Z Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:21<02:42,  1.15s/it]
2026-02-20T18:27:15.0741456Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:15.0742064Z Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:22<02:39,  1.14s/it]
2026-02-20T18:27:16.2246343Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:16.2246817Z Loading safetensors checkpoint shards:  15% Completed | 24/163 [00:23<02:38,  1.14s/it]
2026-02-20T18:27:17.2901623Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:17.2902136Z Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:24<02:34,  1.12s/it]
2026-02-20T18:27:18.5937323Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:18.5937815Z Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:25<02:40,  1.17s/it]
2026-02-20T18:27:19.8142915Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:19.8143368Z Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:27<02:41,  1.19s/it]
2026-02-20T18:27:20.8843498Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:20.8843991Z Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:28<02:35,  1.15s/it]
2026-02-20T18:27:22.0159356Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:22.0159872Z Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:29<02:33,  1.15s/it]
2026-02-20T18:27:23.1479112Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:23.1479568Z Loading safetensors checkpoint shards:  18% Completed | 30/163 [00:30<02:31,  1.14s/it]
2026-02-20T18:27:24.3947810Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:24.3948232Z Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:31<02:34,  1.17s/it]
2026-02-20T18:27:25.6270896Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:25.6271400Z Loading safetensors checkpoint shards:  20% Completed | 32/163 [00:32<02:36,  1.19s/it]
2026-02-20T18:27:26.8487030Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:26.8487552Z Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:34<02:36,  1.20s/it]
2026-02-20T18:27:27.9734415Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:27.9734847Z Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:35<02:31,  1.18s/it]
2026-02-20T18:27:29.1633249Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:29.1633710Z Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:36<02:31,  1.18s/it]
2026-02-20T18:27:30.2495604Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:30.2496037Z Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:37<02:26,  1.15s/it]
2026-02-20T18:27:31.3778254Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:31.3778753Z Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:38<02:24,  1.15s/it]
2026-02-20T18:27:32.4904516Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:32.4905045Z Loading safetensors checkpoint shards:  23% Completed | 38/163 [00:39<02:21,  1.14s/it]
2026-02-20T18:27:33.7094372Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:33.7094829Z Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:41<02:23,  1.16s/it]
2026-02-20T18:27:34.8763410Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:34.8763877Z Loading safetensors checkpoint shards:  25% Completed | 40/163 [00:42<02:22,  1.16s/it]
2026-02-20T18:27:36.0303451Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:36.0303880Z Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:43<02:21,  1.16s/it]
2026-02-20T18:27:37.1692950Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:37.1693385Z Loading safetensors checkpoint shards:  26% Completed | 42/163 [00:44<02:19,  1.15s/it]
2026-02-20T18:27:38.2520249Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:38.2520803Z Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:45<02:15,  1.13s/it]
2026-02-20T18:27:39.3460325Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:39.3460875Z Loading safetensors checkpoint shards:  27% Completed | 44/163 [00:46<02:13,  1.12s/it]
2026-02-20T18:27:40.5409414Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:40.5409982Z Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:47<02:14,  1.14s/it]
2026-02-20T18:27:41.7974718Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:41.7975292Z Loading safetensors checkpoint shards:  28% Completed | 46/163 [00:49<02:17,  1.18s/it]
2026-02-20T18:27:42.5817142Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:42.5817699Z Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:49<02:02,  1.06s/it]
2026-02-20T18:27:43.8375632Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:43.8376050Z Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:51<01:38,  1.16it/s]
2026-02-20T18:27:45.0628939Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:45.0629373Z Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:52<01:47,  1.05it/s]
2026-02-20T18:27:46.2210411Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:46.2210815Z Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:53<01:52,  1.00s/it]
2026-02-20T18:27:47.4346911Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:47.4347317Z Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:54<01:57,  1.06s/it]
2026-02-20T18:27:48.5420935Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:48.5421392Z Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:55<01:58,  1.07s/it]
2026-02-20T18:27:49.7027805Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:49.7028244Z Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:57<01:59,  1.10s/it]
2026-02-20T18:27:50.9460501Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:50.9460989Z Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:58<02:03,  1.14s/it]
2026-02-20T18:27:52.1777363Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:52.1778172Z Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:59<02:04,  1.17s/it]
2026-02-20T18:27:53.3834863Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:53.3835428Z Loading safetensors checkpoint shards:  35% Completed | 57/163 [01:00<02:04,  1.18s/it]
2026-02-20T18:27:54.6738472Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:54.6739028Z Loading safetensors checkpoint shards:  36% Completed | 58/163 [01:02<02:07,  1.21s/it]
2026-02-20T18:27:55.8183974Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:55.8184464Z Loading safetensors checkpoint shards:  36% Completed | 59/163 [01:03<02:03,  1.19s/it]
2026-02-20T18:27:56.9730783Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:56.9731308Z Loading safetensors checkpoint shards:  37% Completed | 60/163 [01:04<02:01,  1.18s/it]
2026-02-20T18:27:58.1224822Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:58.1225238Z Loading safetensors checkpoint shards:  37% Completed | 61/163 [01:05<01:59,  1.17s/it]
2026-02-20T18:27:59.2067936Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:27:59.2068389Z Loading safetensors checkpoint shards:  38% Completed | 62/163 [01:06<01:55,  1.15s/it]
2026-02-20T18:28:00.3176282Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:00.3176731Z Loading safetensors checkpoint shards:  39% Completed | 63/163 [01:07<01:53,  1.13s/it]
2026-02-20T18:28:01.5022759Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:01.5023239Z Loading safetensors checkpoint shards:  39% Completed | 64/163 [01:08<01:53,  1.15s/it]
2026-02-20T18:28:02.6070038Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:02.6070550Z Loading safetensors checkpoint shards:  40% Completed | 65/163 [01:09<01:51,  1.14s/it]
2026-02-20T18:28:03.6974626Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:03.6975086Z Loading safetensors checkpoint shards:  40% Completed | 66/163 [01:11<01:48,  1.12s/it]
2026-02-20T18:28:04.7834906Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:04.7835385Z Loading safetensors checkpoint shards:  41% Completed | 67/163 [01:12<01:46,  1.11s/it]
2026-02-20T18:28:05.8559228Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:05.8559765Z Loading safetensors checkpoint shards:  42% Completed | 68/163 [01:13<01:44,  1.10s/it]
2026-02-20T18:28:06.9080017Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:06.9080561Z Loading safetensors checkpoint shards:  42% Completed | 69/163 [01:14<01:42,  1.09s/it]
2026-02-20T18:28:08.0050428Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:08.0050976Z Loading safetensors checkpoint shards:  43% Completed | 70/163 [01:15<01:41,  1.09s/it]
2026-02-20T18:28:09.1459551Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:09.1460273Z Loading safetensors checkpoint shards:  44% Completed | 71/163 [01:16<01:41,  1.10s/it]
2026-02-20T18:28:10.2706827Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:10.2707403Z Loading safetensors checkpoint shards:  44% Completed | 72/163 [01:17<01:41,  1.11s/it]
2026-02-20T18:28:11.3944516Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:11.3944988Z Loading safetensors checkpoint shards:  45% Completed | 73/163 [01:18<01:40,  1.11s/it]
2026-02-20T18:28:12.4563635Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:12.4564100Z Loading safetensors checkpoint shards:  45% Completed | 74/163 [01:19<01:37,  1.10s/it]
2026-02-20T18:28:13.6317865Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:13.6318348Z Loading safetensors checkpoint shards:  46% Completed | 75/163 [01:20<01:38,  1.12s/it]
2026-02-20T18:28:14.8149523Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:14.8149951Z Loading safetensors checkpoint shards:  47% Completed | 76/163 [01:22<01:39,  1.14s/it]
2026-02-20T18:28:15.9387760Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:15.9388227Z Loading safetensors checkpoint shards:  47% Completed | 77/163 [01:23<01:37,  1.14s/it]
2026-02-20T18:28:17.1452942Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:17.1453621Z Loading safetensors checkpoint shards:  48% Completed | 78/163 [01:24<01:38,  1.16s/it]
2026-02-20T18:28:18.3260727Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:18.3261219Z Loading safetensors checkpoint shards:  48% Completed | 79/163 [01:25<01:37,  1.16s/it]
2026-02-20T18:28:19.4904423Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:19.4904869Z Loading safetensors checkpoint shards:  49% Completed | 80/163 [01:26<01:36,  1.16s/it]
2026-02-20T18:28:20.5814319Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:20.5814859Z Loading safetensors checkpoint shards:  50% Completed | 81/163 [01:27<01:33,  1.14s/it]
2026-02-20T18:28:21.6807969Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:21.6808494Z Loading safetensors checkpoint shards:  50% Completed | 82/163 [01:29<01:31,  1.13s/it]
2026-02-20T18:28:22.8044705Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:23.8723317Z Loading safetensors checkpoint shards:  51% Completed | 83/163 [01:30<01:30,  1.13s/it]
2026-02-20T18:28:23.8723981Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:23.8724437Z Loading safetensors checkpoint shards:  52% Completed | 84/163 [01:31<01:27,  1.11s/it]
2026-02-20T18:28:25.0114148Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:25.0114553Z Loading safetensors checkpoint shards:  52% Completed | 85/163 [01:32<01:27,  1.12s/it]
2026-02-20T18:28:26.2035987Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:26.2036395Z Loading safetensors checkpoint shards:  53% Completed | 86/163 [01:33<01:27,  1.14s/it]
2026-02-20T18:28:27.3145436Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:27.3145944Z Loading safetensors checkpoint shards:  53% Completed | 87/163 [01:34<01:26,  1.13s/it]
2026-02-20T18:28:28.3598454Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:28.3598943Z Loading safetensors checkpoint shards:  54% Completed | 88/163 [01:35<01:22,  1.11s/it]
2026-02-20T18:28:29.4242778Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:29.4243266Z Loading safetensors checkpoint shards:  55% Completed | 89/163 [01:36<01:20,  1.09s/it]
2026-02-20T18:28:30.4442899Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:30.4443379Z Loading safetensors checkpoint shards:  55% Completed | 90/163 [01:37<01:18,  1.07s/it]
2026-02-20T18:28:31.5529354Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:31.5529771Z Loading safetensors checkpoint shards:  56% Completed | 91/163 [01:38<01:17,  1.08s/it]
2026-02-20T18:28:32.5397228Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:32.5397751Z Loading safetensors checkpoint shards:  56% Completed | 92/163 [01:39<01:14,  1.05s/it]
2026-02-20T18:28:33.5529767Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:33.5530192Z Loading safetensors checkpoint shards:  57% Completed | 93/163 [01:40<01:12,  1.04s/it]
2026-02-20T18:28:34.6664624Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:34.6665191Z Loading safetensors checkpoint shards:  58% Completed | 94/163 [01:42<01:13,  1.06s/it]
2026-02-20T18:28:35.7376800Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:35.7377366Z Loading safetensors checkpoint shards:  58% Completed | 95/163 [01:43<01:12,  1.07s/it]
2026-02-20T18:28:36.8282224Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:36.8282767Z Loading safetensors checkpoint shards:  59% Completed | 96/163 [01:44<01:11,  1.07s/it]
2026-02-20T18:28:37.8997567Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:37.8998032Z Loading safetensors checkpoint shards:  60% Completed | 97/163 [01:45<01:10,  1.07s/it]
2026-02-20T18:28:39.0620737Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:39.0621211Z Loading safetensors checkpoint shards:  60% Completed | 98/163 [01:46<01:11,  1.10s/it]
2026-02-20T18:28:40.2366391Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:40.2366846Z Loading safetensors checkpoint shards:  61% Completed | 99/163 [01:47<01:11,  1.12s/it]
2026-02-20T18:28:41.3684883Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:41.3685312Z Loading safetensors checkpoint shards:  61% Completed | 100/163 [01:48<01:10,  1.12s/it]
2026-02-20T18:28:42.7606001Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:42.7606418Z Loading safetensors checkpoint shards:  62% Completed | 101/163 [01:50<01:14,  1.21s/it]
2026-02-20T18:28:43.8795072Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:43.8795524Z Loading safetensors checkpoint shards:  63% Completed | 102/163 [01:51<01:11,  1.18s/it]
2026-02-20T18:28:44.9453061Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:44.9453492Z Loading safetensors checkpoint shards:  63% Completed | 103/163 [01:52<01:08,  1.15s/it]
2026-02-20T18:28:45.9085126Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:45.9085611Z Loading safetensors checkpoint shards:  64% Completed | 104/163 [01:53<01:04,  1.09s/it]
2026-02-20T18:28:46.9214172Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:46.9214610Z Loading safetensors checkpoint shards:  64% Completed | 105/163 [01:54<01:01,  1.07s/it]
2026-02-20T18:28:47.9328299Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:47.9328773Z Loading safetensors checkpoint shards:  65% Completed | 106/163 [01:55<00:59,  1.05s/it]
2026-02-20T18:28:50.4045788Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:50.4046297Z Loading safetensors checkpoint shards:  66% Completed | 107/163 [01:57<01:22,  1.48s/it]
2026-02-20T18:28:51.1035415Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:51.1035839Z Loading safetensors checkpoint shards:  66% Completed | 108/163 [01:58<01:08,  1.24s/it]
2026-02-20T18:28:52.1585508Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:52.1585957Z Loading safetensors checkpoint shards:  67% Completed | 109/163 [01:59<01:04,  1.19s/it]
2026-02-20T18:28:53.2657058Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:53.2657512Z Loading safetensors checkpoint shards:  67% Completed | 110/163 [02:00<01:01,  1.16s/it]
2026-02-20T18:28:54.2989666Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:54.2990124Z Loading safetensors checkpoint shards:  68% Completed | 111/163 [02:01<00:58,  1.12s/it]
2026-02-20T18:28:55.2889475Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:55.2889901Z Loading safetensors checkpoint shards:  69% Completed | 112/163 [02:02<00:55,  1.08s/it]
2026-02-20T18:28:56.2911169Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:56.2911645Z Loading safetensors checkpoint shards:  69% Completed | 113/163 [02:03<00:52,  1.06s/it]
2026-02-20T18:28:57.3618020Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:57.3618963Z Loading safetensors checkpoint shards:  70% Completed | 114/163 [02:04<00:52,  1.06s/it]
2026-02-20T18:28:58.4538968Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:58.4539513Z Loading safetensors checkpoint shards:  71% Completed | 115/163 [02:05<00:51,  1.07s/it]
2026-02-20T18:28:59.6048513Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:28:59.6049105Z Loading safetensors checkpoint shards:  71% Completed | 116/163 [02:06<00:51,  1.10s/it]
2026-02-20T18:29:00.7178376Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:00.7178936Z Loading safetensors checkpoint shards:  72% Completed | 117/163 [02:08<00:50,  1.10s/it]
2026-02-20T18:29:01.7448158Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:01.7448710Z Loading safetensors checkpoint shards:  72% Completed | 118/163 [02:09<00:48,  1.08s/it]
2026-02-20T18:29:02.8471859Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:02.8472529Z Loading safetensors checkpoint shards:  73% Completed | 119/163 [02:10<00:47,  1.09s/it]
2026-02-20T18:29:03.9449872Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:03.9450405Z Loading safetensors checkpoint shards:  74% Completed | 120/163 [02:11<00:46,  1.09s/it]
2026-02-20T18:29:05.1081499Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:05.1082294Z Loading safetensors checkpoint shards:  74% Completed | 121/163 [02:12<00:46,  1.11s/it]
2026-02-20T18:29:06.2806697Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:06.2807118Z Loading safetensors checkpoint shards:  75% Completed | 122/163 [02:13<00:46,  1.13s/it]
2026-02-20T18:29:07.4777653Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:07.4778136Z Loading safetensors checkpoint shards:  75% Completed | 123/163 [02:14<00:46,  1.15s/it]
2026-02-20T18:29:07.8738218Z [0;36m(ApiServer_2 pid=186)[0;0m Process ApiServer_2:
2026-02-20T18:29:07.8804077Z [0;36m(ApiServer_1 pid=185)[0;0m Process ApiServer_1:
2026-02-20T18:29:07.8830330Z [0;36m(ApiServer_2 pid=186)[0;0m Traceback (most recent call last):
2026-02-20T18:29:07.8840494Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-20T18:29:07.8850162Z [0;36m(ApiServer_2 pid=186)[0;0m     self.run()
2026-02-20T18:29:07.8860722Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-20T18:29:07.8870039Z [0;36m(ApiServer_2 pid=186)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-20T18:29:07.8880651Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-20T18:29:07.8889920Z [0;36m(ApiServer_2 pid=186)[0;0m     uvloop.run(
2026-02-20T18:29:07.8902302Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-20T18:29:07.8912174Z [0;36m(ApiServer_2 pid=186)[0;0m     return runner.run(wrapper())
2026-02-20T18:29:07.8923022Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:07.8933087Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-20T18:29:07.8943361Z [0;36m(ApiServer_2 pid=186)[0;0m     return self._loop.run_until_complete(task)
2026-02-20T18:29:07.8952493Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:07.8963407Z [0;36m(ApiServer_2 pid=186)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-20T18:29:07.8974191Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-20T18:29:07.8983465Z [0;36m(ApiServer_2 pid=186)[0;0m     return await main
2026-02-20T18:29:07.8992932Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^
2026-02-20T18:29:07.9004137Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-20T18:29:07.9013567Z [0;36m(ApiServer_2 pid=186)[0;0m     async with build_async_engine_client(
2026-02-20T18:29:07.9023727Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-20T18:29:07.9032844Z [0;36m(ApiServer_2 pid=186)[0;0m     return await anext(self.gen)
2026-02-20T18:29:07.9043324Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:07.9053016Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-20T18:29:07.9062677Z [0;36m(ApiServer_2 pid=186)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-20T18:29:07.9072373Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-20T18:29:07.9133330Z [0;36m(ApiServer_2 pid=186)[0;0m     return await anext(self.gen)
2026-02-20T18:29:07.9133741Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:07.9134570Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-20T18:29:07.9135200Z [0;36m(ApiServer_2 pid=186)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-20T18:29:07.9135636Z [0;36m(ApiServer_2 pid=186)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:07.9136212Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-20T18:29:07.9140405Z [0;36m(ApiServer_2 pid=186)[0;0m     return cls(
2026-02-20T18:29:07.9149998Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^
2026-02-20T18:29:07.9160520Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-20T18:29:07.9170883Z [0;36m(ApiServer_2 pid=186)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-20T18:29:07.9179759Z [0;36m(ApiServer_2 pid=186)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:07.9189394Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-20T18:29:07.9199208Z [0;36m(ApiServer_2 pid=186)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-20T18:29:07.9208828Z [0;36m(ApiServer_2 pid=186)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:07.9218635Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-20T18:29:07.9228511Z [0;36m(ApiServer_2 pid=186)[0;0m     super().__init__(
2026-02-20T18:29:07.9238925Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-20T18:29:07.9248519Z [0;36m(ApiServer_2 pid=186)[0;0m     super().__init__(
2026-02-20T18:29:07.9258535Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-20T18:29:07.9268154Z [0;36m(ApiServer_2 pid=186)[0;0m     super().__init__(
2026-02-20T18:29:07.9278525Z [0;36m(ApiServer_2 pid=186)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-20T18:29:07.9288336Z [0;36m(ApiServer_2 pid=186)[0;0m     raise TimeoutError(
2026-02-20T18:29:07.9297917Z [0;36m(ApiServer_2 pid=186)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-20T18:29:07.9307884Z [0;36m(ApiServer_1 pid=185)[0;0m Traceback (most recent call last):
2026-02-20T18:29:07.9318356Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-20T18:29:07.9327607Z [0;36m(ApiServer_1 pid=185)[0;0m     self.run()
2026-02-20T18:29:07.9337350Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-20T18:29:07.9346529Z [0;36m(ApiServer_1 pid=185)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-20T18:29:07.9359558Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-20T18:29:07.9371453Z [0;36m(ApiServer_1 pid=185)[0;0m     uvloop.run(
2026-02-20T18:29:07.9383292Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-20T18:29:07.9392327Z [0;36m(ApiServer_1 pid=185)[0;0m     return runner.run(wrapper())
2026-02-20T18:29:07.9402068Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:07.9412542Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-20T18:29:07.9422333Z [0;36m(ApiServer_1 pid=185)[0;0m     return self._loop.run_until_complete(task)
2026-02-20T18:29:07.9431827Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:07.9442160Z [0;36m(ApiServer_1 pid=185)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-20T18:29:07.9451106Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-20T18:29:07.9460298Z [0;36m(ApiServer_1 pid=185)[0;0m     return await main
2026-02-20T18:29:07.9470021Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^
2026-02-20T18:29:07.9480394Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-20T18:29:07.9490189Z [0;36m(ApiServer_1 pid=185)[0;0m     async with build_async_engine_client(
2026-02-20T18:29:07.9500430Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-20T18:29:07.9509447Z [0;36m(ApiServer_1 pid=185)[0;0m     return await anext(self.gen)
2026-02-20T18:29:07.9519357Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:07.9529684Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-20T18:29:07.9539193Z [0;36m(ApiServer_1 pid=185)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-20T18:29:07.9548025Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-20T18:29:07.9557731Z [0;36m(ApiServer_1 pid=185)[0;0m     return await anext(self.gen)
2026-02-20T18:29:07.9566811Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:07.9576838Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-20T18:29:07.9586354Z [0;36m(ApiServer_1 pid=185)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-20T18:29:07.9596052Z [0;36m(ApiServer_1 pid=185)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:07.9606028Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-20T18:29:07.9614982Z [0;36m(ApiServer_1 pid=185)[0;0m     return cls(
2026-02-20T18:29:07.9624692Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^
2026-02-20T18:29:07.9635110Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-20T18:29:07.9648310Z [0;36m(ApiServer_1 pid=185)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-20T18:29:07.9659063Z [0;36m(ApiServer_1 pid=185)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:07.9668878Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-20T18:29:07.9678566Z [0;36m(ApiServer_1 pid=185)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-20T18:29:07.9687674Z [0;36m(ApiServer_1 pid=185)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:07.9697712Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-20T18:29:07.9706096Z [0;36m(ApiServer_1 pid=185)[0;0m     super().__init__(
2026-02-20T18:29:07.9715855Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-20T18:29:07.9725873Z [0;36m(ApiServer_1 pid=185)[0;0m     super().__init__(
2026-02-20T18:29:07.9735183Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-20T18:29:07.9744515Z [0;36m(ApiServer_1 pid=185)[0;0m     super().__init__(
2026-02-20T18:29:07.9754280Z [0;36m(ApiServer_1 pid=185)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-20T18:29:07.9765574Z [0;36m(ApiServer_1 pid=185)[0;0m     raise TimeoutError(
2026-02-20T18:29:07.9776103Z [0;36m(ApiServer_1 pid=185)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-20T18:29:07.9785613Z [0;36m(ApiServer_0 pid=184)[0;0m Process ApiServer_0:
2026-02-20T18:29:07.9795949Z [0;36m(ApiServer_3 pid=187)[0;0m Process ApiServer_3:
2026-02-20T18:29:07.9807015Z [0;36m(ApiServer_0 pid=184)[0;0m Traceback (most recent call last):
2026-02-20T18:29:07.9816996Z [0;36m(ApiServer_3 pid=187)[0;0m Traceback (most recent call last):
2026-02-20T18:29:07.9827260Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-20T18:29:07.9837680Z [0;36m(ApiServer_0 pid=184)[0;0m     self.run()
2026-02-20T18:29:07.9847412Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-20T18:29:07.9857719Z [0;36m(ApiServer_0 pid=184)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-20T18:29:07.9866535Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-20T18:29:07.9875379Z [0;36m(ApiServer_0 pid=184)[0;0m     uvloop.run(
2026-02-20T18:29:07.9885541Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-20T18:29:07.9895105Z [0;36m(ApiServer_0 pid=184)[0;0m     return runner.run(wrapper())
2026-02-20T18:29:07.9904966Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:07.9914550Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-20T18:29:07.9924833Z [0;36m(ApiServer_0 pid=184)[0;0m     return self._loop.run_until_complete(task)
2026-02-20T18:29:07.9934139Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:07.9944225Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2026-02-20T18:29:07.9953623Z [0;36m(ApiServer_0 pid=184)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-20T18:29:07.9963232Z [0;36m(ApiServer_3 pid=187)[0;0m     self.run()
2026-02-20T18:29:07.9973542Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-20T18:29:07.9982846Z [0;36m(ApiServer_0 pid=184)[0;0m     return await main
2026-02-20T18:29:07.9991340Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^
2026-02-20T18:29:08.0001116Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/multiprocessing/process.py", line 108, in run
2026-02-20T18:29:08.0010549Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-20T18:29:08.0019430Z [0;36m(ApiServer_3 pid=187)[0;0m     self._target(*self._args, **self._kwargs)
2026-02-20T18:29:08.0029009Z [0;36m(ApiServer_0 pid=184)[0;0m     async with build_async_engine_client(
2026-02-20T18:29:08.0038691Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 300, in run_api_server_worker_proc
2026-02-20T18:29:08.0048645Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-20T18:29:08.0056657Z [0;36m(ApiServer_0 pid=184)[0;0m     return await anext(self.gen)
2026-02-20T18:29:08.0066082Z [0;36m(ApiServer_3 pid=187)[0;0m     uvloop.run(
2026-02-20T18:29:08.0075328Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:08.0085635Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
2026-02-20T18:29:08.0095348Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-20T18:29:08.0105596Z [0;36m(ApiServer_0 pid=184)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-20T18:29:08.0113687Z [0;36m(ApiServer_3 pid=187)[0;0m     return runner.run(wrapper())
2026-02-20T18:29:08.0123689Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:08.0133190Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-20T18:29:08.0159233Z [0;36m(ApiServer_0 pid=184)[0;0m     return await anext(self.gen)
2026-02-20T18:29:08.0169942Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:08.0179830Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-20T18:29:08.0189461Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/asyncio/runners.py", line 118, in run
2026-02-20T18:29:08.0198529Z [0;36m(ApiServer_0 pid=184)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-20T18:29:08.0208417Z [0;36m(ApiServer_3 pid=187)[0;0m     return self._loop.run_until_complete(task)
2026-02-20T18:29:08.0229300Z [0;36m(ApiServer_0 pid=184)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:08.0239091Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:08.0248906Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-20T18:29:08.0258371Z [0;36m(ApiServer_3 pid=187)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
2026-02-20T18:29:08.0267430Z [0;36m(ApiServer_0 pid=184)[0;0m     return cls(
2026-02-20T18:29:08.0277509Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^
2026-02-20T18:29:08.0287680Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
2026-02-20T18:29:08.0297024Z [0;36m(ApiServer_3 pid=187)[0;0m     return await main
2026-02-20T18:29:08.0364919Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-20T18:29:08.0365422Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^
2026-02-20T18:29:08.0365867Z [0;36m(ApiServer_0 pid=184)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-20T18:29:08.0366354Z [0;36m(ApiServer_0 pid=184)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:08.0366938Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
2026-02-20T18:29:08.0367637Z [0;36m(ApiServer_3 pid=187)[0;0m     async with build_async_engine_client(
2026-02-20T18:29:08.0368276Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-20T18:29:08.0376300Z [0;36m(ApiServer_0 pid=184)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-20T18:29:08.0386169Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-20T18:29:08.0395313Z [0;36m(ApiServer_0 pid=184)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:08.0405763Z [0;36m(ApiServer_3 pid=187)[0;0m     return await anext(self.gen)
2026-02-20T18:29:08.0414290Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:08.0424035Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-20T18:29:08.0433350Z [0;36m(ApiServer_0 pid=184)[0;0m     super().__init__(
2026-02-20T18:29:08.0443317Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 147, in build_async_engine_client
2026-02-20T18:29:08.0452750Z [0;36m(ApiServer_3 pid=187)[0;0m     async with build_async_engine_client_from_engine_args(
2026-02-20T18:29:08.0461970Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-20T18:29:08.0470761Z [0;36m(ApiServer_0 pid=184)[0;0m     super().__init__(
2026-02-20T18:29:08.0480979Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/usr/local/python3.11.14/lib/python3.11/contextlib.py", line 210, in __aenter__
2026-02-20T18:29:08.0490537Z [0;36m(ApiServer_3 pid=187)[0;0m     return await anext(self.gen)
2026-02-20T18:29:08.0499599Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-20T18:29:08.0509268Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:08.0519241Z [0;36m(ApiServer_0 pid=184)[0;0m     super().__init__(
2026-02-20T18:29:08.0529240Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/entrypoints/openai/api_server.py", line 188, in build_async_engine_client_from_engine_args
2026-02-20T18:29:08.0538464Z [0;36m(ApiServer_0 pid=184)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-20T18:29:08.0547220Z [0;36m(ApiServer_0 pid=184)[0;0m     raise TimeoutError(
2026-02-20T18:29:08.0557011Z [0;36m(ApiServer_3 pid=187)[0;0m     async_llm = AsyncLLM.from_vllm_config(
2026-02-20T18:29:08.0569784Z [0;36m(ApiServer_3 pid=187)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:08.0575251Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 228, in from_vllm_config
2026-02-20T18:29:08.0584628Z [0;36m(ApiServer_3 pid=187)[0;0m     return cls(
2026-02-20T18:29:08.0594232Z [0;36m(ApiServer_0 pid=184)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-20T18:29:08.0603597Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^
2026-02-20T18:29:08.0616516Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/async_llm.py", line 155, in __init__
2026-02-20T18:29:08.0622823Z [0;36m(ApiServer_3 pid=187)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
2026-02-20T18:29:08.0632292Z [0;36m(ApiServer_3 pid=187)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:08.0642699Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 121, in make_async_mp_client
2026-02-20T18:29:08.0652300Z [0;36m(ApiServer_3 pid=187)[0;0m     return DPLBAsyncMPClient(*client_args)
2026-02-20T18:29:08.0661361Z [0;36m(ApiServer_3 pid=187)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2026-02-20T18:29:08.0670916Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1191, in __init__
2026-02-20T18:29:08.0680537Z [0;36m(ApiServer_3 pid=187)[0;0m     super().__init__(
2026-02-20T18:29:08.0690598Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 1032, in __init__
2026-02-20T18:29:08.0699146Z [0;36m(ApiServer_3 pid=187)[0;0m     super().__init__(
2026-02-20T18:29:08.0708946Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
2026-02-20T18:29:08.0718346Z [0;36m(ApiServer_3 pid=187)[0;0m     super().__init__(
2026-02-20T18:29:08.0728083Z [0;36m(ApiServer_3 pid=187)[0;0m   File "/vllm-workspace/vllm/vllm/v1/engine/core_client.py", line 530, in __init__
2026-02-20T18:29:08.0737442Z [0;36m(ApiServer_3 pid=187)[0;0m     raise TimeoutError(
2026-02-20T18:29:08.0746774Z [0;36m(ApiServer_3 pid=187)[0;0m TimeoutError: Timed out waiting for engines to send initial message on input socket.
2026-02-20T18:29:08.3385689Z [0;36m(ApiServer_2 pid=186)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-20T18:29:08.3629037Z [0;36m(ApiServer_0 pid=184)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-20T18:29:08.3760405Z [0;36m(ApiServer_3 pid=187)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-20T18:29:08.3771128Z [0;36m(ApiServer_1 pid=185)[0;0m sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-20T18:29:08.6940825Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:08.6941270Z Loading safetensors checkpoint shards:  76% Completed | 124/163 [02:16<00:45,  1.17s/it]
2026-02-20T18:29:09.8663330Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:09.8663798Z Loading safetensors checkpoint shards:  77% Completed | 125/163 [02:17<00:44,  1.17s/it]
2026-02-20T18:29:10.8906103Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:10.8906590Z Loading safetensors checkpoint shards:  77% Completed | 126/163 [02:18<00:41,  1.13s/it]
2026-02-20T18:29:11.0237093Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:11.0237590Z Loading safetensors checkpoint shards:  80% Completed | 130/163 [02:18<00:14,  2.29it/s]
2026-02-20T18:29:11.1369235Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:11.1369693Z Loading safetensors checkpoint shards:  82% Completed | 133/163 [02:18<00:08,  3.63it/s]
2026-02-20T18:29:11.2450105Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:11.2450537Z Loading safetensors checkpoint shards:  83% Completed | 136/163 [02:18<00:05,  5.32it/s]
2026-02-20T18:29:11.3537113Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:11.3537542Z Loading safetensors checkpoint shards:  85% Completed | 139/163 [02:18<00:03,  7.36it/s]
2026-02-20T18:29:11.4640974Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:11.4641461Z Loading safetensors checkpoint shards:  87% Completed | 142/163 [02:18<00:02,  9.69it/s]
2026-02-20T18:29:11.5745609Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:11.5746145Z Loading safetensors checkpoint shards:  89% Completed | 145/163 [02:18<00:01, 12.19it/s]
2026-02-20T18:29:11.6841482Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:11.6841934Z Loading safetensors checkpoint shards:  91% Completed | 148/163 [02:19<00:01, 14.76it/s]
2026-02-20T18:29:17.0251326Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:17.0251775Z Loading safetensors checkpoint shards:  93% Completed | 151/163 [02:24<00:07,  1.67it/s]
2026-02-20T18:29:17.1343125Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:17.1343564Z Loading safetensors checkpoint shards:  94% Completed | 154/163 [02:24<00:03,  2.35it/s]
2026-02-20T18:29:17.2429792Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:17.2430347Z Loading safetensors checkpoint shards:  96% Completed | 157/163 [02:24<00:01,  3.26it/s]
2026-02-20T18:29:17.3510136Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:17.3510613Z Loading safetensors checkpoint shards:  98% Completed | 160/163 [02:24<00:00,  4.45it/s]
2026-02-20T18:29:17.4581135Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:17.4581648Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [02:24<00:00,  5.96it/s]
2026-02-20T18:29:17.4603775Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:17.4614114Z Loading safetensors checkpoint shards: 100% Completed | 163/163 [02:24<00:00,  1.13it/s]
2026-02-20T18:29:17.4614557Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:29:17.5052218Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m INFO 02-20 18:29:17 [default_loader.py:291] Loading weights took 144.85 seconds
2026-02-20T18:29:17.5413187Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m INFO 02-20 18:29:17 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-20T18:29:17.5762695Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m INFO 02-20 18:29:17 [default_loader.py:291] Loading weights took 144.92 seconds
2026-02-20T18:29:17.6130111Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m INFO 02-20 18:29:17 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-20T18:29:17.7333607Z [0;36m(Worker_DP1_TP7_EP15 pid=890)[0;0m INFO 02-20 18:29:17 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-20T18:29:17.7986421Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-20 18:29:17 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-20T18:29:17.8246896Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-20 18:29:17 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-20T18:29:17.8658848Z [0;36m(Worker_DP0_TP1_EP1 pid=275)[0;0m INFO 02-20 18:29:17 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-20T18:29:17.8872357Z [0;36m(Worker_DP1_TP2_EP10 pid=372)[0;0m INFO 02-20 18:29:17 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-20T18:29:17.9070395Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-20 18:29:17 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-20T18:29:18.0114970Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-20 18:29:18 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-20T18:29:18.0535796Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-20 18:29:18 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-20T18:29:18.0994155Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-20 18:29:18 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-20T18:29:18.2825287Z [0;36m(Worker_DP0_TP2_EP2 pid=369)[0;0m INFO 02-20 18:29:18 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-20T18:29:18.3321057Z [0;36m(Worker_DP1_TP1_EP9 pid=276)[0;0m INFO 02-20 18:29:18 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-20T18:29:18.3347781Z [0;36m(Worker_DP1_TP6_EP14 pid=785)[0;0m INFO 02-20 18:29:18 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-20T18:29:18.3995332Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-20 18:29:18 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-20T18:29:18.4194409Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-20 18:29:18 [eagle_proposer.py:236] Detected MTP model. Sharing target model embedding weights with the draft model.
2026-02-20T18:29:19.0027954Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-20 18:29:19 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-20T18:29:19.0238454Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m INFO 02-20 18:29:19 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-20T18:29:19.2019758Z [0;36m(Worker_DP1_TP2_EP10 pid=372)[0;0m INFO 02-20 18:29:19 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-20T18:29:19.5142194Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m INFO 02-20 18:29:19 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-20T18:29:19.5632096Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-20 18:29:19 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-20T18:29:19.8097528Z [0;36m(Worker_DP1_TP1_EP9 pid=276)[0;0m INFO 02-20 18:29:19 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-20T18:29:19.8960208Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-20 18:29:19 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-20T18:29:20.1441046Z [0;36m(Worker_DP1_TP7_EP15 pid=890)[0;0m INFO 02-20 18:29:20 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-20T18:29:20.5016431Z [0;36m(Worker_DP0_TP1_EP1 pid=275)[0;0m INFO 02-20 18:29:20 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-20T18:29:20.6656304Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-20 18:29:20 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-20T18:29:20.7843991Z [0;36m(Worker_DP1_TP6_EP14 pid=785)[0;0m INFO 02-20 18:29:20 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-20T18:29:20.8907964Z [0;36m(Worker_DP0_TP2_EP2 pid=369)[0;0m INFO 02-20 18:29:20 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-20T18:29:20.9180853Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-20 18:29:20 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-20T18:29:20.9406099Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-20 18:29:20 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-20T18:29:21.1766340Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-20 18:29:21 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-20T18:29:21.2483534Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-20 18:29:21 [model_runner_v1.py:2323] Loading model weights took 30.8108 GB
2026-02-20T18:29:42.6581879Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m INFO 02-20 18:29:42 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/5f4e5deba5/rank_0_1/backbone for vLLM's torch.compile
2026-02-20T18:29:42.6612598Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m INFO 02-20 18:29:42 [backends.py:865] Dynamo bytecode transform time: 4.63 s
2026-02-20T18:29:42.8256132Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m INFO 02-20 18:29:42 [backends.py:805] Using cache directory: /root/.cache/vllm/torch_compile_cache/5f4e5deba5/rank_0_0/backbone for vLLM's torch.compile
2026-02-20T18:29:42.8285679Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m INFO 02-20 18:29:42 [backends.py:865] Dynamo bytecode transform time: 4.80 s
2026-02-20T18:29:48.6435007Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-20T18:29:48.6443117Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m   warnings.warn(
2026-02-20T18:29:48.6506813Z [0;36m(Worker_DP0_TP2_EP2 pid=369)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-20T18:29:48.6516063Z [0;36m(Worker_DP0_TP2_EP2 pid=369)[0;0m   warnings.warn(
2026-02-20T18:29:48.6654189Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-20T18:29:48.6655375Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m   warnings.warn(
2026-02-20T18:29:48.6710336Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-20T18:29:48.6711484Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m   warnings.warn(
2026-02-20T18:29:48.6721007Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-20T18:29:48.6724414Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m   warnings.warn(
2026-02-20T18:29:48.6752950Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-20T18:29:48.6763618Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m   warnings.warn(
2026-02-20T18:29:48.6779669Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-20T18:29:48.6791075Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m   warnings.warn(
2026-02-20T18:29:48.6940531Z [0;36m(Worker_DP1_TP2_EP10 pid=372)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-20T18:29:48.6950075Z [0;36m(Worker_DP1_TP2_EP10 pid=372)[0;0m   warnings.warn(
2026-02-20T18:29:48.7039377Z [0;36m(Worker_DP1_TP1_EP9 pid=276)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-20T18:29:48.7047165Z [0;36m(Worker_DP1_TP1_EP9 pid=276)[0;0m   warnings.warn(
2026-02-20T18:29:48.7061001Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-20T18:29:48.7069362Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m   warnings.warn(
2026-02-20T18:29:48.7189108Z [0;36m(Worker_DP1_TP6_EP14 pid=785)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-20T18:29:48.7197995Z [0;36m(Worker_DP1_TP6_EP14 pid=785)[0;0m   warnings.warn(
2026-02-20T18:29:48.7381551Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-20T18:29:48.7390965Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m   warnings.warn(
2026-02-20T18:29:48.7436798Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-20T18:29:48.7445463Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m   warnings.warn(
2026-02-20T18:29:48.7502077Z [0;36m(Worker_DP0_TP1_EP1 pid=275)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-20T18:29:48.7511272Z [0;36m(Worker_DP0_TP1_EP1 pid=275)[0;0m   warnings.warn(
2026-02-20T18:29:48.8328001Z [0;36m(Worker_DP1_TP7_EP15 pid=890)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-20T18:29:48.8336142Z [0;36m(Worker_DP1_TP7_EP15 pid=890)[0;0m   warnings.warn(
2026-02-20T18:29:48.8349614Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in reduce-overhead mode: config.experimental_config.enable_view_optimize:True, set_dim_gears, dynamo_export, scope, npu_print
2026-02-20T18:29:48.8359605Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m   warnings.warn(
2026-02-20T18:30:02.4579586Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m INFO 02-20 18:30:02 [backends.py:319] Compiling a graph for compile range (1, 4096) takes 13.72 s
2026-02-20T18:30:02.4725998Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m INFO 02-20 18:30:02 [monitor.py:34] torch.compile takes 18.35 s in total
2026-02-20T18:30:02.9678986Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m INFO 02-20 18:30:02 [backends.py:319] Compiling a graph for compile range (1, 4096) takes 14.14 s
2026-02-20T18:30:02.9698253Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m INFO 02-20 18:30:02 [monitor.py:34] torch.compile takes 18.93 s in total
2026-02-20T18:30:08.6727130Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-20 18:30:08 [worker.py:338] Available memory: 18272887296, total memory: 65787658240
2026-02-20T18:30:08.6757876Z [0;36m(Worker_DP0_TP2_EP2 pid=369)[0;0m INFO 02-20 18:30:08 [worker.py:338] Available memory: 18264507904, total memory: 65787658240
2026-02-20T18:30:08.6935361Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-20 18:30:08 [worker.py:338] Available memory: 18547960524, total memory: 65796046848
2026-02-20T18:30:08.7253138Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-20 18:30:08 [worker.py:338] Available memory: 18265229824, total memory: 65787658240
2026-02-20T18:30:08.7327522Z [0;36m(Worker_DP1_TP1_EP9 pid=276)[0;0m INFO 02-20 18:30:08 [worker.py:338] Available memory: 18545181388, total memory: 65796046848
2026-02-20T18:30:08.7422585Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-20 18:30:08 [worker.py:338] Available memory: 18245128704, total memory: 65787658240
2026-02-20T18:30:08.7498966Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-20 18:30:08 [worker.py:338] Available memory: 18536655564, total memory: 65796046848
2026-02-20T18:30:09.0447126Z [0;36m(Worker_DP0_TP1_EP1 pid=275)[0;0m INFO 02-20 18:30:09 [worker.py:338] Available memory: 18550586060, total memory: 65796046848
2026-02-20T18:30:09.0870715Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m INFO 02-20 18:30:09 [worker.py:338] Available memory: 17871800832, total memory: 65787658240
2026-02-20T18:30:09.1776507Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-20 18:30:09 [worker.py:338] Available memory: 18541171404, total memory: 65796046848
2026-02-20T18:30:09.3291760Z [0;36m(Worker_DP1_TP2_EP10 pid=372)[0;0m INFO 02-20 18:30:09 [worker.py:338] Available memory: 18259104256, total memory: 65787658240
2026-02-20T18:30:09.3436586Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-20 18:30:09 [worker.py:338] Available memory: 18546885324, total memory: 65796046848
2026-02-20T18:30:09.3638910Z [0;36m(Worker_DP1_TP7_EP15 pid=890)[0;0m INFO 02-20 18:30:09 [worker.py:338] Available memory: 18567466700, total memory: 65796046848
2026-02-20T18:30:09.5212114Z [0;36m(Worker_DP1_TP6_EP14 pid=785)[0;0m INFO 02-20 18:30:09 [worker.py:338] Available memory: 18255312384, total memory: 65787658240
2026-02-20T18:30:09.7496673Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-20 18:30:09 [worker.py:338] Available memory: 18549283532, total memory: 65796046848
2026-02-20T18:30:09.7504740Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-20 18:30:09 [kv_cache_utils.py:1307] GPU KV cache size: 204,672 tokens
2026-02-20T18:30:09.7514072Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-20 18:30:09 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 24.98x
2026-02-20T18:30:09.9970633Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m INFO 02-20 18:30:09 [worker.py:338] Available memory: 17877830144, total memory: 65787658240
2026-02-20T18:30:09.9999085Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-20 18:30:09 [kv_cache_utils.py:1307] GPU KV cache size: 204,672 tokens
2026-02-20T18:30:10.0008263Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-20 18:30:09 [kv_cache_utils.py:1312] Maximum concurrency for 8,192 tokens per request: 24.98x
2026-02-20T18:30:30.4263190Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m 
2026-02-20T18:30:30.4265329Z Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s][rank13]:[W220 18:30:30.277310015 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-20T18:30:30.4288824Z [rank14]:[W220 18:30:30.278306654 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-20T18:30:30.4299494Z [rank15]:[W220 18:30:30.278319014 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-20T18:30:30.4310813Z [rank12]:[W220 18:30:30.278376785 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-20T18:30:30.4321926Z [rank8]:[W220 18:30:30.278641817 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-20T18:30:30.4332853Z [rank6]:[W220 18:30:30.279715837 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-20T18:30:30.4343524Z [rank4]:[W220 18:30:30.279715117 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-20T18:30:30.4353942Z [rank5]:[W220 18:30:30.279715107 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-20T18:30:30.4370616Z [rank2]:[W220 18:30:30.279741247 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-20T18:30:30.4377906Z [rank9]:[W220 18:30:30.280604845 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-20T18:30:30.4400891Z [rank7]:[W220 18:30:30.284234747 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-20T18:30:30.4401698Z [rank3]:[W220 18:30:30.284382118 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-20T18:30:30.4408549Z [rank10]:[W220 18:30:30.285357517 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-20T18:30:30.4423383Z [rank0]:[W220 18:30:30.289240362 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-20T18:30:30.4440009Z [rank1]:[W220 18:30:30.296174854 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-20T18:30:30.4466696Z [rank11]:[W220 18:30:30.296787759 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
2026-02-20T18:30:32.8117476Z [rank4]:[W220 18:30:32.663234990 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-20T18:30:32.8135004Z [rank3]:[W220 18:30:32.663246020 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-20T18:30:32.8152643Z [rank1]:[W220 18:30:32.663795245 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-20T18:30:32.8162572Z [rank5]:[W220 18:30:32.665682322 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-20T18:30:32.8172365Z [rank6]:[W220 18:30:32.665730792 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-20T18:30:32.8182547Z [rank0]:[W220 18:30:32.666542740 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-20T18:30:32.8192455Z [rank2]:[W220 18:30:32.666964383 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-20T18:30:32.8203117Z [rank7]:[W220 18:30:32.667457398 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-20T18:30:32.8429313Z [rank14]:[W220 18:30:32.695011854 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-20T18:30:32.8437902Z [rank11]:[W220 18:30:32.695015644 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-20T18:30:32.8450079Z [rank13]:[W220 18:30:32.695175085 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-20T18:30:32.8459520Z [rank8]:[W220 18:30:32.695435588 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-20T18:30:32.8469084Z [rank12]:[W220 18:30:32.696991202 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-20T18:30:32.8479857Z [rank9]:[W220 18:30:32.697246874 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-20T18:30:32.8490214Z [rank10]:[W220 18:30:32.697617307 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-20T18:30:32.8499715Z [rank15]:[W220 18:30:32.699637175 compiler_depend.ts:3667] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
2026-02-20T18:30:35.9304577Z 
2026-02-20T18:30:35.9306879Z Capturing CUDA graphs (decode, FULL):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:20<00:20, 20.27s/it]
2026-02-20T18:30:35.9307486Z Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:23<00:00, 10.09s/it]
2026-02-20T18:30:35.9308015Z Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:23<00:00, 11.62s/it]
2026-02-20T18:30:36.7836990Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m INFO 02-20 18:30:36 [gpu_model_runner.py:5051] Graph capturing finished in 25 secs, took 0.27 GiB
2026-02-20T18:30:37.3043090Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m INFO 02-20 18:30:37 [gpu_model_runner.py:5051] Graph capturing finished in 26 secs, took 0.27 GiB
2026-02-20T18:30:37.3156342Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-20 18:30:37 [core.py:272] init engine (profile, create kv cache, warmup model) took 76.06 seconds
2026-02-20T18:30:37.3760436Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-20 18:30:37 [core.py:272] init engine (profile, create kv cache, warmup model) took 76.41 seconds
2026-02-20T18:30:38.2865472Z INFO 02-20 18:30:38 [coordinator.py:200] All engine subscriptions received by DP coordinator
2026-02-20T18:30:38.2885857Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-20 18:30:38 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-20T18:30:38.2898067Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-20 18:30:38 [vllm.py:624] Asynchronous scheduling is enabled.
2026-02-20T18:30:38.2913525Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-20 18:30:38 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:30:38.2929511Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-20 18:30:38 [ascend_config.py:412] Dynamic EPLB is False
2026-02-20T18:30:38.2940734Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-20 18:30:38 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:30:38.2952832Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-20 18:30:38 [ascend_config.py:413] The number of redundant experts is 0
2026-02-20T18:30:38.2966121Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-20 18:30:38 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:30:38.2977702Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-20 18:30:38 [ascend_config.py:54] Linear layer sharding enabled with config: ['q_b_proj', 'o_proj']. Note: This feature works optimally with FLASHCOMM2 and DSA-CP enabled; using it without these features may result in significant performance degradation.
2026-02-20T18:30:38.2988143Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-20 18:30:38 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-20T18:30:38.2998225Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-20 18:30:38 [platform.py:318] FULL_DECODE_ONLY compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
2026-02-20T18:30:38.3008531Z INFO 02-20 18:30:38 [utils.py:249] Waiting for API servers to complete ...
2026-02-20T18:30:38.3017975Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-20 18:30:38 [platform.py:335] [91m
2026-02-20T18:30:38.3027508Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             **********************************************************************************
2026-02-20T18:30:38.3037454Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-20T18:30:38.3047747Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-20T18:30:38.3058496Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-20T18:30:38.3068431Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-20T18:30:38.3093101Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-20T18:30:38.3093918Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             * batch size for graph capture.
2026-02-20T18:30:38.3098799Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             * For more details, please refer to:
2026-02-20T18:30:38.3109116Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-20T18:30:38.3119474Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             **********************************************************************************[0m
2026-02-20T18:30:38.3129516Z [0;36m(EngineCore_DP1 pid=173)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             
2026-02-20T18:30:38.3138702Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-20 18:30:38 [platform.py:335] [91m
2026-02-20T18:30:38.3149025Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             **********************************************************************************
2026-02-20T18:30:38.3159146Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             * WARNING: You have enabled the *full graph* feature.
2026-02-20T18:30:38.3169305Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             * This is an early experimental stage and may involve various unknown issues.
2026-02-20T18:30:38.3179131Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             * A known problem is that capturing too many batch sizes can lead to OOM
2026-02-20T18:30:38.3188675Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             * (Out of Memory) errors or inference hangs. If you encounter such issues,
2026-02-20T18:30:38.3198924Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             * consider reducing `gpu_memory_utilization` or manually specifying a smaller
2026-02-20T18:30:38.3209042Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             * batch size for graph capture.
2026-02-20T18:30:38.3218841Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             * For more details, please refer to:
2026-02-20T18:30:38.3228704Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             * https://docs.vllm.ai/en/stable/configuration/conserving_memory.html#reduce-cuda-graphs
2026-02-20T18:30:38.3238322Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             **********************************************************************************[0m
2026-02-20T18:30:38.3247554Z [0;36m(EngineCore_DP0 pid=154)[0;0m WARNING 02-20 18:30:38 [platform.py:335]             
2026-02-20T18:30:38.3257556Z [0;36m(EngineCore_DP0 pid=154)[0;0m INFO 02-20 18:30:38 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-20T18:30:38.3266906Z [0;36m(EngineCore_DP1 pid=173)[0;0m INFO 02-20 18:30:38 [platform.py:443] Set PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
2026-02-20T18:30:38.3277008Z ERROR 02-20 18:30:38 [utils.py:290] Exception occurred while running API servers: Process ApiServer_0 (PID: 184) died with exit code 1
2026-02-20T18:30:38.3286690Z ERROR 02-20 18:30:38 [utils.py:290] Traceback (most recent call last):
2026-02-20T18:30:38.3296956Z ERROR 02-20 18:30:38 [utils.py:290]   File "/vllm-workspace/vllm/vllm/v1/utils.py", line 277, in wait_for_completion_or_failure
2026-02-20T18:30:38.3305562Z ERROR 02-20 18:30:38 [utils.py:290]     raise RuntimeError(
2026-02-20T18:30:38.3315845Z ERROR 02-20 18:30:38 [utils.py:290] RuntimeError: Process ApiServer_0 (PID: 184) died with exit code 1
2026-02-20T18:30:38.3325472Z INFO 02-20 18:30:38 [utils.py:293] Terminating remaining processes ...
2026-02-20T18:30:38.5788902Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-20T18:30:38.5803039Z [0;36m(Worker_DP1_TP1_EP9 pid=276)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-20T18:30:38.5813889Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-20T18:30:38.5823430Z [0;36m(Worker_DP1_TP2_EP10 pid=372)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-20T18:30:38.5833188Z [0;36m(Worker_DP0_TP2_EP2 pid=369)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-20T18:30:38.5844236Z [0;36m(Worker_DP0_TP1_EP1 pid=275)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-20T18:30:38.5855376Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-20T18:30:38.5863861Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-20T18:30:38.5874212Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-20T18:30:38.5884029Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-20T18:30:38.5894190Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-20T18:30:38.5904294Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-20T18:30:38.5914162Z [0;36m(Worker_DP1_TP6_EP14 pid=785)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-20T18:30:38.5925085Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-20T18:30:38.5938183Z [0;36m(Worker_DP1_TP7_EP15 pid=890)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-20T18:30:38.5948206Z [0;36m(Worker_DP0_TP2_EP2 pid=369)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-20T18:30:38.5959642Z [0;36m(Worker_DP0_TP4_EP4 pid=578)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-20T18:30:38.5970917Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:730] Parent process exited, terminating worker
2026-02-20T18:30:38.5982342Z [0;36m(Worker_DP1_TP2_EP10 pid=372)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-20T18:30:38.5994790Z [0;36m(Worker_DP1_TP1_EP9 pid=276)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-20T18:30:38.6011675Z [0;36m(Worker_DP0_TP1_EP1 pid=275)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-20T18:30:38.6024913Z [0;36m(Worker_DP1_TP0_EP8 pid=258)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-20T18:30:38.6036226Z [0;36m(Worker_DP0_TP6_EP6 pid=786)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-20T18:30:38.6047369Z [0;36m(Worker_DP0_TP5_EP5 pid=681)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-20T18:30:38.6058547Z [0;36m(Worker_DP0_TP0_EP0 pid=257)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-20T18:30:38.6069561Z [0;36m(Worker_DP1_TP5_EP13 pid=684)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-20T18:30:38.6080911Z [0;36m(Worker_DP1_TP3_EP11 pid=473)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-20T18:30:38.6091179Z [0;36m(Worker_DP1_TP6_EP14 pid=785)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-20T18:30:38.6103044Z [0;36m(Worker_DP1_TP7_EP15 pid=890)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-20T18:30:38.6109643Z [0;36m(Worker_DP0_TP7_EP7 pid=889)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-20T18:30:38.6120251Z [0;36m(Worker_DP1_TP4_EP12 pid=577)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-20T18:30:38.6130090Z [0;36m(Worker_DP0_TP3_EP3 pid=474)[0;0m INFO 02-20 18:30:38 [multiproc_executor.py:774] WorkerProc shutting down.
2026-02-20T18:30:43.6081765Z Traceback (most recent call last):
2026-02-20T18:30:43.6093983Z   File "/usr/local/python3.11.14/bin/vllm", line 7, in <module>
2026-02-20T18:30:43.6105561Z     sys.exit(main())
2026-02-20T18:30:43.6116221Z              ^^^^^^
2026-02-20T18:30:43.6127703Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/main.py", line 73, in main
2026-02-20T18:30:43.6137674Z     args.dispatch_function(args)
2026-02-20T18:30:43.6147096Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 108, in cmd
2026-02-20T18:30:43.6157741Z     run_multi_api_server(args)
2026-02-20T18:30:43.6168629Z   File "/vllm-workspace/vllm/vllm/entrypoints/cli/serve.py", line 282, in run_multi_api_server
2026-02-20T18:30:43.6177217Z     wait_for_completion_or_failure(
2026-02-20T18:30:43.6188298Z   File "/vllm-workspace/vllm/vllm/v1/utils.py", line 277, in wait_for_completion_or_failure
2026-02-20T18:30:43.6202525Z     raise RuntimeError(
2026-02-20T18:30:43.6213950Z RuntimeError: Process ApiServer_0 (PID: 184) died with exit code 1
2026-02-20T18:30:43.6877465Z [ERROR] 2026-02-20-18:30:43 (PID:138, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
2026-02-20T18:30:43.9786749Z sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2026-02-20T18:30:45.7209286Z /usr/local/python3.11.14/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked shared_memory objects to clean up at shutdown
2026-02-20T18:30:45.7217862Z   warnings.warn('resource_tracker: There appear to be %d '
2026-02-20T18:30:50.9072590Z FAILED
2026-02-20T18:30:50.9082432Z 
2026-02-20T18:30:50.9093358Z =================================== FAILURES ===================================
2026-02-20T18:30:50.9103070Z _______________________________ test_multi_node ________________________________
2026-02-20T18:30:50.9112414Z 
2026-02-20T18:30:50.9123177Z     @pytest.mark.asyncio
2026-02-20T18:30:50.9132821Z     async def test_multi_node() -> None:
2026-02-20T18:30:50.9142288Z         config = MultiNodeConfigLoader.from_yaml()
2026-02-20T18:30:50.9151150Z     
2026-02-20T18:30:50.9161267Z         with ProxyLauncher(
2026-02-20T18:30:50.9170747Z                 nodes=config.nodes,
2026-02-20T18:30:50.9180673Z                 disagg_cfg=config.disagg_cfg,
2026-02-20T18:30:50.9189885Z                 envs=config.envs,
2026-02-20T18:30:50.9200405Z                 proxy_port=config.proxy_port,
2026-02-20T18:30:50.9210883Z                 cur_index=config.cur_index,
2026-02-20T18:30:50.9219146Z         ) as proxy:
2026-02-20T18:30:50.9228798Z     
2026-02-20T18:30:50.9238106Z >           with RemoteOpenAIServer(
2026-02-20T18:30:50.9247871Z                     model=config.model,
2026-02-20T18:30:50.9257023Z                     vllm_serve_args=config.server_cmd,
2026-02-20T18:30:50.9266621Z                     server_port=config.server_port,
2026-02-20T18:30:50.9276760Z                     server_host=config.master_ip,
2026-02-20T18:30:50.9286318Z                     env_dict=config.envs,
2026-02-20T18:30:50.9295682Z                     auto_port=False,
2026-02-20T18:30:50.9304540Z                     proxy_port=proxy.proxy_port,
2026-02-20T18:30:50.9314310Z                     disaggregated_prefill=config.disagg_cfg,
2026-02-20T18:30:50.9323913Z                     nodes_info=config.nodes,
2026-02-20T18:30:50.9333588Z                     max_wait_seconds=2800,
2026-02-20T18:30:50.9342992Z             ) as server:
2026-02-20T18:30:50.9351959Z 
2026-02-20T18:30:50.9362276Z tests/e2e/nightly/multi_node/scripts/test_multi_node.py:21: 
2026-02-20T18:30:50.9417904Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-20T18:30:50.9418210Z tests/e2e/conftest.py:306: in __init__
2026-02-20T18:30:50.9418551Z     self._wait_for_multiple_servers(
2026-02-20T18:30:50.9418866Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2026-02-20T18:30:50.9419050Z 
2026-02-20T18:30:50.9419273Z self = <tests.e2e.conftest.RemoteOpenAIServer object at 0xffff1beb03d0>
2026-02-20T18:30:50.9429373Z targets = [('10.0.0.78', 'http://10.0.0.78:8080/health')], timeout = 2800
2026-02-20T18:30:50.9438771Z log_interval = 30.0
2026-02-20T18:30:50.9448081Z 
2026-02-20T18:30:50.9457938Z     def _wait_for_multiple_servers(self,
2026-02-20T18:30:50.9466824Z                                    targets,
2026-02-20T18:30:50.9476350Z                                    timeout: float,
2026-02-20T18:30:50.9486460Z                                    log_interval: float = 30.0):
2026-02-20T18:30:50.9495437Z         """
2026-02-20T18:30:50.9504961Z         targets: List[(node_ip, url)]
2026-02-20T18:30:50.9514708Z         log_interval
2026-02-20T18:30:50.9524846Z         """
2026-02-20T18:30:50.9534479Z         start = time.time()
2026-02-20T18:30:50.9543865Z         client = requests
2026-02-20T18:30:50.9553452Z     
2026-02-20T18:30:50.9563440Z         ready = {node_ip: False for node_ip, _ in targets}
2026-02-20T18:30:50.9572290Z     
2026-02-20T18:30:50.9581101Z         last_log_time = 0.0
2026-02-20T18:30:50.9590457Z     
2026-02-20T18:30:50.9599922Z         while True:
2026-02-20T18:30:50.9610155Z             now = time.time()
2026-02-20T18:30:50.9619340Z             all_ready = True
2026-02-20T18:30:50.9628562Z             should_log = (now - last_log_time) >= log_interval
2026-02-20T18:30:50.9638028Z     
2026-02-20T18:30:50.9647621Z             for node_ip, url in targets:
2026-02-20T18:30:50.9656680Z                 if ready[node_ip]:
2026-02-20T18:30:50.9665554Z                     continue
2026-02-20T18:30:50.9674725Z     
2026-02-20T18:30:50.9684279Z                 try:
2026-02-20T18:30:50.9693560Z                     resp = client.get(url)
2026-02-20T18:30:50.9703179Z                     if resp.status_code == 200:
2026-02-20T18:30:50.9712625Z                         ready[node_ip] = True
2026-02-20T18:30:50.9723108Z                         logger.info(f"[READY] Node {node_ip} is ready.")
2026-02-20T18:30:50.9732357Z                 except RequestException:
2026-02-20T18:30:50.9740785Z                     all_ready = False
2026-02-20T18:30:50.9750067Z                     if should_log:
2026-02-20T18:30:50.9760049Z                         logger.debug(f"[WAIT] {url}: connection failed")
2026-02-20T18:30:50.9769581Z     
2026-02-20T18:30:50.9779192Z                     # check unexpected exit
2026-02-20T18:30:50.9789599Z                     result = self._poll()
2026-02-20T18:30:50.9799269Z                     if result is not None and result != 0:
2026-02-20T18:30:50.9809217Z >                       raise RuntimeError(
2026-02-20T18:30:50.9819413Z                             f"Server at {node_ip} exited unexpectedly."
2026-02-20T18:30:50.9828998Z                         ) from None
2026-02-20T18:30:50.9839266Z E                       RuntimeError: Server at 10.0.0.78 exited unexpectedly.
2026-02-20T18:30:50.9850375Z 
2026-02-20T18:30:50.9858113Z tests/e2e/conftest.py:399: RuntimeError
2026-02-20T18:30:50.9867489Z =============================== warnings summary ===============================
2026-02-20T18:30:50.9877246Z <frozen importlib._bootstrap>:241
2026-02-20T18:30:50.9887413Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute
2026-02-20T18:30:50.9895821Z 
2026-02-20T18:30:50.9905456Z <frozen importlib._bootstrap>:241
2026-02-20T18:30:50.9915624Z   <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute
2026-02-20T18:30:50.9925039Z 
2026-02-20T18:30:50.9934834Z ../../usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647
2026-02-20T18:30:50.9945399Z   /usr/local/python3.11.14/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
2026-02-20T18:30:50.9953707Z     warnings.warn(
2026-02-20T18:30:50.9963297Z 
2026-02-20T18:30:50.9972912Z ../../usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8
2026-02-20T18:30:51.0004417Z   /usr/local/python3.11.14/lib/python3.11/site-packages/torch_npu/dynamo/torchair/__init__.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
2026-02-20T18:30:51.0005110Z     import pkg_resources
2026-02-20T18:30:51.0005256Z 
2026-02-20T18:30:51.0033381Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2026-02-20T18:30:51.0033879Z =========================== short test summary info ============================
2026-02-20T18:30:51.0034335Z FAILED tests/e2e/nightly/multi_node/scripts/test_multi_node.py::test_multi_node
2026-02-20T18:30:51.0040129Z ================== 1 failed, 4 warnings in 771.81s (0:12:51) ===================
2026-02-20T18:30:52.7048230Z [0;31mFAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml âœ— ERROR: Some tests failed, please check the error stack above for details. If this is insufficient to pinpoint the error, please download and review the logs of all other nodes from the job's summary.[0m
2026-02-20T18:30:52.8565487Z Cleaning up background log streams...
2026-02-20T18:30:52.9287665Z ##[error]Error: failed to run script step: Error: command terminated with non-zero exit code: command terminated with exit code 1
2026-02-20T18:30:52.9328247Z ##[error]Process completed with exit code 1.
2026-02-20T18:30:52.9423522Z ##[error]Executing the custom container implementation failed. Please contact your self hosted runner administrator.
2026-02-20T18:30:52.9811676Z ##[group]Run actions/upload-artifact@v6
2026-02-20T18:30:52.9811922Z with:
2026-02-20T18:30:52.9812339Z   name: DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs
2026-02-20T18:30:52.9812666Z   path: /tmp/vllm*_logs.txt
2026-02-20T18:30:52.9812882Z   retention-days: 7
2026-02-20T18:30:52.9813125Z   if-no-files-found: warn
2026-02-20T18:30:52.9813375Z   compression-level: 6
2026-02-20T18:30:52.9813570Z   overwrite: false
2026-02-20T18:30:52.9813811Z   include-hidden-files: false
2026-02-20T18:30:52.9814032Z env:
2026-02-20T18:30:52.9814277Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-20T18:30:52.9814795Z ##[endgroup]
2026-02-20T18:30:52.9841233Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-20T18:30:52.9842135Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-20T18:30:52.9842400Z ##[endgroup]
2026-02-20T18:30:53.3528012Z (node:928) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-20T18:30:53.3528771Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-20T18:30:54.3476121Z With the provided path, there will be 1 file uploaded
2026-02-20T18:30:54.3480357Z Artifact name is valid!
2026-02-20T18:30:54.3480675Z Root directory input is valid!
2026-02-20T18:30:55.2754280Z Beginning upload of artifact content to blob storage
2026-02-20T18:30:56.3835357Z Uploaded bytes 12528
2026-02-20T18:30:56.6119839Z Finished uploading artifact content to blob storage!
2026-02-20T18:30:56.6120358Z SHA256 digest of uploaded artifact zip is 6184c12cab215f050cef4139c3bfbd8618de8fcc764d0b4d6a929253cfde6e23
2026-02-20T18:30:56.6120817Z Finalizing artifact upload
2026-02-20T18:30:57.5006417Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs.zip successfully finalized. Artifact ID 5594303285
2026-02-20T18:30:57.5007262Z Artifact DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml-pod-logs has been successfully uploaded! Final size is 12528 bytes. Artifact ID is 5594303285
2026-02-20T18:30:57.5009503Z Artifact download URL: https://github.com/vllm-project/vllm-ascend/actions/runs/22231958921/artifacts/5594303285
2026-02-20T18:30:57.9704307Z ##[group]Run kubectl get pods -n "$NAMESPACE" --ignore-not-found=true
2026-02-20T18:30:57.9704718Z [36;1mkubectl get pods -n "$NAMESPACE" --ignore-not-found=true[0m
2026-02-20T18:30:57.9705057Z [36;1mkubectl delete -f ./lws.yaml --ignore-not-found=true || true[0m
2026-02-20T18:30:57.9705421Z shell: bash -el {0}
2026-02-20T18:30:57.9705581Z env:
2026-02-20T18:30:57.9705785Z   FAIL_TAG: FAIL_TAG_DeepSeek-V3_2-W8A8-A3-dual-nodes.yaml
2026-02-20T18:30:57.9706038Z ##[endgroup]
2026-02-20T18:30:57.9807002Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-20T18:30:57.9807774Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-20T18:30:57.9808084Z ##[endgroup]
2026-02-20T18:30:58.3282135Z (node:1040) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-20T18:30:58.3282808Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-20T18:30:58.9691620Z NAME                                             READY   STATUS    RESTARTS     AGE
2026-02-20T18:30:58.9692104Z linux-aarch64-a3-0-n4cwm-runner-qpgps            1/1     Running   0            14m
2026-02-20T18:30:58.9692549Z linux-aarch64-a3-0-n4cwm-runner-qpgps-workflow   1/1     Running   0            14m
2026-02-20T18:30:58.9692910Z vllm-0                                           1/1     Running   1 (6s ago)   13m
2026-02-20T18:30:58.9693203Z vllm-0-1                                         1/1     Running   0            13m
2026-02-20T18:30:59.0362498Z leaderworkerset.leaderworkerset.x-k8s.io "vllm" deleted from vllm-project namespace
2026-02-20T18:30:59.0564112Z service "vllm-leader" deleted from vllm-project namespace
2026-02-20T18:30:59.5337452Z Post job cleanup.
2026-02-20T18:30:59.5357780Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-20T18:30:59.5358635Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-20T18:30:59.5358863Z ##[endgroup]
2026-02-20T18:30:59.8860450Z (node:1166) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
2026-02-20T18:30:59.8861127Z (Use `node --trace-deprecation ...` to show where the warning was created)
2026-02-20T18:31:00.5141561Z [command]/usr/bin/git version
2026-02-20T18:31:00.5379212Z git version 2.34.1
2026-02-20T18:31:00.5409786Z Copying '/root/.gitconfig' to '/__w/_temp/f8b77cbd-218b-4609-8c81-eaad24b699d2/.gitconfig'
2026-02-20T18:31:00.5421280Z Temporarily overriding HOME='/__w/_temp/f8b77cbd-218b-4609-8c81-eaad24b699d2' before making global git config changes
2026-02-20T18:31:00.5421858Z Adding repository directory to the temporary git global config as a safe directory
2026-02-20T18:31:00.5425307Z [command]/usr/bin/git config --global --add safe.directory /__w/vllm-ascend/vllm-ascend
2026-02-20T18:31:00.5460317Z Removing SSH command configuration
2026-02-20T18:31:00.5464679Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2026-02-20T18:31:00.5520861Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2026-02-20T18:31:00.6023973Z Removing HTTP extra header
2026-02-20T18:31:00.6027182Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2026-02-20T18:31:00.6054284Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2026-02-20T18:31:00.6247399Z Removing includeIf entries pointing to credentials config files
2026-02-20T18:31:00.6250450Z [command]/usr/bin/git config --local --name-only --get-regexp ^includeIf\.gitdir:
2026-02-20T18:31:00.6268844Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-20T18:31:00.6269227Z includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-20T18:31:00.6269525Z includeif.gitdir:/github/workspace/.git.path
2026-02-20T18:31:00.6269795Z includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-20T18:31:00.6276077Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path
2026-02-20T18:31:00.6293784Z /__w/_temp/git-credentials-0ad1268f-cd13-4f46-b1f8-db2541803ef4.config
2026-02-20T18:31:00.6301789Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git.path /__w/_temp/git-credentials-0ad1268f-cd13-4f46-b1f8-db2541803ef4.config
2026-02-20T18:31:00.6329003Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path
2026-02-20T18:31:00.6346941Z /__w/_temp/git-credentials-0ad1268f-cd13-4f46-b1f8-db2541803ef4.config
2026-02-20T18:31:00.6354140Z [command]/usr/bin/git config --local --unset includeif.gitdir:/__w/vllm-ascend/vllm-ascend/.git/worktrees/*.path /__w/_temp/git-credentials-0ad1268f-cd13-4f46-b1f8-db2541803ef4.config
2026-02-20T18:31:00.6385677Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git.path
2026-02-20T18:31:00.6402127Z /github/runner_temp/git-credentials-0ad1268f-cd13-4f46-b1f8-db2541803ef4.config
2026-02-20T18:31:00.6408524Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git.path /github/runner_temp/git-credentials-0ad1268f-cd13-4f46-b1f8-db2541803ef4.config
2026-02-20T18:31:00.6435523Z [command]/usr/bin/git config --local --get-all includeif.gitdir:/github/workspace/.git/worktrees/*.path
2026-02-20T18:31:00.6453162Z /github/runner_temp/git-credentials-0ad1268f-cd13-4f46-b1f8-db2541803ef4.config
2026-02-20T18:31:00.6459488Z [command]/usr/bin/git config --local --unset includeif.gitdir:/github/workspace/.git/worktrees/*.path /github/runner_temp/git-credentials-0ad1268f-cd13-4f46-b1f8-db2541803ef4.config
2026-02-20T18:31:00.6486751Z [command]/usr/bin/git submodule foreach --recursive git config --local --show-origin --name-only --get-regexp remote.origin.url
2026-02-20T18:31:00.6665572Z Removing credentials config '/__w/_temp/git-credentials-0ad1268f-cd13-4f46-b1f8-db2541803ef4.config'
2026-02-20T18:31:19.2210874Z ##[group]Run '/home/runner/k8s/index.js'
2026-02-20T18:31:19.2211674Z shell: /home/runner/externals/node20/bin/node {0}
2026-02-20T18:31:19.2211959Z ##[endgroup]
2026-02-20T18:31:19.6103013Z Cleaning up orphan processes
